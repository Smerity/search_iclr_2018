{"notes":[{"tddate":null,"ddate":null,"tmdate":1515871752596,"tcdate":1515871752596,"number":6,"cdate":1515871752596,"id":"rJWf00wEf","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Comment","forum":"SJJQVZW0b","replyto":"Sye2eNDxM","signatures":["ICLR.cc/2018/Conference/Paper666/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper666/AnonReviewer3"],"content":{"title":"Review revision","comment":"I am happy with the updates the authors made to the paper. The video and additional experiments are valuable, and I will increase my score accordingly. \n\nHowever, the paper would strongly benefit from either a second test environment or a more complex grammar to showcase generality.\n\n(nitpick): “We do not include the standard deviations since there are no noticeable difference among them.” including standard deviations is not just to compare the magnitude of the standard deviations, but to see whether the differences in means of the methods are within the standard deviations."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515026969846,"tcdate":1515026969846,"number":4,"cdate":1515026969846,"id":"H1GX9limf","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Comment","forum":"SJJQVZW0b","replyto":"Sye2eNDxM","signatures":["ICLR.cc/2018/Conference/Paper666/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper666/Authors"],"content":{"title":"Author response","comment":"Thank you for your reviews and insights.\n\n1. “Providing this reward requires knowing the full state of the world”\n\nLearning curves of training without the penalty have been added to Figure 5. We find that the penalty does not have a significantly effect on the learning efficiency as shown in Figure 5.\n\nReviewer may be confused by our phrasing. Sorry about that. Actually we want to stress that we do not provide the full state when using this penalty. Instead, a penalty only includes information about the agent’s physical capacity and the nature of the target task, since it is given i) when the agent attempts to execute tasks that exceeds its physical capacity, such as trying to put down a block when it is not carrying one or trying to pick-up another block where there is already one in its hands, ii) or when it attempts to execute tasks that are irrelevant to that tasks. \n\nAlso, we only use this during training under the assumption that a given task in the training is always executable (the necessary blocks are present in the environment). So when a penalty was given to a task of finding an object that does not exist in the environment, it was meant to save time in game playing and also gives training signical of what old tasks are relevant for the agent.\n\nFinally, in testing, we do not use this penalty, and it does not affect the performance.\n\nTherefore, we do not think that this will prevent us to apply the approach to the real world for the aforementioned reasons.\n\n2. “There are no comparisons to other multitask or hierarchical methods”\nWe have also evaluated H-DRLN (Tessler et al., 2017). \n\n3. “A video … would be helpful”\nPlease refer to this link for the video (with audio): https://www.dropbox.com/s/j5nw2cljpoofo9j/hrl_demo.mov?dl=0\n\n4. “The performances of the different ablations are rather close. Please a standard deviation over multiple training runs.”\nWe do not include the standard deviations since there are no noticeable difference among them. The alternating and 2 value functions mainly help accelerating the learning phase 1. As shown in Figure 4a, the acceleration is significant (full model is the first one that switches to phase 2). In phase 2 (see Figure 4b), the advantage of using the STG is very clear (the blue curve reaches a plateau around an average reward of 0.8) and the others do not show a large improvement over the full model. So none of them reduces the training variance, but they all help increase the training efficiency.\n\n5. “why does figure 4.b not include a flat policy””\nWe have added the flat policy.\n\n6. “the authors claim that the order is arbitrary”\nSorry for the confusion. We will clarify this. What we meant was for arbitrary order, we can still train the hierarchical policy, but a semantically meaningful order is indeed very important for the training efficiency. And we do provide this order as weak supervision.\n\n7. “unclear which hacks are for the method generally, and which hacks are for the method to work on the environment.”\nThey are not environment dependent.\ni) The order of the stages comes from semantic meanings of the tasks, so it depends on the tasks but does not depend on the environment. E.g., you may train a real robot on the same tasks in the same order in a real environment. In fact, the generated interpretable hierarchical plans can be directly used for the same tasks in different environments without additional training as long as there are equivalent primitive actions.\nii) Alternating is purely for the optimization of the neural nets. As the experimental results show, we can also train the hierarchical policies without it.\niii) The first phase in the 2-phase curriculum is for the global policy to learn what the goals of the previous tasks are, so that in the second phase, it knows when to repeat the same task and when to stop. This is also not tailored to any specific environment."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515103116265,"tcdate":1515026936082,"number":3,"cdate":1515026936082,"id":"H1lZ5eo7z","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Comment","forum":"SJJQVZW0b","replyto":"S1hvkpKxf","signatures":["ICLR.cc/2018/Conference/Paper666/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper666/Authors"],"content":{"title":"Author response","comment":"Thank you for your comments and suggestions. \n\n1. “performance against existing methods”\nWe have added the comparison with Tessler et al. (2017), i.e., H-DRLN, as suggested (see Table 1 and Figure 4). H-DRLN indeed also has the concept of reusing previously learned skills but the advantages of our approach are very obvious:\n1) Each H-DRLN in Tessler et al. (2017) can only learn one task like “Stack blue,” whereas ours can learn a set of tasks, like {“Stack x”}, where x can be different colors. In fact, each curve of H-DRLN we show in Figure 4 is only for training one particular task (i.e., “Get white” and “Stack white” respectively), whereas all the other curves are for training the whole set of tasks.\n2) H-DRLN treats all the old tasks {DSN_i} and the actions on the same level and learns Q(s, a) and Q(s, DSN_i). This setting is clearly not scalable. In the original paper, the learning was done where there were only 4 old tasks. However, in our settings, we have as many as 18 old tasks. And clearly from the results, H-DRLN can not learn good policies for new tasks as the input space of the Q(s, DSN_i) function becomes unbearably large (it is similar to learning a policy based on a very large action space). \n3) Our policy is also learning the semantics of the tasks from the instructions, thus it has better generalization. As we show in the experiments on the {“Put x on top of y”} tasks, ours generalizes well in noval (x, y) combinations unseen in training whereas H-DRLN does not have this capability.\n\n2. “one more experiment in a different, harder domain”\nWe have tested more difficult tasks {“Put x on top of y”}, and have also evaluated our hierarchical policy in a zero-shot setting for unseen (x, y) combinations. See Table 1, Figure 4 and also Section 5.3 and Section 5.4. As shown in the results, flat policy and H-DRLN (Tessler et al., 2017) fail to learn good policies for {”Stack x”} tasks and the new {“Put x on top of y”} tasks. This manifests that the tasks are actually quite challenging for current RL methods.\n\n3. Questions about the paper:\n\n1) “I do not see how this approach avoids suffering from the same problem”\nPolicies on different levels are learned on different corpus (e.g., pi_0 does not know the word “get”) and on different action spaces (e.g., pi_0 can be trained on actions excluding “pick up” and “put down”).  They also do not need to share a same state encoding module, so the forms of states can be different (e.g., we may use symbolic states for a global policy while its local policy uses raw pixels as states).\n\n2) “It sounds like this is saying that the inter-option policy can pick only options, and not primitive actions”\nWe are referring to some of the existing methods like Kulkarni et al. (2016), Andreas et al. (2017) where the set of necessary options for a global policy is predefined and a task is executed only based on these given options. Other methods like Tessler et al. (2017) do not have this limitation. We will clarify this.\n\n3) “are there any drawbacks to the instruction policy being defined as two independent distributions”\nIt is for simplicity. For more complex instructions, we may use GRUs or LSTMs to train the instruction generator, but the training will be slower.\n\n4) “How is the sampling from the switch and instruction policies done in this case?”\nAs we stated in the text, the sampling was done w.r.t. equation (1) and (2) when not using the STG.\n\nWe agree with the other editing comments and have fixed them in the updated version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515026894009,"tcdate":1515026894009,"number":2,"cdate":1515026894009,"id":"BkURFxoQz","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Comment","forum":"SJJQVZW0b","replyto":"HJQ8haFxz","signatures":["ICLR.cc/2018/Conference/Paper666/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper666/Authors"],"content":{"title":"Author response","comment":"Thank you for your reviews.\n\n1. “a new meta policy feeds in task id to the previous policy”\nActually, the global policy feeds an instruction in human language rather than a task ID to the previous policy. Compared to using task IDs, this i) improves the interpretability of the policy and ii) facilitates the generalization of the policy in noval scenarios/tasks thanks to the semantics of the instructions.\n\n2. “appropriate subgoals and curriculum must be hand designed”\nActually we let the global policy at each level to explore the appropriate subgoals for the new tasks among all previously learned tasks. The learning efficiency of our approach does depend on the given curriculum as existing curriculum-based RL training approaches do. We regard this as a weak supervision from human knowledge. In fact, compared to some recent work (Kulkarni  et al., 2016; Andreas et al., 2017) which specifically provide the necessary sub-goals and/or the order of the subgoals for a task, we do not think our setting is any less general.\n\n3. “The comparison to non-hierarchical models is not totally fair”\nFirst, the training of flat policy is also strictly following the curriculum we use for our model. It is always finetuned based on the policy trained for the previous task set.  Second, the biggest benefit of our hierarchical policy comes from the more efficient exploration thanks to reusing old skills and learning the STG. As the updated Figure 4b shows, when training for more complex tasks, the flat policy can not yield any positive reward as achieving the goals requires a fairly long sequence of primitive actions and precise operations (e.g., picking up and putting down the correct blocks at the correct locations).\n\n4. “I could not find the definitions of mu, and regarding the STG I could not find the definition of q and rho”\nMu was introduced in the second paragraph of Section 4.1. q and rho were defined in the second paragraph of Section 3.3.\n\n5. “The experimental results are a bit confusing”\nSorry for the confusion. As we explained in Section 4, we adopt a 2-phase curriculum learning where the task set was expanded in the second phase. For curriculum-based training, the dip of a curve comes from the switch from phase 1 to phase 2, thus also indicates when the task set was expanded. For non-curriculum based training where there is no dip in the reward, the expansion starts from the first episode. Note that in Figure 4b, we only show the phase 2 of our curriculum, so the expansion starts from the first episode for all curves in this figure. \n\nWe have also added the flat baseline for the Figure 4b. The reason we didn’t include that in the previous version was that the flat policy failed to learn anything meaningful for the complex tasks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515027196999,"tcdate":1515026572722,"number":1,"cdate":1515026572722,"id":"rJScuximM","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Comment","forum":"SJJQVZW0b","replyto":"SJJQVZW0b","signatures":["ICLR.cc/2018/Conference/Paper666/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper666/Authors"],"content":{"title":"Submission revision","comment":"Our updated submission has included extensive experiments as suggested by the reviewers. We have added a set of more challenging tasks with a zero-shot evaluation and also results of new baselines including an existing multi-task hierarchical policy method, H-DRLN in Tessler et al. (2017). The updated results can be seen in Section 5. A demo video (audio included) is available here: https://www.dropbox.com/s/j5nw2cljpoofo9j/hrl_demo.mov?dl=0"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515642488976,"tcdate":1511803979477,"number":3,"cdate":1511803979477,"id":"HJQ8haFxz","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Review","forum":"SJJQVZW0b","replyto":"SJJQVZW0b","signatures":["ICLR.cc/2018/Conference/Paper666/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting model but not very clear explanation, a bit weak experimental section.","rating":"6: Marginally above acceptance threshold","review":"This paper introduces an iterative method to build hierarchical policies. At every iteration, a new meta policy feeds in task id to the previous policy and mixes the results with an 'augmented' policy. The resulting policy is somewhat interpretable as the task id being sampled by the meta policy corresponds to one of the subgoals that are manually designed.\n\nOne of the limitation of the method is that appropriate subgoals and curriculum must be hand designed. Another one is that the model complexity grows linearly with the number of meta iterations. \n\nThe comparison to non-hierarchical models is not totally fair in my opinion. According to the experiment, the flat policy performs much worse than the hierarchical, but it is unclear how much of this is due to the extra capacity of the model of the unfolded hierarchical policy and how much of that is due to the hierarchy. In other words, it is unclear if hierarchy is actually useful, or just the task curriculum and model capacity staging.\n\nThe paper does not appear to be  fully self contained in term of notations, in particular regarding the importance sampling I could not find the definitions of mu, and regarding the STG I could not find the definition of q and rho. \n\nThe experimental results are a bit confusing. In the learning curves that are shown, it is not clear exactly when the set of task is expanded, nor when the hierarchical policy iteration occurs. Also, some curves are lacking the flat baseline.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515791823373,"tcdate":1511800675736,"number":2,"cdate":1511800675736,"id":"S1hvkpKxf","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Review","forum":"SJJQVZW0b","replyto":"SJJQVZW0b","signatures":["ICLR.cc/2018/Conference/Paper666/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-Task Reinforcement Learning","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThis paper proposes an approach to learning hierarchical policies in a lifelong learning context. This is achieved by stacking policies - an explicit \"switch\" policy is then used to decide whether to execute a primitive action or call the policy of the layer below it. Additionally, each task is encoded in a human-readable template, which provides interpretability.\nReview:\nOverall, I found the paper to be generally well-written and the core idea to be interesting. My main concern is about the performance against existing methods (no empirical results are provided), and while it does provide interpretability, I am not sure that other approaches (e.g. Tessler et al. 2017) could not be slightly modified to do the same. I think the paper could also benefit from at least one more experiment in a different, harder domain.\n\nI have a few questions and comments about the paper:\n\nThe first paragraph claims \"This precludes transfer of previously learned simple skills to a new policy defined over a space with differing states or actions\". I do not see how this approach avoids suffering from the same problem? Additionally, approaches such as agent-space options [Konidaris and Barto. Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI 2007] get around at least the state part.\n\nI do not quite follow what is meant by \"a global policy is assumed to be executable by only using local policies over specific options\". It sounds like this is saying that the inter-option policy can pick only options, and not primitive actions, which is obviously untrue. Can you clarify this sentence?\n\nIn section 3.1, it may be best to mention that the policy accepts both a state and task and outputs an action. This is stated shortly afterwards, but it was confusing because section 3.1 says that there is a single policy for a set of tasks, and so obviously a normal state-action policy would not work here.\n\nAt the bottom of page 6, are there any drawbacks to the instruction policy being defined as two independent distributions? What if not all skills are applicable to all items?\n\nIn section 5, what does the \"without grammar\" agent entail? How is the sampling from the switch and instruction policies done in this case?\n\nWhile the results in Figures 4 and 5 show improvement over a flat policy, as well as the value of using the grammar, I am *very* surprised there is no comparison to existing methods. For example, Tessler's H-DRLN seems like one obvious comparison here, since it learns when to execute a primitive action and when to reuse a skill.\n\nThere were also some typos/small issues (I may have missed some):\n\npg 3: \"In addition, previous work usually useS...\"\npg 3. \"we encode a human instruction to LEARN A...\" (?)\npg 4. \"...with A stochastic temporal grammar...\"\npg 4. \"... described above through A/THE modified...\"\npg 6. \"...TOTALLING six colors...\"\nThere are some issues with the references (capital letters missing e.g. Minecraft)\n\nIt also would be preferable if the figures could appear after they are referenced in the text, since it is quite confusing otherwise. For example, Figure 2 contains V(s,g), but that is only defined much later on. Also, I struggled to make out the yellow box in Figure 2, and the positioning of Figure 3 on the side is not ideal either.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515871764030,"tcdate":1511633064075,"number":1,"cdate":1511633064075,"id":"Sye2eNDxM","invitation":"ICLR.cc/2018/Conference/-/Paper666/Official_Review","forum":"SJJQVZW0b","replyto":"SJJQVZW0b","signatures":["ICLR.cc/2018/Conference/Paper666/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A good idea, but with limited evaluation","rating":"6: Marginally above acceptance threshold","review":"This paper aims to learn hierarchical policies by using a recursive policy structure regulated by a stochastic temporal grammar. The experiments show that the method is better than a flat policy for learning a simple set of block-related skills in minecraft (find, get, put, stack) and generalizes better to a modification of the environment (size of room). The sequence of subtasks generated by the policy are interpretable.\n\nStrengths:\n- The grammar and policies are trained using a sparse reward upon task completion. \n- The method is well ablated; Figures 4 and 5 answered most questions I had while reading.\n- Theoretically, the method makes few assumptions about the environment and the relationships between tasks.\n- The interpretability of the final behaviors is a good result. \n\nWeaknesses:\n- The implementation gives the agent a -0.5 reward if it generates a currently unexecutable goal g’. Providing this reward requires knowing the full state of the world. If this hack is required, then this method would not be useful in a real world setting, defeating the purpose of the sparse reward mentioned above. I would really like to see how the method performs without this hack. \n- There are no comparisons to other multitask or hierarchical methods. Progressive Networks or Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning seem like natural comparisons.\n- A video to show what the environments and tasks look like during execution would be helpful.\n- The performances of the different ablations are rather close. Please a standard deviation over multiple training runs. Also, why does figure 4.b not include a flat policy?\n- The stages are ordered in a semantically meaningful order (find is the first stage), but the authors claim that the order is arbitrary. If this claim is going to be included in the paper, it needs to be proven (results shown for random orderings) because right now I do not believe it. \n\nQuality:\nThe method does provide hierarchical and interpretable policies for executing instructions, this is a meaningful direction to work on.\n\nClarity:\nAlthough the method is complicated, the paper was understandable.\n\nOriginality and significance:\nAlthough the method is interesting, I am worried that the environment has been too tailored for the method, and that it would fail in realistic scenarios. The results would be more significant if the tasks had an additional degree of complexity, e.g. “put blue block next to the green block” “get the blue block in room 2”. Then the sequences of subtasks would be a bit less linear (e.g., first need to find blue, then get, then find green, then put). At the moment the tasks are barely more than the actions provided in the environment.\n\nAnother impedance to the paper’s significance is the number of hacks to make the method work (ordering of stages, alternating policy optimization, first training each stage on only tasks of previous stage). Because the method is only evaluated on one simple environment, it unclear which hacks are for the method generally, and which hacks are for the method to work on the environment.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1515026398509,"tcdate":1509131287307,"number":666,"cdate":1509739169330,"id":"SJJQVZW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJJQVZW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning","abstract":"Learning policies for complex tasks that require multiple different skills is a major challenge in reinforcement learning (RL). It is also a requirement for its deployment in real-world scenarios. This paper proposes a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill. This enables agents to continually acquire new skills during different stages of training. Each learned task corresponds to a human language description. Because agents can only access previously learned skills through these descriptions, the agent can always provide a human-interpretable description of its choices. In order to help the agent learn the complex temporal dependencies necessary for the hierarchical policy, we provide it with a stochastic temporal grammar that modulates when to rely on previously learned skills and when to execute new skills. We validate our approach on Minecraft games designed to explicitly test the ability to reuse previously learned skills while simultaneously learning new skills.","pdf":"/pdf/30d20d78d8149ccd365179489f1e9cd0b5427f1c.pdf","TL;DR":"A novel hierarchical policy network which can reuse previously learned skills alongside and as subcomponents of new skills by discovering the underlying relations between skills.","paperhash":"anonymous|hierarchical_and_interpretable_skill_acquisition_in_multitask_reinforcement_learning","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJQVZW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper666/Authors"],"keywords":["Hierarchical Policy","Interpretable Policy","Deep Reinforcement Learning","Multi-task Reinforcement Learning","Skill Acquisition","Language Grounding"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}