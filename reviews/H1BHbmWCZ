{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642386323,"tcdate":1511976859203,"number":3,"cdate":1511976859203,"id":"B1XsJ_3lf","invitation":"ICLR.cc/2018/Conference/-/Paper1126/Official_Review","forum":"H1BHbmWCZ","replyto":"H1BHbmWCZ","signatures":["ICLR.cc/2018/Conference/Paper1126/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Paper violates double bind","rating":"2: Strong rejection","review":"The authors disclosed their identity and violated the terms of double blind reviews.\nPage 2 \"In our previous work (Aly & Dugan, 2017)\n\nAlso the page 1 is full of typos and hard to read.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING","abstract":"n this paper we present a thrust in three directions of visual development us- ing supervised and semi-supervised techniques. The first is an implementation of semi-supervised object detection and recognition using the principles of Soft At- tention and Generative Adversarial Networks (GANs). The second and the third are supervised networks that learn basic concepts of spatial locality and quantity respectively using Convolutional Neural Networks (CNNs). The three thrusts to- gether are based on the approach of Experiential Robot Learning, introduced in previous publication. While the results are unripe for implementation, we believe they constitute a stepping stone towards autonomous development of robotic vi- sual modules.","pdf":"/pdf/00e5c4aefc80d0396ee745c032d27e0bccb43079.pdf","TL;DR":"3 thrusts serving as stepping stones for robot experiential learning of vision module","paperhash":"anonymous|towards_robot_vision_module_development_with_experiential_robot_learning","_bibtex":"@article{\n  anonymous2018towards,\n  title={TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BHbmWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1126/Authors"],"keywords":["Deep Learning","Robotics","Artificial Intelligence","Computer Vision"]}},{"tddate":null,"ddate":null,"tmdate":1515642386358,"tcdate":1511794958460,"number":2,"cdate":1511794958460,"id":"B1IfKjYgM","invitation":"ICLR.cc/2018/Conference/-/Paper1126/Official_Review","forum":"H1BHbmWCZ","replyto":"H1BHbmWCZ","signatures":["ICLR.cc/2018/Conference/Paper1126/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Random ideas about object detection with poor discussion","rating":"2: Strong rejection","review":"This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image. These three proposals are presented in a framework of a robot vision module, although neither the experiments nor the dataset correspond to this domain.\n\nFrom my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges. The proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other technique. This important flaw in the evaluation prevents any fair comparison with the state of the art.\n\nThe text is also difficult to follow. The three contributions seem disconnected and could have been presented in separate works with a more deeper discussion. In particular, I have serious problems understanding:\n\n1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ? The reference to a GAN architecture seems very forced and out of the scope.\n\n2. What is the interest of the localization network ? The task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the 3x3 grid.\n\n3. The authors talk about an autoencoder architecture, but also on a classification network where the labels correspond to the object count. I could not undertstand what is exactly assessed in this section.\n\nFinally, the authors violate the double-bind review policy by clearly referring to their previous work on Experiental Robot Learning.\n\nI would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it, with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vision.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING","abstract":"n this paper we present a thrust in three directions of visual development us- ing supervised and semi-supervised techniques. The first is an implementation of semi-supervised object detection and recognition using the principles of Soft At- tention and Generative Adversarial Networks (GANs). The second and the third are supervised networks that learn basic concepts of spatial locality and quantity respectively using Convolutional Neural Networks (CNNs). The three thrusts to- gether are based on the approach of Experiential Robot Learning, introduced in previous publication. While the results are unripe for implementation, we believe they constitute a stepping stone towards autonomous development of robotic vi- sual modules.","pdf":"/pdf/00e5c4aefc80d0396ee745c032d27e0bccb43079.pdf","TL;DR":"3 thrusts serving as stepping stones for robot experiential learning of vision module","paperhash":"anonymous|towards_robot_vision_module_development_with_experiential_robot_learning","_bibtex":"@article{\n  anonymous2018towards,\n  title={TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BHbmWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1126/Authors"],"keywords":["Deep Learning","Robotics","Artificial Intelligence","Computer Vision"]}},{"tddate":null,"ddate":null,"tmdate":1515642386395,"tcdate":1511695071005,"number":1,"cdate":1511695071005,"id":"S1w1mX_xG","invitation":"ICLR.cc/2018/Conference/-/Paper1126/Official_Review","forum":"H1BHbmWCZ","replyto":"H1BHbmWCZ","signatures":["ICLR.cc/2018/Conference/Paper1126/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nothing new","rating":"3: Clear rejection","review":"The paper is motivated with building robots that learn in an open-ended way, which is really interesting. What it actually investigates is the performance of existing image classifiers and object detectors. I could not find any technical contribution or something sufficiently mature and interesting for presenting in ICLR.\n\nSome issues:\n- submission is supposed to be double blind but authors reveal their identity at the start of section 2.1.\n- implementation details all over the place (section 3. is called \"Implementation\", but at that point no concrete idea has been proposed, so it seems too early for talking about tensorflow and keras).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING","abstract":"n this paper we present a thrust in three directions of visual development us- ing supervised and semi-supervised techniques. The first is an implementation of semi-supervised object detection and recognition using the principles of Soft At- tention and Generative Adversarial Networks (GANs). The second and the third are supervised networks that learn basic concepts of spatial locality and quantity respectively using Convolutional Neural Networks (CNNs). The three thrusts to- gether are based on the approach of Experiential Robot Learning, introduced in previous publication. While the results are unripe for implementation, we believe they constitute a stepping stone towards autonomous development of robotic vi- sual modules.","pdf":"/pdf/00e5c4aefc80d0396ee745c032d27e0bccb43079.pdf","TL;DR":"3 thrusts serving as stepping stones for robot experiential learning of vision module","paperhash":"anonymous|towards_robot_vision_module_development_with_experiential_robot_learning","_bibtex":"@article{\n  anonymous2018towards,\n  title={TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BHbmWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1126/Authors"],"keywords":["Deep Learning","Robotics","Artificial Intelligence","Computer Vision"]}},{"tddate":null,"ddate":null,"tmdate":1510092380273,"tcdate":1509138751536,"number":1126,"cdate":1510092359678,"id":"H1BHbmWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1BHbmWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING","abstract":"n this paper we present a thrust in three directions of visual development us- ing supervised and semi-supervised techniques. The first is an implementation of semi-supervised object detection and recognition using the principles of Soft At- tention and Generative Adversarial Networks (GANs). The second and the third are supervised networks that learn basic concepts of spatial locality and quantity respectively using Convolutional Neural Networks (CNNs). The three thrusts to- gether are based on the approach of Experiential Robot Learning, introduced in previous publication. While the results are unripe for implementation, we believe they constitute a stepping stone towards autonomous development of robotic vi- sual modules.","pdf":"/pdf/00e5c4aefc80d0396ee745c032d27e0bccb43079.pdf","TL;DR":"3 thrusts serving as stepping stones for robot experiential learning of vision module","paperhash":"anonymous|towards_robot_vision_module_development_with_experiential_robot_learning","_bibtex":"@article{\n  anonymous2018towards,\n  title={TOWARDS ROBOT VISION MODULE DEVELOPMENT WITH EXPERIENTIAL ROBOT LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BHbmWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1126/Authors"],"keywords":["Deep Learning","Robotics","Artificial Intelligence","Computer Vision"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}