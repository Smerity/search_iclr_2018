{"notes":[{"tddate":null,"ddate":null,"tmdate":1516549821916,"tcdate":1516549821916,"number":12,"cdate":1516549821916,"id":"HyUTUNfHz","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"HJR6IerZf","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Any remaining Concerns ?","comment":"Dear Reviewer,\n\nWe have tried addressing all your concerns in our latest response, please let us know if you still have any remaining concerns ? \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1516150789414,"tcdate":1516150789414,"number":11,"cdate":1516150789414,"id":"SyaZlQnEG","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"rJTFwew4M","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Response to reviewer concerns","comment":"> Rather, I'm saying that other algorithms have similar theoretical guarantees\nCould the reviewer be more specific on which algorithms have similar theoretical guarantees with ours? We believe we have clearly distinguish our analysis from the references mentioned before. We would like to emphasize theoretical results (especially upper bound) only provides worst-case guarantee. The empirical performance may vary for different algorithms under same guarantees. Again, we agree GANs may not satisfy our assumptions that have been widely used in GAN optimization, but the analysis is not unnecessary.\n\n> the proof of Theorem 1 seems like it would apply to simultaneous gradient descent\nThe main purpose of theorem 1 is to show the prediction method converges in a convex-concave setting. It is correct that similar analysis can be applied to general alternating gradients and simultaneous gradients without the prediction step. However, we are unaware of previous convergence analysis for alternative gradients with prediction step except for the (non-stochastic) bilinear problems discussed in related work. \n\n> Why should I expect it to use more significantly more RAM?\nIn most current implementation frameworks, we need to store weights of generator, weights of discriminator, gradients of generator, and gradients of discriminator, and  all the four variables need to be stored in RAM (GPU memory) for simultaneous gradient descent. However, only one of the gradients (either generator or discriminator) needs to be stored in GPU anytime for alternate updates. It can make a big difference for training large networks on GPU with limited memory.\n\n>> prediction makes really difficult problems really easy\nThe reviewer is simply nitpicking the part of our response. In our earlier response we have clearly quoted, \n\n“The purpose of the experiments is not to show that we can train things that are *impossible* to train via other methods (indeed, almost anything is possible if you tune the hyperparameters and network architecture enough), but rather that prediction makes really difficult problems really easy.” \n\nThis do suggest that the models considered in our work can be solved by various methods. However, each of these models requires different tricks to actually make them work (For more details please refer section 3.2). In our work, three different GAN models were considered. For each of these models, single method i.e., prediction has been shown to work equally well for the default setting and remains stable for wide range of hyper-parameters. Moreover, it is not clear whether the tricks mentioned in Improved WGAN paper works well when applied to other GAN models or loss functions.\n\n> Your measure of 'really difficult' is  behind the GAN literature in an empirical sense.\nCould the reviewer be more specific ? Pointer to any references ?\n\nRegarding Imagenet:\nThe reported variance was the outcome of the inception score code. In our latest revision, the updated  figure is now an average over five different instances.\n\nWe also thank reviewer for suggestions on improving our paper presentation.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515812741485,"tcdate":1515812741485,"number":10,"cdate":1515812741485,"id":"rJTFwew4M","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"HkUSllGXM","signatures":["ICLR.cc/2018/Conference/Paper422/AnonReviewer4"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/AnonReviewer4"],"content":{"title":"Response to both author comments","comment":"REGARDING THEORY RESPONSE:\nI'm not specifically criticizing the lack of realism in the assumptions - I agree that such a criticism would be unreasonable.\nRather, I'm saying that other algorithms have similar theoretical guarantees (proved using similar assumptions) to your algorithm, yet those guarantees don't seem to correspond in general to serious improvements in empirical performance.\nThus, I estimate that the value of your particular guarantee is low.\nIf instead it were true that all GAN training procedures proven to have property P seem to do really well in practice, and you proved that your (admittedly simple and easy to implement) algorithm had property P, I would feel differently.\n\nYou also didn't address my complaint that the proof of Theorem 1 seems like it would apply to simultaneous gradient descent (another commenter has now also made this claim).\nThis seems like an easy complaint to address - either I'm right about this or I'm wrong.\nIf I'm right, I don't see what value Theorem 1 adds (except as a sanity check), if I'm wrong, I'm happy to increase the score.\n\nFinally, I'm confused by your statements about simultaneous gradient descent.\nWhy should I expect it to use more significantly more RAM?\nPerhaps we are talking about different things when we say Simultaneous Gradient Descent (maybe there is a usage in the saddle-point optimization literature I'm not familiar with)?\n\nREGARDING EXPERIMENTAL RESPONSE:\n\n> prediction makes really difficult problems really easy\nRight - I claim that you haven't showed this.\nAll the problems you solved (admittedly with the exception of the 100 Mixture Components - but I can think of other methods that could solve this problem even more easily :)) have already been 'solved' in my estimation.\nYour measure of 'really difficult' is  behind the GAN literature in an empirical sense.\nI would also claim that the experiments where you've modified the hyperparameters in a variety of ways and shown that things are generally better behaved have already been done, e.g. in the Improved WGAN paper.\nThat doesn't necessarily mean that doing them again is useless, but certainly it decreases their value.\n\n> below we also report the performance score measured at the fewer number of epochs for higher learning rates.\nI think this is much better empirical support for your method than anything else in the paper.\nI will raise my score slightly for this reason, and I would raise it even more for a version that de-emphasized the claims to have SOTA inception score and more heavily explored this benefit (I'm aware you might not have time for that).\n\nI don't really know what to make of the Imagenet experiment.\nFor one thing, you have error bars, but surely you've conducted only one instance of the experiment (presumably all of your baseline runs didn't dip and then recover exactly at epoch 5)?\n\nMISC:\n\nHere are ways that I think this paper could be improved (which you are of course free to disregard):\n\n1. Move Thm 1 to an appendix - I don't think it really does anything.\n2. Get rid of the bit about the oscillator - I don't think it's wrong per se, but the relevance is questionable.\nThe pathologies you're claiming to get rid of correspond more to divergence than to well-behaved cycling about a fixed point.\n3. Emphasize the speed-up you can get from using higher learning rates. This is a good result!\n4. Do proper imagenet experiments. I know they're a pain, but the state of the art has moved there at this point.\n5. Give a more complete story about why your method should prevent certain pathologies, and maybe study more deeply the nature of those pathologies? (I don't have a great suggestion about how to do this because I don't know what the story is!)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515780731132,"tcdate":1515780731132,"number":9,"cdate":1515780731132,"id":"rk7Fq_IEM","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Any questions or concerns ?","comment":"Dear ACs and Reviewers, \n\nDo you have any questions? \nAre there any remaining concerns?\n\nBest regards, \nThe Authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515776756644,"tcdate":1515776756644,"number":8,"cdate":1515776756644,"id":"SkpeoDLEG","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"rJvkqPIVz","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Thanks ","comment":"Thanks for trying it out, please let me know if you need any help implementing it ?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515776478701,"tcdate":1515776478701,"number":4,"cdate":1515776478701,"id":"rJvkqPIVz","invitation":"ICLR.cc/2018/Conference/-/Paper422/Public_Comment","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Works for my non-standard Adversarial network architecture !!","comment":"I would like to just add that I am working on a lighting related project which uses Adversarial network architecture. Because my architecture is bit non-standard GAN architecture, I was finding very difficult to train my network. But after implementing this simple idea (it look hardly 2 hours for me to code), at-least I am able to train my network and get some reasonable results (although far from what I wanted)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515055463379,"tcdate":1515055463379,"number":6,"cdate":1515055463379,"id":"SyyOtPi7G","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"Bk1l-xfmf","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Results on Imagenet","comment":"As per the suggestion, we experimented with Imagenet dataset using AC-GAN [1] model with and without prediction. Please find the result in the supplementary material. Note that unlike the model used in spectral regularization [2] article, AC-GAN model do not use conditional BN, resnet blocks, hinge loss etc. Thus compared to [2], the reported inception score is low. We stick to AC-GAN as it is the only publicly available model which works best on Imagenet dataset.\n\n[1] https://arxiv.org/abs/1610.09585 (AC-GAN)\n[2] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515048050590,"tcdate":1515048050590,"number":5,"cdate":1515048050590,"id":"rkou2HjXf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Results on Imagenet","comment":"As per the suggestion from Reviewer4, we experimented with Imagenet dataset using AC-GAN [1] model with and without prediction. Please find the result in the supplementary material. Note that unlike the model used in spectral regularization [2] article, AC-GAN model do not use conditional BN, resnet blocks, hinge loss etc. Thus compared to [2], the reported inception score is low. We stick to AC-GAN as it is the only publicly available model which works best on Imagenet dataset.\n\n[1] https://arxiv.org/abs/1610.09585 (AC-GAN)\n[2] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514971690352,"tcdate":1514971690352,"number":3,"cdate":1514971690352,"id":"BJMVz7qQG","invitation":"ICLR.cc/2018/Conference/-/Paper422/Public_Comment","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"A relevant reference","comment":"Hi, just wanted to point out a very relevant paper from a different community.\n\nhttps://link.springer.com/article/10.1134%2FS0965542511120050\n\nI guess it deserves to be mentioned in your paper.  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514435392801,"tcdate":1514434791501,"number":4,"cdate":1514434791501,"id":"Bk1l-xfmf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"HJR6IerZf","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Experimental comments","comment":"The purpose of the experiments is not to show that we can train things that are *impossible* to train via other methods (indeed, almost anything is possible if you tune the hyperparameters and network architecture enough), but rather that prediction makes really difficult problems really easy.  Compared to simple alternating gradient methods, prediction methods are more stable than other methods, work with a much wider range of hyperparameters than classical schemes, and don’t suffer from the collapse phenomenon that make it difficult to use other methods. \n   \n Below, we address reviewer’s comments that seem to pertain to specific dataset and architecture:\n \nRegarding DCGAN experiments (Without using label information):\nFigure 4 uses the finely tuned learning rate and momentum parameters that come with the pre-packaged DCGAN code distribution. This figure shows that DCGAN collapses frequently; even with these fine tuned parameters it still requires a carefully chosen stopping time/epoch to avoid collapse.  With prediction it does not collapse at all.  The purpose of increasing the learning rate is not to show that “better” results could be had, but rather to show that prediction methods don’t require finely tuned parameters.  If you have a look at the additional experiments in the appendix (page 18), we train DCGAN with a litany of different learning rate and momentum parameters.  The prediction method succeeds without any collapse events in all of these cases, while non-prediction is unstable as soon as we move away from the carefully tuned parameter choices.\n\nRegarding Stacked GAN (With using label information):  \n   We reproduced this experiment using the Stacked Gan author’s publicly available code, but were not able to get the same inception scores for Stacked GAN as the original authors.  Note the release code did not come with code for computing inception scores, and we used a well-known Tensor Flow implementation that may differ from what the original author’s used. \n   We ran all the scenarios for a fixed number of epochs (200 epochs, which is default in the Stacked GAN’s released code) to ensure a fair comparison. Indeed, prediction method was able to achieve the best inception score of 8.83 at lesser epoch than 200. Having said that, as per the suggestion, below we also report the performance score measured at the fewer number of epochs for higher learning rates. The quantitative comparison based on the inception score for learning rates of 0.0005 (200/5 = 40 epochs) and 0.001 (200/10 = 20 epochs) are as follows-\n\nLearning Rate\t\t\t              0.0005 (epochs=40)\t\t         0.001 (epochs=20)\nStacked GAN (joint)                                 5.80 +/- 0.15                                    1.42 +/- 0.01\nStacked GAN (joint) + Prediction           8.10 +/- 0.10                                    7.79 +/- 0.07\n\nRegarding the absence of problems that are “hard” without prediction:  In Figure 8 of the appendix, we solve a toy problem that is famously hard:  trying to recover all of the modes in a Gaussian mixture model.  The prediction method does this easily, while the method without prediction fails to capture all the modes.  We also “turn the dial up” on this problem by using 100 Gaussian components in Figure 9, and the non-prediction method produces highly irregular results unless a batch size of over 6000 (which is very much larger than the number of components) is used.  In contrast, the prediction method represents the distribution well for a wide range of batch sizes and learning rates."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514434622442,"tcdate":1514434622442,"number":3,"cdate":1514434622442,"id":"HkUSllGXM","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"HJR6IerZf","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Response for theoretical comments","comment":"We agree with the reviewer that theory in this area (and in deep learning in general) often requires assumptions that don’t hold for neural networks.  Nonetheless, we think it is worth taking time to explore conditions under which algorithms are guaranteed to work, because this provides a theoretical proof-of-concept, and thinking through theoretical properties of a new algorithm makes it more than just another hack.   The purpose of our result is to do just that for our proposed algorithm.  We don’t disagree that analysis exists for other algorithms, but we don’t think the existence of other algorithms gets us “off the hook” from thinking about the theoretical implications of our approach.  \n\nThat being said, we think the reviewer is overestimating the state of the art in theory for GANs.  There is currently no theoretical result that does not make strong assumptions, and many results (including those referenced by the reviewer) are quite different from (and in many ways weaker than) our own.  The result in [1] shares certain assumptions with our own (convex-concave assumptions, bounded problem domain, and an ergodic measure of convergence).  However, the result in [1] does not prove convergence in the usual sense, but rather that the error will decay to within an o(1) constant.  In contrast, our result shows that the error decays to zero.  The result in [1] also requires simultaneous gradient descent, which is not commonly used in practice (because it requires more RAM to store [extremely large] iterates and it uses a stale iterate when updating the generator and discriminator one-at-a-time).  In contrast, our result concerns the commonly used alternating direction approach.\n   The result in [0] shows stability using a range of assumptions that are different from (but not necessarily stronger or weaker than) our own.  They require the discriminator to be a linear classifier, and make a strict concavity assumption on the loss function.  They also require an assumption (called Property I) that is analogous to the “strict saddle” assumption in the saddle-point literature (see, e.g. Lee 2016, “Gradient Descent Converges to Minimizers”), which is known not to hold for general neural nets.  Also, note that the result in [0] is only a local stability result (it only holds once the iterates get close to a saddle satisfying the assumptions), whereas our result is a global convergence result that holds for any initialization.\n\tFinally, we emphasize that both [0] and [1] are great works that make numerous important contributions to this field and address a host of issues beyond just convergence proofs.  Our purpose here is not to make any claims that our result is “better” than theirs, but rather to state what differentiates our result from the literature, and why we felt it was worth putting it in the paper.  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514434491202,"tcdate":1514434491202,"number":2,"cdate":1514434491202,"id":"HyXpJeMQz","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"HJgHrDugM","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Regarding accelerated methods","comment":"We thank the reviewer for the thoughtful comments and suggestions for future work.   We think the idea of pursuing accelerated methods is particularly interesting.  We have actually already done some experiments with Nesterov-type acceleration (as described for saddle-point problems by Chambolle and Pock), however it seems that the benefits of acceleration vanish when we move from deterministic to stochastic updates.  We’ve made similar observations for standard convex (non-saddle) problems.  That being said, we’re still interested in this direction, and are keeping our eyes peeled for possible ways forward."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514434369027,"tcdate":1514434303559,"number":1,"cdate":1514434303559,"id":"HkOZkgzQf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Comment","forum":"Skj8Kag0Z","replyto":"ry-q9ZOlf","signatures":["ICLR.cc/2018/Conference/Paper422/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper422/Authors"],"content":{"title":"Regarding step size gamma","comment":"Thanks for the thoughtful comments!  To answer your question:  it is indeed possible to generalize this method by adding an extra stepsize parameter for the prediction step, and this is something that we have experimented with extensively.  It can be shown that your proposed “gamma” parameter method is stable (under convexity assumptions) whenever gamma is between 0 and 2.  However, we have not been able to find any worthwhile advantages to choosing any gamma different from 1.  Choosing a smaller gamma weakens the stability benefits of prediction, and choosing a larger gamma seems to slow down convergence a bit.  The latter effect can be compensated for by choosing a larger learning rate, but even in this case the method doesn’t run noticeably faster than with gamma=1.  For this reason, including this “gamma” seemed like unnecessarily added complexity, so we removed it and went with a cleaner presentation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512607548572,"tcdate":1512607293997,"number":2,"cdate":1512607293997,"id":"HJUBAbUbf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Public_Comment","forum":"Skj8Kag0Z","replyto":"SyMkklU-G","signatures":["~Leon_Boellmann1"],"readers":["everyone"],"writers":["~Leon_Boellmann1"],"content":{"title":"faster convergence","comment":"I just pass by and see this question. This paper is actually very close to what I have been doing (not published yet), so I would like to share some of my understandings. \n \nThe convergence rate for simultaneous gradient descent is generally slow and with the prediction step the order of the convergence rate could be increased. Details can be found in the paper in the reference list: Chen et al, \"Optimal primal-dual methods for a class of saddle point problems\". \n\nActually I think this is a very good paper (probably because I am doing very similar work). Although one of the reviewers gave a very low score, these questions are very reasonable and are what I have in mind too. I believe the authors should be aware of them too. I hope the authors can justify the contributions of the paper. \n\nFor the convex-concave assumption, this paper is not the only one that assumes that. The f-GAN paper also derived a theorem based on this assumption. The recent paper \"Training GANs with Optimism\", which is submitted to this ICLR, also derives the main algorithm based on this assumption. They are still very excellent papers. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512599574208,"tcdate":1512599258408,"number":1,"cdate":1512599258408,"id":"SyMkklU-G","invitation":"ICLR.cc/2018/Conference/-/Paper422/Public_Comment","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Question regarding Theorem 1","comment":"It seems to me that the proof of Theorem 1 would go through even without the prediction step, i.e. it would be equally valid for, say, simultaneous gradient descent (instead of Lemma 2, one only needs to use a similar argument as in Lemma 1). In that sense, the theorem provides no support for the proposed method."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515812958766,"tcdate":1512535749641,"number":3,"cdate":1512535749641,"id":"HJR6IerZf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Review","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Some issues with both experiments and theoretical claims.","rating":"4: Ok but not good enough - rejection","review":"NOTE:\nI'm very willing to change my recommendation if I turn out to be wrong \nabout the issues I'm addressing and if certain parts of the experiments are fixed.\n\nHaving said that, I do (think I) have some serious issues: \nboth with the experimental evaluation and with the theoretical results.\nI'm pretty sure about the experimental evaluation and less sure about the theoretical results.\n\n\nTHEORETICAL CLAIMS:\n\nThese are the complaints I'm not as sure about:\n\nTheorem 1 assumes that L is convex/concave.\nThis is not generally true for GANs.\nThat's fine and it doesn't necessarily make the statement useless, but:\n\nIf we are willing to assume that L is convex/concave, \nthen there already exist other algorithms that will provably converge\nto a saddle point (I think). [1] contains an explanation of this.\nGiven that there are other algorithms with the same theoretical guarantees,\nand that those algorithms don't magically make GANs work better, \nI am much less convinced about the value of your theorem.\n\nIn [0] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable, \neven when L is not convex/concave. \nThis seems like it makes your result a lot less interesting, though perhaps I'm wrong to think this?\n\nFinally, I'm not totally sure you can show that simultaneous gradient descent won't converge \nas well under the assumptions you made.\nIf you actually can't show that, then the therom *is* useless, \nbut it's also the thing I've said that I'm the least sure about.\n\n\nEXPERIMENTAL EVALUATION:\n\nRegarding the claims of being able to train with a higher learning rate:\nI would consider this a useful contribution if it were shown that (by some measure of GAN 'goodness')\na high goodness was achieved faster because a higher learning rate was used.\nYour experiments don't support this claim presently, because you evaluate all the models at the same step.\nIn fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate.\nThis calls into question the usefulness of training with a higher learning rate.\nThe performance is not a huge amount worse though (based on my understanding of Inception Scores),\nso if it turns out that you could get that performance\nin 1/10th the time then that wouldn't be so bad.\n\nRegarding the experiment with Stacked GANs, the scores you report are lower than what they report [2].\nTheir reported mean score for joint training is 8.59.\nAre the baseline scores you report from an independent reproduction?\nAlso, the model they have trained uses label information. \nDoes your model use label information?\nGiven that your reported improvements are small, it would be nice to know what the proposed mechanism is by \nwhich the score is improved. \nWith a score of 7.9 and a standard deviation of 0.08, presumably none of the baseline model runs\nhad 'stability issues', so it doesn't seem like 'more stable training' can be the answer.\n\nFinally, papers making claims about fixing GAN stability should support those claims by solving problems\nwith GANs that people previously had a hard time solving (due to instability).\nI don't believe this is true of CIFAR10 (especially if you're using the class information).\nSee [3] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator.\n\nI didn't pay as much attention to the non-GAN experiments because\na) I don't have as much context for evaluating them, because they are a bit non-standard.\nb) I had a lot of issues with the GAN experiments already and I don't think the paper should be accepted unless those are addressed.\n\n\n[0] https://arxiv.org/abs/1706.04156 (Gradient Descent GAN Optimization is Locally Stable)\n\n[1] https://arxiv.org/pdf/1705.07215.pdf (On Convergence and Stability of GANs)\n\n[2] https://arxiv.org/abs/1612.04357 (Stacked GAN)\n\n[3] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs)\n\nEDIT: \nAs discussed below, I have slightly raised my score. \nI would raise it more if more of my suggestions were implemented (although I'm aware that the authors don't have much (any?) time for this - and that I am partially to blame for that, since I didn't respond that quickly).\nI have also slightly raised my confidence.\nThis is because now I've had more time to think about the paper, and because the authors didn't really address a lot of my criticisms (which to me seems like evidence that some of my criticisms were correct).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642446696,"tcdate":1511712056423,"number":2,"cdate":1511712056423,"id":"HJgHrDugM","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Review","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good work","rating":"7: Good paper, accept","review":"This work proposes a framework for stabilizing adversarial nets using a prediction step. The prediction step is motivated by primal-dual algorithms in convex optimization where the term having both variables is bi-linear.  \n\nThe authors prove a convergence result when the function is convex in one variable and concave in the other. This problem is more general than the previous one in convex optimization.  Then this prediction step is applied in many recent applications in training adversarial nets and compared with state-of-the-art solvers. The better performance of this simple step is shown in most of the numerical experiments. \n\nThough this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance,  there is more work to be done. Whether there is a better generalization of this prediction step? There are also other variants of primal-dual algorithms in convex optimization; can other modification including the accelerated variants be applied?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642446732,"tcdate":1511688841272,"number":1,"cdate":1511688841272,"id":"ry-q9ZOlf","invitation":"ICLR.cc/2018/Conference/-/Paper422/Official_Review","forum":"Skj8Kag0Z","replyto":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference/Paper422/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple modification to alternating stochastic gradient for GAN training, which stabilizes training, essentially for free. Clever and useful idea, solid and insightful analysis, good presentation. ","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training, which stabilizes training, by adding a prediction step. \n\nThis is a clever and useful idea, and the paper is very well written. The proposed method is very clearly motivated, both intuitively and mathematically, and the authors also provide theoretical guarantees on its convergence behavior. I particularly liked the analogy with the damped harmonic oscillator.  \n\nThe experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique. I believe that the method proposed in this paper will have a significant impact in the area of GAN training.\n\nI have only one minor question: in the prediction step, why not use a step size, say \n$\\bar{u}_k+1 = u_{k+1} + \\gamma_k (u_{k+1} − u_k)$, such that the \"amount of predition\" may be adjusted?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1516149392051,"tcdate":1509116243358,"number":422,"cdate":1509739309510,"id":"Skj8Kag0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Skj8Kag0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stabilizing Adversarial Nets with Prediction Methods","abstract":"Adversarial neural networks solve many important problems in data science, but are notoriously difficult to train. These difficulties come from the fact that optimal weights for adversarial nets correspond to saddle points, and not minimizers, of the loss function. The alternating stochastic gradient methods typically used for such problems do not reliably converge to saddle points, and when convergence does happen it is often highly sensitive to learning rates. We propose a simple modification of stochastic gradient descent that stabilizes adversarial networks. We show, both in theory and practice, that the proposed method reliably converges to saddle points. This makes adversarial networks less likely to \"collapse,\" and enables faster training with larger learning rates.","pdf":"/pdf/baf773174024532dfaa0e64c2857d522362bd92d.pdf","TL;DR":"We present a simple modification to the alternating SGD method, called a prediction step, that improves the stability of adversarial networks.","paperhash":"anonymous|stabilizing_adversarial_nets_with_prediction_methods","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Adversarial Nets with Prediction Methods},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skj8Kag0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper422/Authors"],"keywords":["adversarial networks","optimization"]},"nonreaders":[],"replyCount":18,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}