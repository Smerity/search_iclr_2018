{"notes":[{"tddate":null,"ddate":null,"tmdate":1515645566827,"tcdate":1515645566827,"number":12,"cdate":1515645566827,"id":"SkwY9v4VG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some details on CIFAR10 experiments","comment":"Do the four rows in Table 2 (#GPUs in total = 4, 8, 16, 32) correspond to 1, 2, 4 and 8 training nodes? Could you please also say what is the compression ratio for these four cases? Thank you."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515201140759,"tcdate":1515201053209,"number":19,"cdate":1515201053209,"id":"B1HmMiTXz","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"ryuVTiCWG","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: more improvements can be done on the momentum correction justification","comment":"We thank the reviewer for the suggestions. We have revised our paper.\n\n- The author should emphasize that the sparsification + thresholding have async nature, as the update only trigger when sparse condition applies and causes the mismatch problem. \nWe talked about the asynchrony nature in another way in the Sec 3.3: “Because we delay the update of small gradients, when these updates do occur, they are outdated or \\emph{stale}.”\n\n- It is a bit hard to understand the vanilla baseline(I might not do it in that way). It can be explained as local gradient aggregation and only apply momentum update at the trigger point. The update rule is no longer equivalent to SGD with momentum. \nWe revised our paper to describe the vanilla sparse SGD + momentum baseline in Sec 3.2: \"If SGD with the momentum is directly applied to the sparse gradient scenario (line 15 in Algorithm \\ref{alg:ssgd}), the update rule is no longer equivalent to Equation \\ref{eq:msgd}, which becomes:\".\n\n- The paper did not explicitly mention that the value of v_t gets reset after triggering a communication, it should be explicitly mentioned in the update equation. Justify the correctness: \nWe revised our paper to explicitly mention resetting the value of v_t by the mask in Sec 3.2: \"Similarly to the line 12 in Algorithm \\ref{alg:ssgd}, the accumulation result $v_{k,t}$ gets cleared by the mask in the $sparse\\left( \\right)$ function.\"\n\nRough intuition gives most of the current justification. We can know that one is better than another, because of the equivalence. The author should try to do more formal justification, at least in some special cases, instead of leaving it as a debt for yourself or the potential readers\n- For example, the author should be able to prove that, under only one machine and one weight value. The weight value after K updates using the vanilla approach vs. the new approach. \nWe have shown that the sparsification + local gradient accumulation can be considered as “increasing the batch size over time” in Sec 3.1.\n\n-A fundamental question that needs to be answered, is that why thresholding trigger method(which is async) works as good as sync SGD. A proof sketch to show the loss change after K step update would shed light on this and may give insights on what a good update rule is(possibly borrow some analysis from aync update and stale gradient)\n[1] has shown that running stochastic gradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration and a smaller momentum coefficient can work as good as sync SGD. We introduced the momentum factor masking to dynamically reduce the momentum term in Sec 3.3. Meanwhile, the asynchrony nature in the sparsification + local gradient accumulation can be considered as “increasing the batch size over time” in Sec 3.1, and there’s numerous work showing that increasing the batch size is feasible.\n\nReferences\n[1] Mitliagkas, I., Zhang, C., Hadjis, S., & Re, C. (2017). Asynchrony begets momentum, with an application to deep learning. In 54th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2016. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1514927601074,"tcdate":1514927601074,"number":17,"cdate":1514927601074,"id":"S1te8dFmM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"r1U-Go0WG","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/AnonReviewer2"],"content":{"title":"Re","comment":"Thanks for the response, I feel confident this contribution should be accepted."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515124052088,"tcdate":1513979928348,"number":11,"cdate":1513979928348,"id":"ByemeWoMz","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"Byn_brkfG","signatures":["~Chia-Yu_Chen1"],"readers":["everyone"],"writers":["~Chia-Yu_Chen1"],"content":{"title":"Thanks for your response","comment":"Hi DGC,\n\nThanks for your comments.  I read the paper again.  The idea is quite interesting, but I still cannot say that I totally understood the results.  Need more time for me to digest.\n\n\nSincerely,\n\nChia-Yu"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513822504705,"tcdate":1513822504705,"number":16,"cdate":1513822504705,"id":"H1eVtcdff","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"HkRogNzGf","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: question about Momentum Factor Masking","comment":"Thank you for your comments. \n\nThe momentum factor masking does not reset the momentum correction. It only blocks the momentum of delayed gradients from misleading the optimization.\n\nSuppose the last update is at iteration t-1, the next update at iteration t+T-1, and we only consider the gradients  { g_{t}, g_{t+1}, ..., g_{t+T-1} }\n\n         - Dense Update\n            w_{t+T} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m +  ... + m^{\\tau-1}) x g_{t} +  (1 + m +  ... + m^{\\tau-2}) x g_{t+1} + ... + (1 + m +  ... + m^{\\tau-T}) x g_{t+T-1} + ...], where \\tau > T\n\n         - Only local gradient accumulation\n            the coefficients of  { g_{t}, g_{t+1}, ..., g_{t+T-1} } are always the same.\n            w_{t+T} = w_{t} - lr x [ ... + 1 x g_{t} +  1 x g_{t+1} + ... +  1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m + m^2 + ... + m^{\\tau-T}) x (g_{t} + g_{t+1} + ... + g_{t+T-1}) + ...]\n\n         - With the momentum correction,\n            the coefficients of  { g_{t}, g_{t+1}, ..., g_{t+T-1} } are always the same as the dense update.\n            w_{t+T} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m +  ... + m^{\\tau-1}) x g_{t} +  (1 + m +  ... + m^{\\tau-2}) x g_{t+1} + ... + (1 + m +  ... + m^{\\tau-T}) x g_{t+T-1} + ...], where \\tau > T\n\n         - With the momentum correction and momentum factor masking\n            we clear the local u_{t} to prevent the delayed gradients from misleading the optimization after they are used for the update.\n            w_{t+T} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1} + ...], where \\tau > T"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513822402776,"tcdate":1513822402776,"number":15,"cdate":1513822402776,"id":"H1o6dcuff","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"S1mTqdWzM","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: requires more work to justify momentum factor masking","comment":"        Thank you for your question.\n        This example is to show how the momentum correction works, and therefore we do not consider the staleness effect in this example. \n        Suppose the last update is at iteration t-1, the next update at iteration t+T-1, and we only consider the gradients  { g_{t}, g_{t+1}, ..., g_{t+T-1} }\n         - Dense Update\n            w_{t+T} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m +  ... + m^{\\tau-1}) x g_{t} +  (1 + m +  ... + m^{\\tau-2}) x g_{t+1} + ... + (1 + m +  ... + m^{\\tau-T}) x g_{t+T-1} + ...], where \\tau > T\n\n         - Only local gradient accumulation\n            the coefficients of  { g_{t}, g_{t+1}, ..., g_{t+T-1} } are always the same.\n            w_{t+T} = w_{t} - lr x [ ... + 1 x g_{t} +  1 x g_{t+1} + ... +  1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m + m^2 + ... + m^{\\tau-T}) x (g_{t} + g_{t+1} + ... + g_{t+T-1}) + ...]\n\n         - With the momentum correction,\n            the coefficients of  { g_{t}, g_{t+1}, ..., g_{t+T-1} } are always the same as the dense update.\n            w_{t+T} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m +  ... + m^{\\tau-1}) x g_{t} +  (1 + m +  ... + m^{\\tau-2}) x g_{t+1} + ... + (1 + m +  ... + m^{\\tau-T}) x g_{t+T-1} + ...], where \\tau > T\n\n         - With the momentum correction and momentum factor masking\n            we clear the local u_{t} to prevent the delayed gradients from misleading the optimization after they are used for the update.\n            w_{t+T} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1}]\n            w_{t+\\tau} = w_{t} - lr x [ ... + (1 + m +  ... + m^{T-1}) x g_{t} +  (1 + m +  ... + m^{T-2}) x g_{t+1} + ... + 1 x g_{t+T-1} + ...], where \\tau > T"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513403914516,"tcdate":1513402533930,"number":10,"cdate":1513402533930,"id":"HkRogNzGf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"question about Momentum Factor Masking","comment":"Once we applied momentum factor masking; the momentum correction becomes useless.  The  accumulated discounting factor in Eq(6) was masked and become the same as original way.  It is not clear about momentum factor mask.  Please clarify it.  As reviewer's comments, it seems that momentum factor mask resets the momentum correction.  From the content, this is really confusing!  Thanks a lot. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513355962779,"tcdate":1513355962779,"number":9,"cdate":1513355962779,"id":"S1mTqdWzM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"BkCsfi0bG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"requires more work to justify momentum factor masking","comment":"- Therefore, we do not keep a running sum of v_t overall history, but keep a running sum of u_t. v_t is the running sum result and will be cleared after update (with or without momentum factor masking).  For example, at iteration t-1, \nu_{t-1}  = m^{t-2} g_{1}+ … + m g_{t-2} + g_{t-1}, \nv_{t-1}  = (1+…+m^{t-2}) g_{1} + … + (1+m) g_{t-2} + g_{t-1}. \nUpdate, w_{t} = w_{1} – lr x v_{t-1}\nAfter update, v_{t-1}  = 0. \nNext iteration, \nu_{t} = m^{t-1} g_{1} + … + m g_{t-1} + g_{t}, \nv_{t}  = m^{t-1} g_{1} + … + m g_{t-1} + g_{t}.\nUpdate, w_{t+1} = w_{t} – lr x v_{t}  \n                              = w_{1} – lr x (v_{t-1} + v_{t} )\n                              = w_{1} - lr x [ (1+…+m^{t-1}) g_{1} + … + (1+m) g_{t-1} + g_{t} ]\nWhich is the same as the dense momentum SGD.\n\nIn the momentum factor masking section, both v_t and u_t get reset after trigerring a communication. Why only v_t gets reset in your example?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513358096163,"tcdate":1513210228063,"number":14,"cdate":1513210228063,"id":"Byn_brkfG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"HySlhQ1GM","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Fully maintaining the accuracy on ImageNet at high compression ratio is not easy, but Deep Gradient Compression made it.","comment":"Thanks for the comments. The accuracy degradation on ImageNet are quoted from the Table 2 of AdaComp [1]:\nResNet18: baseline top1 error=32.41%, AdaComp top1 error=32.87% (0.46% accuracy degradation) \nResNet50: baseline top1 error=28.91%, AdaComp top1 error=29.15% (0.24% accuracy degradation)\n\nIn our DGC work:\nResNet50: baseline top1 error=24.04%, DGC top1 error=23.85% \n\nWe respect your argument and would be happy to adjust the citation to your paper. However, we believe ImageNet results are more interesting than MNIST. The 0.5% Top1 accuracy degradation on ImageNet is significant, not noise. Fully maintaining the accuracy on ImageNet at a much higher compression ratio is not easy, while the bag of four techniques introduced in DGC achieved this.\n\nThe worker-scalability of deep gradient compression is described in Figure6 with up to 64 workers. \n\nReferences:\n[1] Chen, Chia-Yu, et al. \"AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training.\" arXiv preprint arXiv:1712.02679 (2017)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513204882578,"tcdate":1513204717334,"number":8,"cdate":1513204717334,"id":"HySlhQ1GM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~Chia-Yu_Chen1"],"readers":["everyone"],"writers":["~Chia-Yu_Chen1"],"content":{"title":"Hi from AdaComp","comment":"Hi DeepGradientCompression,\n\nThis is an interesting paper.  I think that it is misleading to mention that AdaComp shows 0.2-0.4% degradation.  AdaComp sometimes actually shows ~0.3% improvement and it always <0.5% difference compared to baseline.  The difference is from randomness of SGD; not from AdaComp itself.  It is not very meaningful to quote SGD difference within 0.5%.  Please correct it.\n\nBy the way, I have some questions: how many workers do you use in the experiments?  What is worker-scalability of deep gradient compression?\n\nBest,\n\nChia-Yu\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513172272322,"tcdate":1513172272322,"number":13,"cdate":1513172272322,"id":"ryuVTiCWG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"BkCsfi0bG","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/AnonReviewer3"],"content":{"title":"more improvements can be done on the momentum correction justification","comment":"This is the comment after author's updated a revised version of the paper.\n\nI now understand the proposed momentum correction method(with some effort). However, the presentation can be further improved.  I list my suggestions here:\n\nClarify the approach:\n- The author should emphasize that the sparsification + thresholding have async nature, as the update only trigger when sparse condition applies and causes the mismatch problem.\n- It is a bit hard to understand the vanilla baseline(I might not do it in that way). It can be explained as local gradient aggregation and only apply momentum update at the trigger point. The update rule is no longer equivalent to SGD with momentum.\n- The paper did not explicitly mention that the value of v_t gets reset after triggering a communication, it should be explicitly mentioned in the update equation.\n\nJustify the correctness:\nRough intuition gives most of the current justification. We can know that one is better than another, because of the equivalence.\nThe author should try to do more formal justification, at least in some special cases, instead of leaving it as a debt for yourself or the potential readers \n\n- For example, the author should be able to prove that, under only one machine and one weight value. The weight value after K updates using the vanilla approach vs. the new approach.\n-A fundamental question that needs to be answered, is that why thresholding trigger method(which is async) works as good as sync SGD. A proof sketch to show the loss change after K step update would shed light on this and may give insights on what a good update rule is(possibly borrow some analysis from aync update and stale gradient)\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"ddate":null,"tddate":1513169586568,"tmdate":1513169604141,"tcdate":1513169574083,"number":12,"cdate":1513169574083,"id":"BkCsfi0bG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"BJDSSJ5-M","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: good empirical results, but requires work to justify the proposed techniques","comment":"We thank the reviewer for the comments. We have revised our paper.\n\n    -    Did you keep a running sum of v_t overall history? Such sum without damping(the m term in momentum update) is likely lead to the growing dominance of noise and divergence.  Do you do non-sparse global synchronization of momentum term? It seems that local update of momentum is likely going to diverge, and the momentum masking somehow reset that.\n\nWe already revised our paper, and described the momentum correction more precisely in Section 3.2. \n\nBasically, the momentum correction performs the momentum SGD without update locally, and accumulates the velocity u_t locally. The optimization performs SGD with v_t instead of momentum SGD with G_t after momentum correction.  We add figure 2 to illustrate the difference.\n\nTherefore, we do not keep a running sum of v_t overall history, but keep a running sum of u_t. v_t is the running sum result and will be cleared after update (with or without momentum factor masking).  For example, at iteration t-1, \nu_{t-1}  = m^{t-2} g_{1}+ … + m g_{t-2} + g_{t-1}, \nv_{t-1}  = (1+…+m^{t-2}) g_{1} + … + (1+m) g_{t-2} + g_{t-1}. \nUpdate, w_{t} = w_{1} – lr x v_{t-1}\nAfter update, v_{t-1}  = 0. \nNext iteration, \nu_{t} = m^{t-1} g_{1} + … + m g_{t-1} + g_{t}, \nv_{t}  = m^{t-1} g_{1} + … + m g_{t-1} + g_{t}.\nUpdate, w_{t+1} = w_{t} – lr x v_{t}  \n                              = w_{1} – lr x (v_{t-1} + v_{t} )\n                              = w_{1} - lr x [ (1+…+m^{t-1}) g_{1} + … + (1+m) g_{t-1} + g_{t} ]\nWhich is the same as the dense momentum SGD.\n\n    -    Did you perform local aggregations of gradients between GPU cards before send out to do all0reduce in a network?\n\nYes."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513219611225,"tcdate":1513169405760,"number":11,"cdate":1513169405760,"id":"r1U-Go0WG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"rJ9crpElM","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: A useful contribution","comment":"We thank the reviewer for the comments.\n\n    -    Several Figures and Tables are never referenced in the text, making it a little harder to properly follow text. Pointing to them from appropriate places would improve clarity I think.\n\nWe revised our paper. All the figures and tables are referenced properly in the text.\n\n    -    Algorithm 1 line 14: You never seem to explain what is sparse(G). Sec 3.1: What is it exactly that gets communicated? How do you later calculate the Compression Ratio?\n\nWe have change the name of function to encode(G). The encode() function packs 32-bit nonzero gradient values and 16-bit run lengths of zeros in the flattened gradients. The encoded sparse gradients get communicated. These are described in the Sec 3.1 now.\nThe compression ratio is calculated as follows:\n       The Gradient Compression Ratio = Size[ encode( sparse( G_k ) ) ] / Size [G_k]\nIt is defined in the Sec 4.1 now.\n\n    -    Sec 3.2 you mention 1% loss of accuracy. A pointer here would be good, at that point it is not clear if it is in your work later, or in another paper.\n\nWe pointed to the Figure 3(a) in the updated draft, and also cite the paper AdaComp [1].\n\n    -    Pointer to Table 1 at the end of Sec 3 would probably be ideal along with some summary comments.\n\nWe make a summary at the end of Sec 3 and add Appendix D to show the overall algorithm of DGC in the updated draft.\n\n    -    I find it really confusing why you sometimes compare against Gradient Dropping, sometimes against TernGrad, sometimes against neither, sometimes include Gradient Sparsification with momentum correction (not clear again what is the difference from DGC).\n\nBecause related work didn’t cover them all.  Gradient Dropping [2] only performed experiments on 2-layer LSTM for NMT, and 3-layer DNN for MNIST; TernGrad [3] only performed experiments on AlexNet, GoogleNet and VGGNet.  Therefore, we compared our AlexNet result with TernGrad. \n\nDGC contains not only momentum correction but also momentum factor masking and warm-up training. Momentum correction and Local gradient clipping are proposed to improve local gradient accumulation. Momentum factor masking and warm-up training are proposed to overcome the staleness effect. Comparison between Gradient Sparsification with momentum correction and DGC shows their impact on training respectively.\n\n    -    Why do you only work with 99.9% sparsity? Does 99% with 64 training nodes lead to almost dense total updates, making it inefficient in your communication model? If yes, does that suggest a scaling limit in terms of number of training nodes? If not, how important is the 99.9% sparsity if you care about communication cost dominating the total runtime?\n\nYes, 99% with 128 training nodes lead to almost dense total updates, making it inefficient in communication. The scaling limit N in terms of number of training nodes depends on the gradient sparsity s: N ≈1/(1-s). When the gradient sparsity is 99.9%, the scaling limit is 1024 training nodes.\n\n    -    When you say what you do has the effect of increasing stepsize, why don't you just increase the stepsize? What would be your reason for using DGC as opposed to just increasing the batch size?\n\nSince the memory on GPU is limited, the way to increase the stepsize is to increase training nodes. Previous work in increasing the stepsize focus on how to deal with very large mini-batch training, while our work focus on how to reduce the communication consumption among increased nodes under poor network bandwidth. DGC can be considered as increasing the stepsize temporally on top of increasing the actual stepsize spatially.\n\nReferences:\n[1] Chen, Chia-Yu, et al. \"AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training.\" arXiv preprint arXiv:1712.02679 (2017).\n[2] Aji, Alham Fikri, and Kenneth Heafield. Sparse Communication for Distributed Gradient Descent. In Empirical Methods in Natural Language Processing (EMNLP), 2017.\n[3] Wen, Wei, et al. TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning. In Advances in Neural Information Processing Systems, 2017."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513219450225,"tcdate":1513169331639,"number":10,"cdate":1513169331639,"id":"rknhZsRbf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"B1lk3Ojxf","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Study on gradient compression","comment":"       We thank the reviewer for the comments.\n\n    -    What exactly is the problem without this correction? Can the issue be described more precisely?\n\nWe already revised our paper, and described the momentum correction more precisely in Section 3.2. Basically, the momentum correction performs the momentum SGD without update locally and accumulates the velocity u_t locally. \n\n    -    What exactly is the issue of Gradient clipping?\n\nWhen training RNN, people usually use Gradient Clipping to avoid the exploding gradient problem. The hyper-parameter for Gradient Clipping is the threshold thr_G of the gradients L2-norm. The gradients for optimization is scaled by a coefficient depending on their L2-norm. \n\nBecause we accumulate gradients over iterations on each node independently, we need to scale the gradients before adding them to the previous accumulation, in order to scale the gradients by the correct coefficient. The threshold for local gradient clipping thr_Gk should be set to N^{-1/2} x thr_G. We add Appendix C to explain how N^{-1/2} comes.\n\n    -    What is the expected performance of the 1-bit SGD method proposed by Seide et al.?\n\n1-bit SGD [1] encodes the gradients as 0 or 1, so the data volume is reduced by 32x. Meanwhile, since 1-bit SGD quantizes the gradients column-wise, a floating-point scaler per column is required, and thus it cannot yield much speed benefit on convolutional neural networks.\n\n    -    What exactly is \"layer normalization\"\n\n“Layer Normalization” is similar to batch normalization but computes the mean and variance from the summed inputs in a layer on a single training case. [2]\n\n-\t What are \"drastic gradients\"?\n\nIt means the period when the network weight changes dramatically.\n\nReferences:\n[1] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.\n[2] J. Lei Ba, J. R. Kiros, and G.E.Hinton, Layer Normalization. ArXiv e-prints, July 2016"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513219489926,"tcdate":1513169219564,"number":9,"cdate":1513169219564,"id":"r13BWoAZM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"Bk5Gd4sxf","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Hi from TernGrad","comment":"Hi, Wei. Thank you for your comments.\n\n    First of all, all the hyper-parameters, including the learning rate and momentum, are the same as the default settings.\n\n    -    The loss because of TernGrad is just 0.04% instead of 0.89%? \n\n     In the paper of TernGrad [1], the baseline AlexNet is trained with dropout ratio of 0.5, while the TernGrad AlexNet is trained with dropout ratio of 0.2. The paper claims that quantization introduces randomness and less dropout ratio avoids over-randomness. However, when we trained the baseline AlexNet with dropout ratio of 0.2, we gained 1 point improvement in top-1 accuracy. It indicates that the TernGrad might incur more loss of accuracy than expected. Therefore, to be fair, we use the dropout ratio of 0.2 in all experiments relating to AlexNet.\n\n   -    does the same warmup scheme work in general for all experiments?\n\n   Yes. Warm-up training takes only 4 out of 90 epochs for ImageNet, 1 out of 70 epochs for Librispeech. The gradient sparsity increases exponentially  75% -> 93.75% -> 98.4375% -> 99.6%.\n\nReferences:\n[1] Wen, Wei, et al. TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning. In Advances in Neural Information Processing Systems, 2017."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513169069928,"tcdate":1513169069928,"number":8,"cdate":1513169069928,"id":"H18nes0Wz","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"S1vhAambf","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Some details on distributed training of language model with Deep Gradient Compression","comment":"Thank you for your comments.\nThe batch size is 80 and the number of iterations per epoch is 332."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513169046132,"tcdate":1513169046132,"number":7,"cdate":1513169046132,"id":"BJRcgoCbG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"ryR_PyE-f","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: details on implementation of Deep Gradient Compression","comment":"Thank you for your comments.\n    -     For parameter server, communication is reduced when push sparse gradient to parameter server. Is it possible to pull sparsified gradient and applied locally? \n\n      Yes, you can pull sparsified gradient and applied locally.\n\n   -     For All-reduce, since the sparse gradients may be of different size, the standard MPI All-reduce operation won't work for this. Do you implement your own All-reduce operation?\n\n      In our experiments, we force the size of the sparse gradients to be same as 0.1% of the number of gradients. We use hierarchical top-k selection not only to speed up sparsification but also to control the sparse gradients size. If the number of gradients is smaller than 0.1%, we filled the buffer with zeros. If it is much larger, we re-calculate the threshold. However, an efficient All-reduce operation for sparse communication is one of our future work.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513219273839,"tcdate":1513168936653,"number":6,"cdate":1513168936653,"id":"HJbNlsRbM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"S1SGgoA-f","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"continue","comment":"(continue)\n\n    -    However this paper added several hyper-parameters (momentum correction, learning rate correction, warm up, and momentum factor mask etc..)\n\nThe *only* hyper-parameters introduced by DGC are the warm-up training strategy. However, we use the same settings in all experiments as answered above. Momentum correction and Momentum factor masking are equation changes, they do not introduce any hyper-parameters. \n\n    -    The compression rate and performance ignore several important factors such as sparsity representation, different directions of compression, computation overhead (parallel or not)\n\nNo, Figure 6 in the Sec 5 already takes the sparsity representation, computation overhead, communication overhead into account. \n  \n    -    Results are from simple model only\n\nNo, we have broadly experimented on state-of-the-art, complex models across CNN, RNN, CNN and RNN mixture. We extensively experimented with ResNet110 on Cifar10, AlexNet/ResNet50 on ImageNet, 2-layer LSTM with the size of 195MB on PTB, 7-layer GRU following 3-layer CNN (DeepSpeech) with the size of 488MB on LibriSpeech. \nIn comparison, previous work Gradient Dropping [4] performed experiments on 2-layer LSTM with size of 476MB for NMT, and 3-layer DNN with size of 80MB on MNIST; \nTernGrad [3] performed experiments on AlexNet, GoogleNet, and VGGNet on ImageNet; \nAdacomp [2] performed experiments on 4-layer CifarCNN with the size of 0.3MB on Cifar10, AlexNet, ResNet18, ResNet50 on ImageNet, BN50-DNN with the size of 43MB on BN50, and 2-layer LSTM with the size of 13MB on Shakespeare Dataset.\n\nReferences:\n[1] Cormen, Thomas H. Introduction to algorithms. MIT press, 2009\n[2] Chen, Chia-Yu, et al. \"AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training.\" arXiv preprint arXiv:1712.02679 (2017).\n[3] Wen, Wei, et al. TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning. In Advances in Neural Information Processing Systems, 2017.\n[4] Aji, Alham Fikri, and Kenneth Heafield. Sparse Communication for Distributed Gradient Descent. In Empirical Methods in Natural Language Processing (EMNLP), 2017."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513219204731,"tcdate":1513168908794,"number":5,"cdate":1513168908794,"id":"S1SGgoA-f","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"SkhtLE9Zz","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Some suggestions about deep gradient compression","comment":"Thank you for your suggestions.\nWe appreciate your reminding us of citing these excellent papers, and we have already cited these work in the newest version of our paper.\n\n    -    Although sorting could be Nlog(N), this method is not easy to be parallel.  Thus, large computation overhead still exists.\n\nWe use top-k selection, *NOT* sorting. The complexity of top-k selection is O(N), not O(NlogN) [1]. To further reduce computation, we perform the top-k selection on samples in stride. The sample rate is 0.1% to 1%. In practice, without any code optimization, the extra computation takes less than 10% of total communication time when training AlexNet with 64 nodes under 1Gbps Ethernet. We have already included this in Figure 6.\n\n    -    From previous works, gradient residue compression is pretty robust, it is not surprising that the compression rate is high.\n\nIn fact, gradient residue compression does not preserve  the accuracy of the model.\nFigure 4 in the related work [2] shows that gradient residue compression brings around 2% to 5% loss of accuracy when the compression ratio is less than 500x, and even damages the training when the compression ratio is higher. It is our bag of 4 techniques that enables no loss of accuracy.\n\n    -    What happened in the direction from parameter to workers?  This could reduce their compression rate by learner number.\n\nFirst, we use all-reduce communication model in system performance analysis. \n\nWith N=2^k workers and s sparsity. We need k step to gather these gradients. The density doubles at every step, so the average communication volume is \\sum_{i=0}^{k-1} 2^{i}*M*s/k = (N-1)/log(N)*M*s. The average density increases sub-linearly with the number of nodes by N/log(N), not exponentially. \n\nWe already considered this non-ideal effect, including the extra computation cost on top-k selection, in the second paragraph of Section 5: \"the density of sparse data doubles at every aggregation step in the worst case. However, even considering this effect, Deep Gradient Compression still significantly reduces the network communication time, as implied in Figure 6.\" \"For instance, when training AlexNet with 64 nodes, conventional training only achieves nearly 30× speedup with 10Gbps Ethernet (Apache, 2016), while with DGC, more than 40× speedup is achieved even with 1Gbps Ethernet\". With 1Gbps Ethernet, the speedup of TernGrad is 30x, our worse case is 44x (considering this non-ideal effect), our best case is 58x. We reported the worse case, which is 44x speedup (see Figure 6).\n\nWhen it comes to parameter server communication model, we only pull the sum of sparse gradients, which is the same as TernGrad [3]. With the gradient compression ratio of 500x, it requires at least 500 training nodes to pull the same data size as in the dense scenario.\n  \n    -    What is the sparse representation?\n\nWe already discussed the sparse representation strategy in section 3.1. We used the simple run-length encoding: we pack the 32-bit float nonzero gradient values and 16-bit run lengths of zeros of the flattened gradients. The overhead is only 0.5x, not 10x. We already considered the overhead when reporting the compression ratio. \n\n    -    How much warm up period do you need to use for each examples?\n\nWarm-up training takes only 4 out of 90 epochs for ImageNet, 1 out of 70 epochs for Librispeech, which is only 1.4%-4% of the total training epochs. The time impact of warmup training is negligible. \n \n    -    Is the compression layer-wise or whole model (including FC layers and convolution layers)?\n\nUnlike AdaComp [2] has “~200X for fully-connected and recurrent layers, and ~40X for convolutional layers”, our compression rate is the same for the WHOLE model, where sparsity=99.9% for ALL layers."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512946288280,"tcdate":1512879748191,"number":7,"cdate":1512879748191,"id":"SkhtLE9Zz","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"some suggestions about deep gradient compression","comment":"This paper has strong experimental results. Momentum and learning rate correction make sense for effective larger mini-batch size.  However there are some suggestions about this work.\n\n\n1. This submission should cite other papers well.  The main algorithm of this submission is very similar as Dryden's work in 2016 ( Communication quantization for data-parallel training of deep neural networks) and Strom in 2015 (Strom,  N.   2015.   Scalable  distributed  dnn  training  using commodity gpu cloud computing. In Sixteenth Annual Conference of the International Speech Communication Association).  Moreover, recently an ArXiv paper (AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training, accepted in AAAI18) also reported similar gradient compression scheme and shows excellent experimental results in all different NNs (ResNet50, ResNet18, AlexNet, RNN, DNN, LSTM etc..).  This paper should cite relevant work properly. \n\n2. In section5, authors proposed sampling to reduce sorting time (the same as Dryden's work in 2016).  Although sorting could be Nlog(N), this method is not easy to be parallel.  Thus, large computation overhead still exists.\n\n3.  Learning rate correction (estimate T), momentum correction, momentum factor mask, and warm up are very empirical.  From previous works, gradient residue compression is pretty robust, it is not surprising that the compression rate is high.  \n\n4. The paper just focuses on compression from workers to parameter server.  What happened in the direction from parameter to workers?  This could reduce their compression rate by learner number (as described in TernGrad).\n\n5. What is the sparse representation?  The overhead of sparse representation should be discussed.  It is easy to lose compression rate by >10x here.  The high compression rate may be confusing if detailed sparse representation is not discussed. \n\n6. How much warm up period do you need to use for each examples? Warm-up makes the experiments much easier since they do not clearly mention the warm-up epoch number.  \n\n7.  Is the compression layer-wise or whole model (including FC layers and convolution layers)?\n\nIn general, this paper reused previous gradient residue idea and added momentum and learning rate correction for effective larger mini-batch size.  This paper did a lot of experiments and has strong experimental results for NN convergence.  However this paper added several hyper-parameters (momentum correction, learning rate correction, warm up, and momentum factor mask etc..) and should clearly list values of these parameters in the test cases.  It is also important to guide users ways to put these extra hyper-parameters.  The compression rate and performance ignore several important factors such as sparsity representation, different directions of compression, computation overhead (parallel or not); results are from simple model only.  Look forward to seeing more exciting papers from this team!\n\n\n\n\n \n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512858943277,"tcdate":1512858943277,"number":4,"cdate":1512858943277,"id":"BJDSSJ5-M","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"rJmrmQ5lG","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/AnonReviewer3"],"content":{"title":"the author  really need to provide justification to the momentum correction","comment":"I take a look at the other reviews after they get online. While most of them give accept to the paper given the strong empirical results provided by the paper.\n\nHowever, the problem is mentioned by all the reviewers(it is unclear why momentum correction is needed and the intuition behind this). I would like to emphasize that the current rule seems will is likely lead to the growing dominance of noise and divergence (because of no damping).\n\nI strongly encourage the author to clarify this. We cannot simply accept a paper for great empirical result without justification of why the rule works (all the reviewers are confused by this point) "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512466294282,"tcdate":1512466294282,"number":6,"cdate":1512466294282,"id":"ryR_PyE-f","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"details on implementation of Deep Gradient Compression","comment":"In the appendix, you mention two ways to aggregate gradients: parameter server and All-reduce. \n1.For parameter server, communication is reduce when push sparse gradient to parameter server. Is it possible to pull sparsified gradient and applied locally? \n2. For All-reduce, since the sparse gradients may be of different size, the standard MPI All-reduce operation won't work for this. Do you implement  your own All-reduce operation?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512460129098,"tcdate":1512459951095,"number":5,"cdate":1512459951095,"id":"S1vhAambf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some details on distributed training of language model with Deep Gradient Compression","comment":"\nHello!\n\nCould you please tell what was the batch size and the number of iterations per epoch on a node during distributed training of the language model on PTB? This is necessary to get an idea of total amount of communication that was sufficient to reach perplexity 72.24 at the end of 40-th epoch.\n\nThank you!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642518382,"tcdate":1511914456468,"number":3,"cdate":1511914456468,"id":"B1lk3Ojxf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Review","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Study on gradient compression","rating":"7: Good paper, accept","review":"The paper is thorough and on the whole clearly presented. However, I think it could be improved by giving the reader more of a road map w.r.t. the guiding principle. The methods proposed are heuristic in nature, and it's not clear what the guiding principle is. E.g., \"momentum correction\". What exactly is the problem without this correction? The authors describe it qualitatively, \"When the gradient sparsity is high, the interval dramatically increases, and thus the significant momentum effect will harm the model performance\". Can the issue be described more precisely? Similarly for gradient clipping, \"The method proposed by Pascanu et al. (2013) rescales the gradients whenever the sum of their L2-norms exceeds a threshold. This step is conventionally executed after gradient aggregation from all nodes. Because we accumulate gradients over iterations on each node independently, we perform the gradient clipping locally before adding the current gradient... \" What exactly is the issue here? It reads like a story of what the authors did, but it's not really clear why they did it.\n\nThe experiments seem quite thorough, with several methods being compared. What is the expected performance of the 1-bit SGD method proposed by Seide et al.?\n\nre. page 2: What exactly is \"layer normalization\"?\n\nre. page 4: What are \"drastic gradients\"?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511897281655,"tcdate":1511897105554,"number":4,"cdate":1511897105554,"id":"Bk5Gd4sxf","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~Wei_Wen1"],"readers":["everyone"],"writers":["~Wei_Wen1"],"content":{"title":"Hi from TernGrad","comment":"Hi from TernGrad, \n\nImpressive result, really! \n\nFor the top-1 accuracy in Table 3, I guess the 0.89% accuracy difference of TernGrad comes from the different ways we trained the standard AlexNet? In our work, the baseline AlexNet is trained using the same hyper-parameters of caffe (https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet), and converges to 57.32%. Your baseline got 58.17% because you used different training hyper-parameters in Wilber 2016 as you pointed out?\nReplacing floating SGD by TernGrad, it converges to 57.28%. The loss because of TernGrad is just 0.04% instead of 0.89%? \n\nIs it easy to implement all of the techniques? Do you plan to open source it? I may want to try this.\nThe core of TernGrad can be done within several lines (https://github.com/wenwei202/terngrad/blob/master/terngrad/inception/bingrad_common.py#L159-L166).\n\nAnd just be curious about how does the warmup stage generalize, does the same warmup scheme work in general for all experiments? I am asking since we may not want to tune the warmup stage for several times when training a DNN, which essentially is wasting training time. TernGrad converges with the same hyper-parameters of standard SGD.\n\nThanks,\n-Wei "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642518419,"tcdate":1511826235241,"number":2,"cdate":1511826235241,"id":"rJmrmQ5lG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Review","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer3"],"readers":["everyone"],"content":{"title":"good empirical results, but requires work to justify the proposed techniques","rating":"6: Marginally above acceptance threshold","review":"This paper proposes additional improvement over gradient dropping(Aji & Heafield) to improve communication efficiency.  \n\n- First of all, the experimental results are thorough and seem to suggest the advantage of the proposed techniques.\n- The result for gradient dropping(Aji & Heafield) should be included in the ImageNet experiment.\n- I am having a hard time understanding the intuition behind v_t introduced in the momentum correction. The authors should provide some form of justifications.\n   - For example, provide an equivalence provide to the original update rule or some error analysis would be great\n   - Did you keep a running sum of v_t overall history? Such sum without damping(the m term in momentum update) is likely lead to the growing dominance of noise and divergence.\n- The momentum masking technique seems to correspond to stop momentum when a gradient is synchronized. A discussion about the relation to asynchronous update is helpful.\n- Do you do non-sparse global synchronization of momentum term? It seems that local update of momentum is likely going to diverge,  and the momentum masking somehow reset that.\n- In the experiment, did you perform local aggregations of gradients between GPU cards before send out to do all0reduce in a network? since doing so will reduce bandwidth requirement.\n\nIn general, this is a paper shows good empirical results. But requires more work to justify the proposed correction techniques.\n\n\n---\n\nI have read the authors updates and changed my score accordingly(see series of discussions)\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642518456,"tcdate":1511474578925,"number":1,"cdate":1511474578925,"id":"rJ9crpElM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Review","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference/Paper833/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A useful contribution","rating":"7: Good paper, accept","review":"I think this is a good work that I am sure will have some influence in the near future. I think it should be accepted and my comments are mostly suggestions for improvement or requests for additional information that would be interesting to have.\n\nGenerally, my feeling is that this work is a little bit too dense, and would like to encourage the authors in this case to make use of the non-strict ICLR page limit, or move some details to appendix and focus more on more thorough explanations. With increased clarity, I think my rating (7) would be higher.\n\nSeveral Figures and Tables are never referenced in the text, making it a little harder to properly follow text. Pointing to them from appropriate places would improve clarity I think.\n\nAlgorithm 1 line 14: You never seem to explain what is sparse(G). Sec 3.1: What is it exactly that gets communicated? How do you later calculate the Compression Ratio? This should surely be explained somewhere.\n\nSec 3.2 you mention 1% loss of accuracy. A pointer here would be good, at that point it is not clear if it is in your work later, or in another paper. The efficient momentum correction is great!\n\nAs I was reading the paper, I got to the experiments and realized I still don't understand what is it that you refer to as \"deep gradient compression\". Pointer to Table 1 at the end of Sec 3 would probably be ideal along with some summary comments.\n\nI feel the presentation of experimental results is somewhat disorganized. It is not clear what is immediately clear what is the baseline, that should be somewhere stressed. I find it really confusing why you sometimes compare against Gradient Dropping, sometimes against TernGrad, sometimes against neither, sometimes include Gradient Sparsification with momentum correction (not clear again what is the difference from DGC). I recommend reorganizing this and make it more consistent for sake of clarity. Perhaps show here only some highlights, and point to more in the Appendix.\n\nSec 5: Here I feel would be good to comment on several other things not mentioned earlier. \nWhy do you only work with 99.9% sparsity? Does 99% with 64 training nodes lead to almost dense total updates, making it inefficient in your communication model? If yes, does that suggest a scaling limit in terms of number of training nodes? If not, how important is the 99.9% sparsity if you care about communication cost dominating the total runtime? I would really like to better understand how does this change and what is the point beyond which more sparsity is not practically useful. Put differently, is DGC with 600x size reduction in total runtime any better than DGC with 60x reduction?\n\n\nFinally, a side remark:\nUnder eq. (2) you point to something that I think could be more discussed. When you say what you do has the effect of increasing stepsize, why don't you just increase the stepsize? \nThere has recently been this works on training ImageNet in 1 hour, then in 24 minutes, latest in 15 minutes... You cite the former, but highlight different part of their work. Broader idea is that this is trend that potentially makes this kind of work less relevant. While I don't think that makes your work bad or misplaced, I think mentioning this would be useful as an alternative approach to the problems you mention in the introduction and use to motivate your contribution.\n...what would be your reason for using DGC as opposed to just increasing the batch size?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511901508167,"tcdate":1510990581749,"number":3,"cdate":1510990581749,"id":"H1Rx7PTJG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"B1wc_Z5JG","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Hello from your baseline","comment":"Dear Kenneth Heafield,\n\n     Thank you for clarifying the Gradient Dropping, it's very helpful. We will describe the Gradient Dropping in a more rigorous way in the final version.\n     We also appreciate your reminding us of citing these two excellent papers.\n     Here are the answers to your questions.\n\n     - Warm-up training works in general, so was it included in your baseline experiments as well? \n\n       Warm-up training was previously used for improving the large minibatch training proposed by Goyal et. al. They warm up the learning rate linearly in the first several epochs. However, we are the first to warm up the sparsity during the gradient pruning. Therefore, only experiments with DGC adopted warm-up sparsity. It is a simple but effective technique.\n\n      - \"Implementing DGC requires gradient sorting.\"  To be pedantic, it requires top-k selection which we have been talking to NVIDIA about implementing more efficiently in the context of beam search.  I like the hierarchical add-on to the sampling we've been doing too; if too few gradients pass the threshold, do you sample more?  \n\n      We indeed use top-k selection instead of sorting. We do not sample more if too few gradients are selected. Since hierarchical selection is designed to control the communication data size, we will perform top-k selection twice only when too many gradients pass the threshold."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510770831328,"tcdate":1510770831328,"number":3,"cdate":1510770831328,"id":"B1wc_Z5JG","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~Kenneth_Heafield1"],"readers":["everyone"],"writers":["~Kenneth_Heafield1"],"content":{"title":"Hello from your baseline","comment":"I'm Kenneth Heafield, one of the authors cited.   \n\nIt's an interesting bag of 4 tricks here and I will likely use them going forward.  \n\n\"Gradient Dropping requires adding a layer normalization.\" Figure 5 in our paper shows that gradient dropping works, admittedly slower, without layer normalization if we determine the threshold locally to each parameter/matrix rather than globally.  \n\nI feel like you're giving us too much credit.  Strom https://s3-us-west-2.amazonaws.com/amazon.jobs-public-documents/strom_interspeech2015.pdf and Dryden et al https://ornlcda.github.io/MLHPC2016/papers/3_Dryden.pdf deserve to be cited too.  \n\nWarm-up training works in general, so was it included in your baseline experiments as well?  \n\n\"incurring 0.3% loss of accuracy on a machine translation task\" It would be better to say BLEU score here, rather than a vague metric.  Parsing people fight over 0.3% while translation people shrug over 0.3% BLEU.  \n\n\"Implementing DGC requires gradient sorting.\"  To be pedantic, it requires top-k selection which we have been talking to NVIDIA about implementing more efficiently in the context of beam search.  I like the hierarchical add-on to the sampling we've been doing too; if too few gradients pass the threshold, do you sample more?  \n\nAbstracts should compare to the strongest baseline, not just the stock baseline.  \n\nLet's talk when you're less anonymous.  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510740814691,"tcdate":1510740814691,"number":2,"cdate":1510740814691,"id":"SJPLQcF1M","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"H1cZgIYyM","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"Re: Typos and clarification questions","comment":"We really appreciate your comments.\n\n- Equation 1 & 2: shouldn’t k start from 1 if N is the number of training node?\n       Yes. It's a typo. k should start from 1.\n\n- Related Work section: Graidient typo\n- Section 4.2: csparsity typo\n       Thank you for pointing out these typos. They should be \"Gradient\" and \"sparsity\".\n\n- Line 8, 9 in Algorithm 4 in Appendix B: shouldn’t line 8 be U <- mU + G and line 9 be V_t <- V_{t-1} + mU + G\n       These two lines are equivalent to those in Algorithm 4 in Appendix B. \n\n- Is \"Gradient Size\" referring to the average size of the gradient that's larger than the threshold?\n       Yes. \"Gradient Size\" is referring to the size of the sparse gradient, which contains both the gradient values that are larger than the threshold and 16-bit index distances when it comes to DGC."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510739778308,"tcdate":1510739778308,"number":1,"cdate":1510739778308,"id":"SyqSy9YyM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Official_Comment","forum":"SkhQHMW0W","replyto":"HJOI6ndJz","signatures":["ICLR.cc/2018/Conference/Paper833/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper833/Authors"],"content":{"title":"we have considered these non-ideal effects. ","comment":"we thank the reviewer for the comments.\n\n(1) \nFirst, warm-up training takes only 4 out of 90 epochs for imagenet, 1 out of 70 epochs for librispeech, which is only 1.4%-4% of the total training epochs. Therefore the impact of warmup training is negligible.\n\nSecond, during warm-up training the gradient is also very sparse. On Imagenet, the sparsity for the 4 warm-up epochs are: 75% -> 93.75% -> 98.4375% -> 99.6% (exponentially increase), then 99.9% for the rest 86 epochs. Same warm-up sparsity rule applies to the first four quarter epochs on Librispeech, then 99.9% for the rest 69 epochs. \n\n\n(2) Yes, we already considered the a larger communication volume of summed gradients. \n\nWith N=2^k workers and s sparsity. We need k step to gather these gradients. The density doubles at every step, so the average communication volume is \\sum_{i=0}^{k-1} 2^{i}*M*s/k = (N-1)/log(N)*M*s. The average density increases sub-linearly with the number of nodes by N/log(N), not exponentially. \n\nWe already considered this non-ideal effect in the second paragraph of Section 5: \"the density of sparse data doubles at every aggregation step in the worst case. However, even considering this effect, Deep Gradient Compression still significantly reduces the network communication time, as implied in Figure 6.\" \"For instance, when training AlexNet with 64 nodes, conventional training only achieves nearly 30× speedup with 10Gbps Ethernet (Apache, 2016), while with DGC, more than 40× speedup is achieved even with 1Gbps Ethernet\". With 1Gbps Ethernet, the speedup of TernGrad is 30x, our worse case is 44x (considering this non-ideal effect), our best case is 58x. We reported the worse case, which is 44x speedup (see Figure 6)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510731958467,"tcdate":1510723585550,"number":2,"cdate":1510723585550,"id":"H1cZgIYyM","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["~quan_vuong1"],"readers":["everyone"],"writers":["~quan_vuong1"],"content":{"title":"Typos and clarification questions","comment":"Thank you for a great paper! The author's intuition really shines through. I just have a few clarifying points:\n\n- Equation 1 & 2: shouldn’t k start from 1 if N is the number of training node ?\n- Related Work section: Graidient typo\n- Section 4.2: csparsity typo\n- Line 8, 9 in Algorithm 4 in Appendix B: shouldn’t line 8 be U <- mU + G and line 9 be V_t <- V_{t-1} + mU + G\n- Is \"Gradient Size\" referring to the average size of the gradient that's larger than the threshold ?\n\nedit 1: add question about gradient size."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510686032179,"tcdate":1510686032179,"number":1,"cdate":1510686032179,"id":"HJOI6ndJz","invitation":"ICLR.cc/2018/Conference/-/Paper833/Public_Comment","forum":"SkhQHMW0W","replyto":"SkhQHMW0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Does compression ratio consider the larger communication overhead in warm-up training and the smaller sparsity after summing?","comment":"For compression ratio in Table 3 & 4, does this work consider the larger communication volume during warm-up training?\nPlease clarify how many iterations it took to \"warm-up\" and what was the sparsity of gradients during warm-up. If it did warm up for 20% of total epochs with 50% sparsity, the compression ratio is bounded by 10x.\n\nDid this work consider a larger communication volume of summed gradients? Suppose there are gradients from k workers  to sum up and the sparsity of gradients is s, the expectation of the sparsity of summed gradients is s^n which exponentially decreases with n. Please clarify this.\n\nThanks"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515200228740,"tcdate":1509135651724,"number":833,"cdate":1509739074092,"id":"SkhQHMW0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkhQHMW0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training","abstract":"Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.","pdf":"/pdf/23e8363e1f8badcba5dff25f5b1b347ac44cb4b3.pdf","TL;DR":"we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy. ","paperhash":"anonymous|deep_gradient_compression_reducing_the_communication_bandwidth_for_distributed_training","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkhQHMW0W}\n}","keywords":["distributed training"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper833/Authors"]},"nonreaders":[],"replyCount":33,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}