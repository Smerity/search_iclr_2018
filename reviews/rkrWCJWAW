{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222681031,"tcdate":1511813495168,"number":3,"cdate":1511813495168,"id":"r11tZl5xG","invitation":"ICLR.cc/2018/Conference/-/Paper533/Official_Review","forum":"rkrWCJWAW","replyto":"rkrWCJWAW","signatures":["ICLR.cc/2018/Conference/Paper533/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper, would like to see more experiments","rating":"5: Marginally below acceptance threshold","review":"This is an interesting paper.\n\nIt is well known that TBPTT is biased because of a fixed truncation length. The authors propose to make it  unbiased by sampling different truncation lengths and hence changing  the optimization procedure which corresponds to adding noise in the gradient estimates which leads to  unbiased gradients. \n\nPros:\n\n- Its a well written and easy to follow paper.\n- If I understand correctly, they are changing the optimization procedure so that the proposed approach is able to find a local minima, which was not possible by using truncated backpropagation through time.  \n- Its interesting to see in there PTB results that they get better validation score as compared to truncated BPTT.\n\nCons: \n\n- Though the approach is interesting, the results are quite preliminary. And given the fact there results are worse than the LSTM baseline (1.40 v/s 1.38). The authors note that it might be because of they are applying without sub-sequence shuffling. \n\n- I'm not convinced of the approach yet. The authors could do some large scale experiments on datasets like Text8 or speech modelling. \n\n\nFew points\n\n- If I'm correct that the proposed approach indeed changes the optimization procedure, than I'd like to know what the authors think about exposure bias issue. Its a well known[1, 2] that we can't sample from RNN's for more number of steps, than what we used for trained (difference b/w teacher forcing and free running RNN). I'd like to know how does there method perform in such a regime (where you sample for more number of steps than you have trained for)\n\n- Another thing, I'd like to see is the results of this model as compared to truncated backpropagation when you increase the sequence length. For example, Lets say you are doing language modelling on PTB, how the result varies when you change the length of the input sequence. I'd like to see a graph where on X axis is the length of the input sequence and on the Y axis is the bpc score (for PTB) and how does it compare to truncated backpropagation through time. \n\n-  PTB dataset has still not very long term dependencies, so I'm curious what the authors think about using there method for something like speech modelling or some large scale experiments.\n\n- I'd expect the proposed approach to be more computationally expensive as compared to Truncated Back-propagation through time. I dont think the authors mentioned this somewhere in the paper. How much time does a single update takes as compared to Truncated Back-propagation through time ?\n\n- Does the proposed approach help in flow of gradients?  \n\n- In practice for training RNN's people use gradient clipping which also makes the gradient biased. Can the proposed method be used for training longer sequences?  \n\n[1] Scheduled Sampling For Sequence Prediction with RNN's https://arxiv.org/abs/1506.03099\n[2] Professor Forcing  https://arxiv.org/abs/1610.09038\n\n\nOverall, Its an interesting paper which requires some more analysis to be published in this conference. I'd be very happy to increase my score if the authors can provide me results what I have asked for. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiasing Truncated Backpropagation Through Time","abstract":"\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.\n","pdf":"/pdf/f4012cab3e797bf0f0bf75b6aa46f7721504310a.pdf","TL;DR":"Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly.","paperhash":"anonymous|unbiasing_truncated_backpropagation_through_time","_bibtex":"@article{\n  anonymous2018unbiasing,\n  title={Unbiasing Truncated Backpropagation Through Time},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkrWCJWAW}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper533/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222681071,"tcdate":1511805445754,"number":2,"cdate":1511805445754,"id":"rJRZM0txz","invitation":"ICLR.cc/2018/Conference/-/Paper533/Official_Review","forum":"rkrWCJWAW","replyto":"rkrWCJWAW","signatures":["ICLR.cc/2018/Conference/Paper533/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper introduces a new approximation to backpropagation through time (BPTT) to overcome the computational and memory load that arise when having to learn from long sequences. \nRather than chopping the sequence into subsequences of equal length as in truncated BPTT, the authors suggest to segment the sequence into subsequences of differing lengths according to an a priori specified distribution for the segment length. The gradient estimator is made unbiased through a weighting procedure.\n\nWhilst the proposed method is interesting and relevant, I find the analysis quite superficial and limited.\n\n1) The distribution for the segment length is fully specified a priori. Depending on the problem at hand, different specifications could give rise to very different results. It would be good to suggest an approach for more automatically determine the (parameters of the) distribution.\n\n2) Whilst unbiased, the proposed estimator could have high variance. This point is not investigated in the experimental section.\n\n3) For an experimental paper as this one, it would be good to have many more problems analysed and a deeper analysis than the one given for the language problem.  \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiasing Truncated Backpropagation Through Time","abstract":"\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.\n","pdf":"/pdf/f4012cab3e797bf0f0bf75b6aa46f7721504310a.pdf","TL;DR":"Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly.","paperhash":"anonymous|unbiasing_truncated_backpropagation_through_time","_bibtex":"@article{\n  anonymous2018unbiasing,\n  title={Unbiasing Truncated Backpropagation Through Time},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkrWCJWAW}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper533/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222681110,"tcdate":1511695199335,"number":1,"cdate":1511695199335,"id":"rkvD7QulM","invitation":"ICLR.cc/2018/Conference/-/Paper533/Official_Review","forum":"rkrWCJWAW","replyto":"rkrWCJWAW","signatures":["ICLR.cc/2018/Conference/Paper533/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Sufficient contribution, though little bad written","rating":"6: Marginally above acceptance threshold","review":"This paper proposes stochastic determination methods for truncation points in backpropagation through time. The previous truncation methods naively determine truncation points with fixed intervals, however, these methods cannot ensure the unbiasedness of gradients. The proposed methods stochastically determine truncation points with importance sampling. This framework ensures the unbiasedness of gradients, which contribute to the reliable convergence. Moreover, this paper investigates how the proposed methods work effectively by carefully tuning the sampling probability. This paper shows two experimental results, in which one is a simple synthetic task and the other is a real-data task. These results validate the effectiveness of the proposed methods.\n\nOverall, I think the constitution and the novelty of this paper are above the bar. The proposed methods are simple extensions of the Truncated BPTT to ensure the unbiasedness. In particular, the investigation on the choice of the sampling probability is very helpful to consider how to enhance benefits of the proposed truncated BPTT methods. However, the written quality of this paper is not good at some points. I think the authors should re-check the manuscript and modify mistakes before the publication.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiasing Truncated Backpropagation Through Time","abstract":"\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.\n","pdf":"/pdf/f4012cab3e797bf0f0bf75b6aa46f7721504310a.pdf","TL;DR":"Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly.","paperhash":"anonymous|unbiasing_truncated_backpropagation_through_time","_bibtex":"@article{\n  anonymous2018unbiasing,\n  title={Unbiasing Truncated Backpropagation Through Time},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkrWCJWAW}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper533/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739250649,"tcdate":1509125629390,"number":533,"cdate":1509739247997,"id":"rkrWCJWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkrWCJWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unbiasing Truncated Backpropagation Through Time","abstract":"\\emph{Truncated Backpropagation Through Time} (truncated BPTT, \\cite{jaeger2002tutorial}) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of \\emph{Backpropagation Through Time} (BPTT \\cite{werbos:bptt}) while relieving the need for a complete backtrack through the whole data sequence at every step.  However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce \\emph{Anticipated Reweighted Truncated Backpropagation} (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling \\cite{ptb_proc}, ARTBP slightly outperforms truncated BPTT.\n","pdf":"/pdf/f4012cab3e797bf0f0bf75b6aa46f7721504310a.pdf","TL;DR":"Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly.","paperhash":"anonymous|unbiasing_truncated_backpropagation_through_time","_bibtex":"@article{\n  anonymous2018unbiasing,\n  title={Unbiasing Truncated Backpropagation Through Time},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkrWCJWAW}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper533/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}