{"notes":[{"tddate":null,"ddate":null,"tmdate":1515449876490,"tcdate":1515185354679,"number":9,"cdate":1515185354679,"id":"HkQCEwaXM","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"The New Revision","comment":"We uploaded the revised version of our paper.\n\nAs you can see, over 80% of the paper is major edited to improve clarity while the claim is same as the previous version.\n\nEspecially, we make it clear the motivation and the method.\nPlease read throughout the paper again."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1514703866504,"tcdate":1514703866504,"number":8,"cdate":1514703866504,"id":"SyM-3W87f","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"Bk3zRoBGz","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Helpful comments. Thank you so much","comment":"Thank you for reading and commenting our paper.  \nWe really appreciate your detailed comments. Most of them were very helpful to brush up our paper. \nWe are about to finalize the paper, and will upload a version which highly improved clarity at 5th Jan. \nSo, please look forward it.\n\nEnjoy the holidays & Have a happy new year."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642511096,"tcdate":1513631252182,"number":3,"cdate":1513631252182,"id":"Bk3zRoBGz","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Review","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference/Paper787/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Interesting and promising ideas, but need significant polishing and substantiation ","rating":"5: Marginally below acceptance threshold","review":"This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning). The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem. This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology.\n \nThis work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning. Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks, there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims. These issues may need a significant amount of effort to fix as I will elaborate more below.\n \n1. Introduction\nThere are several important concepts, such as reward distribution, credit assignment, which are used (from the very beginning of the paper) without explanation until the final part of the paper.\n \nThe motivation of the work is not very clear. There seems to be a gap between the first paragraph and the second paragraph. The authors mentioned that “From a micro perspective, the abstraction capability of each unit contribute to the return of the entire system. Therefore, we address the following questions. Will reinforcement learning work even if we consider each unit as an autonomous agent ”\nIs there any citation for the claim “From a micro perspective, the abstraction capability of each unit contribute to the return of the entire system” ?  It seems to me this is a very general claim. Even RL methods with linear function approximations use abstractions.  Also, it is unclear to me why this is an interest question. Does it have anything to do with existing issues in DRL? Moreover, The definition of autonomous agent is not clear, do you mean learning agent or policy execution agent?\n \n“it uses \\epsilon-greedy as a policy, …” Do you mean exploration policy?\nI also have some concerns regarding the claim that “We confirm that optimization with the framework of NaaA leads to better performance of RL”. Since there are only two baselines are compared to the proposed method, this claim seems too general to be true.\n \nIt is not clear to why the authors mention that “negative result that the return decreases if we naively consider units as agents”. What is the big picture behind this claim?\n \n“The counterfactual return is that by which we extend reward …” need to be rewritten.\n \nThe last paragraph of introduction discussed the possible applications of the proposed methods without any substantiation, especially neither citations nor any related experiments of the authors are provided.\n \n2 Related Work\n \n“POSG, a class of reinforcement learning with multiple ..” -> reinforcement learning framework\n \n“Another one is credit assignment. Instead of reward.. ” Two sentences are disconnected and need to be rewritten.\n \n“This paper unifies both issues” sounds very weird. Do you mean “solves/considers both issues in a principled way”?\n \nThe introduction of GAN is very abrupt. Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs.\n \n“,which we propose in a later part of this paper” -> which we propose in this paper\n \n \n3. Background\n \n“a function from the state and the action of an agent to the real value” -> a reward function  \n \nShould provide a citation for DRQN\n \nThere is a big gap between the last two paragraphs of section 3.\n \n4. Neuro as an agent\n \n“We add the following assumption for characteristics of the v_i” -> assumptions for characterizing v_i\n \n“to maximize toward maximizing its own return” -> to maximize its own return\n \nWe construct the framework of NaaA from the assumptions -> from these assumptions\n \n“indicates that the unit gives additional value to the obtained data. …” I am not sure what this sentence means, given that \\rho_ijt is not clearly defined.\n \n5. Optimization\n \n“NaaA assumes that all agents are not cooperative but selfish” Why? Is there any justification for such a claim?\n \n \nWhat is the relation between \\rho_jit and q_it ?\n \n“A buyer which cannot receive the activation approximates x_i with …” It is unclear why a buyer need to do so given that it cannot receive the activation anyway.\n \n“Q_it maximizing the equation is designated as the optimal price.” Which equation?\n \ne_j and 0 are not defined in equation 8\n \n \n6 Experiment\nsetare -> set are\n \nwhat is the std for CartPole in table 1\n \nIt is hard to judge the significance of the results on the left side of figure 2. It might be better to add errorbars to those curves\n \nMore description should be provided to explain the reward visualization on the right side of figure 2. What reward? External/internal?\n \n“Specifically, it is applicable to various methods as described below …” Related papers should be cited.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1512288231787,"tcdate":1512288231787,"number":7,"cdate":1512288231787,"id":"BJgxl4WbM","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"H12VRW9gM","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"All right. We will improve the mathematical formulation","comment":"Thank you for reading and commenting our paper.  We will polish our mathematical formulation to improve your understanding during this period.\n\n> Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later. \n\nAn external environment is the original environment such as Doom and Atari, and an internal environment is a set of units. From an agent's perspective, other units are considered as an environment.\n\nThe quick reference can also be helpful.\n\n                   Environment for a unit         State for a unit                               Observation for a unit                   \n                   -----------------------------------   -------------------------------------------   ---------------------------------------\nExternal    original environment            original state                                   original observation            \nInternal     other units                              activation of all the other units   activation of allocated units\nBoth           -                                                -                                                         be used to predict o_{ijt}\n\n                   Reward per unit                     Total reward over units\n                   -----------------------------------   -------------------------------------------------------------\nExternal    original reward                       total original reward (designer's objective)\nInternal     revenue from units - cost     0\nBoth           units' objective                       total original reward (designer's objective)\n\n\n> “environment that the multi-agent system itself touches”\n\nTypically, there is boundary between an agent and an environment (e.g., a robot in a room). We wrote this situation that the agent with a NN (as the multi-agent system) touches the environment. \n\n> In section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). \n> Many equations were unclear to me for similar reasons to the point \n> Without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.\n\nAs we will reflect the comments to our paper, please wait for it."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1512278614586,"tcdate":1512278614586,"number":6,"cdate":1512278614586,"id":"Bykv9-bWG","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"HJSqWxjez","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"A month is enough to meet your requirements","comment":"Thank you for reading and commenting our paper.  Let us answer the question first:\n\n> The model itself is also never really motivated. Why is this an interesting problem? \n\nDo you know “Blockchain”, the technology supporting most of virtual currencies such as Bitcoin and Ethereum, which has vast market cap of $100 billions? Also, there was a news that the price of one Bitcoin exceeded $10,000 last week though the price was $1,000 at the beginning of this year. The emerging technology enable us to send incentive among agents in a decentralized environment. If an agent can earn money with realistic way such as automatic financial trading for stock, debt and coins, the way agent takes will be reinforcement learning, and it would face a problem of POMDP, because the market is inefficient in which an agent who has  informative data can gain advantage. Hence, the agent will buy the informative data from other agent by paying incentive over the multi-agent setting, and the incentive will be distributed in the Blockchain environment.\n\nSuch background raises the question in a face of the paper: \n     “will reinforcement learning work even if we consider each unit as an autonomous agent?”\nwhich motivates our framework. To answer the question, there are several issues to address such as “how much is appropriate reward the agent should pay?” and “how to address the social dilemma?”. All the answers are written in the paper.\n\nIf the major problem is clarity as you mentioned, a month will be enough to solve it. \n\n\nResponse to Paragraph 3:\n\n> GANs\n\nAlthough our paper had nothing to do with GANs directly, we mentioned GAN as a game-theoretic approach to model the real environment.\n\n> POMDPs, never to be used again in the entirety of the paper \n\nNotation of POMDP is used in methods such as S_O (below Eq (3)) and \\gamma (in Eq (4)). \nBesides, Eq (1) in the section of PODMP is used to derive Eq (9).\n\n\nResponse to Paragraph 4:\n\n> It is never explained how this reward is allocated even in the authors’ own experiments. \n\nIn the classification and the single-agent setting, the reward is given only to the endpoint of agent. \n\nIn the multi-agent setting,  the external reward (reward from the Doom environment) is given to the agents (a main player and a cameraman) with following ways.\n    1. Baseline: endpoint of the main player.\n    2. Comm: endpoint of the main player and the cameraman (the same configuration to the original paper of CommNet) \n    3. NaaA: endpoint of the main player. The reward is pour from the main player to the cameraman as an internal reward.\n\n> The authors state that all units playing NOOP is an equilibrium. While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology. None of this is discussed. \n\nAlthough the few agents which can gain the external reward can survive, most of the agents whose R_{it}^{ex} equals to 0 becomes NOOP regardless of its network topology because \\sigma_{jit} will equal 0 at the convergence. As we will post the revised version which contains the proof, please wait for it. \n\n> The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary. This is pervasive throughout the paper, and is detrimental to the reader’s understanding.  \n\nThe objective function is return (discounted cumulative reward). That is,\n                  Σ_{t=0}^T [ γ^t R_t^{ex} ],\nwhere R_{ex,t} := Σ_i R_{it}^{ex} is overall reward from the external environment. Pareto optimal is defined for the objective functions of all the agents.\n\n(continues...)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1512278426562,"tcdate":1512278426562,"number":5,"cdate":1512278426562,"id":"H1mjFWW-G","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"HJSqWxjez","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"(cont'd) Response","comment":"Response to Paragraph 5:\n\n> There are many ways to create rational incentives for neurons in a neural net. \n\nAs I don’t think there are many methods for our problem setting, please provide a link.\n \n> The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction.\n\nAuction is more than auction as it used in mechanism design to orchestrate the actions of agents with mechanism. So, think out of the box, and throw away the typical image of auction.\n\n> Is there an economic motivation? Is it just a different way to train a NN? \n\nYes, there is an economic motivation as well as to improve training a NN.\n\n\nResponse to Paragraph 6 (the detailed comments):\n\n> “passing its activation to the unit as cost” => Unclear. What does this mean?\n\n\"to observe their activation\" is correct.\n(As it was a mistake in the native check process, we will change the native checker later)\n\n> “performance decreases if we naively consider units as agents” => Performance on what?\n\nPerformance on the total cumulative external reward.\n\n> “Subsequently, we present that learning counterfactual return leads the model to learning optimal topology” => Do you mean  “maximizing” instead of learning.  Optimal with respect to what task?\n\nJust like the above answer, it will be optimal with respect to the total cumulative external reward."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642511134,"tcdate":1511879053397,"number":2,"cdate":1511879053397,"id":"HJSqWxjez","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Review","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference/Paper787/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Requires major editing to be fit for publication. ","rating":"3: Clear rejection","review":"The authors consider a Neural Network where the neurons are treated as rational agents. In this model, the neurons must pay to observe the activation of neurons upstream. Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task).  \n\nWhile this is an interesting idea on its surface, the paper suffers from many problems in clarity, motivation, and technical presentation. It would require very major editing to be fit for publication. \n\nThe major problem with this paper is its clarity. See detailed comments below for problems just in the introduction. More generally, the paper is riddled with non sequiturs. The related work section mentions Generative Adversarial Nets. As far as I can tell, this paper has nothing to do with GANs. The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells. \n\nThere is also a general lack of attention to detail. For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task. This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}). It is never explained how this reward is allocated even in the authors’ own experiments. The authors state that all units playing NOOP is an equilibrium. While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology. None of this is discussed. The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary. This is pervasive throughout the paper, and is detrimental to the reader’s understanding.  \n\nWhile this might be lost because of the clarity problems described above, the model itself is also never really motivated. Why is this an interesting problem? There are many ways to create rational incentives for neurons in a neural net. Why is paying to observe activations the one chosen here? The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction. Is there an economic motivation? Is it just a different way to train a NN? \n\nDetailed Comments:\n“In the of NaaA” => remove “of”?\n“passing its activation to the unit as cost” => Unclear. What does this mean?\n“performance decreases if we naively consider units as agents” => Performance on what?\n“.. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium“ => Perhaps, this should be rewritten as “Agents maximize their counterfactual return in equilibrium. \n“Subsequently, we present that learning counterfactual return leads the model to learning optimal topology” => Do you mean  “maximizing” instead of learning. Optimal with respect to what task?\n“pure-randomly” => “randomly”\n “with adaptive algorithm” => “with an adaptive algorithm”\n“the connection” => “connections”\n“In game theory, the outcome maximizing overall reward is named Pareto optimality.” => This is simply incorrect. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642511174,"tcdate":1511820853935,"number":1,"cdate":1511820853935,"id":"H12VRW9gM","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Review","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference/Paper787/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good work. Presentation needs further polishing.","rating":"7: Good paper, accept","review":"In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward. The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks. For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline.\n\nThe idea proposed in the paper is quite interesting, but the presentation is severely lacking. In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works. For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). Many equations were unclear to me for similar reasons to the point I decided to only skim those parts. Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later. Like, what does it mean when we say, “environment that the multi-agent system itself touches”?\n\nOverall, I think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1511084185266,"tcdate":1511083528139,"number":3,"cdate":1511083528139,"id":"BJxz0TA1f","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"BkTpRSRkf","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Demo and sample code","comment":"Thank you for being interested in.\n\n> Demo\n\nYou can see our demo for the multi-agent Doom in the following URL.\nhttps://youtu.be/paT2n40QHOA\n\n> Implementation of Adaptive Dropconnect\n\nImplementation is easy because you can use it by just replace a layer.\n\nHere is a sample vanilla code w/o Adaptive DropConnect in pytorch:\n\n 1  class Net(nn.Module):                                                           \n 2     def __init__(self):                                           \n 3         super(Net, self).__init__()                                             \n 4         self.conv1 = nn.Conv2d(3, 10, kernel_size=5)                            \n 5         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)                           \n 6         self.fc1 = nn.Linear(500,100)                                 \n 7         self.fc2 = nn.Linear(100, 10)                                           \n 8                                                                                 \n 9     def forward(self, x):                                                       \n 10         x = F.relu(F.max_pool2d(self.conv1(x), 2))                              \n 11         x = F.relu(F.max_pool2d(self.conv2(x), 2))                              \n 12         x = x.view(-1, 500)                                                     \n 13         x = F.relu(self.fc1(x, training=self.training))                         \n 14         x = F.dropout(x, training=self.training)                                \n 15         x = self.fc2(x)                                                                                                                                                                                                                   \n 16         return F.log_softmax(x)   \n\nYou can turn on Adaptive DropConnnect by just replace a line with\n  6         self.fc1 = nn.Linear(500,100)                                 \n                                  vvv\n  6         self.fc1 = TradeLinear(500,100,eps=0.2)                                 \nTradeLinear is contained in our provided library, which supports Adaptive DropConnnect and NaaA."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1511075637330,"tcdate":1511074917224,"number":2,"cdate":1511074917224,"id":"BJTvnoCJG","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"SJPHs661G","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Re: External/internal","comment":"Good questions.\n\n> External/internal\n\nIn reinforcement learning (RL), there are two parts: an environment and an agent.\nIn \"deep\" RL, there is a neural network inside the agent as a value/policy function approximator.\nThe network contains bunch of units,\nand NaaA considers the network as a multi-agent system, and each unit as an agent.\nFrom perspective of the unit, the other units are considered as an environment.\nTo distinguish from the original environment, we call it an internal environment,\nand call the original environment an external environment.\n\nHere is a quick reference which can also be helpful.\n\n                   Environment for a unit         State for a unit                               Observation for a unit                   \n                   -----------------------------------   -------------------------------------------   ---------------------------------------\nExternal    original environment            original state                                   original observation            \nInternal     other units                              activation of all the other units   activation of allocated units\nBoth           -                                                -                                                         be used to predict o_{ijt}\n\n                   Reward per unit                     Total reward over units\n                   -----------------------------------   -------------------------------------------------------------\nExternal    original reward                       total original reward (designer's objective)\nInternal     revenue from units - cost     0\nBoth           units' objective                       total original reward (designer's objective)\n\n\n> Why not use simple neural network\n\nSuppose AIs had ego. That is, they maximize not total reward but their own reward in a multi-agent system.\nAlthough recent works such as CommNet supposed cooperate setting, say, all the agents have obtain total reward R,\nif the agents were selfish, there would be no incentive to cooperate, and hence they would not communicate each other.\nThe problem is known as social dilemma (e.g., prisoner's dilemma), and leads the overall reward.\nNaaA enables us to design such a multi-agent setting.\nAlso, NaaA can be used multi-agent setting in which the agents are made by other people."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1511050949046,"tcdate":1511050949046,"number":3,"cdate":1511050949046,"id":"BkTpRSRkf","invitation":"ICLR.cc/2018/Conference/-/Paper787/Public_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["~Luis_Garcia1"],"readers":["everyone"],"writers":["~Luis_Garcia1"],"content":{"title":"Cool work!","comment":"The method is general and that makes it widely applicable for many problems.\nI hope that the design concept will be a new basis of studies such as GAN.\n \nI found very challenging trying to find alternative AI patterns or routines\nbased on the cooperation of two AIs. I would like to see this happening more often in videogames.\n Are there any demo videos?\n \nHowever, your idea, Adaptive Dropconnect seems to be complicated to Implement.\nHow can we implement it?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1511017279511,"tcdate":1511017279511,"number":2,"cdate":1511017279511,"id":"SJPHs661G","invitation":"ICLR.cc/2018/Conference/-/Paper787/Public_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"External/internal","comment":"Why do you just divide the environment into two types: external/internal?\nThere is also another way to simply use neural network."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1510403363870,"tcdate":1510403327633,"number":1,"cdate":1510403327633,"id":"rkdWpPEJz","invitation":"ICLR.cc/2018/Conference/-/Paper787/Official_Comment","forum":"BkfEzz-0-","replyto":"ByRdDLV1z","signatures":["ICLR.cc/2018/Conference/Paper787/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper787/Authors"],"content":{"title":"Thank you for being interested in our work","comment":"\n1) The objective function is return (discounted cumulative reward). That is,\n                  Σ_{t=0}^T [ γ^t R_t^{ex} ],\n    where R_{ex,t} := Σ_i R_{it}^{ex} is overall reward from the external environment.\n\n   > POMDP/MDP\n   The actual problem we want to solve is POMDP.\n   However, we extended it to POSG, multi-agent problem, because we consider bunch of neurons as agents.\n   That's why we distinguished it external/internal environment in the paper.\n\n2) Yes, we used DQN-like architecture (Q-learning with neural net and experience replay) to predict counterfactual return of j for i  at t o_{ijt}. The detail is as below.\n     - The input is a state s_{it}, a coupled vector made of an external state, input vector, and parameter (weight and bias).\n     - The output is Q-value Q(s_{it}, g_{ijt}), where g_{ijt} \\in {0, 1} is allocation. Hence, there are 2 |N_i^{in}| Q-values per unit, where |N_i^{in}| is number of j's (indices of connected units from a unit v_i).\n     - o_{ijt} is calculated from a pair of scalars from the output: Q(s_{it}, 1) - Q(s_{it}, 0).\n     - The model made of one layer, but also deeper architectures can be introduced.\n\n3) ViZDoom partially supports multi-agent environment, but it does not supports communication among the agents.\n    So, we extended it with writing the original code which supports communication.\n\n> def of (s_it^ex, \\tilde{x}, \\theta_i)\n\nThe coupled vectors are designed as a state to predict Q-values for o_{ijt}. \nHere is the definition of the each notation.\n- s_it^ex: external state\n- \\tilde{x}: the predicted input vector from limited information. \\tilde{x} := x * g + \\bar{x} * (1-g). \n   \\bar{x} is mean value of x.\n- \\theta_i: the parameter of v_i. For example, weight and bias for linear unit.\nPlease also see our answer (2) in this post."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1510397814415,"tcdate":1510397814415,"number":1,"cdate":1510397814415,"id":"ByRdDLV1z","invitation":"ICLR.cc/2018/Conference/-/Paper787/Public_Comment","forum":"BkfEzz-0-","replyto":"BkfEzz-0-","signatures":["~Xin_Yang1"],"readers":["everyone"],"writers":["~Xin_Yang1"],"content":{"title":"Interesting paper. I have three questions","comment":"Very interesting paper. It shows a novel framework to consider all the units as agents.\nEven though the problem setting is challenging, the paper solved it by converting it into a scheme of counterfactual return maximization using an elegant trick from auction theory.\n\nNonetheless, I have several questions about the paper.\n1. What is the objective function?  While the author states the problem is POSG, I guess the problem is POMDP/MDP since the paper introduced a Doom-based environment as the experiment. I'm not sure to what the algorithm want to maximize actually.\n2. I'm unsure how to predict o_it actually. Though it seems to use Q-learn according to the paper, I want you to provide detail information of the architecture.\n3. As I guess ViZDoom is a single-agent platform, how do you realize the multi-agent setting?  I mean, are there some special implementations?\n\nThere are minor comments which may improve your paper:\n - Definition of R and \\pi is missing. I supposed they are a reward function and a policy. \n - Provide definition of (s_it^ex, \\tilde x, \\theta_i) in line 12, algo 1."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]}},{"tddate":null,"ddate":null,"tmdate":1515469077298,"tcdate":1509134889628,"number":787,"cdate":1509739100430,"id":"BkfEzz-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkfEzz-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neuron as an Agent","abstract":"Existing multi-agent reinforcement learning (MARL) communication methods have relied on a trusted third party (TTP) to distribute reward to agents, leaving them inapplicable in peer-to-peer environments. This paper proposes reward distribution using {\\em Neuron as an Agent} (NaaA) in MARL without a TTP with two key ideas: (i) inter-agent reward distribution and (ii) auction theory. Auction theory is introduced because inter-agent reward distribution is insufficient for optimization. Agents in NaaA maximize their profits (the difference between reward and cost) and, as a theoretical result, the auction mechanism is shown to have agents autonomously evaluate counterfactual returns as the values of other agents. NaaA enables representation trades in peer-to-peer environments, ultimately regarding unit in neural networks as agents. Finally, numerical experiments (a single-agent environment from OpenAI Gym and a multi-agent environment from ViZDoom) confirm that NaaA framework optimization leads to better performance in reinforcement learning.","pdf":"/pdf/888d31698082eb121137da7477bc6a38f567fea0.pdf","TL;DR":"Neuron as an Agent (NaaA) enable us to train multi-agent communication without a trusted third party.","paperhash":"anonymous|neuron_as_an_agent","_bibtex":"@article{\n  anonymous2018neuron,\n  title={Neuron as an Agent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkfEzz-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper787/Authors"],"keywords":["Multi-agent Reinforcement Learning","Communication","Reward Distribution","Trusted Third Party","Auction Theory"]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}