{"notes":[{"tddate":null,"ddate":null,"tmdate":1514672853555,"tcdate":1514672853555,"number":7,"cdate":1514672853555,"id":"rkR0McB7M","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Revision posted","comment":"We thank the reviewers and other discussants for their comments. In order to address points discussed in OpenReview reviews, comments, and our responses, we have updated our paper. The key changes are as follows:\n\n1. We’ve added content to the Related Work section. This content elaborates on the relationship to prior work (e.g., non-dilated gated ConvNets, convolutional models for sequence to sequence prediction, etc.), in accordance with our responses to OpenReview reviews and comments. As highlighted in the revision, the TCN model we focus on avoids much of the specialized machinery present in prior work and is evaluated on an extremely diverse set of tasks rather than a specific domain or application.\n\n2. We have added experiments on the LAMBADA dataset, as suggested by Reviewer 3, which in fact show very strong performance for the TCN models.  LAMBADA is an especially challenging task where each data sample consists of a long context segment (4.6 sentences on average) and a target sentence, the last word of which needs to be predicted. In this setting, a human can perfectly predict the last word when given the context, but most of the existing models (e.g., LSTM, vanilla RNN) fail to do so. As shown in Table 1 of Section 4 in the revision, without much tuning (due to limited rebuttal time), TCN can achieve a perplexity of < 1300 on LAMBADA, substantially outperforming LSTMs (~4000 ppl) and vanilla RNNs (~15000 ppl), as listed in prior works. This is a strong result that suggests that TCNs are able to recall from a much larger context than recurrent networks, and thus may be more suitable for tasks where long dependencies are required.\n\n3. The appendix now includes a new section that compares the baseline TCN to a TCN that uses a gating mechanism.  This mainly serves as a comparison point to the Dauphin et al. paper, which one reviewer pointed out was not sufficiently addressed in our original draft.  Our experiments show that a gating mechanism can indeed be useful on certain language modeling tasks, but such benefits may not generalize well to other tasks (e.g., polyphonic music and other benchmark tasks).  Thus, while we do absolutely agree with the relevance of the Dauphin et al. paper, and stress this more in the update, we also feel that much the same considerations apply here as to e.g., the WaveNet paper, where the focus of the previous work was really on a single domain, whereas our paper stresses the generality of convolutional sequence models.\n\n4. The revision includes the latest results on certain large experiments (e.g., Wikitext-103).  Specifically, as mentioned in our responses, the TCN achieves a perplexity of 45.2 on this dataset (the only change from our original result was simple optimizing the model for longer), compared to an LSTM that achieves 48.4 perplexity.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513468961017,"tcdate":1513468961017,"number":6,"cdate":1513468961017,"id":"SyYmNNQzf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"BJ7tE5OCb","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Response to Comment","comment":"Thanks for your note.  We will certainly update the paper to include this arXiv report.  However, we also believe that the precise conclusions of this report are somewhat orthogonal as it applies an architecture virtually identical to WaveNet to one particular time series prediction task; thus, from an architectural standpoint, we think that the WaveNet paper is the more relevant prior work, which of course we do cite and discuss.  In contrast, the goal of our current work is to highlight a simpler architecture and empirically study it across a wide range of sequence modeling tasks.  But as mentioned, we're happy to include the reference and explain this connection.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513468929103,"tcdate":1513468929103,"number":5,"cdate":1513468929103,"id":"S1FZN4mzf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"SkdHpQDez","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Thank you very much for the review, we agree with virtually all your points.   As per your suggestion, we are currently integrating experiments on the LAMBADA dataset into the paper, and will post a revision with these results shortly.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513468888442,"tcdate":1513468888442,"number":4,"cdate":1513468888442,"id":"SyxkEEQfM","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"HkUwN_Ylf","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thank you very much for this review.  We agree on most points, except in the ultimate conclusions and assessment of the current \"default\" mindset of temporal modeling in RNNs.\n\nFirst, we agree that speech data in particular (or perhaps audio data more broadly), is indeed one instance where CNNs do appear to have a historical edge over recurrent models, and we can emphasize this in the background section.  Indeed, as you mention, the success of WaveNet has certainly made clear the power of CNNs in this application domain.\n\nThe question, then, is to what extent the community already feels that the success of WaveNet in the speech setting is sufficient to \"standardize\" the use of CNNs across all sequence prediction tasks.  And our genuine impression here is that these ideas have yet to permeate the mindset of the community for generic sequence prediction.  Numerous resources (e.g., Goodfellow et al.'s deep learning book, with its chapter \"Sequence Modeling: Recurrent and Recursive Nets\", plus virtually all current papers on recurrent networks), still highlight LSTMs and other similar architectures as the \"standard\" for sequence modeling.  The precise goal of our work is to highlight the fact that WaveNet-like architectures (though substantially simplified too, as we describe below) can indeed work well across the many other settings we consider.  And we feel that this is an important point to make empirically, even if the results or conclusion may seem \"unsurprising\" to people who are very familiar with CNN architectures.\n\nThe second point, also, is that the architecture we consider is indeed simpler than WaveNet in many respects: e.g. no gated activation but just ReLUs (which, as we highlighted in our response to a previous reviewer, we will include more experimentation on in a forthcoming update), no context stacks, etc; and residual units and dilation structure that more directly mirror the corresponding \"standard\" architectures in convolutional image networks.  Thus, a practitioner wishing to apply WaveNet-style architectures to some new sequence prediction task may be unclear about which elements of the architecture are really necessary, and we attempt to distill this as much as possible in our current paper.\n\nOverall, therefore, we agree that the significance of our current work is largely making the empirical point that TCN architectures are not just for audio, but really for any sequence modeling problem.  But we do feel that this is an important point to make and thoroughly substantiate, even given the success of WaveNet.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513468839972,"tcdate":1513468839972,"number":3,"cdate":1513468839972,"id":"rkehXEQff","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"HkTNLM5gM","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thanks for your note, though we honestly found it a bit surprising.  The entire point of our paper _is_ to evaluate the improved TCN performance over a large and diverse set of experiments, and on this point it is by far the single _most diverse_ study of CNN vs. RNN performance that we are aware of.  And while many of the particular benchmarks are indeed \"small-sized\" in and of themselves, they are standard benchmarks for evaluating the performance of recurrent networks (see appendix A for some references to papers that used these benchmark tests); and we include experiments on domains such as Wikitext-103, which is certainly not a small dataset.\n\nRegarding arXiv:1703.04691, see our comments in the response to the discussant who originally brought this up.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513468750653,"tcdate":1513468750653,"number":2,"cdate":1513468750653,"id":"HkDIQN7ff","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"Bk2tcOm-f","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Response to Comment","comment":"Thanks for the note.  We believe this note is addressing the same points as the note above (with a few additional follow-on points), so we refer to our comment above."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513468709960,"tcdate":1513468709960,"number":1,"cdate":1513468709960,"id":"ByC7mNQGf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Comment","forum":"rk8wKk-R-","replyto":"B1lh3uXbf","signatures":["ICLR.cc/2018/Conference/Paper501/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper501/Authors"],"content":{"title":"Response to Comment","comment":"Thanks very much for your note.  We absolutely agree with your general comments about the related work. We respond to two different points here, because in our mind there are two different categories in the papers you mention.\n\nFirst, the Kalchbrenner et al., and Gehring et al., papers both relate to convolutional sequence to sequence models. While we absolutely agree that this work is related to our topic, we made the explicit choice not to consider seq2seq models in this paper.  The rationale for us is that these models differ in substantial ways from \"pure\" temporal convolutional models.  Since the input to the model is the entire input sentence (captured by non-causal convolutions), and only the autoregressive output network needs to follow causal generation, the task itself is quite different from pure temporal sequence modeling, even if it may be an extension.  Specifically, the two-stage encoder/decoder architecture (first to encode the entire input sentence, then to autoregressively generate the translation) of typical seq2seq models seems so fundamental to these approaches that we felt it was substantially more specialized than the generic temporal modeling problem.\n\nHowever, we also of course concede that the work is related, especially given the machine translation community's departure from pure recurrent networks to convolutional (or even pure attention-based) models.  Thus we will edit the paper to cite these works and address these points (we'll be posting a revised version within a week or so).\n\nSecond, there is the work of Dauphin et al., which more directly relates to a language modeling task.  And while we _do_ cite this work, we believe your point combined with the point in the comment below is more that we don't devote sufficient attention to this previous work.  We agree that the relationship is not clarified enough in the paper and are currently revising to fix this, but let us briefly mention here the connections and how we see this relationship.\n\nFirst, we should mention that while we did include the 48.9 PPL figure on one GPU, running the TCN model for more epochs (still on one GPU) actually achieves a PPL of 45.2, which isn't far off from Dauphin’s 44.9. (Note that we use a network approximately half the size of Dauphin et al.’s, and little tuning.) We'll naturally update the paper on this point.  Second, the main technical contribution of the paper of Dauphin et al. is the combination of (non-dilated) convolutional networks with a gating mechanism.  We experimented quite extensively with this gating mechanism combined with our generic TCN architecture, but didn’t see significant overall performance improvements due to the gating mechanism.  We can include these results in an appendix.  Indeed, a main characteristic of our work is simply the claim that the generic TCN architecture (which is quite simple in nature, as we highlight) is _sufficient_ to achieve most of the benefits proposed by more complex convolutional architectures, without the need for attention, gating mechanisms, and other architectural elaborations.  We believe that the comparison to the Dauphin et al. work actually supports this conclusion, and we will update the paper accordingly (we will post a follow-up note here once the paper has been updated).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512438952355,"tcdate":1512438952355,"number":3,"cdate":1512438952355,"id":"B1lh3uXbf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Public_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"needs to acknowledge existence of very similar work and the better result presented by other papers for language modeling","comment":"(1) It is already known that convolutional architectures perform well on sequence modeling tasks (for example, Oord et al 2016, Wavenet). This paper does not discuss many related works on convolutional sequence modeling for text that should be addressed specifically in the related work section, for example Kalchbrenner et al 2016, Dauphin et al 2017, and Gehring et al, 2017. \n\n(2) This paper tests on Wikitext-103 but it does not cite that https://arxiv.org/pdf/1612.08083.pdf already published better results on Wikitext-103 with a very similar convolutional model (44.9 PPL on 1 GPU/37.2 PPL on 4 GPU, compared to the reported 48.9 PPL here, a significant difference). \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512438403564,"tcdate":1512438403564,"number":2,"cdate":1512438403564,"id":"Bk2tcOm-f","invitation":"ICLR.cc/2018/Conference/-/Paper501/Public_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Does not acknowledge recent work on CNNs for text generation","comment":"This paper does not acknowledge most very prominent recent work on CNNs for text generation, e.g.,\n\nKalchbrenner et al. \"Neural Machine Translation in Linear Time\". 2016.\nGehring et al. \"Convolutional Sequence to Sequence Moedeling\". 2017.\n\nThose papers make precisely the same points and have much stronger empirical results. The authors cite Dauphin et al.  (2016) at the very end of the paper but do not acknowledge that many of the points made are already covered by recent other work."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642457627,"tcdate":1511822901224,"number":3,"cdate":1511822901224,"id":"HkTNLM5gM","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Review","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Convolutional networks are good for solving sequence tasks","rating":"4: Ok but not good enough - rejection","review":"This paper argues that convolutional networks should be the default\napproach for sequence modeling.\n\nThe paper is nicely done and rather easy to understand. Nevertheless, I find\nit difficult to assess its significance. In order to support the original hypothesis,\nI think that a much larger and more diverse set of experiments should have\nbeen considered. As pointed out by another reviewer please add  https://arxiv.org/abs/1703.04691\nto your references.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642457665,"tcdate":1511781470002,"number":2,"cdate":1511781470002,"id":"HkUwN_Ylf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Review","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice results but not much novelty and I don't think that the views are as contrarian as the paper claims.","rating":"5: Marginally below acceptance threshold","review":"The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The paper describes an architecture similar to wavenet with residual connections. Empirical results are presented on a large number of tasks where the convolutional network often outperforms modern recurrent baselines or reaches similar performance.\n\nThe biggest strength of the paper is the large number of tasks on which the models are evaluated. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. That said, I don’t think that all the tasks are very relevant for comparing convolutional and recurrent architectures. While the time windows that RNNs can deal with are infinite in principle, it is common knowledge that the effective length of the dependencies RNNs can model is quite limited in practise. Many of the artificial task like the adding problem and sequential MNIST have been designed to highlight this weakness of RNNs. I don’t find it very surprising that these tasks are easy to solve with a feedforward architecture with a large enough context window. The more impressive results are in my opinion those on the language modelling tasks where one would indeed expect RNNs to be more suitable for capturing dependencies that require stack-like memory functionality. \n\nWhile the related work is quite comprehensive, it downplays the popularity of convolutional architectures throughout history a bit. Especially in speech recognition, RNNs have only recently started to gain popularity while deep feedforward networks applied to overlapping time windows (i.e., 1D convolutions) have been the state-of-the-art for years. Of course the recent successes of dilated convolutions are likely to change the landscape in this application domain yet again.\n\nThe paper is well-structured and written. If anything, it is perhaps a little bit wordy at times but I prefer that over obscurity due to brevity.\n\nThe ideas in the paper are not novel and neither do the authors claim that they are. Unfortunately, I also think that the impact of the work is also somewhat limited due to the enormous success of the wavenet architecture. I do think that the results on the real-world tasks are valuable and worthy of publication. However, I feel that the authors exaggerate the extent to which researchers in this field still consider RNNs superior models for sequences. \n\n+ Many experiments and tasks.\n+ Well-written and clear.\n+ Good results\n- Somewhat exaggerated claims about the extent to which RNNs are still being considered more suitable sequence models\n than dilated convolutions. Especially in light of the success of Wavenet.\n- Not much novelty/originality.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642457703,"tcdate":1511632191825,"number":1,"cdate":1511632191825,"id":"SkdHpQDez","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Review","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The authors benchmark a general-purpose convolutional architecture on several sequence modeling tasks across a variety of domains. The results will be of broad use to the community, although some of the claims in the paper could do with more justification.","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. They start by proposing a generic temporal convolution sequence model which leverages recent advances in the field, discuss the respective advantages of convolutional and recurrent networks, and benchmark their architecture on a number of different tasks.\n\nThe paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works.\n\nThe only part which is slightly less convincing is the section about effective memory size. While it is true that learning longer term dependencies can be difficult in standard RNN architectures, it is interesting to notice that the SoTA results presented in appendix B.3 for language modeling on larger data sets are architectures which focus on remedying this difficulty (cache model and hierarchical LSTM). It would also be interesting to see how TCN works on word prediction tasks which are devised explicitly to test for longer memory, such as Lambada (1) or Children Books Test (2).\n\nAs a minor point, adding a measure of complexity in terms of number of operations could be a useful hardware-independent indication of the computational cost of the architecture.\n\nPros:\n- Clearly written, well executed paper\n- Makes a strong point for the use of convolutional architecture for sequences\n- Provides useful benchmarks for the community\n\nCons:\n- The claims on effective memory size need more context and justification\n\n1: The LAMBADA dataset: Word prediction requiring a broad discourse context, Paperno et al. 2016\n2: The Goldilocks principle: reading children's books with explicit memory representation, Hill et al. 2016","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509627003298,"tcdate":1509627003298,"number":1,"cdate":1509627003298,"id":"BJ7tE5OCb","invitation":"ICLR.cc/2018/Conference/-/Paper501/Public_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Close resemblance with another already published paper","comment":"This work very closely resembles the work that was first done by the authors in https://arxiv.org/abs/1703.04691. The network structure employed seems almost identical. Furthermore the conclusion that CNNs can be an efficient alternative to RNNs was also already reached in the above mentioned paper. Thus it would be advisable to cite this work in your paper. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1514672741968,"tcdate":1509124445918,"number":501,"cdate":1509739265765,"id":"rk8wKk-R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.","pdf":"/pdf/4f72202970a15efa844bae2729d7d0a4f2ed503a.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}