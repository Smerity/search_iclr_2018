{"notes":[{"tddate":null,"ddate":null,"tmdate":1512438952355,"tcdate":1512438952355,"number":3,"cdate":1512438952355,"id":"B1lh3uXbf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Public_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"needs to acknowledge existence of very similar work and the better result presented by other papers for language modeling","comment":"(1) It is already known that convolutional architectures perform well on sequence modeling tasks (for example, Oord et al 2016, Wavenet). This paper does not discuss many related works on convolutional sequence modeling for text that should be addressed specifically in the related work section, for example Kalchbrenner et al 2016, Dauphin et al 2017, and Gehring et al, 2017. \n\n(2) This paper tests on Wikitext-103 but it does not cite that https://arxiv.org/pdf/1612.08083.pdf already published better results on Wikitext-103 with a very similar convolutional model (44.9 PPL on 1 GPU/37.2 PPL on 4 GPU, compared to the reported 48.9 PPL here, a significant difference). \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512438403564,"tcdate":1512438403564,"number":2,"cdate":1512438403564,"id":"Bk2tcOm-f","invitation":"ICLR.cc/2018/Conference/-/Paper501/Public_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Does not acknowledge recent work on CNNs for text generation","comment":"This paper does not acknowledge most very prominent recent work on CNNs for text generation, e.g.,\n\nKalchbrenner et al. \"Neural Machine Translation in Linear Time\". 2016.\nGehring et al. \"Convolutional Sequence to Sequence Moedeling\". 2017.\n\nThose papers make precisely the same points and have much stronger empirical results. The authors cite Dauphin et al.  (2016) at the very end of the paper but do not acknowledge that many of the points made are already covered by recent other work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222676514,"tcdate":1511822901224,"number":3,"cdate":1511822901224,"id":"HkTNLM5gM","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Review","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Convolutional networks are good for solving sequence tasks","rating":"4: Ok but not good enough - rejection","review":"This paper argues that convolutional networks should be the default\napproach for sequence modeling.\n\nThe paper is nicely done and rather easy to understand. Nevertheless, I find\nit difficult to assess its significance. In order to support the original hypothesis,\nI think that a much larger and more diverse set of experiments should have\nbeen considered. As pointed out by another reviewer please add  https://arxiv.org/abs/1703.04691\nto your references.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222676613,"tcdate":1511781470002,"number":2,"cdate":1511781470002,"id":"HkUwN_Ylf","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Review","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice results but not much novelty and I don't think that the views are as contrarian as the paper claims.","rating":"5: Marginally below acceptance threshold","review":"The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The paper describes an architecture similar to wavenet with residual connections. Empirical results are presented on a large number of tasks where the convolutional network often outperforms modern recurrent baselines or reaches similar performance.\n\nThe biggest strength of the paper is the large number of tasks on which the models are evaluated. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. That said, I don’t think that all the tasks are very relevant for comparing convolutional and recurrent architectures. While the time windows that RNNs can deal with are infinite in principle, it is common knowledge that the effective length of the dependencies RNNs can model is quite limited in practise. Many of the artificial task like the adding problem and sequential MNIST have been designed to highlight this weakness of RNNs. I don’t find it very surprising that these tasks are easy to solve with a feedforward architecture with a large enough context window. The more impressive results are in my opinion those on the language modelling tasks where one would indeed expect RNNs to be more suitable for capturing dependencies that require stack-like memory functionality. \n\nWhile the related work is quite comprehensive, it downplays the popularity of convolutional architectures throughout history a bit. Especially in speech recognition, RNNs have only recently started to gain popularity while deep feedforward networks applied to overlapping time windows (i.e., 1D convolutions) have been the state-of-the-art for years. Of course the recent successes of dilated convolutions are likely to change the landscape in this application domain yet again.\n\nThe paper is well-structured and written. If anything, it is perhaps a little bit wordy at times but I prefer that over obscurity due to brevity.\n\nThe ideas in the paper are not novel and neither do the authors claim that they are. Unfortunately, I also think that the impact of the work is also somewhat limited due to the enormous success of the wavenet architecture. I do think that the results on the real-world tasks are valuable and worthy of publication. However, I feel that the authors exaggerate the extent to which researchers in this field still consider RNNs superior models for sequences. \n\n+ Many experiments and tasks.\n+ Well-written and clear.\n+ Good results\n- Somewhat exaggerated claims about the extent to which RNNs are still being considered more suitable sequence models\n than dilated convolutions. Especially in light of the success of Wavenet.\n- Not much novelty/originality.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222676664,"tcdate":1511632191825,"number":1,"cdate":1511632191825,"id":"SkdHpQDez","invitation":"ICLR.cc/2018/Conference/-/Paper501/Official_Review","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference/Paper501/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The authors benchmark a general-purpose convolutional architecture on several sequence modeling tasks across a variety of domains. The results will be of broad use to the community, although some of the claims in the paper could do with more justification.","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. They start by proposing a generic temporal convolution sequence model which leverages recent advances in the field, discuss the respective advantages of convolutional and recurrent networks, and benchmark their architecture on a number of different tasks.\n\nThe paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works.\n\nThe only part which is slightly less convincing is the section about effective memory size. While it is true that learning longer term dependencies can be difficult in standard RNN architectures, it is interesting to notice that the SoTA results presented in appendix B.3 for language modeling on larger data sets are architectures which focus on remedying this difficulty (cache model and hierarchical LSTM). It would also be interesting to see how TCN works on word prediction tasks which are devised explicitly to test for longer memory, such as Lambada (1) or Children Books Test (2).\n\nAs a minor point, adding a measure of complexity in terms of number of operations could be a useful hardware-independent indication of the computational cost of the architecture.\n\nPros:\n- Clearly written, well executed paper\n- Makes a strong point for the use of convolutional architecture for sequences\n- Provides useful benchmarks for the community\n\nCons:\n- The claims on effective memory size need more context and justification\n\n1: The LAMBADA dataset: Word prediction requiring a broad discourse context, Paperno et al. 2016\n2: The Goldilocks principle: reading children's books with explicit memory representation, Hill et al. 2016","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509627003298,"tcdate":1509627003298,"number":1,"cdate":1509627003298,"id":"BJ7tE5OCb","invitation":"ICLR.cc/2018/Conference/-/Paper501/Public_Comment","forum":"rk8wKk-R-","replyto":"rk8wKk-R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Close resemblance with another already published paper","comment":"This work very closely resembles the work that was first done by the authors in https://arxiv.org/abs/1703.04691. The network structure employed seems almost identical. Furthermore the conclusion that CNNs can be an efficient alternative to RNNs was also already reached in the above mentioned paper. Thus it would be advisable to cite this work in your paper. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739268431,"tcdate":1509124445918,"number":501,"cdate":1509739265765,"id":"rk8wKk-R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk8wKk-R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Convolutional Sequence Modeling Revisited","abstract":"This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default'' mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory'' advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to'' architecture for sequence modeling.","pdf":"/pdf/05eb672dda044e36a99887a04c15171cdf6ab731.pdf","TL;DR":"We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.","paperhash":"anonymous|convolutional_sequence_modeling_revisited","_bibtex":"@article{\n  anonymous2018convolutional,\n  title={Convolutional Sequence Modeling Revisited},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8wKk-R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper501/Authors"],"keywords":["Temporal Convolutional Network","Sequence Modeling","Deep Learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}