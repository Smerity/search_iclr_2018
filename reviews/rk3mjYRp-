{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222831433,"tcdate":1511804837791,"number":3,"cdate":1511804837791,"id":"rk0iJ0FgM","invitation":"ICLR.cc/2018/Conference/-/Paper98/Official_Review","forum":"rk3mjYRp-","replyto":"rk3mjYRp-","signatures":["ICLR.cc/2018/Conference/Paper98/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Important topic but the work is a presentation of known material","rating":"4: Ok but not good enough - rejection","review":"The main object of the paper is the (entropy regularized) policy updates. Policy iterations are viewed as a gradient flow in the small timestep limit. Using this, (and following Jordan et al. (1998)) the desired PDE (Equation 21) is obtained. The rest of the paper discusses the implications of Equation 21 including but not limited to what happens when the time derivative of the policy is zero, and the link to noisy gradients.\n\nEven though the topic is interesting and would be of interest to the community, the paper mainly presents known results and provides an interpretation from the point of view of policy dynamics. I fail to see the significance nor the novelty in this work (esp. in light of  Jordan et al. (1998) and Peyre (2015)).\n\nThat said, I believe that exposing such connections will prove to be useful, and I encourage the authors to push the area forward. In particular, it would be useful to see demonstrations of the idea, and experimental justifications even in the form of references would be a welcome addition to the literature.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Diffusing Policies : Towards Wasserstein Policy Gradient Flows","abstract":"Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance $W_2$, policy dynamics are governed by the heat equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.","pdf":"/pdf/611b3366b2035a540b49e01de82d22d1275a1539.pdf","TL;DR":"Linking Wasserstein-trust region entropic policy gradients, and the heat equation.","paperhash":"anonymous|diffusing_policies_towards_wasserstein_policy_gradient_flows","_bibtex":"@article{\n  anonymous2018diffusing,\n  title={Diffusing Policies : Towards Wasserstein Policy Gradient Flows},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk3mjYRp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper98/Authors"],"keywords":["Optimal transport","policy gradients","entropy regularization","reinforcement learning","heat equation","Wasserstein","JKO","gradient flows"]}},{"tddate":null,"ddate":null,"tmdate":1512222831473,"tcdate":1511761798869,"number":2,"cdate":1511761798869,"id":"Syy9DXtef","invitation":"ICLR.cc/2018/Conference/-/Paper98/Official_Review","forum":"rk3mjYRp-","replyto":"rk3mjYRp-","signatures":["ICLR.cc/2018/Conference/Paper98/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Diffusing Policies : Towards Wasserstein Policy Gradient Flows","rating":"5: Marginally below acceptance threshold","review":"In this paper the authors studied policy gradient with change of policies limited by a trust region of Wasserstein distance in the multi-armed bandit setting. They show that in the small steps limit, the policy dynamics are governed by the heat equation (Fokker-Planck equation). This theoretical result helps us understand both the convergence property and the probability matching property in policy gradient using concepts in diffusion and advection from the heat equation. To the best of my knowledge, this line of research was dated back to the paper by Jordan et al in 1998, where they showed that the continuous control policy transport follows the Fokker-Planck equation. In general I found this line of research very interesting as it connects the convergence of proximal policy optimization to optimal transport, and I appreciate seeing recent developments on this line of work. \n\nIn terms of theoretical contributions, I see that this paper contains some novel ideas in connecting gradient flow with Wasserstein distance regularization to the Fokker-Planck equation. Furthermore its interpretation on the Brownian diffusion processes justifies the link between entropy-regularization and noisy gradients (with isotropic Gaussian noise regularization for exploration). I also think this paper is well-written and mathematically sound. While I understand the knowledge of this paper based on standard knowledge in PDE of diffusion processes and Ito calculus, I am not experienced enough in this field to judge whether these contributions are significant enough for a standalone contribution, as the problem setting is limited to multi-armed bandits.\n\nMy major critic to this paper is its practical value. Besides the proposed Sinkhorn-Knopp based algorithm in the Appendix that finds the optimal policy as fixed point of (44), I am unsure how these results lead to more effective policy gradient algorithms (with lower variance in gradient estimators, or with quasi-monotonic performance improvement etc.). There are also no experiments in this paper (for example to compare the standard policy gradient algorithm with the one that solves the Fokker-Planck equation) to demonstrate the effectiveness of the theoretical findings.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Diffusing Policies : Towards Wasserstein Policy Gradient Flows","abstract":"Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance $W_2$, policy dynamics are governed by the heat equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.","pdf":"/pdf/611b3366b2035a540b49e01de82d22d1275a1539.pdf","TL;DR":"Linking Wasserstein-trust region entropic policy gradients, and the heat equation.","paperhash":"anonymous|diffusing_policies_towards_wasserstein_policy_gradient_flows","_bibtex":"@article{\n  anonymous2018diffusing,\n  title={Diffusing Policies : Towards Wasserstein Policy Gradient Flows},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk3mjYRp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper98/Authors"],"keywords":["Optimal transport","policy gradients","entropy regularization","reinforcement learning","heat equation","Wasserstein","JKO","gradient flows"]}},{"tddate":null,"ddate":null,"tmdate":1512222831516,"tcdate":1511734702573,"number":1,"cdate":1511734702573,"id":"Sywhphuez","invitation":"ICLR.cc/2018/Conference/-/Paper98/Official_Review","forum":"rk3mjYRp-","replyto":"rk3mjYRp-","signatures":["ICLR.cc/2018/Conference/Paper98/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting insights on policy gradient flows but the novel contributions are unclear","rating":"4: Ok but not good enough - rejection","review":"The paper ‘Diffusing policies: Towards Wasserstein policy gradient flows’ explores \nthe connections between reinforcement learning and the theory of quadratic optimal transport (i.e.\nusing the Wasserstein_2 as a regularizer of an iterative problem that converges toward\nan optimal policy). Following a classical result from Jordan-Kinderlehrer-Otto, they show that \nthe policy dynamics are governed by the heat equation, that translates in an advection-diffusion \nscheme. This allows to draw insights on the convergence of empirical practices in the field.\n\nThe paper is clear and well-written, and provides a comprehensive survey of known results in the \nfield of Optimal Transport. The insights on why empirical strategies such as additive gradient noise\nare very interesting and helps in understanding why they work in practical settings. That being said, \nmost of the results presented in the paper are already known (e.g. from the book of Samtambrogio or the work \nof G. Peyré on entropic Wasserstein gradient flows) and it is not exactly clear what are the original\ncontributions of the paper. The fact that the objective is to learn policies\nhas little to no impact on the derivations of calculus. It clearly suggests that the entropy \nregularized Wasserstein_2 distance should be used in numerical experiments but this point is not \nsupported by experimental results. Their direct applications is rapidly ruled out by highlighting the \ncomputational complexity of solving such gradient flows but in the light of recent papers (see \nthe work of Genevay https://arxiv.org/abs/1706.00292 or another paper submitted to ICLR on large scale optimal transport \nhttps://openreview.net/forum?id=B1zlp1bRW) numerical applications should be tractable. For these reasons \nI feel that the paper would clearly be more interesting for the practitioners (and maybe to some extent \nfor the audience of ICLR) if numerical applications of the presented theory were discussed or sketched \nin classical reinforcement learning settings.  \n\nMinor comments:\n - in Equation (10) why is there a ‘d’ in front of the coupling \\gamma ? \n - in Section 4.5, please provide references for why numerical estimators of gradient of Wasserstein distances\nare biased. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Diffusing Policies : Towards Wasserstein Policy Gradient Flows","abstract":"Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance $W_2$, policy dynamics are governed by the heat equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.","pdf":"/pdf/611b3366b2035a540b49e01de82d22d1275a1539.pdf","TL;DR":"Linking Wasserstein-trust region entropic policy gradients, and the heat equation.","paperhash":"anonymous|diffusing_policies_towards_wasserstein_policy_gradient_flows","_bibtex":"@article{\n  anonymous2018diffusing,\n  title={Diffusing Policies : Towards Wasserstein Policy Gradient Flows},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk3mjYRp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper98/Authors"],"keywords":["Optimal transport","policy gradients","entropy regularization","reinforcement learning","heat equation","Wasserstein","JKO","gradient flows"]}},{"tddate":null,"ddate":null,"tmdate":1509739485619,"tcdate":1508969252434,"number":98,"cdate":1509739482964,"id":"rk3mjYRp-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk3mjYRp-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Diffusing Policies : Towards Wasserstein Policy Gradient Flows","abstract":"Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance $W_2$, policy dynamics are governed by the heat equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.","pdf":"/pdf/611b3366b2035a540b49e01de82d22d1275a1539.pdf","TL;DR":"Linking Wasserstein-trust region entropic policy gradients, and the heat equation.","paperhash":"anonymous|diffusing_policies_towards_wasserstein_policy_gradient_flows","_bibtex":"@article{\n  anonymous2018diffusing,\n  title={Diffusing Policies : Towards Wasserstein Policy Gradient Flows},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk3mjYRp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper98/Authors"],"keywords":["Optimal transport","policy gradients","entropy regularization","reinforcement learning","heat equation","Wasserstein","JKO","gradient flows"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}