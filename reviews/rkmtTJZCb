{"notes":[{"tddate":null,"ddate":null,"tmdate":1512368856180,"tcdate":1512368856180,"number":3,"cdate":1512368856180,"id":"B1e1owMZM","invitation":"ICLR.cc/2018/Conference/-/Paper529/Official_Review","forum":"rkmtTJZCb","replyto":"rkmtTJZCb","signatures":["ICLR.cc/2018/Conference/Paper529/AnonReviewer2"],"readers":["everyone"],"content":{"title":"insufficient novelty and significance","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method for predicting future video frames. The method is based on Villegas et al. (2017), with the main difference being that no ground truth pose is needed to train the network.\n\nThe novelty of the method is limited. It seems that there is very little innovation in terms of network architecture compared to Villegas et al. The difference is mainly on how the network is trained. But it is straightforward to train the architecture of Villegas et al. without pose -- just use any standard choice of loss that compares the predicted frame versus the ground truth frame. I don't see what is non-trivial or difficult about not using pose ground truth in training.\n\nOverall I think the contribution is not significant enough. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Hierarchical Video Prediction","abstract":"Long term video prediction is a challenging machine learning problem, whose solution would enable intelligent agents to plan sophisticated interactions with their environment and assess the effect of their actions. Much recent research has been devoted to video prediction and generation, but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video predictions.  However,their method has limited applicability in practical settings as it requires annotation of structures (e.g., pose) at every time step.  This paper presents a long term hierarchical video prediction model that doesn’t have such a restriction. We show that the network learns its own learned higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where that higher-level structure doesn’t  capture  all  of  the  information  needed  to  predict  the  next  frames. This method gives sharper results than other video prediction methods which don’t re-quire a groundtruth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets","pdf":"/pdf/9b4a556f8c9d5bb7ada1795e5c7d965a4a362889.pdf","TL;DR":"We show ways to train a hierarchical video prediction model without needing pose labels.","paperhash":"anonymous|unsupervised_hierarchical_video_prediction","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Hierarchical Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmtTJZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper529/Authors"],"keywords":["video prediction","visual analogy network","unsupervised"]}},{"tddate":null,"ddate":null,"tmdate":1512248126429,"tcdate":1512248126429,"number":2,"cdate":1512248126429,"id":"Sk8SX9e-G","invitation":"ICLR.cc/2018/Conference/-/Paper529/Official_Comment","forum":"rkmtTJZCb","replyto":"HyuKwSixM","signatures":["ICLR.cc/2018/Conference/Paper529/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper529/Authors"],"content":{"title":"Addressing your qualitative results comment","comment":"Would adding this section to the paper sufficiently address your comment “In H3.6M, the results are only qualitative. The conclusions from the paper are uncertain, partly  due to the difficulty of evaluating the video prediction results.”? Or is more still needed to address this?\n\nPaper section:\n\nOne way to compare the methods quantitatively is on how clear the generated human images are. To measure this, we ran a person detector on each of the generated frames and recorded how confident the detector was that a person is in the image. We call this the person score in the paper. The results on each frame averaged over 1k runs are shown in figure 7.\n\nFigure 7: https://drive.google.com/file/d/1rEavoCIPK_ZeMWtSiheLaJRuLYcrlrC_/view?usp=sharing Confidence of the person detector that a person is in the image (person score). The baseline method is CDNA from Finn et al. (2016):\n\nThe person score on the ground truth is about 40% on each frame.  It is probably so low because the frames have a low resolution.  The person score is 26% on average for the images generated by the EPEV method, and 18% for CDNA from Finn et al. (2016).  The person score degrades very rapidly in the first 8 frames of CDNA, but degrades more slowly in the EPEV method. The person's core of the EPEV method on frame 63 is about the same as on frame 8 of CDNA. This confirms our visual analysis that the EPEV method produces clearer predictions further into the future.  The EPEV method was only trained to predict 32 frames into the future but there isn’t a significant drop in the person score at frame 32, showing that the EPEV method generalizes well to predicting longer sequences."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Hierarchical Video Prediction","abstract":"Long term video prediction is a challenging machine learning problem, whose solution would enable intelligent agents to plan sophisticated interactions with their environment and assess the effect of their actions. Much recent research has been devoted to video prediction and generation, but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video predictions.  However,their method has limited applicability in practical settings as it requires annotation of structures (e.g., pose) at every time step.  This paper presents a long term hierarchical video prediction model that doesn’t have such a restriction. We show that the network learns its own learned higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where that higher-level structure doesn’t  capture  all  of  the  information  needed  to  predict  the  next  frames. This method gives sharper results than other video prediction methods which don’t re-quire a groundtruth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets","pdf":"/pdf/9b4a556f8c9d5bb7ada1795e5c7d965a4a362889.pdf","TL;DR":"We show ways to train a hierarchical video prediction model without needing pose labels.","paperhash":"anonymous|unsupervised_hierarchical_video_prediction","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Hierarchical Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmtTJZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper529/Authors"],"keywords":["video prediction","visual analogy network","unsupervised"]}},{"tddate":null,"ddate":null,"tmdate":1512214396910,"tcdate":1512214350858,"number":1,"cdate":1512214350858,"id":"BkDU1MxWz","invitation":"ICLR.cc/2018/Conference/-/Paper529/Official_Comment","forum":"rkmtTJZCb","replyto":"BJ1X3tYgf","signatures":["ICLR.cc/2018/Conference/Paper529/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper529/Authors"],"content":{"title":"Clarifying toy dataset comment from AnonReviewer1","comment":"I was thinking of doing the following to address your comment “I think a toy example where this is clearly the case because we know exactly the factors of variations and they are inferred by the algorithm automatically or some better ones are discovered by the algorithm, that would make it a very strong submission.”:\n\nCreate a toy dataset where a generated ball bounces around the frame in a predictable way. Train my algorithm on predicting the first 10 or so frames, and show that the model can generalize to predicting very far into the future.\n\nIs this what you had in mind?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Hierarchical Video Prediction","abstract":"Long term video prediction is a challenging machine learning problem, whose solution would enable intelligent agents to plan sophisticated interactions with their environment and assess the effect of their actions. Much recent research has been devoted to video prediction and generation, but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video predictions.  However,their method has limited applicability in practical settings as it requires annotation of structures (e.g., pose) at every time step.  This paper presents a long term hierarchical video prediction model that doesn’t have such a restriction. We show that the network learns its own learned higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where that higher-level structure doesn’t  capture  all  of  the  information  needed  to  predict  the  next  frames. This method gives sharper results than other video prediction methods which don’t re-quire a groundtruth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets","pdf":"/pdf/9b4a556f8c9d5bb7ada1795e5c7d965a4a362889.pdf","TL;DR":"We show ways to train a hierarchical video prediction model without needing pose labels.","paperhash":"anonymous|unsupervised_hierarchical_video_prediction","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Hierarchical Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmtTJZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper529/Authors"],"keywords":["video prediction","visual analogy network","unsupervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222680371,"tcdate":1511901056146,"number":2,"cdate":1511901056146,"id":"HyuKwSixM","invitation":"ICLR.cc/2018/Conference/-/Paper529/Official_Review","forum":"rkmtTJZCb","replyto":"rkmtTJZCb","signatures":["ICLR.cc/2018/Conference/Paper529/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting architecture alternatives, but lack of strong empirical conclusions regarding the lack of supervising the high level structure","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method for hierarchical future frame prediction in monocular videos. It builds upon the recent method of Villegas et al. 2017, which generates future RGB frames in two stages: in the first stage, it predicts a human body pose sequence, then it conditions on the pose sequence to predict RGB content, using an image analogy network. This current paper, does not constrain the first stage (high level) prediction to be human poses, but instead it can be any high level representation. Thus, the method does not require human annotations.\n\nThe method has the following two sub-networks:\n1) An image encoder, that given an RGB image, predicts a deep feature encoding. \n2) An LSTM predictor, that conditioned on the last observed frame's encoding,  predicts future high level structure p_t. Once enough frames are generated though, it conditions on its own predictions. \n3) A visual analogy network (VAN), that given predicted high level structure p_t, it predicts the pixel image I_t, by applying the transformation from the first to tth frame, as computed by the vector subtraction of the corresponding high level encodings (2nd equation of the paper). VAN is trained to preserve parallelogram relationships in the joint RGB image and high level structure embedding.\n\nThe authors experiment with  many different neural network connectivities, e.g., not constraining the predicted high level structure to match the encoder's outputs, constraining the predicted high level structure to match the encoder's output (EPEV), and training together the VAN  and predictor so that VAN can tolerate mistakes of the predictor. Results are shown in H3.6m and the pushobject datasets, and are compared against the method of Villegas et all (INDIVIDUAL). The conclusion seems to be that not constraining the predicted high level structure to match the encoder’s output, but biasing the encoder’s output in the observed frames to represent ground-truth pose information, gives the best results. \n\nPros\n1) Interesting alternative training schemes are tested\n\nCons:\n1)Numerous English mistakes, e.g.,  ''an intelligent agents\", ''we explore ways generate\" etc.\n\n2) Equations are not numbered (and thus is hard to refer to them.) E.g., i do not understand the first equation, shouldn’t it be that e_{t-1} is always fixed and equal to the encoding of the last observed (not predicted) frame? Then the subscript cannot be t-1.\n\n3) In H3.6M, the results are only qualitative. The conclusions from the paper are uncertain, partly  due to the difficulty of evaluating the video prediction results.\n\n\nGiven the difficulty of assessing the experimental results quantitatively (one possibility to do so is asking a set of people of which one they think is the most plausible video completion), and given the limited novelty of the paper, though interesting alternative architectures are tried out, it may not be suitable to be part of  ICLR proceedings as a conference paper.  \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Hierarchical Video Prediction","abstract":"Long term video prediction is a challenging machine learning problem, whose solution would enable intelligent agents to plan sophisticated interactions with their environment and assess the effect of their actions. Much recent research has been devoted to video prediction and generation, but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video predictions.  However,their method has limited applicability in practical settings as it requires annotation of structures (e.g., pose) at every time step.  This paper presents a long term hierarchical video prediction model that doesn’t have such a restriction. We show that the network learns its own learned higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where that higher-level structure doesn’t  capture  all  of  the  information  needed  to  predict  the  next  frames. This method gives sharper results than other video prediction methods which don’t re-quire a groundtruth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets","pdf":"/pdf/9b4a556f8c9d5bb7ada1795e5c7d965a4a362889.pdf","TL;DR":"We show ways to train a hierarchical video prediction model without needing pose labels.","paperhash":"anonymous|unsupervised_hierarchical_video_prediction","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Hierarchical Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmtTJZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper529/Authors"],"keywords":["video prediction","visual analogy network","unsupervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222680415,"tcdate":1511787542574,"number":1,"cdate":1511787542574,"id":"BJ1X3tYgf","invitation":"ICLR.cc/2018/Conference/-/Paper529/Official_Review","forum":"rkmtTJZCb","replyto":"rkmtTJZCb","signatures":["ICLR.cc/2018/Conference/Paper529/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting paper but I find the claims are not backed up by the experimental evidence","rating":"4: Ok but not good enough - rejection","review":"The paper treats the interesting problem of long term video prediction in complex video streams. I think the approach of adding more structure to their representation before making longer term prediction is also a reasonable one. Their approach combines an RNN that predicts an encoding of scene and then generating an image prediction using a VAN (Reed et al.). They show some results on the Human3.6M and the Robot Push dataset. \n\nI find the submission lacking clarity in many places. The main lack of clarity source I think is about what the contribution is. There are sparse mentions in the introduction but I think it would be much more forceful and clear if they would present VAN or Villegas et al method separately and then put the pieces together for their method in a separate section. This would allow the author to clearly delineate their contribution and maybe why those choices were made. Also the use of hierarchical is non-standard and leads to confusion I recommend maybe \"semantical\" or better \"latent structured\" instead. Smaller ambiguities in wording are also in the paper : e.g. related work -> long term prediction \"in this work\" refers to the work mentioned but could as well be the work that they are presenting.  \n\nI find some of the claims not clearly backed by a thorough evaluation and analysis. Claiming to be able to produce encodings of scenes that work well at predicting many steps into the future is a very strong claim. I find the few images provided very little evidence for that fact. I think a toy example where this is clearly the case because we know exactly the factors of variations and they are inferred by the algorithm automatically or some better ones are discovered by the algorithm, that would make it a very strong submission. Reed et al. have a few examples that could be adapted to this setting and the resulting representation, analyzed appropriately, would shed some light into whether this is the right approach for long term video prediction and what are the nobs that should be tweaked in this system. \n\nIn the current format, I think that the authors are on a good path and I hope my suggestions will help them improve their submission, but as it stands I recommend rejection from this conference.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Hierarchical Video Prediction","abstract":"Long term video prediction is a challenging machine learning problem, whose solution would enable intelligent agents to plan sophisticated interactions with their environment and assess the effect of their actions. Much recent research has been devoted to video prediction and generation, but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video predictions.  However,their method has limited applicability in practical settings as it requires annotation of structures (e.g., pose) at every time step.  This paper presents a long term hierarchical video prediction model that doesn’t have such a restriction. We show that the network learns its own learned higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where that higher-level structure doesn’t  capture  all  of  the  information  needed  to  predict  the  next  frames. This method gives sharper results than other video prediction methods which don’t re-quire a groundtruth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets","pdf":"/pdf/9b4a556f8c9d5bb7ada1795e5c7d965a4a362889.pdf","TL;DR":"We show ways to train a hierarchical video prediction model without needing pose labels.","paperhash":"anonymous|unsupervised_hierarchical_video_prediction","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Hierarchical Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmtTJZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper529/Authors"],"keywords":["video prediction","visual analogy network","unsupervised"]}},{"tddate":null,"ddate":null,"tmdate":1509739252904,"tcdate":1509125499067,"number":529,"cdate":1509739250240,"id":"rkmtTJZCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkmtTJZCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unsupervised Hierarchical Video Prediction","abstract":"Long term video prediction is a challenging machine learning problem, whose solution would enable intelligent agents to plan sophisticated interactions with their environment and assess the effect of their actions. Much recent research has been devoted to video prediction and generation, but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video predictions.  However,their method has limited applicability in practical settings as it requires annotation of structures (e.g., pose) at every time step.  This paper presents a long term hierarchical video prediction model that doesn’t have such a restriction. We show that the network learns its own learned higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where that higher-level structure doesn’t  capture  all  of  the  information  needed  to  predict  the  next  frames. This method gives sharper results than other video prediction methods which don’t re-quire a groundtruth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets","pdf":"/pdf/9b4a556f8c9d5bb7ada1795e5c7d965a4a362889.pdf","TL;DR":"We show ways to train a hierarchical video prediction model without needing pose labels.","paperhash":"anonymous|unsupervised_hierarchical_video_prediction","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Hierarchical Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmtTJZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper529/Authors"],"keywords":["video prediction","visual analogy network","unsupervised"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}