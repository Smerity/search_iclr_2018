{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222802995,"tcdate":1511744764416,"number":2,"cdate":1511744764416,"id":"H1EWH1KxG","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Review","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"1) Summary\nThis paper proposes a study on the robustness of one low-precision neural networks class - binarized neural networks (BNN) - against adversarial attacks. Specifically, the authors show that these low precision networks are not just efficient in terms of memory consumption and forward computation, but also more immune to adversarial attacks than their high-precision counterparts. In experiments, they show the advantage of BNNs by conducting experiments based on black-box and white-box adversarial attacks without the need to artificially mask gradients.\n\n\n2) Pros:\n+ Introduced, studied, and supported the novel idea that BNNs are robust to adversarial attacks.\n+ Showed that BNNs are robust to the Fast Gradient Sign Method (FGSM) and Carlini-Wagner attacks in white-box adversarial attacks by presenting evidence that BNNs either outperform or perform similar to the high-precision baseline against the attacks.\n+ Insightful analysis and discussion of the advantages of using BNNs against adversarial attacks.\n\n3) Cons:\nMissing full-precision model trained with PGD in section 3.2:\nThe authors mention that the full-precision model would also likely improve with PGD training, but do not have the numbers. It would be useful to have such numbers to make a better evaluation of the BNN performance in the black-box attack setting.\n\n\nAdditional comments:\nCan the authors provide additional analysis on why BNNs perform worse than full-precision networks against black-box adversarial attacks? This could be insightful information that this paper could provide if possible.\n\n\n4) Conclusion:\nOverall, this paper proposes great insightful information about BNNs that shows the additional benefit of using them besides less memory consumption and efficient computation. This paper shows that the used architecture for BBNs makes them less susceptible to known white-box adversarial attack techniques.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/d4c00615dadb457c43c13512fcaef9098b19ee72.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222803758,"tcdate":1511709617735,"number":1,"cdate":1511709617735,"id":"Sy5hsUOlG","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Review","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"\nThis paper starts by gently going over the concept of adversarial attacks on neural networks (black box vs white box, reactive vs proactive, transfer of attacks, linearity hypothesis), as well as low-precision nets and their deployement advantages. \nAdversarial examples are introduced as a norm-measurable deviation from natural inputs to a system. We are reminded of adversarial training, and of the fact that binarized nets are highly non linear due to the nature of their weights and activations.\n\nThis paper then proposes to examine the robustness of binarized neural networks to adversarial attacks on MNIST and CIFAR-10.\n\nThe quantization scheme used here is v32 Conv2D -> ReLU -> BNorm -> sign -> bit Conv2D -> ReLU -> Scalar -> BNorm -> sign, but sign is really done with the stochastic quantization method of Courbariaux et al, even at test time (in order to make it more robust).\n\nWhat the experimental results show:\n- High capacity BNNs are usually more robust to white-box attacks than normal networks, probably because the gradient information that an adversary would use becomes very poor as training progresses.\n- BNNs are harder to properly train with adversarial examples because of the polarized weight distribution that they induce\n- Against black-box attacks, it seems there is little difference between NNs and BNNs.\n\nSome comments and questions:\n- In figure 1 I'm not sure what \"Scalar\" refers to, and it is not explained in the paper (nor could I find it explained in Papernot et al 2017a).\n- Do you adopt the \"Shift based Batch Normalizing Transform\" of Courbariaux et al? If not, why?\n- It might be worth at least _quickly_ explaining what the 'Carlini-Wagner L2 from CleverHans' is rather than simply offering a citation with no explanation. Idem for 'smooth substitute model black-box misclassificiation attack'. We often assume our readers know most of what we know, but I find this is often not the case and can discourage the many newcomers of our field.\n- \"Running these attacks to 1000 iterations [...], therefore we believe this targeted attack represents a fairly substantial level of effort on behalf of the adversary.\" while true for us researchers, computational difficulty will not be a criterion to stop for state actor or multinational tech companies, unless it can be proven that e.g. the number of iterations needs to grow exponentially (or in some other unreasonable way) in order to get reliable attacks.\n- \"MLP with binary units 3 better\", 'as in Fig.' is missing before '3', or something of the sort.\n- You say \"We postpone a formal explanation of this outlier for the discussion.\" but really you're explaining it in the next paragraph (unless there's also another explanation I'm missing). Training BNNs with adversarial examples is hard.\n- You compare stochastic BNNs with deterministic NNs, but not with stochastic NNs. What do you think would happen? Some of arguments that you make in favour of BNNs could also maybe be applied to stochastic NNs.\n\nMy opinions on this paper:\n- Novelty: it certainly seems to be the first time someone has tackled BNNs and adversarial examples\n- Relevance: BNNs can be a huge deal when deploying applications, it makes sense to study their vulnerabilities\n- Ease of understanding: To me the paper was mostly easy to understand, yet, considering there is no page limit in this conference, I would have buffed up the appendix, e.g. to include more details about the attacks used and how various hyperparameters affect things.\n- Clarity: I feel like some details are lacking that would hinder reproducing and extending the work presented here. Mostly, it isn't always clear why the chosen prodecures and hyperparameters were chosen (wrt the model being a BNN)\n- Method: I'm concerned by the use of MNIST to study such a problem. MNIST is almost linearly separable, has few examples, and given the current computational landscape, much better alternatives are available (SVHN for example if you wish to stay in the digits domain). Concerning black-box attacks, it seems that BNNs less beneficial in a way; trying more types of attacks and/or delving a bit deeping into that would have been nice. The CIFAR-10 results are barely discussed.\n\nOverall I think this paper is interesting and relevant to ICLR. It could have stronger results both in terms of the datasets used and the variety of attacks tested, as well as some more details concerning how to perform adversarial training with BNNs (or why that's not a good idea).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/d4c00615dadb457c43c13512fcaef9098b19ee72.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1510205458939,"tcdate":1510205260757,"number":2,"cdate":1510205260757,"id":"B1SUPvZ1z","invitation":"ICLR.cc/2018/Conference/-/Paper887/Public_Comment","forum":"HkTEFfZRb","replyto":"Sy0k0EkJf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"That doesn't matter","comment":"The submission guideline does not prohibit anything about arXiv preprint.\nRecently there is a movement toward tackling the real double-blindness of review process (e.g. ACL), however as far as I know ICLR 2018 is not arguing any restriction on that.\nThe anonymity policy is applied only to papers submitted to the conference via OpenReview platform; if one can't find out who the authors are from the version uploaded on OpenReview, then it is not considered to violate double-blindness.\nYou may easily find other papers in arXiv which are submitted to ICLR 2018.\nAlso, see the submission guideline: http://www.iclr.cc/doku.php?id=iclr2018:conference_cfp; in 3), the document states that \"While ICLR is double blind, we will not forbid authors from posting their paper on arXiv or any other public forum.\""},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/d4c00615dadb457c43c13512fcaef9098b19ee72.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512445234330,"tcdate":1509136692980,"number":887,"cdate":1509739044736,"id":"HkTEFfZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/d4c00615dadb457c43c13512fcaef9098b19ee72.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}