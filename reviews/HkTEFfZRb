{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642926450,"tcdate":1515017020620,"number":3,"cdate":1515017020620,"id":"BkSH7A5Qz","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Comment","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper887/Authors"],"content":{"title":"Response to all reviewers (part 1)","comment":"We thank the reviewers for their positive and constructive feedback. We believe that we have addressed all of the main questions and concerns in the most recent revision of the paper. These are detailed below:\n\nR2 - Higher dimensional data\n\nTo confirm that our findings hold for higher dimensional data, and further show that performance on clean inputs does not necessarily translate to the same on adversarial examples, we conducted ImageNet experiments with AlexNet and ResNet-18 using 1-4 bits for weights and activations. Models were trained as in DoReFa-Net by Zhou et al., 2016, with the addition of L2 weight decay on the first layer (conv0) of all models which isn’t quantized. This L2 norm penalty is inspired by the boundary tilting perspective (Tanay & Griffin, 2016). A regularization constant of 1e-5 was used for AlexNet and 1e-4 for ResNets.\n\nWe found that:\n\nFor AlexNet, despite some accuracy degradation for clean inputs, all low-precision variants had the same or less top-1 error against FGSM, and all but two had less top-5 error across a typical range of perturbation magnitudes (epsilon in [2.0 - 16.0]). A binarized AlexNet had 4.5% and 8.0% less top-1 and top-5 error respectively than a full-precision equivalent for epsilon=2.0, and performed the same or better when sweeping across the full range of epsilon. \n\nResNet experienced a 6.5% and 4.2% reduction in top-1 and top-5 error respectively on clean inputs when going to 2-bits, however this performance gap shrinks to within +/- 0.2% for FGSM with epsilon=4.0 (in favour of low-precision for top-1 error, and in favour of full-precision for top-5). A binarized ResNet was slightly less optimal than the 2-bit case, but still managed to reduce the performance gap on clean inputs, resulting in 3.4% higher top-5 error, but 0.4% lower top-1 error, for FGSM with epsilon=4.0, as compared to full-precision.\n\nThe small 3x3 kernels in ResNets are less likely to be unique for very low bitwidths, as there are only 512 possible binary 3x3 kernels, vs 65k possible binary 4x4 kernels. This could explain some of the differences between binarizing AlexNet which uses a mix of large and small kernels, vs ResNet which makes exclusive use of small kernels.\n\nAlthough one of the main contributions of DoReFa-Net was to train with low bitwidth gradients, we’re reporting the 32-bit gradient case here as this is what was done originally in our paper, and is more representative of “black-box” vulnerability in an ideal FGSM transfer attack. Models trained with low bitwidth gradients (e.g 6-bits) further reduced top-5 error under FGSM by 6-7% on AlexNet, however this gain was found to be caused by gradient masking, as it was overcome by subsequently attacking with 32-bit gradients.\n\nOur preference is to report these results here informally to address the questions/concerns of R2, to keep the paper at a reasonable length, and because the use of mixed precision and larger dataset could be seen as a departure from the original scope of the paper (c.f. the instructions to authors). The majority of related works report CIFAR-10 and MNIST since adversarial robustness is generally an unsolved problem even in low-dimensions (e.g see https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge). Several of the targeted attacks and adversarial training methods used in the paper (e.g PGD, CWL2, Papernot black-box transfer attack) are currently very slow to run on ImageNet.\n\nIn the time since submission, we have also been analyzing the difference between compression by low-precision parameters and compression by other means such as pruning. Pruning introduces sparsity which can result in loss of rank in weight matrices, effectively removing coordinate axis with which to express the decision boundary and causing it to collapse and lie near the data, leaving the model vulnerable to adversarial examples. Low-precision is less likely to result in loss of rank unless used in conjunction with very small kernels.\n\nR1 - Stochastic BNN vs Stochastic NN\n\nWe conducted but did not report some additional experiments for the full-precision case where weights are sampled from a Gaussian distribution with a learned mean. This type of model achieved 20% accuracy against 100 iterations of CWL2, compared to 70% for SBNNs. Having weights flip signs is more destructive and noisy wrt the progress of an iterative attack. Weights sampled from a Gaussian are less likely to change sign between iterations. In the literature outside low-precision implementations, typically stochastic NNs refers to stochastic activations (e.g. [Tang and Salakhutdinov 2013, Tapani et al. 2014]) and this is likely what R1 was referencing. While we agree with the reviewer that networks with stochastic activations are indeed a relevant comparison, we have not yet carried out the experiments."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515024778904,"tcdate":1515015728256,"number":2,"cdate":1515015728256,"id":"BJO40TcmM","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Comment","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper887/Authors"],"content":{"title":"Response to all reviewers (part 2)","comment":"R1 - Computational effort of running CWL2 to 1000 iterations\n\nThe purpose of our statement was to provide some context for the level of effort spent attacking a network in terms of the effort required to train it originally. We agree that this is not a formal notion of security, although for high dimensional problems such as ImageNet, an attack that is an order of magnitude more expensive than training from scratch could be prohibitive for the layperson. \n\nR1 - More detail on attacks\n\nWe agree that more detail regarding the attacks makes the paper more accessible. As such, we have added a brief description of the Carlini-Wagner L2 attack. We note that the Papernot “smooth substitute model-black box attack” was briefly outlined in Section 3.2 and do not feel that we have much more space to elaborate given the target paper length.\n\nR3 - Full-precision model with PGD training in Section 3.2\n\nWe can confirm that the full-precision model (A) does indeed perform much better with PGD adversarial training rather than with FGSM, but so does the scaled binarized model (C), which we did not report originally in Table 4. We find no significant difference between models A and C when PGD training is used for models of varying capacity. Please refer to the latest revision for the updated Table 4.\n\nR3 - Can the authors provide additional analysis on why BNNs perform worse than full-precision networks against black-box adversarial attacks? This could be insightful information that this paper could provide if possible.\n\nWe wish to clarify that BNNs are not worse than NNs against black-box attacks, which should be more clear in the updated Table 4 (see C+*), and from Table 5 (C+). When using delayed PGD training, and scaling binarized activations by a single small tunable parameter per layer, similar performance to full-precision with PGD training is achieved on MNIST. For CIFAR-10, the scaled BNN with FGSM adversarial training (C+) achieved 8.6% higher black-box accuracy than full-precision with FGSM training (A+) for the high capacity model, and C+ maintained a small edge over A+ at the lowest capacity tested. A possible explanation for the improvement of the scaled binarized model on CIFAR-10 is that it has limited representation power to cheat by learning image statistics and other non-salient consistencies between training and test sets, an explanation inspired by Jo & Bengio, 2017. We have observed this effect in preliminary MNIST experiments with simple classifiers and a small set of feature-preserving frequency domain transformations applied to either training or test split, but a more detailed explanation is still in progress. \n\nR1 - In figure 1 I'm not sure what \"Scalar\" refers to, and it is not explained in the paper (nor could I find it explained in Papernot et al 2017a).\n\nWe have updated the second paragraph in Section 3 with additional detail about the nature and purpose of this scalar. The original description referred to a “scaling factor” but we have updated the language. This scalar does not come from Papernot et al., rather it is a modification to vanilla BNNs that reduces the range of hidden activations so they align more closely to NNs. This prevents numerical instabilities at the softmax layer, which has implications for gradient based attacks, and leads to improved accuracy on clean inputs as originally reported by Tang et al., 2017.\n\nR1 - Do you adopt the \"Shift based Batch Normalizing Transform\" of Courbariaux et al? If not, why?\n\nWe did not use shift based batch normalization (SBN) as this was viewed as a performance trick for reducing the number of multiplications required by “vanilla” batch normalization (BN) during training. The original Courbariaux paper reported no loss in accuracy when using SBN rather than BN for the same datasets. Traditional aspects of BNN performance were outside the scope of this paper. \n\nAdditional References:\n\nThomas Tanay and Lewis Griffin. A Boundary Tilting Perspective on the Phenomenon of Adversarial Examples, 2016 -- https://arxiv.org/abs/1608.07690\n\nYichuan Tang and Ruslan Salakhutdinov. Learning Stochastic Feedforward Neural Networks, 2013  -- http://www.cs.toronto.edu/~tang/papers/sfnn.pdf\n\nTapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for Learning Binary Stochastic Feedforward Neural Networks, 2014  -- https://arxiv.org/abs/1406.2989\n\nJason Jo and Yoshua Bengio. Measuring the tendency of CNNs to learn surface statistical regularities, 2017 -- https://arxiv.org/abs/1711.11561\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1513398033367,"tcdate":1513398033367,"number":3,"cdate":1513398033367,"id":"ByKGyXMfz","invitation":"ICLR.cc/2018/Conference/-/Paper887/Public_Comment","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Replication of 'Attacking Binarized Neural Networks'","comment":"1) Introduction: As a ﬁnal project for the course ”Applied Machine Learning” at McGill University, Canada, we were tasked with reproducing a paper submitted to the International Conference on Learning Representation (iclr.cc). We chose the paper ”Attacking Binarized Neural Networks” because of its application to mass-produced and available technology. Low-precision networks can be deployed on more cost-effective hardware and can therefore be more widely utilized.\n\n2) Analysis of Paper: The paper is well written and provides many novel approaches and ﬁndings:\n• The authors of this paper conducted many experiments.\nEach experiment was well deigned and focused on one aspect of the model.\n• The paper demonstrated that binary neural networks are often more robust against adversarial attacks.\n• The paper found that binary neural network were harder to properly train with adversarial examples than the full-precision network.\n\nHowever, there were limitations to this paper:\n• In the paper, the author stated that they ran the training and attacking in both MNIST and CIFAR-10 datasets, however, in the white-box attack portion, they only showed the results of performing MNIST dataset. They did not mention the CIFAR-10 dataset.\n• The authors applied the low-precision neural network to the datasets only in a relatively low dimension. It will be more convincing if they can test this neural network architecture on some higher dimensional datasets, such as Imagenet.\n\n3) Reproduction Methodology: The reproduction is realized with TensorFlow and CleverHans. We conducted attacks on similarly modeled BNNs and full-precision networks and compared their performance. Both white-box and black-box attacks were reproduced running under similar parameters in the original work.\nIn addition, we further veriﬁed BNNs’ robustness against adversarial attacks by experimenting on CIFAR-10 dataset. Details of our reproduction can be seen in paper linked below.\n\n4) Reproduction Results: To replicate the original paper’s white-box attack methods we ran both Fast-Gradient Sign Method (FGSM) and Carlini-Wagner L2 attacks on different neural network setups. From these attacks, we found that we were able to replicate the original papers ﬁndings. Most of our results consistently fell within the ranges that the author has listed. However, for some tests, we have received accuracies drastically different than those reported in the original paper. These inconsistencies may be due to us using slightly different model parameters as they were not well speciﬁed in the original paper. They may also be reduced by rerunning our replicated tests until we have more conﬁdence in our values.\nReplication of Black-Box attacks consisted of running the same substitute model training procedure from Papernot et al. using CleverHans v2.0.0 on the MNIST dataset with FGSM adversarial training.\nSimilar to our White-box replication, some of our Black-box results had accuracies similar to those originally reported. However, when we attacked the binary neural network with learned scalar, our result differed far from the authors’ experimental results.\nFull-precision networks had a moderate advantage over binary model classes B and C, which was similar to the authors’ result. However, our binary neural network with learned scalar performed worse than binary network without the learned scalar. We think that the difference is due to due to the different number of adversarial instances.\n\n5) Conclusion: We reproduced the main ﬁndings of the paper Attacking Binarized Neural Networks. We found out that neural networks with low-precision weights and activations, such as binarized neural networks, would indeed improve robustness against some adversarial attacks like FGSM and Carlini-Wagner L2.\n\n6) Nota Bene: A copy of the original paper can be found at https://goo.gl/rQvXig."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642527370,"tcdate":1512696923602,"number":3,"cdate":1512696923602,"id":"HkNP3wvWM","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Review","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/AnonReviewer2"],"readers":["everyone"],"content":{"title":"elegant method, less convincing experiments","rating":"6: Marginally above acceptance threshold","review":"his work presents an empirical study demonstrating that binarized networks are more robust to adversarial examples. The authors follow the stochastic binarization procedure proposed by Courbariaux et al. The robustness is tested with various attacks such as the fast gradient sign method and the projected gradient method on MNIST and CIFAR.\n\nThe experimental results validate the main claims of the paper on some datasets. While reducing the precision can intuitively improve the robustness, It remains unclear if this method would work on higher dimensional inputs such as Imagenet. Indeed: \n\n(1) state of the art architectures on Imagenet such as Residual networks are known to be very fragile to precision reduction. Therefore, reducing the precision can also reduce the robustness as it is positively correlated with accuracy. \n\n(2) Compressing reduces the size of the hypothesis space explored. Therefore, larger models may be needed to make this method work for higher dimensional inputs. \n\nThe paper is well written overall and the main idea is simple and elegant. I am less convinced by the experiments.  ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642527408,"tcdate":1511744764416,"number":2,"cdate":1511744764416,"id":"H1EWH1KxG","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Review","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"1) Summary\nThis paper proposes a study on the robustness of one low-precision neural networks class - binarized neural networks (BNN) - against adversarial attacks. Specifically, the authors show that these low precision networks are not just efficient in terms of memory consumption and forward computation, but also more immune to adversarial attacks than their high-precision counterparts. In experiments, they show the advantage of BNNs by conducting experiments based on black-box and white-box adversarial attacks without the need to artificially mask gradients.\n\n\n2) Pros:\n+ Introduced, studied, and supported the novel idea that BNNs are robust to adversarial attacks.\n+ Showed that BNNs are robust to the Fast Gradient Sign Method (FGSM) and Carlini-Wagner attacks in white-box adversarial attacks by presenting evidence that BNNs either outperform or perform similar to the high-precision baseline against the attacks.\n+ Insightful analysis and discussion of the advantages of using BNNs against adversarial attacks.\n\n3) Cons:\nMissing full-precision model trained with PGD in section 3.2:\nThe authors mention that the full-precision model would also likely improve with PGD training, but do not have the numbers. It would be useful to have such numbers to make a better evaluation of the BNN performance in the black-box attack setting.\n\n\nAdditional comments:\nCan the authors provide additional analysis on why BNNs perform worse than full-precision networks against black-box adversarial attacks? This could be insightful information that this paper could provide if possible.\n\n\n4) Conclusion:\nOverall, this paper proposes great insightful information about BNNs that shows the additional benefit of using them besides less memory consumption and efficient computation. This paper shows that the used architecture for BBNs makes them less susceptible to known white-box adversarial attack techniques.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642527456,"tcdate":1511709617735,"number":1,"cdate":1511709617735,"id":"Sy5hsUOlG","invitation":"ICLR.cc/2018/Conference/-/Paper887/Official_Review","forum":"HkTEFfZRb","replyto":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference/Paper887/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"\nThis paper starts by gently going over the concept of adversarial attacks on neural networks (black box vs white box, reactive vs proactive, transfer of attacks, linearity hypothesis), as well as low-precision nets and their deployement advantages. \nAdversarial examples are introduced as a norm-measurable deviation from natural inputs to a system. We are reminded of adversarial training, and of the fact that binarized nets are highly non linear due to the nature of their weights and activations.\n\nThis paper then proposes to examine the robustness of binarized neural networks to adversarial attacks on MNIST and CIFAR-10.\n\nThe quantization scheme used here is v32 Conv2D -> ReLU -> BNorm -> sign -> bit Conv2D -> ReLU -> Scalar -> BNorm -> sign, but sign is really done with the stochastic quantization method of Courbariaux et al, even at test time (in order to make it more robust).\n\nWhat the experimental results show:\n- High capacity BNNs are usually more robust to white-box attacks than normal networks, probably because the gradient information that an adversary would use becomes very poor as training progresses.\n- BNNs are harder to properly train with adversarial examples because of the polarized weight distribution that they induce\n- Against black-box attacks, it seems there is little difference between NNs and BNNs.\n\nSome comments and questions:\n- In figure 1 I'm not sure what \"Scalar\" refers to, and it is not explained in the paper (nor could I find it explained in Papernot et al 2017a).\n- Do you adopt the \"Shift based Batch Normalizing Transform\" of Courbariaux et al? If not, why?\n- It might be worth at least _quickly_ explaining what the 'Carlini-Wagner L2 from CleverHans' is rather than simply offering a citation with no explanation. Idem for 'smooth substitute model black-box misclassificiation attack'. We often assume our readers know most of what we know, but I find this is often not the case and can discourage the many newcomers of our field.\n- \"Running these attacks to 1000 iterations [...], therefore we believe this targeted attack represents a fairly substantial level of effort on behalf of the adversary.\" while true for us researchers, computational difficulty will not be a criterion to stop for state actor or multinational tech companies, unless it can be proven that e.g. the number of iterations needs to grow exponentially (or in some other unreasonable way) in order to get reliable attacks.\n- \"MLP with binary units 3 better\", 'as in Fig.' is missing before '3', or something of the sort.\n- You say \"We postpone a formal explanation of this outlier for the discussion.\" but really you're explaining it in the next paragraph (unless there's also another explanation I'm missing). Training BNNs with adversarial examples is hard.\n- You compare stochastic BNNs with deterministic NNs, but not with stochastic NNs. What do you think would happen? Some of arguments that you make in favour of BNNs could also maybe be applied to stochastic NNs.\n\nMy opinions on this paper:\n- Novelty: it certainly seems to be the first time someone has tackled BNNs and adversarial examples\n- Relevance: BNNs can be a huge deal when deploying applications, it makes sense to study their vulnerabilities\n- Ease of understanding: To me the paper was mostly easy to understand, yet, considering there is no page limit in this conference, I would have buffed up the appendix, e.g. to include more details about the attacks used and how various hyperparameters affect things.\n- Clarity: I feel like some details are lacking that would hinder reproducing and extending the work presented here. Mostly, it isn't always clear why the chosen prodecures and hyperparameters were chosen (wrt the model being a BNN)\n- Method: I'm concerned by the use of MNIST to study such a problem. MNIST is almost linearly separable, has few examples, and given the current computational landscape, much better alternatives are available (SVHN for example if you wish to stay in the digits domain). Concerning black-box attacks, it seems that BNNs less beneficial in a way; trying more types of attacks and/or delving a bit deeping into that would have been nice. The CIFAR-10 results are barely discussed.\n\nOverall I think this paper is interesting and relevant to ICLR. It could have stronger results both in terms of the datasets used and the variety of attacks tested, as well as some more details concerning how to perform adversarial training with BNNs (or why that's not a good idea).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1510205458939,"tcdate":1510205260757,"number":2,"cdate":1510205260757,"id":"B1SUPvZ1z","invitation":"ICLR.cc/2018/Conference/-/Paper887/Public_Comment","forum":"HkTEFfZRb","replyto":"Sy0k0EkJf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"That doesn't matter","comment":"The submission guideline does not prohibit anything about arXiv preprint.\nRecently there is a movement toward tackling the real double-blindness of review process (e.g. ACL), however as far as I know ICLR 2018 is not arguing any restriction on that.\nThe anonymity policy is applied only to papers submitted to the conference via OpenReview platform; if one can't find out who the authors are from the version uploaded on OpenReview, then it is not considered to violate double-blindness.\nYou may easily find other papers in arXiv which are submitted to ICLR 2018.\nAlso, see the submission guideline: http://www.iclr.cc/doku.php?id=iclr2018:conference_cfp; in 3), the document states that \"While ICLR is double blind, we will not forbid authors from posting their paper on arXiv or any other public forum.\""},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515186564913,"tcdate":1509136692980,"number":887,"cdate":1509739044736,"id":"HkTEFfZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkTEFfZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Attacking Binarized Neural Networks","abstract":"Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise\noperations. We propose a third benefit of very low-precision neural networks:\nimproved robustness against some adversarial attacks, and in the worst case,\nperformance that is on par with full-precision models. We focus on the very\nlow-precision case where weights and activations are both quantized to $\\pm$1,\nand note that stochastically quantizing weights in just one layer can sharply\nreduce the impact of iterative attacks. We observe that non-scaled binary neural\nnetworks exhibit a similar effect to the original \\emph{defensive distillation}\nprocedure that led to \\emph{gradient masking}, and a false notion of security.\nWe address this by conducting both black-box and white-box experiments with\nbinary models that do not artificially mask gradients.","pdf":"/pdf/c0be535ade85efb2842e2f566a726f2c6dfa777b.pdf","TL;DR":"We conduct adversarial attacks against binarized neural networks and show that we reduce the impact of the strongest attacks, while maintaining comparable accuracy in a black-box setting","paperhash":"anonymous|attacking_binarized_neural_networks","_bibtex":"@article{\n  anonymous2018attacking,\n  title={Attacking Binarized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkTEFfZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper887/Authors"],"keywords":["adversarial examples","adversarial attacks","binary","binarized neural networks"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}