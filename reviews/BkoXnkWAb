{"notes":[{"tddate":null,"ddate":null,"tmdate":1515769786167,"tcdate":1515769786167,"number":4,"cdate":1515769786167,"id":"SkGpJUL4G","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Comment","forum":"BkoXnkWAb","replyto":"SJNVMViZG","signatures":["ICLR.cc/2018/Conference/Paper518/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper518/AnonReviewer1"],"content":{"title":"Thanks for your answer","comment":"Thanks for your answer,\n\n I like the general idea of  bipolar activation,  but I think the empirical evaluation still need to be improved. Although authors show that bipolar activation improve the trainability of deep-stack RNN and simple convolutional networks, their approach tends to underperform other methods that also focus on in networks trainability (gating, batch-norm).  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512944259037,"tcdate":1512944259037,"number":3,"cdate":1512944259037,"id":"SJiFf4jZG","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Comment","forum":"BkoXnkWAb","replyto":"SkBvfy5lz","signatures":["ICLR.cc/2018/Conference/Paper518/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper518/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thank you for your review.\n\nWe agree that it would be nice to show results with LSTMs or GRUs. However, it is not obvious to us how to do best do so, since LSTM and GRU do not use ReLU-family activation functions, but instead use the tanh and sigmoid functions. Properly introducing bipolar activations to gated network seems to raise enough questions to warrant a paper on its own. It certainly seems like a fruitful direction for future research.\n\nYour review raises some valid concerns about our stacked RNN architecture. The many layers makes it computationally expensive, and it is outperformed by other architectures. However, our paper is fundamentally about bipolar activation functions, not about the RNN architecture. \n\nOur intent is to argue in favor of BReLU over ReLU and BELU over ELU. It is not to argue in favor of stacked Elman-RNNs over LSTMs. \n\nMost successful RNNs use gates and bounded activation functions (tanh and sigmoid). RNNs with unbounded activation functions have a potential for exploding activations. Stacking such models in depth compounds this problem, as exploding dynamics can happen depthwise as well as across time. \n\nIn other words, the architecture we chose is one that makes learning hard, not one that makes it easy. The propensity for exploding dynamics makes it a good testbed for a self-centering activation function. Indeed, in more than half of our experiments on RNNs, we find that bipolar activation functions are required for the training to work at all. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512944172308,"tcdate":1512944172308,"number":2,"cdate":1512944172308,"id":"SJNVMViZG","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Comment","forum":"BkoXnkWAb","replyto":"rJC1hZqxf","signatures":["ICLR.cc/2018/Conference/Paper518/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper518/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Thank you for your review.\n\nWe address your questions and comments below:\n\n* Which layer mean and variance are reported in Figure 2? What is the difference between the left and right plots?\n- These graphs show the development of a repeated application of matrix-multiplication + non-linearity on a random vector. This is like a single layer RNN without any input and without any learning. It is an idealized case which serves to isolate the effect of the recurrent dynamics. The left graph show ReLU vs BReLU while the right graph show ELU vs BELU. In every case, the bipolar variants lead to more stable dynamics.\n\n* In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances. It would be nice to report the training loss to see if this is an optimization or a generalization problem.\n- Agreed. As noted in our reply to reviewer 3 above, we have updated the paper with a training error curve for ReLU-RNN vs BReLU-RNN, which shows lower training error with the bipolar variants.\n\n* How does bipolar activation compare to model train with BN on CIFAR10?\n- We note the original results with BN in the last sentence in the section on CIFAR-10 (a test error of 2.98% for ORN and 4.17% for WRN). These results come from an extensive hyperparameter search over networks with BN. We simply copied the hyperparameters for their best results, i.e. we have not attempted to compensate for the loss of regularization due to removing BN.\n\n* Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?\n- It is not clear how to introduce bipolar activations to such networks, since both GRU and LSTM use only bounded activation functions like tanh and sigmoid. Most RNNs use bounded activation functions, which avoid exploding dynamics. This explosion risk makes deeply stacked RNNs with unbounded activations a good testing ground for a self-centering activation function.\n\n* As stated in the text, BELU-RNN outperforms BN-LSTM for PTB. However, BN-LSTM outperforms BELU-RNN on Text8. Do you know why the trend is not consistent across datasets?\n- We have not specifically investigated this question. A possible explanation is that we did not do any hyperparameter tuning on the Text8 dataset.\n\n* Performance of the stacked RNN\n\nIt is true that other architectures outperform our stacked RNN. However, our intent was not to introduce a new RNN architecture, but to introduce bipolar activation functions. Indeed, our RNN architecture is simply a stacked Elman-RNN with skip connections.\n\nWe argue that bipolar activations help learning over non-bipolar ReLU-family activations functions. To show this, we compare ReLU vs BReLU and ELU vs BELU in various architectures, and find bipolarity to be helpful in both RNNs and ConvNets. This argument does not rely on stacked RNNs beng superior to LSTMs or other recurrent architectures.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512943876437,"tcdate":1512943876437,"number":1,"cdate":1512943876437,"id":"Sy3ZW4sZz","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Comment","forum":"BkoXnkWAb","replyto":"B1qbgHcxz","signatures":["ICLR.cc/2018/Conference/Paper518/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper518/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thanks for your review. It is a fair criticism that we have not included enough evidence of faster training in the RNN setting. \n\nWhat follows is a summary of the evidence we do present that bipolar activations help learning in RNNs. There are two cases to consider: ELU vs BELU and ReLU vs BReLU.\n\n- For the ELU vs BELU case, we find that in every experiment the ELU-RNN diverges, while the BELU-RNN does not.\n- For the ReLU vs BReLU case, we find that the ReLU-RNN diverges in the Text8 experiment, while BReLU-RNN does not.\n\nIn most of our experiments, the non-bipolar RNNs do not converge at all, while the bipolar variant does. \n\nHowever on PennTreebank, both ReLU-RNN and BReLU-RNN do converge. Here the bipolar version achieves higher generalization accuracy on deeper models. This higher accuracy may be because bipolarity helps optimization, and it may be because of better generalization when using the bipolar versions. It is right to point out that the paper does not adequately establish which of these two are happening.\n\nThe way to establish that the PennTreebank results are due to ease of optimization would be to present the training error curve for the two variants. While we don't present it in the paper, we do have this curve. For the 36-layer network we focus on, what it shows is that the bipolar variant has lower training error for the first 88 epochs, until the learning rate is cut in the ReLU-RNN. In other words, at every point where the curves are comparable, the BReLU-RNN error is lower than the ReLU-RNN error, and the BReLU-RNN also ends up with a lower training error in the end.\n\nThat these curves are not in the paper is an omission, and we have updated the paper to include it. Even without this curve, we believe that the remaining evidence makes a strong case that bipolar activations help learning:\n\n- With ConvNets on CIFAR-10, the bipolar version achieved substantially lower training error than the non-bipolar versions.\n- On Text8, both non-bipolar version diverge, while the bipolar versions do not.\n- On PennTreebank, the ELU diverges, while the BELU does not.\n\nFor the remaining case, BReLU vs ReLU on PennTreebank we have updated the paper with the learning curve that shows faster learning in the bipolar case."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642459915,"tcdate":1511833602190,"number":3,"cdate":1511833602190,"id":"B1qbgHcxz","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Review","forum":"BkoXnkWAb","replyto":"BkoXnkWAb","signatures":["ICLR.cc/2018/Conference/Paper518/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple idea to encourage zero mean activations, but results focus on accuracy instead of learning speed-up","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks. The proposal is to replace the non-linearity in half of the units in each layer with its \"bipolar\" version -- one that is obtained by flipping the function on both axes.\nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained. \n\nClarity:\nThe paper is easy to read. The plots in Fig. 2 and the appendix are quite helpful in improving presentation. The experimental setups are explained in detail. \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive. However, the experiments to support the idea do not seem to match the motivation of the paper. As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent. However, the presented results focus on the performance on held-out data instead of improvements in training speed. This is especially the case for the RNN experiments.\n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning. However, it is unclear that overall training time can be reduced with the help of this technique. To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered.\n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases. The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset), but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642459955,"tcdate":1511820263156,"number":2,"cdate":1511820263156,"id":"rJC1hZqxf","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Review","forum":"BkoXnkWAb","replyto":"BkoXnkWAb","signatures":["ICLR.cc/2018/Conference/Paper518/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice idea but weak empirical performances","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a self-normalizing bipolar extension for the ReLU activation family. For every neuron out of two, authors propose to preserve the negative inputs. Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU.\n\nCombined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN.\n\nAuthors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN.  They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions. They also show that they can train deep residual network architecture on CIFAR without the use of BN.\n\nQuestion:\n- Which layer mean and variance are reported in Figure 2? What is the difference between the left and right plots?\n- In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances. It would be nice to report the training loss to see if this is an optimization or a generalization problem.\n- How does bipolar activation compare to model train with BN on CIFAR10?\n- Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?\n- As stated in the text, BELU-RNN outperforms BN-LSTM for PTB. However, BN-LSTM outperforms BELU-RNN on Text8. Do you know why the trend is not consistent across datasets?\n\n-Clarity/Quality\nThe paper is well written and pleasant to read\n\n\n- Originality:\nSelf-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel.\n\n- Significance:\nActivation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost. In this paper, bipolar activations are used to train very deep stacked RNN. However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures. It is not clear what are the advantage of deep stacked RNN in that context.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642459996,"tcdate":1511809629674,"number":1,"cdate":1511809629674,"id":"SkBvfy5lz","invitation":"ICLR.cc/2018/Conference/-/Paper518/Official_Review","forum":"BkoXnkWAb","replyto":"BkoXnkWAb","signatures":["ICLR.cc/2018/Conference/Paper518/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting paper, would like to see more experiments","rating":"4: Ok but not good enough - rejection","review":"The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs. The activation function keeps the activation roughly zero-centered. \n\nIn general, this is an interesting direction to explore, the idea is interesting, however, I would like to see more experiments\n\n1. The authors tested out this new activation function on RNNs. It would be interesting to see the results of the new activation function on LSTM.\n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers. For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.  \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.\n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm.\n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without.\n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512943404481,"tcdate":1509125155343,"number":518,"cdate":1509739256321,"id":"BkoXnkWAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkoXnkWAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Shifting Mean Activation Towards Zero with Bipolar Activation Functions","abstract":"We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.","pdf":"/pdf/7c0f599ec93cc356acb0b1c1b7cb572d84ff92c2.pdf","paperhash":"anonymous|shifting_mean_activation_towards_zero_with_bipolar_activation_functions","_bibtex":"@article{\n  anonymous2018shifting,\n  title={Shifting Mean Activation Towards Zero with Bipolar Activation Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkoXnkWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper518/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}