{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642414875,"tcdate":1513635513867,"number":3,"cdate":1513635513867,"id":"HJzp02rMM","invitation":"ICLR.cc/2018/Conference/-/Paper24/Official_Review","forum":"SJvrXqvaZ","replyto":"SJvrXqvaZ","signatures":["ICLR.cc/2018/Conference/Paper24/AnonReviewer5"],"readers":["everyone"],"content":{"title":"Ok but not good enough","rating":"4: Ok but not good enough - rejection","review":"Clarity \nThe paper is clear in general. \n\nOriginality\nThe novelty of the method is limited. The proposed method is a simple extension of L. Pinto et al. by replacing TRPO with A3C. No evidence is provided to show the proposed method is competitive with the original TRPO version. \n\nSignificance\n- The empirical results on the hardware are valuable. \n- The simulated results are very limited. The neural networks used in the simulation have only one hidden layer. The method is tested on the Pendulum domain. \n\nPros:\n- Real hardware results are provided. \n\nCons:\n- Limited simulation results. \n- Lacking technical novelty. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversary A3C for Robust Reinforcement Learning","abstract":"Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent’s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ","pdf":"/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf","paperhash":"anonymous|adversary_a3c_for_robust_reinforcement_learning","_bibtex":"@article{\n  anonymous2018adversary,\n  title={Adversary A3C for Robust Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvrXqvaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper24/Authors"],"keywords":["Adversary","Robust","Reinforcement Learning","A3C"]}},{"tddate":null,"ddate":null,"tmdate":1515642414914,"tcdate":1511831233423,"number":2,"cdate":1511831233423,"id":"r1tT8Ncez","invitation":"ICLR.cc/2018/Conference/-/Paper24/Official_Review","forum":"SJvrXqvaZ","replyto":"SJvrXqvaZ","signatures":["ICLR.cc/2018/Conference/Paper24/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  ","rating":"4: Ok but not good enough - rejection","review":"The authors propose an extension of adversarial reinforcement learning to A3C. The proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.  \n\nThe authors propose extending A3C to produce more robust policies by training a zero-sum game with two agents: a protagonist and an antagonist. The protagonist is attempting to achieve the given task while the antagonist's goal is for the task to fail. \n\nThe contribution of this work, AR-A3C, is extending adversarial reinforcement learning, namely robust RL (RRL) and robust adversarial RL (RARL), to A3C. In the context of this prior work the novelty is extending the family of adversarial RL methods. However, the proposed method is still within the same family methods as demonstrated by RARL.\n\nThe authors state that AR-A3C requires half as many rollouts as compared to RARL. However, no empirical comparison between the two methods is performed. The paper only performs analysis against the A3C and no other adversarial baseline and on only one environment: cartpole.  While they show transfer to the real world cartpole with this technique, there is not sufficient analysis to satisfactorily demonstrate the benefits of the proposed technique. \n\nThe paper reads well. There are a few notational issues in the paper that should be addressed. The authors mislabel the value function V as the  action value, or Q function. The action value function is action dependent where the value function is not.  As a much more minor issue, the authors introduce y as the discount factor, which deviates from the standard notation of \\gamma without any obvious reason to do so.\n\nDouble blind was likely compromised with the youtube video, which was linked to a real name account instead of an anonymous account.\n\nOverall, the proposed technique is of modest contribution and the experimental results do not provide sufficient validation of the approach.    ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversary A3C for Robust Reinforcement Learning","abstract":"Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent’s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ","pdf":"/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf","paperhash":"anonymous|adversary_a3c_for_robust_reinforcement_learning","_bibtex":"@article{\n  anonymous2018adversary,\n  title={Adversary A3C for Robust Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvrXqvaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper24/Authors"],"keywords":["Adversary","Robust","Reinforcement Learning","A3C"]}},{"tddate":null,"ddate":null,"tmdate":1515642414950,"tcdate":1511818971888,"number":1,"cdate":1511818971888,"id":"S14kDbqlG","invitation":"ICLR.cc/2018/Conference/-/Paper24/Official_Review","forum":"SJvrXqvaZ","replyto":"SJvrXqvaZ","signatures":["ICLR.cc/2018/Conference/Paper24/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach and hardware validation, but methods and comparisons are lacking","rating":"4: Ok but not good enough - rejection","review":"Positive:\n- Interesting approach\n- Hardware validation (the RL field needs more of this!)\n\nNegative:\n- Figure 2: what is the reward here? The one from Section 5.1?\n- No comparisons to other methods: Single pendulum swing-up is a very easy task that has been solved with various methods (mostly in a cart-pole setup). Please compare to existing methods such as PILCO, basic Q-learning, classical methods... \n- I'm not sure what's going on with the grammar in Section 5.3 (\"like crazy\", \"super hot\"...). This section also seems irrelevant (move to an appendix/supplementary or remove).\n- You should plot a typical control curve for the motors (requested torques). This might explain your heat problem (I'm guessing the motor is effectively controlled by a bang-bang controller).\n- Why did you pick this task? It's fine to only validate on a single task in hardware, but why not include additional simulation results (e.g. double pendulum)?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversary A3C for Robust Reinforcement Learning","abstract":"Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent’s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ","pdf":"/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf","paperhash":"anonymous|adversary_a3c_for_robust_reinforcement_learning","_bibtex":"@article{\n  anonymous2018adversary,\n  title={Adversary A3C for Robust Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvrXqvaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper24/Authors"],"keywords":["Adversary","Robust","Reinforcement Learning","A3C"]}},{"tddate":null,"ddate":null,"tmdate":1509739525353,"tcdate":1508512574690,"number":24,"cdate":1509739522693,"id":"SJvrXqvaZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJvrXqvaZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adversary A3C for Robust Reinforcement Learning","abstract":"Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent’s performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments. ","pdf":"/pdf/c25bf035784d1264c1a41f3000bfe219c202b361.pdf","paperhash":"anonymous|adversary_a3c_for_robust_reinforcement_learning","_bibtex":"@article{\n  anonymous2018adversary,\n  title={Adversary A3C for Robust Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvrXqvaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper24/Authors"],"keywords":["Adversary","Robust","Reinforcement Learning","A3C"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}