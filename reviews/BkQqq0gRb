{"notes":[{"tddate":null,"ddate":null,"tmdate":1515064100906,"tcdate":1515064100906,"number":4,"cdate":1515064100906,"id":"Hy67jKomG","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Comment","forum":"BkQqq0gRb","replyto":"SyF0odSef","signatures":["ICLR.cc/2018/Conference/Paper463/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper463/Authors"],"content":{"title":"Extensions to more complex tasks","comment":"Extensions to more complex tasks:\n\nIn the existing discriminative model experiments, we use shallow networks that are comparable to those considered in previous work (Kirkpatrick et al., 2017; Zenke et al., 2017) so that our reimplementation fairly represents the previous work. In the updated version of the paper, we have added an additional Split notMNIST experiment (see page 7 of the new version and Figure 5). The notMNIST dataset is much larger and more noisy than the MNIST dataset. It contains 400,000 images of 10 characters written in different font styles, where each character has 40,000 images. This dataset is considered more difficult than the MNIST dataset. In this new experiment, we investigate a deeper network with 4 hidden layers and our method also performs well compared to EWC and SI.\n\nExtension to computer vision applications:\n\nThe paper shows that VCL performs very well for MLPs in a variety of settings which we believe is an important contribution. To apply our method to many large-scale computer vision applications, the method needs to be extended to handle CNNs. In general, accurate approximate variational inference methods have not been developed for CNNs and this is an outstanding goal of the area of Bayesian Deep Learning. We therefore leave this development for future research. However, once a good general variational inference method has been developed for CNNs, it will be straightforward to apply the VCL framework.\n\nAlthough MC dropout (Gal & Ghahramani, 2016) is one candidate for Bayesian inference in CNNs, the nature of this approximation makes vanilla application of the VCL framework difficult. MC dropout uses a Gaussian prior over the weights and (the limit of) a mixture of Gaussians with shared parameters for the variational distribution. These two distributions are not of the same form and therefore a second approximation step would be required to apply VCL. Moreover, the impoverished representation of posterior uncertainty retained by MC dropout is likely to result in poor continual learning performance since nuanced and parameter specific information about parameter uncertainty is required in this setting. Approximations that employ a single global variance parameter in the q distribution, such as those employed by Kingma et al., 2015, will suffer similar problems.\n\nReferences:\n\nY. Gal and Z. Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. ICML 2016.\n\nD.P. Kingma, T. Salimans, M. Welling. Variational Dropout and the Local Reparameterization Trick. NIPS 2015."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515063767648,"tcdate":1515063767648,"number":3,"cdate":1515063767648,"id":"rkgk9tsmG","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Comment","forum":"BkQqq0gRb","replyto":"H1T4epKeM","signatures":["ICLR.cc/2018/Conference/Paper463/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper463/Authors"],"content":{"title":"New experiments on coreset sizes and the novelty of VCL","comment":"New experiments showing that coresets can significantly improve VCL's performance:\n\nThe use of a coreset can *significantly* improve VCL over the vanilla version. We have added a more comprehensive comparison to the updated version of the paper to make this completely clear (see Figure 3 and the last paragraph on page 6). For example, on the permuted MNIST task when the coreset size is 200 examples per task, the final accuracy of VCL improves from 90% to 93% and when the coreset size is increased to 5,000 examples per task, the performance further improves to 95.5%. These are significant improvements for this dataset. Crucially, using just the coreset alone (and no online inference) still performs significantly worse. Thus, although we agree that VCL alone is effective for continual learning the combination with a coreset can be critical. \n\nMoreover, as now noted in the paper, from a more general perspective, coreset VCL is equivalent to a message-passing implementation of variational inference in which the coreset data point message updates are scheduled last, only after the contributions from other data have been incorporated. This opens the door to versions of VCL which revisit the coreset points several times through learning (rather than just at the end).\n\nNovelty of VCL and contributions of the paper:\n\nThe novelty of our VCL method compared to online variational inference (Broderich et al., 2013; Ghahramani & Attias, 2000) is two-fold. \n\nFirst, online VI has only previously been applied to simple conjugate models. Here instead we consider deep neural networks and variational auto-encoders. Indeed, a Bayesian treatment of the parameters of variational auto-encoders, in addition to the latent variables, is challenging in and of itself. These more complex models require a fusion of online VI and Monte Carlo VI which is technically challenging. \n\nSecond, previous work on online VI considers very simple tasks, most typically where the data arrive in iid fashion. Here instead, we consider much more general continual learning tasks that were not previously considered for online VI. The increased inhomogeneity in the data necessitated the development of coreset VI which is more natural and simpler than previous work on coresets for continual learning such as Lopez-Paz and Ranzato (2017) which requires an additional constraint on the optimization objective for every new task.\n\nAt a more general level, we also feel that it is important to point out to the continual learning community that standard methods of (approximate) Bayesian inference provide a rich mathematical and algorithmic framework for attacking continual learning that has hitherto been largely overlooked.\n\nAppropriateness of the Title:\n\nGiven the two points addressed in the above responses, we believe that the title is appropriate. We have endeavoured to explain the relationship to prior work in the first line of the abstract, “a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks”, which we hope clearly explains the positioning of the paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515063385708,"tcdate":1515063385708,"number":2,"cdate":1515063385708,"id":"S1GDuYi7G","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Comment","forum":"BkQqq0gRb","replyto":"BkgsE19xz","signatures":["ICLR.cc/2018/Conference/Paper463/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper463/Authors"],"content":{"title":"Experiments on Harder Tasks","comment":"New Experiment on a harder task:\n\nIn order to further assess the efficacy of VCL on larger scale and more complex tasks we have added an additional experiment to the paper: the new Split notMNIST task on page 7 of the updated paper and Figure 5. The notMNIST dataset is much larger and more noisy than the MNIST dataset. It contains 400,000 images of 10 characters written in different font styles, where each character has 40,000 images. This dataset is generally considered more difficult than the MNIST dataset. In this new experiment, we investigate a deeper network and show that VCL still performs well compared to EWC and SI.\n\nDeployment on tasks requiring CNNs:\n\nThe application of VCL to the Atari or Split CIFAR tasks is also a sensible suggestion. However, this requires the development of reliable variational inference methods for convolutional neural networks (CNNs). This is still an outstanding research goal of Bayesian Deep Learning and so we leave this for future research. However, once a good variational inference method has been developed for CNNs, it is straightforward to apply the VCL framework to the above tasks. \n\nPlease see more relevant discussions of the points above in the response to Reviewer 3."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515063147123,"tcdate":1515063147123,"number":1,"cdate":1515063147123,"id":"SyQOwYi7M","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Comment","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper463/Authors"],"content":{"title":"Thanks and some clarifications.","comment":"Dear Reviewers,\n\nMany thanks for your detailed reviews. We really appreciate the time and effort you have put into reading and commenting on our paper. \n\nSorry for not responding to your comments more quickly, but we have been working on a set of new experimental results that have been inspired by your suggestions and which we believe strengthen the paper.\n\nPlease also note that there were some errors in the original plots of EWC and K-center Coreset Only methods in Figure 2. We have corrected the plots in our updated paper. The updated results are now consistent with previous findings in Zenke et al. (2017), where EWC and SI are comparable in the Permuted MNIST experiment. The updated results do not change our conclusions in this paper.\n\nWe will now address each of your reviews individually."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515642452459,"tcdate":1511810199658,"number":3,"cdate":1511810199658,"id":"BkgsE19xz","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Review","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/AnonReviewer1"],"readers":["everyone"],"content":{"title":"New framework for an important problem, supported with experiments in basic cases.","rating":"6: Marginally above acceptance threshold","review":"The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning. The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations. \n\nThe paper is well written, the problem is described neatly in conjunction with the past work, and the proposed algorithm is supported by experiments. The work is a useful addition to the community.\n\nMy main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017). Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515642452496,"tcdate":1511800885096,"number":2,"cdate":1511800885096,"id":"H1T4epKeM","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Review","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Seemingly significant finding, but the title should be rephrased","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new method, called VCL, for continual learning. This method is a combination of the online variational inference for streaming environment with Monte Carlo method. The authors further propose to maintain a coreset which consists of representative data points from the past tasks. Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning. Extensive experiments shows that VCL performs very well, compared with some state-of-the-art methods. \n\nThe authors present two ideas for continual learning in this paper: (1) Combination of online variational inference and sampling method, (2) Use of coreset to deal with the catastrophic forgetting problem. Both ideas have been investigated in Bayesian literature, while (2) has been recently investigated in continual learning. Therefore, the authors seems to be the first to investigate the effectiveness of (1) for continual learning. From extensive experiments, the authors find that the first idea results in VCL which can outperform other state-of-the-art approaches, while the second idea plays little role. \n\nThe finding of the effectiveness of idea (1) seems to be significant. The authors did a good job when providing a clear presentation, a detailed analysis about related work, an employment to deep discriminative models and deep generative models, and a thorough investigation of empirical performance.\n\nThere are some concerns the authors should consider:\n- Since the coreset plays little role in the superior performance of VCL, it might be better if the authors rephrase the title of the paper. When the coreset is empty, VCL turns out to be online variational inference [Broderich et al., 2013; Ghahramani & Attias, 2000]. Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well.\n- It is unclear about the sensitivity of VCL with respect to the size of the coreset. The authors should investigate this aspect.\n- What is the trade-off when the size of the coreset increases?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515642452534,"tcdate":1511521232840,"number":1,"cdate":1511521232840,"id":"SyF0odSef","invitation":"ICLR.cc/2018/Conference/-/Paper463/Official_Review","forum":"BkQqq0gRb","replyto":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference/Paper463/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper introduces a varaitional continual learning framework for neural networks. ","rating":"6: Marginally above acceptance threshold","review":"Overall, the idea of this paper is simple but interesting. Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty.\n\nThe paper is written well, and literature review is sufficient. My comment is mainly about its importance for large-scale computer vision applications. The neural networks in the experiments are shallow. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]}},{"tddate":null,"ddate":null,"tmdate":1515062340983,"tcdate":1509120651023,"number":463,"cdate":1509739287438,"id":"BkQqq0gRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkQqq0gRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variational Continual Learning","abstract":"This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that variational continual learning outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.","pdf":"/pdf/05ecc1c6bd884953b97790e700ab845bafe542a9.pdf","TL;DR":"This paper develops a principled method for continual learning in deep models.","paperhash":"anonymous|variational_continual_learning","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkQqq0gRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper463/Authors"],"keywords":["continual learning","online variational inference"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}