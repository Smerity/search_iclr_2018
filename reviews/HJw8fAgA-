{"notes":[{"tddate":null,"ddate":null,"tmdate":1512451734440,"tcdate":1512451734440,"number":3,"cdate":1512451734440,"id":"BkCqRjmZz","invitation":"ICLR.cc/2018/Conference/-/Paper444/Official_Review","forum":"HJw8fAgA-","replyto":"HJw8fAgA-","signatures":["ICLR.cc/2018/Conference/Paper444/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Could be impactful","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors provide a deeper exploration of Imagination Agents, by looking more closely at a variety of state-space models.  They examine what happens as both the representation of state, update algorithm, as well as the concept of time, is changed.  As in the original I2A paper, they experiment with learning how best to take advantage of a learned dynamics model.\n\nTo me, this strain of work is very important.  Because no model is perfect, I think the idea of learning how best to use an approximate model will become increasingly important.  The original I2A of learning-to-interpret models is here extended with the idea of learning-to-query, and a variety of solid variants on a few basic themes are well worked out.\n\nOverall, I think the strongest part of this paper is in its conceptual contributions - I find the work thought-provoking and inspiring.  On the negative side, I felt that the experiments were thin, and that the work was not well framed in terms of the literature of state-space identification and planning (there are a zillion ways to plan using a model; couldn't we have compared to at least one of them?  Or discussed a few popular ones, and why they aren't likely to work?  Since your model is fully differentiable, vanilla MPC would be a natural choice [In other words, instead of learning a rollout policy, do something simple like run MPC on the approximate model, and pass the resulting optimized action trajectory back to the agent as an input feature]).\n\nOf course, while we can always demand more and more experiments, I felt that this paper did a good enough job to merit publication.\n\nMinor quibble: I wasn't sure what to make of the sSSM ideas.  My understanding is that in any dynamical system model, the belief state update is always a deterministic function of previous belief state and observation; this suggests to me that the idea of \"state\" here differs from my definition of \"state\".  I don't think you should have to sample anything if you've represented your state cleanly.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Dynamic State Abstractions for Model-Based Reinforcement Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed models that learn predictive and compact state representations, also called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment (ALE) from raw pixels. Furthermore, RL agents that use Monte-Carlo rollouts of these models as features for decision making outperform strong model-free baselines on the game MS_PACMAN, demonstrating the benefits of planning using learned dynamic state abstractions.","pdf":"/pdf/ce968e9b48c159ec367d6c3037fb859cb397a15f.pdf","paperhash":"anonymous|learning_dynamic_state_abstractions_for_modelbased_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Dynamic State Abstractions for Model-Based Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJw8fAgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper444/Authors"],"keywords":["generative models","probabilistic modelling","reinforcement learning","state-space models","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512385495313,"tcdate":1512385495313,"number":2,"cdate":1512385495313,"id":"BkJknsz-f","invitation":"ICLR.cc/2018/Conference/-/Paper444/Official_Review","forum":"HJw8fAgA-","replyto":"HJw8fAgA-","signatures":["ICLR.cc/2018/Conference/Paper444/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Unclear contribution","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a method for inferring dynamical models from partial observations, that can later be used in model-based RL algorithms such as I2A. The essence of the method is to perform variational inference of the latent state, representing its distribution as Gaussian, and to use it in an ELBO to infer the dynamics of state and observation.\n\nWhile this is an interesting approach, many of the architectural choices involved seem arbitrary and unjustified. This wouldn't be so bad if they were justified by empirical success rather than principled design, but I'm also a bit skeptical of the strength of the results.\n\nA few examples of such architectural choices:\n1. What's the significance of separating the stochastic state transition into a stochastic choice of z and a transition g deterministic in z?\n2. How is the representational power affected by having only the observations depend on z? What's the intuition behind calling this model VAE, when sSSM is also trained with variational inference?\n3. What is gained by using pool-and-inject layers? By the way, is this a novel component? If so please elaborate, if not please cite.\n\nAs for the strength of the results, in Table 1 the proposed methods don't seem to outperform \"RAR\" (i.e., RNN) in expected value. They do seem to have lower variance, and the authors would do well to underline the importance of this.\nIn Figure 3, it's curious that the model-free baseline remains unnamed, as it also does in the text and appendix. This makes it hard to evaluate whether the significant wins are indicative of the strength of the proposed method.\n\nFinally, a notational point that the authors should really get right, is that conditioning on future actions naively changes the distribution of the current state or observation in ways they didn't intend. The authors intended for actions to be used as \"interventions\", i.e. a-causally, and should denote this conditioning by some sort of \"do\" operator.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Dynamic State Abstractions for Model-Based Reinforcement Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed models that learn predictive and compact state representations, also called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment (ALE) from raw pixels. Furthermore, RL agents that use Monte-Carlo rollouts of these models as features for decision making outperform strong model-free baselines on the game MS_PACMAN, demonstrating the benefits of planning using learned dynamic state abstractions.","pdf":"/pdf/ce968e9b48c159ec367d6c3037fb859cb397a15f.pdf","paperhash":"anonymous|learning_dynamic_state_abstractions_for_modelbased_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Dynamic State Abstractions for Model-Based Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJw8fAgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper444/Authors"],"keywords":["generative models","probabilistic modelling","reinforcement learning","state-space models","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222654204,"tcdate":1511761152767,"number":1,"cdate":1511761152767,"id":"H1KbSmYeM","invitation":"ICLR.cc/2018/Conference/-/Paper444/Official_Review","forum":"HJw8fAgA-","replyto":"HJw8fAgA-","signatures":["ICLR.cc/2018/Conference/Paper444/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple idea that seems to work well on Ms Pacman, but paper needs more quantitative results/qualitative inspections","rating":"6: Marginally above acceptance threshold","review":"Summary:\n\nThis paper studies how to learn (hidden)-state-space models of environment dynamics, and integrate them with Imagination-Augmented Agents (I2A). The paper considers single-agent problems and tests on Ms Pacman etc.\n\nThere are several variations of the hidden-state space [ds]SSM model: using det/stochastic latent variables + using det/stochastic decoders. In the stochastic case, learning is done using variational methods. \n\n[ds]SSM is integrated with I2A, which generates rollouts of future states, based on the inferred hidden states from the d/sSSM-VAE model. The rollouts are then fed into the agent's policy / value function.\n\nMain results seem to be:\n1. Experiments on learning the forward model, show that latent forward models work better and faster than naive AR models on several Atari games, and better than fully model-free baselines. \n2. I2A agents with latent codes work better than model-free models or I2A from pixels. Deterministic latent models seem to work better than stochastic ones.\n\nPro:\n- Relatively straightforward idea: learn the forward model on hidden states, rather than raw states.\n- Writing is clear, although a bit dense in places.\n\nCon:\n- Paper only shows training curves for MS Pacman. What about the other games from Table 1?\n- The paper lacks any visualization of the latent codes. What do they represent? Can we e.g. learn a raw-state predictor from the latent codes?\n- Are the latent codes relevant in the stochastic model? See e.g. the discussion in \"Variational Lossy Autoencoder\" (Chen et al. 2016)\n- Experiments are not complete (e.g. for AR, as noted in the paper).\n- The games used are fairly reactive (i.e. do not require significant long-term planning), and so the sequential hidden-state-space model does not have to capture long-term dependencies. It would be nice to see how this technique fares on Montezuma's revenge, for instance.\n\nOverall:\nThe paper proposes a simple idea that seems to work well on reactive 1-agent games. However, the paper could give more insights into *how* this works: e.g. a better qualitative inspection of the learned latent model, and how existing questions surrounding sequential stochastic model affect the proposed method. Also, not all baseline experiments are done, and the impact on training is only evaluated on 1 game. \n\nDetailed:\n-\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Dynamic State Abstractions for Model-Based Reinforcement Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed models that learn predictive and compact state representations, also called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment (ALE) from raw pixels. Furthermore, RL agents that use Monte-Carlo rollouts of these models as features for decision making outperform strong model-free baselines on the game MS_PACMAN, demonstrating the benefits of planning using learned dynamic state abstractions.","pdf":"/pdf/ce968e9b48c159ec367d6c3037fb859cb397a15f.pdf","paperhash":"anonymous|learning_dynamic_state_abstractions_for_modelbased_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Dynamic State Abstractions for Model-Based Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJw8fAgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper444/Authors"],"keywords":["generative models","probabilistic modelling","reinforcement learning","state-space models","planning"]}},{"tddate":null,"ddate":null,"tmdate":1509739300652,"tcdate":1509118542685,"number":444,"cdate":1509739297992,"id":"HJw8fAgA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJw8fAgA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Dynamic State Abstractions for Model-Based Reinforcement Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed models that learn predictive and compact state representations, also called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment (ALE) from raw pixels. Furthermore, RL agents that use Monte-Carlo rollouts of these models as features for decision making outperform strong model-free baselines on the game MS_PACMAN, demonstrating the benefits of planning using learned dynamic state abstractions.","pdf":"/pdf/ce968e9b48c159ec367d6c3037fb859cb397a15f.pdf","paperhash":"anonymous|learning_dynamic_state_abstractions_for_modelbased_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Dynamic State Abstractions for Model-Based Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJw8fAgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper444/Authors"],"keywords":["generative models","probabilistic modelling","reinforcement learning","state-space models","planning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}