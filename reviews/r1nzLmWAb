{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642394055,"tcdate":1511794432810,"number":3,"cdate":1511794432810,"id":"B1tWwoKxz","invitation":"ICLR.cc/2018/Conference/-/Paper1168/Official_Review","forum":"r1nzLmWAb","replyto":"r1nzLmWAb","signatures":["ICLR.cc/2018/Conference/Paper1168/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Official Reviewer 2","rating":"4: Ok but not good enough - rejection","review":"This paper discusses the problem of action segmentation in long videos, up to 10 minutes long. The basic idea is to use a temporal convolutional encoder-decoder architecture, where in the enconder 1-D temporal convolutions are used. In the decoder three variants are studied:\n\n(1) One that uses only several bidirectional LSTMs, one after the other.\n(2) One that first applies successive layers of deconvolutions to produce per frame feature maps. Then, in the end a bidirectional LSTM in the last layer.\n(3) One that first applies a bidirectional LSTM, then applies successively 1-D deconvolution layer.\n\nAll variants end with a \"temporal softmax\"  layer, which outputs a class prediction per frame.\n\nOverall, the paper is of rather limited novelty, as it is very similar to the work of Lea et al., 2017, where now the decoder part also has the deconvolutions smoothened by (bidirectional) LSTMs. It is not clear what is the main novelty compared to the aforementioned paper, other than temporal smoothing of features at the decoder stage.\n\nAlthough one of the proposed architectures (TricorNet) produces some modest improvements, it is not clear why the particular architectures are a good fit. Surely, deconvolutions and LSTMs can help incorporate some longer-term temporal elements into the final representations. However, to begin with, aren't the 1-D deconvolutions and the LSTMs (assuming they are computed dimension-wise) serving the same purpose and therefore overlapping? Why are both needed?\n\nSecond, what makes the particular architectures in Figure 3 the most reasonable choice for encoding long-term dependencies, is there a fundamental reason? What is the difference of the L_mid from the 1-D deconv layers afterward? Currently, the three variants are motivated in terms of what the Bi-LSTM can encode (high or low level details). \n\nThird, the qualitative analysis can be improved. For instance, the experiment with the \"cut lettuce\" vs \"peel cucumber\" is not persuasive enough. Indeed, longer temporal relationships can save incorrect future predictions. However, this works both ways, meaning that wrong past predictions can persist because of the long-term modelling. Is there a mechanism in the proposed approach to account for that fact?\n\nAll in all, I believe the paper indeed improves over existing baselines. However, the novelty is insufficient for a publication at this stage.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Video Action Segmentation with Hybrid Temporal Networks","abstract":"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","pdf":"/pdf/9cb1db4642c01584e6ca3c886e730f3743542a24.pdf","TL;DR":"We propose a new hybrid temporal network that achieves state-of-the-art performance on video action segmentation on three public datasets.","paperhash":"anonymous|video_action_segmentation_with_hybrid_temporal_networks","_bibtex":"@article{\n  anonymous2018video,\n  title={Video Action Segmentation with Hybrid Temporal Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1nzLmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1168/Authors"],"keywords":["action segmentation","video labeling","temporal networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642394092,"tcdate":1511774001591,"number":2,"cdate":1511774001591,"id":"HJ5VPLYxG","invitation":"ICLR.cc/2018/Conference/-/Paper1168/Official_Review","forum":"r1nzLmWAb","replyto":"r1nzLmWAb","signatures":["ICLR.cc/2018/Conference/Paper1168/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Video Action Segmentation with Hybrid Temporal Networks","rating":"3: Clear rejection","review":"I will be upfront: I have already reviewed this paper when it was submitted to NIPS 2017, so this review is based heavily on the NIPS submission. \n\nI am quite concerned that this paper has been resubmitted as it is, word by word, character by character. The authors could have benefited from the feedback they obtained from the reviewers of their last submissions to improved their paper, but nothing has been done. Even very easy remarks, like bolding errors (see below) have been kept in the paper.\n\nThe proposed paper describes a method for video action segmentation, a task where the video must be temporally densely labeled by assigned an action (sub) class to each frame. The method proceeds by extracting frame level features using convolutional networks and then passing a temporal encoder-decoder in 1D over the video, using fully supervised training.\n\nOn the positive side, the method has been tested on 3 different datasets, outperforming the baselines (recent methods from 2016) on 2 of them.\n\nMy biggest concern with the paper is novelty. A significant part of the paper is based on reference [Lea et al. 2017], the differences being quite incremental. The frame-level features are the same as in [Lea et al. 2017], and the basic encoder-decoder strategy is also taken from [Lea et al. 2017]. The encoder is also the same. Even details are reproduced, as the choice of normalized Relu activations.\n\nThe main difference seems to me that the decoder is not convolutional, but a recurrent network.\n\nThe encoder-decoder architecture seems to be surprisingly shallow, with only K=2 layers at each side.\n\nThe paper is well written and can be easily understood. However, a quite large amount of space is wasted on obvious and known content, as for example the basic equation for a convolutional layer (equation (1)) and the following half page of text and equations of LSTM and Bi-directional LSTM networks. This is very well known and the space can be used for more details on the paper's contributions.\n\nWhile the paper is generally well written, there are a couple of exceptions in the form of ambiguous sentences, for example the lines before section 3.\n\nThere is a bolding error in table 2, where the proposed method is not state of the art (as indicated) w.r.t. to the accuracy metric.\n\nTo sum it up, the positive aspect of nicely executed experiments is contrasted by low novelty of the method.  To be honest, I am not totally sure whether the contribution of the paper should be considered as a new method or as architectural optimizations of an existing one. This is corroborated by the experimental results on the first two datasets (tables 2 and 3): on 50 salads, where ref. [Lea et al. 2017]. seems currently to obtain state of the art performance, the improvement obtained by the proposed method allows it to get state of the art performance. On GTEA, where [Lea et al. 2017] does not currently deliver state of the art performance, the proposed method performs (slightly) better than [Lea et al. 2017] but does not obtain state of the art performance.\n\nOn the third dataset, JIGSAWS, reference [Lea et al. 2017]. has not been tested, which is peculiar given the closeness.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Video Action Segmentation with Hybrid Temporal Networks","abstract":"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","pdf":"/pdf/9cb1db4642c01584e6ca3c886e730f3743542a24.pdf","TL;DR":"We propose a new hybrid temporal network that achieves state-of-the-art performance on video action segmentation on three public datasets.","paperhash":"anonymous|video_action_segmentation_with_hybrid_temporal_networks","_bibtex":"@article{\n  anonymous2018video,\n  title={Video Action Segmentation with Hybrid Temporal Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1nzLmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1168/Authors"],"keywords":["action segmentation","video labeling","temporal networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642394128,"tcdate":1511714232303,"number":1,"cdate":1511714232303,"id":"H1lTTDulG","invitation":"ICLR.cc/2018/Conference/-/Paper1168/Official_Review","forum":"r1nzLmWAb","replyto":"r1nzLmWAb","signatures":["ICLR.cc/2018/Conference/Paper1168/AnonReviewer3"],"readers":["everyone"],"content":{"title":"lacking in terms of novelty","rating":"4: Ok but not good enough - rejection","review":"The paper proposed a combination of temporal convolutional and recurrent network for video action segmentation. Overall this paper is written and easy to follow.\n\nThe novelty of this paper is very limited. It just replaces the decoder of ED-TCN (Lea et al. 2017) with a bi-directional LSTM. The idea of applying bi-directional LSTM is also not new for video action segmentation. In fact, ED-TCN used it as one of the baselines. The results also do not show much improvement over ED-TCN, which is much easier and faster to train (as it is fully convolutional model) than the proposed model. Another concern is that the number of layers parameter 'K'. The authors should show an analysis on how the performance varies for different values of 'K' which I believe is necessary to judge the generalization of the proposed model. I also suggest to have an analysis on entire convolutional model (where the decoder has 1D-deconvolution) to be included in order to get a clear picture of the improvement in performance due to bi-directional LSTM . Overall, I believe the novelty, contribution and impact of this work is sub-par to what is expected for publication in ICLR. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Video Action Segmentation with Hybrid Temporal Networks","abstract":"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","pdf":"/pdf/9cb1db4642c01584e6ca3c886e730f3743542a24.pdf","TL;DR":"We propose a new hybrid temporal network that achieves state-of-the-art performance on video action segmentation on three public datasets.","paperhash":"anonymous|video_action_segmentation_with_hybrid_temporal_networks","_bibtex":"@article{\n  anonymous2018video,\n  title={Video Action Segmentation with Hybrid Temporal Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1nzLmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1168/Authors"],"keywords":["action segmentation","video labeling","temporal networks"]}},{"tddate":null,"ddate":null,"tmdate":1510092379066,"tcdate":1509139988227,"number":1168,"cdate":1510092359181,"id":"r1nzLmWAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1nzLmWAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Video Action Segmentation with Hybrid Temporal Networks","abstract":"Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.","pdf":"/pdf/9cb1db4642c01584e6ca3c886e730f3743542a24.pdf","TL;DR":"We propose a new hybrid temporal network that achieves state-of-the-art performance on video action segmentation on three public datasets.","paperhash":"anonymous|video_action_segmentation_with_hybrid_temporal_networks","_bibtex":"@article{\n  anonymous2018video,\n  title={Video Action Segmentation with Hybrid Temporal Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1nzLmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1168/Authors"],"keywords":["action segmentation","video labeling","temporal networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}