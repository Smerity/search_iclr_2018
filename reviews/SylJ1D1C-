{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222565876,"tcdate":1512038545191,"number":3,"cdate":1512038545191,"id":"SJt5gvplM","invitation":"ICLR.cc/2018/Conference/-/Paper128/Official_Review","forum":"SylJ1D1C-","replyto":"SylJ1D1C-","signatures":["ICLR.cc/2018/Conference/Paper128/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A promising approach on nonparametric modelling of partial differential equations with deep architectures that requires more details.","rating":"5: Marginally below acceptance threshold","review":"This paper addresses complex dynamical systems modelling through nonparametric Partial Differential Equations using neural architectures. It falls down within the context of a recent and growing literature on the subject.\n\nThe most important idea of the papier (PDE-net) is to learn both differential operators and the function that governs the PDE. To achieve this goal, the approach relies on the approximation of differential operators by convolution of filters of appropriate order.  This is really the strongest point of the paper.\n\nMoreover, a basic system called delta t block implements one level of full approximation and is stoked several times.\nA short section relates this work to recent existing work and numerical results are deployed on simulated data.\nIn particular, the interest of learning the filters involved in the approximation of the differential operators is tested against a frozen variant of the PDE-net.\n\nComments:\nThe paper is badly structured and is sometimes hard to read because it does not present in a linear way the classic ingredients of Machine Learning, expression of the full function to be estimated, equations of each layer, description of the set of parameters to be learned and the loss function. Parts of the puzzle have to be found in the core of the paper as well as in simulations.\n\nAbout the loss function, I was surprised not to see a sparsity constraint on the different filters in order to select the order of the differential operators themselves. If one want to achieve interpretability of the resulting PDE, this is very important.\n\nI also found difficult to measure the degree of novelty of the approach considering the recent works and  the related work section should have been much more precise in terms of comparison. \n\nFor  the simulations, it is perfectly fine to rely on simulated datasets. However the approach is not compared to the closest works (Sonoda et al., for instance).\n\nFinally, I’ve found the paper very interesting and promising but regarding the standard of scientific publication, it requires additional attention to provide a better description the model and discuss the learning scheme to get a strongest and reproducible approach.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PDE-Net: Learning PDEs from Data","abstract":"Partial differential equations (PDEs)  play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.","pdf":"/pdf/21e36c078c021899092b15982787e2eaf2890789.pdf","TL;DR":"This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data. ","paperhash":"anonymous|pdenet_learning_pdes_from_data","_bibtex":"@article{\n  anonymous2018pde-net:,\n  title={PDE-Net: Learning PDEs from Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SylJ1D1C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper128/Authors"],"keywords":["deep convolution network","partial differential equation","physical laws"]}},{"tddate":null,"ddate":null,"tmdate":1512222565916,"tcdate":1511774091963,"number":2,"cdate":1511774091963,"id":"rJVcvUYlz","invitation":"ICLR.cc/2018/Conference/-/Paper128/Official_Review","forum":"SylJ1D1C-","replyto":"SylJ1D1C-","signatures":["ICLR.cc/2018/Conference/Paper128/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novel method, interesting approach, interpretability might be over-emphasized. ","rating":"8: Top 50% of accepted papers, clear accept","review":"Authors propose a neural network based algorithm for learning from data that arises from dynamical systems with governing equations that can be written as partial differential equations.  The network architecture is constrained such that regardless of the parameters, it always implements discretization of an arbitrary PDE.  Through learning, the network adapts itself to solve a specific PDE.  Discretization is finite difference in space and forward Euler in time.  \n\nThe article is quite novel in my opinion.  To the best of my knowledge, it is the first article that implements a generic method for learning arbitrary PDE models from data.  In using networks, the method differs from previously proposed approaches for learning PDEs.  Experiments are only presented with synthetic data but given the potential for the method and its novelty, I believe this can be accepted.  However, it would have been a stronger article if authors have applied to a real life model with real initial and boundary conditions, and real observations. \n\nI have three main criticism about the article: \n\n 1.   Authors do not cite Chen and Pock’s article on learning diffusion filters with networks that first published in CVPR 2015 and then authors published a PAMI article this year.  To the best of my knowledge, they are the first to show the connection between res-net type architecture and numerical solutions of PDEs. I think proper credit should be given. [I need to emphasize that I am not an author in that article.] \n 2.   Authors emphasize the importance of interpretability, however, the constraint on the moment matrices might cripple this aspect.  The frozen filters have clear interpretations. They are in the end finite difference approximations with some level of accuracy depending on the size and the number of zeros.  When M(q) matrix is free to change, it is unclear what the effect will be on the filters.  Are the numbers that replace stars in Equation 6 for instance, will be absorbed in the O(\\epsilon) term?  Can one really interpret the final c_{ij} for filters whose M(q) have many non-zeros? \n 3.   The introduction and results sections are well written.  The method section on the other hand, needs improvement.  The notation is not easy to follow due to missing definitions.  I believe with proper definitions — amounting to small modifications — readability of the article can substantially improve. \n\n\nIn addition to the main criticisms, I have some other questions and concerns: \n\n  1.  How sensitive is the model?  In real life, one cannot expect to get observations every delta t.  Data is most often very sparse.  Can the model learn in that regime?  Can it differentiate between different PDEs and find the correct one with sparse data? \n  2.  The average operations decreases the interpretability of the proposed model as a PDE.  Depending on the filter size, D_{0}u can deviate from u, which should be the term that should be used in the residual block.  Why do authors need this?  How does the model behave without it? \n  3.  The statement “Thus, the PDE-Net with bigger n owns a longer time stability.” is a very vague statement.  I understand with larger n, training would be easier since more data would be used to estimate parameters.  However, it is not clear how this relates to “time stability”, which is also not defined in the article. \n 4.  How is the relative error computed?  Values in relative error plots goes as high as 10^2.  That would be a huge error if it is relative. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PDE-Net: Learning PDEs from Data","abstract":"Partial differential equations (PDEs)  play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.","pdf":"/pdf/21e36c078c021899092b15982787e2eaf2890789.pdf","TL;DR":"This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data. ","paperhash":"anonymous|pdenet_learning_pdes_from_data","_bibtex":"@article{\n  anonymous2018pde-net:,\n  title={PDE-Net: Learning PDEs from Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SylJ1D1C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper128/Authors"],"keywords":["deep convolution network","partial differential equation","physical laws"]}},{"tddate":null,"ddate":null,"tmdate":1512222565956,"tcdate":1511638246770,"number":1,"cdate":1511638246770,"id":"B1JeHrDlf","invitation":"ICLR.cc/2018/Conference/-/Paper128/Official_Review","forum":"SylJ1D1C-","replyto":"SylJ1D1C-","signatures":["ICLR.cc/2018/Conference/Paper128/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper on the use of deep learning for modeling and identifying dynamical PDE based systems","rating":"7: Good paper, accept","review":"The paper explores the use of deep learning machinery for the purpose of identifying dynamical systems specified by PDEs.\n\nThe paper advocates the following approach:\nOne assumes a dynamic PDE system involving differential operators up to a given order. Each differential operator term is approximated by a filter whose values are properly constrained so as to correspond to finite difference approximations of the corresponding term. The paper discusses the underlying wavelet-related theory in detail. A certain form of the dynamic function and/or source terms is also assumed. An explicit Euler scheme is adopted for time discretization. The parameters of the system are learned by minimizing the approximation error at each timestep. In the experiments reported in the paper the reference signal is provided by numerical simulation of a ground truth system and the authors compare the prediction quality of different versions of their system (eg, for different kernel size).\n\nOverall I find the paper good, well written and motivated. The advocated approach should be appealing for scientific applications of deep learning where not only the quality of approximation but also the interpretability of the identified model is important.\n\nSome suggestions for improvement:\n* The paper doesn't discuss the spatial boundary conditions. Please clarify this.\n* The paper adopts a hybrid approach that lies in between the classic fully analytical PDE approach and the data driven machine learning. I would like to see a couple more experiments comparing the proposed approach with those extremes. (1) in the first experiment, the underlying model order is 2 but the experiment allows filters up to order 4. Can you please report if generalization quality improves if the correct order 2 is specified? (2) On the other side, what happens if no sum (vanishing order) constraints are enforced during model training? This abandons the interpretability of the model as approximating a PDE of given order but I am curious to see what is the generalization error of this less constrained system.\n\nNit: bibtex error in Weinan (2017) makes the paper appear as E (2017).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PDE-Net: Learning PDEs from Data","abstract":"Partial differential equations (PDEs)  play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.","pdf":"/pdf/21e36c078c021899092b15982787e2eaf2890789.pdf","TL;DR":"This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data. ","paperhash":"anonymous|pdenet_learning_pdes_from_data","_bibtex":"@article{\n  anonymous2018pde-net:,\n  title={PDE-Net: Learning PDEs from Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SylJ1D1C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper128/Authors"],"keywords":["deep convolution network","partial differential equation","physical laws"]}},{"tddate":null,"ddate":null,"tmdate":1509739469895,"tcdate":1509023447988,"number":128,"cdate":1509739467241,"id":"SylJ1D1C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SylJ1D1C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"PDE-Net: Learning PDEs from Data","abstract":"Partial differential equations (PDEs)  play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.","pdf":"/pdf/21e36c078c021899092b15982787e2eaf2890789.pdf","TL;DR":"This paper proposes a new feed-forward network, call PDE-Net, to learn PDEs from data. ","paperhash":"anonymous|pdenet_learning_pdes_from_data","_bibtex":"@article{\n  anonymous2018pde-net:,\n  title={PDE-Net: Learning PDEs from Data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SylJ1D1C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper128/Authors"],"keywords":["deep convolution network","partial differential equation","physical laws"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}