{"notes":[{"tddate":null,"ddate":null,"tmdate":1515676310014,"tcdate":1515676310014,"number":4,"cdate":1515676310014,"id":"H1A5fJSNf","invitation":"ICLR.cc/2018/Conference/-/Paper298/Official_Comment","forum":"H1YynweCb","replyto":"BkrBLwSMz","signatures":["ICLR.cc/2018/Conference/Paper298/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper298/AnonReviewer1"],"content":{"title":"Rebuttal comments","comment":"-Experiments varying 'pf' and 'qf':\n\nOk! It would have been nice to have it in the text (when presenting the hyper-parameters of the networks) instead of just the raw number of parameters, and show how changing them influences the performances.\n\n-\"Are you using soft unitary constraints?\":\n\nAgain, adding in the text (or the appendix) the values that you used for the soft unitary constraints would have been nice. \nThanks for the experiments in section 4.7. It really shows that there is a sweet spot for the constraint and that it needs to be carefully tuned.\n\n-Regarding correctness (\"Why does your LSTM in pMNIST performs so poorly?\") :\n\nOh yeah, my bad! I was off by one order of magnitude in the number of training steps!\n\n-Regarding experiments on copy memory problem:\n\nSince you also compare with the unitary RNN family, I now think your comparisons are fair. So yeah maybe you don't need a one page experiment to show that we don't need to train the Wh matrix if it is unitary.\n\n-\"How does your method compares against other factorization approaches?\"\n\"2) It gives us more fine-grained control over the number of parameters, from O(log(N)) to O(N^2) and hence on the speed-accuracy trade-off.\"\n\nAgain, an experiment showing this trade-off would have been nice.\n\n-\"How does KRU compares to other parameterization, in term of wall-clock time?, Add computation time.\"\n\nNice! Now I can really appreciate how faster your method is.\n\n-\"Compare more clearly setups where you fix the hidden size and number of parameters\"\n\nOk! My concern was that sometimes more parameters does not lead to better generalization (since the model can over-fit more easily), and since you only show the validation curves and we don't know all the hyper-parameters, it is difficult to asses what is the cause of the difference in performances.\nOverall, I still can't asses if the gains in performances are due to the unitary constraint, the reduced number of parameters, or a combination of both.\n\n-\"Present results on large scale applications\":\n\nI understand, but in the worse case you can still  perform the Kronecker product of the matrices before the recurrence and then call the cuDNN RNN cell. It is clearly not ideal, but I doubt it will slow down the training process up to the point it is impossible to run experiments.\n\n- \"Thanks a lot for the detailed comments we have fixed the typos in the paper.\"\n\nSome of them haven't been fixed :)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kronecker Recurrent Units","abstract":"Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.","pdf":"/pdf/6840ec92b489a3f4342e7cb4c71c633428162fbd.pdf","TL;DR":"Out work presents a Kronecker factorization of recurrent weight matrices for parameter efficient and well conditioned recurrent neural networks.","paperhash":"anonymous|kronecker_recurrent_units","_bibtex":"@article{\n  anonymous2018kronecker,\n  title={Kronecker Recurrent Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1YynweCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper298/Authors"],"keywords":["Recurrent neural network","Vanishing and exploding gradients","Parameter efficiency","Kronecker matrices","Soft unitary constraint"]}},{"tddate":null,"ddate":null,"tmdate":1515642427328,"tcdate":1511822105335,"number":3,"cdate":1511822105335,"id":"HkZmXGcxf","invitation":"ICLR.cc/2018/Conference/-/Paper298/Official_Review","forum":"H1YynweCb","replyto":"H1YynweCb","signatures":["ICLR.cc/2018/Conference/Paper298/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A nice idea, well presented","rating":"7: Good paper, accept","review":"Typical recurrent neural networks suffer from over-paramterization. Additionally, standard RNNs (non-gated versions) have an ill-conditioned recurrent weight matrix, leading to vanishing/exploding gradients during training. This paper suggests to factorize the recurrent weight matrix as a Kronecker product of matrices. Additionally, in order to avoid vanishing/exploding gradients in standard RNNs, a soft unitary constraint is used. The regularizer is specifically nice in this setting, as it suffices to have the Kronecker factors be unitary. In the empirical section, several RNNs are trained using this approach, using only ~ 100 recurrent parameters, and still achieve comparable results to state-of-the-art approaches. The paper argues that the recurrent state should be high-dimensional (in order to be able to encode the input and extract predictive information) but the recurrent dynamic should be realized by a low-capacity model.\n\nQuality: The paper is well written.\n\nClarity: Main ideas are clearly presented.\n\nOriginality/Significance: Kronecker factorization was introduced for Convolutional networks (citation is in the paper). Soft unitary constraints also have been introduced in earlier work (citations are also in the paper). Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution. Additionally, this approach allows a significant reduction of training time it seems.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kronecker Recurrent Units","abstract":"Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.","pdf":"/pdf/6840ec92b489a3f4342e7cb4c71c633428162fbd.pdf","TL;DR":"Out work presents a Kronecker factorization of recurrent weight matrices for parameter efficient and well conditioned recurrent neural networks.","paperhash":"anonymous|kronecker_recurrent_units","_bibtex":"@article{\n  anonymous2018kronecker,\n  title={Kronecker Recurrent Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1YynweCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper298/Authors"],"keywords":["Recurrent neural network","Vanishing and exploding gradients","Parameter efficiency","Kronecker matrices","Soft unitary constraint"]}},{"tddate":null,"ddate":null,"tmdate":1515642427368,"tcdate":1511809876185,"number":2,"cdate":1511809876185,"id":"Hy28Xy9lM","invitation":"ICLR.cc/2018/Conference/-/Paper298/Official_Review","forum":"H1YynweCb","replyto":"H1YynweCb","signatures":["ICLR.cc/2018/Conference/Paper298/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea, but weak experimental setup","rating":"5: Marginally below acceptance threshold","review":"\nSummary of the paper\n-------------------------------\n\nThis paper proposes to factorize the hidden-to-hidden matrix of RNNs into a Kronecker product of small matrices, thus reducing the number of parameters, without reducing the size of the hidden vector. They also propose to use a soft unitary constraint on those small matrices (which is equivalent to a soft unitary constraint on the Kronecker product of those matrices), that is fast to compute. They evaluate their model on 6 small scale RNN experiments.\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: The main idea is clearly motivated and presented, but the experiment section failed to convince me (see details below).\n\nSignificance: The idea of using factorization for RNNs is not particularly novel. However, it is really nice to be able to decouple the hidden size and the number of recurrent parameters in a simple way. Also, the combination of Kronecker product and soft unitary constraint is really interesting.\n\nCorrectness: There are minor flaws. Some of the baselines seems to perform poorly, and some comparisons with the baselines seems unfair (see the questions below).\n\nQuestions\n--------------\n\n1. Section 3: You say that you can vary 'pf' and 'qf' to set the trade-off between computational budget and performances. Have you run some experiments where you vary those parameters?\n2. Section 4: Are you using the soft unitary constraint in your experiments? Do you have an hyper-parameter that sets the amplitude of the constraint? If yes, what is its value? Are you using it also on the vanilla RNN or the LSTM?\n3. Section 4.1: You say that you don't train the recurrent matrix in the KRU version. Do you also not train the recurrent matrix in the other models (RNN, LSTM,...)? If yes, how do you explain the differences? If no, I don't see how those curves compare.\n4. Section 4.3: Why does your LSTM in pMNIST performs so poorly? There are way better curves reported in the literature (eg in \"Unitary Evolution Recurrent Neural Netwkrs\" or \"Recurrent Batch Normalization\").\n5. General: How does your method compares with other factorization approaches, such as in \"Factorization Tricks for LSTM Networks\"?\n6. Section 4: How does the KRU compares to the other parametrizations, in term of wall-clock time?\n\nRemarks\n------------\n\nThe main claim of the paper is that RNN are over-parametrized and take a long time to train (which I both agree with), but you didn't convinced me that your parametrization solve any of those problems. I would suggest to:\n1. Compare more clearly setups where you fix the hidden size.\n2. Compare more clearly setups where you fix the number of parameters.\nWith systematic comparisons like that, it would be easier to understand where the gains in performances are coming from.\n3. Add an experiment where you vary 'pf' and 'qf' (and keep the hidden size fixed) to show how the optimization/generalization performances can be tweaked.\n4. Add computation time (wall-clock) for all the experiments, to see how it compares in practice (this could definitively weight in your favor, since you seems to have a nice CUDA implementation).\n5. Present results on larger-scale applications (Text8, Teaching Machines to Read and Comprehend, 3 layers LSTM speech recognition setup on TIMIT, DRAW, Machine Translation, ...), especially because your method is really easy to plug in any existing code available online.\n\nTypos / Form\n------------------\n\n1. sct 1, par 3: \"using Householder reflection vectors, it allows a fine-grained\" -> \"using Householder reflection vectors, which allows a fine-grained\"\n2. sct 1, par 3: \"This work called as Efficient\" -> \"This work, called Efficient\"\n5. sct 1, par 5: \"At the heart of KRU is the use of Kronecker\" -> \"At the heart of KRU, we use Kronecker\"\n6. sct 1, par 5: \"Thanks to the properties of Kronecker matrices\" -> \"Thanks to the properties of the Kronecker product\"\n7. sct 1, par 5: \"vanilla real space RNN\" -> \"vanilla RNN\"\n8. sct 2, par 1: \"Consider a standard recurrent\" -> \"Consider a standard vanilla recurrent\"\n9. sct 2, par 1: \"step t RNN\" -> \"step t, a vanilla RNN\"\n11. sct 2.1, par 1: \"U and V, this is efficient using modern BLAS\" -> \"U and V, which can be efficiently computed using modern BLAS\"\n12. sct 2.3, par 2: \"matrices have a determinant of 1 or −1, i.e., the set of all rotations and reflections respectively\" -> \"matrices, i.e., the set of all rotations and reflections, have a determinant of 1 or −1.\"\n13. sct 3, par 1: \"are called as Kronecker\" -> \"are called Kronecker\"\n14. sct 3, par 3: \"used it's spectral\" -> \"used their spectral\"\n15. sct 3, par 3: \"Kronecker matrices\" -> \"Kronecker products\"\n18. sct 4.4, par 3: \"parameters are increased\" -> \"parameters increases\"\n19. sct 5: There is some more typos in the conclusion (\"it's\" -> \"its\")\n20. Some plots are hard to read / interpret, mostly because of the round \"ticks\" you use on the curves. I suggest you remove them everywhere. Also, in the adding problem, it would be cleaner if you down-sampled a bit the curves (as they are super noisy). In pixel by pixel MNIST, some of the legends might have some typos (FC uRNN), and you should use \"N\" instead of \"n\" to be consistent with the notation of the paper.\n21. Appendix A to E are not necessary, since they are from the literature.\n22. sct 3.1, par 2: \"is approximately unitary.\" -> \"is approximately unitary (cf Appendix F).\"\n23. sct 4, par 1: \"and backward operations.\" -> \"and backward operations (cf Appendix G and H).\"\n\nPros\n------\n\n1. Nice Idea that allows to decouple the hidden size with the number of hidden-to-hidden parameters.\n2. Cheap soft unitary constraint\n3. Efficient CUDA implementation (not experimentally verified)\n\nCons\n-------\n\n1. Some experimental setups are unfair, and some other could be clearer\n2. Only small scale experiments (although this factorization has huge potential on larger scale experiments)\n3. No wall-clock time that show the speed of the proposed parametrization.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Kronecker Recurrent Units","abstract":"Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.","pdf":"/pdf/6840ec92b489a3f4342e7cb4c71c633428162fbd.pdf","TL;DR":"Out work presents a Kronecker factorization of recurrent weight matrices for parameter efficient and well conditioned recurrent neural networks.","paperhash":"anonymous|kronecker_recurrent_units","_bibtex":"@article{\n  anonymous2018kronecker,\n  title={Kronecker Recurrent Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1YynweCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper298/Authors"],"keywords":["Recurrent neural network","Vanishing and exploding gradients","Parameter efficiency","Kronecker matrices","Soft unitary constraint"]}},{"tddate":null,"ddate":null,"tmdate":1516385916693,"tcdate":1511321587764,"number":1,"cdate":1511321587764,"id":"ByhgguzeM","invitation":"ICLR.cc/2018/Conference/-/Paper298/Official_Review","forum":"H1YynweCb","replyto":"H1YynweCb","signatures":["ICLR.cc/2018/Conference/Paper298/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice idea, more exploration is needed","rating":"6: Marginally above acceptance threshold","review":"The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices. Given N inputs and output, this method allows one to specify a linear transformation with O(log(N)) parameters, and perform a forward and backward pass in O(Nlog(N)) time.  \nIn addition a relaxation is performed allowing each constituent to deviate a bit from unitarity (“soft unitary constraint”).\nThe paper shows nice results on a number of small tasks. \n\nThe idea is original to the best of my knowledge and is presented clearly.\nI especially like the idea of “soft unitary constraint” which can be applied very efficiently in this factorized setup. I think this is the main contribution of this work.\n\nHowever the paper in its current form has a number of problems:\n\n- The authors state that a major shortcoming of previous (efficient) unitary RNN methods is the lack of ability to span the entire space of unitary matrices. This method presents a family that can span the entire space, but the efficient parts of this family (which give the promised speedup) only span a tiny fraction of it, as they require only O(log(N)) params to specify an O(N^2) unitary matrix. Indeed in the experimental section only those members are tested.\n\n- Another claim that is made is that complex numbers are key, and again the argument is the need to span the entire space of unitary matrices, but the same comment still hold - that is not the space this work is really dealing with, and no experimental evidence is provided that using complex numbers was really needed.\n\n- In the experimental section an emphasis is made as to how small the number of recurrent params are, but at the same time the input/output projections are very large, leaving the reader wondering if the workload simply shifted from the RNN to the projections. This needs to be addressed.\n\n- Another aspect of the previous points is that it’s not clear if stacking KRU layers will work well. This is important as stacking LSTMs is a common practice. Efficient KRU span a restricted subspace whose elements might not compose into structures that are expressive enough. One way to overcome this potential problem is to add projection matrices between layers that will do some mixing, but this will blow the number of parameters. This needs to be explored.\n\n- The authors claim that the soft unitary constraint was key for the success of the network, yet no details are provided as to how this constraint was applied, and no analysis was made for its significance. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Kronecker Recurrent Units","abstract":"Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.","pdf":"/pdf/6840ec92b489a3f4342e7cb4c71c633428162fbd.pdf","TL;DR":"Out work presents a Kronecker factorization of recurrent weight matrices for parameter efficient and well conditioned recurrent neural networks.","paperhash":"anonymous|kronecker_recurrent_units","_bibtex":"@article{\n  anonymous2018kronecker,\n  title={Kronecker Recurrent Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1YynweCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper298/Authors"],"keywords":["Recurrent neural network","Vanishing and exploding gradients","Parameter efficiency","Kronecker matrices","Soft unitary constraint"]}},{"tddate":null,"ddate":null,"tmdate":1513612393289,"tcdate":1509092321243,"number":298,"cdate":1509739375933,"id":"H1YynweCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1YynweCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Kronecker Recurrent Units","abstract":"Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.","pdf":"/pdf/6840ec92b489a3f4342e7cb4c71c633428162fbd.pdf","TL;DR":"Out work presents a Kronecker factorization of recurrent weight matrices for parameter efficient and well conditioned recurrent neural networks.","paperhash":"anonymous|kronecker_recurrent_units","_bibtex":"@article{\n  anonymous2018kronecker,\n  title={Kronecker Recurrent Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1YynweCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper298/Authors"],"keywords":["Recurrent neural network","Vanishing and exploding gradients","Parameter efficiency","Kronecker matrices","Soft unitary constraint"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}