{"notes":[{"tddate":null,"ddate":null,"tmdate":1512424276020,"tcdate":1512424276020,"number":3,"cdate":1512424276020,"id":"BynUQBQZM","invitation":"ICLR.cc/2018/Conference/-/Paper253/Official_Review","forum":"SyXNErg0W","replyto":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference/Paper253/AnonReviewer3"],"readers":["everyone"],"content":{"title":"simple tweak to softmax","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a regularization to the softmax layer, which try to make the distribution of feature representation (inputs fed to the softmax layer) more meaningful according to the Euclidean distance. The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean, so the features are encouraged to lie close to a sphere. Overall, the proposed method is a relatively simple tweak to softmax. The authors show that empirically, features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks.\n\nMy main concern with this paper is the motivation: what are the practical scenarios in which one would want to used proposed method?\n1. It is true that features learned with the pure softmax loss may not presents the ideal  similarity under the  Euclidean metric (e.g. the problem depicted in Figure 1),  because they are not trained to do so: their purpose is just to predict the correct label.  While the proposed regularization does lead to a nicer Euclidean geometry, there is not sufficient motivation and evidence showing this regularization improves classification accuracy.\n\n2. In table 2, the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage. But this does not matter since you already use the labels in the softmax loss.\n\n3. I can not easily think of scenarios in which, we would like to perform KNN in the feature space (Table 3) after training a softmax layer. In fact, Table 3 shows KNN is almost always worse than softmax in terms of classification accuracy. \n\n4. Running kmeans or agglomerative clustering in the feature space (Table 5) *using the Euclidean metric* is again ill-posed, because the softmax layer is not trained to do this. If one really wants good clustering performance, one shall always try to learn a good metric, or , why do not you perform clustering on the softmax output (a probability vector?)\n\n5.  The experiments on adversarial robustness and face verification seems more interesting to me, but the tasks were not carefully explained for someone not familiar with that literature. Perhaps for these tasks, multi-class classification is not the most correct objective, and maybe the proposed regularization can help, but the motivations are not given. \n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222600362,"tcdate":1511856332034,"number":2,"cdate":1511856332034,"id":"S1EAO5qxM","invitation":"ICLR.cc/2018/Conference/-/Paper253/Official_Review","forum":"SyXNErg0W","replyto":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference/Paper253/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper presented a modification to the Center loss softmax for feature learning","rating":"4: Ok but not good enough - rejection","review":"In the centre loss, the centre is learned. Now it's calculated as the average of the last layer's features\nTo enable training with SGD, the authors calculate the centre within a mini batch","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222600407,"tcdate":1511680399560,"number":1,"cdate":1511680399560,"id":"Hyd9YyOlf","invitation":"ICLR.cc/2018/Conference/-/Paper253/Official_Review","forum":"SyXNErg0W","replyto":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference/Paper253/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A solid empirical study on an isotropic  softmax loss function","rating":"6: Marginally above acceptance threshold","review":"The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space. The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class. The proposed loss has been compared extensively against a number of closely related approaches in methodology. Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means. \n\nPros: \n\n- The idea of isotropic normalization for enhancing compactness of class is well motivated\n\n- The paper is mostly clearly organized and presented.\n\n- Numerical study shows some promise of the proposed method.\n\nCons:\n\n-  The novelty of method is mostly incremental given the prior work of (Wen et al., 2016) which has provided a slightly different isotropic variant of softmax loss.\n\n- The training procedure of the proposed method remains unclear in this paper. \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739402549,"tcdate":1509082154917,"number":253,"cdate":1509739399887,"id":"SyXNErg0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}