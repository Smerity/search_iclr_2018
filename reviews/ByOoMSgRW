{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222599598,"tcdate":1512000538618,"number":3,"cdate":1512000538618,"id":"S1Q73Tnlf","invitation":"ICLR.cc/2018/Conference/-/Paper249/Official_Review","forum":"ByOoMSgRW","replyto":"ByOoMSgRW","signatures":["ICLR.cc/2018/Conference/Paper249/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper needs to be better situated and more clearly focused on its main contribution and the results need to be more convincing in terms of generalization/novelty and, ideally, musicality as well.","rating":"5: Marginally below acceptance threshold","review":"The paper proposes to facilitate generation of melody by representing notes as \"words\", representing all of the note's properties.  Using this representation allows the generation of musical \"sentences\", conditioned on a harmonic chord sequence or part/voice information.\n\nThe paper seems to be presenting two principle ideas: first, the idea of representing a note and all of its properties as a unique \"word\" and thus generating melody as a sort of \"sentence\" on these types. This is not particularly novel as it echoes back to the idea of derived types from primitive types in multiple viewpoint systems of Conklin and Witten 1993 (which should probably be cited here). It has also been done in RNNs previously (Colombo, Florian, et al. \"Algorithmic composition of melodies with deep recurrent neural networks.\" arXiv preprint arXiv:1606.07251 (2016).)\n\nThe second idea presented is using a LSTM for generating melodies using this data representation conditioned on chord sequences. It is not readily clear, however, how this paper differentiates its approach from others that use LSTMs and, if the title and abstract are any indication, the LSTM method of generating melodies does not appear to be the intended contribution of this paper. \n\nThough the first idea is not novel per se, it may be novel when used in LSTMs. This is where I might suggest the authors focus revisions to the paper, situating the work clearly within the domain of LSTMs, clearly differentiating it from models which purport to do something similar. I am not very familiar with the work related to LSTMs in generating melodies conditioned on harmony, so the work may be novel in that sense, but I suspect there is a plethora of related works, and likely several more closely related than the two cited in the paper as it now stands.\n\nThe paper is well-written as far as grammaticality and spelling. The related works deal with \"automatic music composition\" in general, thus making it difficult to know whether the contribution relates more to the \"word\" representation of notes or to their use in LSTMs. \n\nA full paragraph in the related works is dedicated to WaveNet, which, as the authors mention, is out of scope of this paper, and raises questions as to why it is included and further distracts from what might be the paper's focus.\n\nOne could argue that combining viewpoints into a single \"word\" representation will result in the model doing more memorization than learning, which could also explain the model's reported improvements over other models. This is particularly true given that the training data is relatively small (46 songs) and comes from \"unpublished materials from semiprofessional musicians,\" and thus a model which was overfit to such data would likely not be readily detectable by any except those familiar with the training data. Is there any way to compare originality between the models to ensure that it is both better than and equally original to previous models?\n\nI do like the fact that they provided listenable examples.  However, I didn't find the examples very compelling or convincing (particularly in the sense of overall structure and diversity).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Melody Generation for Pop Music via Word Representation of Musical Properties","abstract":"Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors. Representation\nof multivariate property of notes has been one of the primary challenges.\nIt is also difficult to remain in the permissible spectrum of musical variety, outside\nof which would be perceived as a plain random play without auditory pleasantness.\nObserving the conventional structure of pop music poses further challenges.\nIn this paper, we propose to represent each note and its properties as a unique\n‘word,’ thus lessening the prospect of misalignments between the properties, as\nwell as reducing the complexity of learning. We also enforce regularization policies\non the range of notes, thus encouraging the generated melody to stay close\nto what humans would find easy to follow. Furthermore, we generate melody\nconditioned on song part information, thus replicating the overall structure of a\nfull song. Experimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models.","pdf":"/pdf/407c081b7c28c1f60b501cae992aefab2bf0e48f.pdf","TL;DR":"We propose a novel model to represent notes and their properties, which can enhance the automatic melody generation.","paperhash":"anonymous|melody_generation_for_pop_music_via_word_representation_of_musical_properties","_bibtex":"@article{\n  anonymous2018melody,\n  title={Melody Generation for Pop Music via Word Representation of Musical Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOoMSgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper249/Authors"],"keywords":["music","lstm","gan","generation","rnn","hmm"]}},{"tddate":null,"ddate":null,"tmdate":1512222599644,"tcdate":1511803885198,"number":2,"cdate":1511803885198,"id":"HkSl26tgM","invitation":"ICLR.cc/2018/Conference/-/Paper249/Official_Review","forum":"ByOoMSgRW","replyto":"ByOoMSgRW","signatures":["ICLR.cc/2018/Conference/Paper249/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Some interesting ideas and results, but unclear in parts and could use more thorough evaluations.","rating":"4: Ok but not good enough - rejection","review":"Summary\n-------\nThis paper proposes a generative model of symbolic (MIDI) melody in western popular music.\nThe model uses an LSTM architecture to map sequences of chord symbols and structural identifiers (e.g., verse or chorus) to predicted note sequences which constitute the melody line.\nThe key innovation proposed in this work is to jointly encode note symbols along with timing and duration information to form musical \"words\" from which melodies are composed.\nThe proposed model compares compared favorably to prior work in listener preference and Turing-test studies, and performs.\n\n\nQuality\n-------\n\nOverall, I found the paper interesting, and the provided examples generated by the\nmodel sound relatively good.  The quantitative evaluations seem promising, though\ndifficult to interpret fully due to a lack of provided detail (see below).\n\nApart from clarification issues enumerated below, where the paper could be most\nsubstantially improved is in the evaluation of the various ingredients of the model.\nMany ideas are presented with some abstract motivation, but there is no comparative\nevaluation to demonstrate what happens when any one piece is removed from the system.\nSome examples:\n\n- How important is the \"song part\" contextual input?\n- What happens if the duration or timing information is not encoded with the note?\n- How important is the pitch range regularization?\n\nSince the authors claim these ideas as novel, I would expect to see more evaluation of\ntheir independent impact on the resulting system.  Without such an evaluation, it is\ndifficult to take any general lessons away from this paper.\n\n\nClarity\n-------\n\nWhile the main ideas of this paper are presented clearly, I found the details difficult to follow.  Specifically, the following points need to be substantially clarified:\n\n- The note encoding described in Section 3.1: \"w_i = (p_i, t_i, l_i)\" describes the\n  pitch, timing, and duration of the i'th chord, but it is not explained how time and\n  duration are represented.  Since these are derived from MIDI, I would expect either\n  ticks or seconds -- or maybe a tempo-normalized variant -- but Figure 2 suggests\n  staff notation, which is not explicitly coded in MIDI.  Please explain precisely\n  how the data is represented.\n\n- Also in Section 3, several references are made to a \"previous\" model, but no\n  citation is given.  Was this a specific published work?\n\n- Equation 1 is missing a variable (j) for the range of the summation.  It took a few\n  passes for me to parse what was going on here.  One could easily mistake it for\n  summing over i to describe partial subsequences, but I don't think this is what is\n  intended.\n\n- \"... our model does not have to consider intervals that do not contain notes\" --\n  this contradicts the implication of Figure 2b, where a rest is explicitly notated in\n  the generated sequence.  Since MIDI does not explicitly encode rests (they must be\n  inferred from the absence of note events), I'd suggest wording this more carefully,\n  and being more explicit about what is produced by the model and what is notational\n  embellishment for expository purposes.\n\n- Equation 2 describes the LSTM gate equations, but there is no concrete description\n  of the model architecture used in this paper.  How are the hidden states mapped to\n  note predictions?  What is the loss function and optimizer?  These details are\n  necessary to facilitate replication.\n\n- Equation 3 and the accompanying text implies that song part states (x_i) are\n  conditionally independent given the current chord state (z_i).  Is that correct?\n  If so, it seems like a strange choice, since I would expect a part state to persist across multiple chord state transitions.  Please explain this part in more detail.  Also a\n  typo in the second factor: p(z_N | z_{N-1}) should be p(z_n | z_{n-1}); likewise\n  p(x_n | z_N).\n\n- The regularization penalty (Alg. 1) is also difficult to follow.  Is S derived from\n  P by viterbi decoding, or independent (point-wise argmax) decoding?  What exactly is\n  the \"E\" that results in the derivative at step 8, and why does the derivative for\n  p_i depend on the total sum C?  This all seems non-obvious, and worth describing in\n  more detail since it seems critical to the performance of the model.\n\n\n- Table 2: what does \"# samples\" mean in this context?  Why is it different from \"#\n  songs\"?\n\n- Section 4.2: the description of the evaluation suggests that the proposed model's\n  output was always played before the baseline.  Is that correct?  If so, does that\n  bias the results?\n\n- Section 4.2: are the examples provided to listeners just the melody lines, or full\n  mixes on top of the input chord sequence?  It's unclear from the text, and it seems\n  like a relevant detail to correctly assess the fairness of the comparison to the\n  baselines.\n\n- Section 4.2: how many generated examples were included in this evaluation?\n\n- Section 4.3: there are a couple of references to being \"in\" or \"out of\" tune.\n  Should this instead be in or out of key, since the tuning is presumably fixed by\n  MIDI synthesis?\n\n\nOriginality\n-----------\n\nAs far as I know, the proposed method is novel, though strongly related to (cited)\nprior work.\n\nThe key idea seems to be encoding of notes and properties as analogous to \"words\".  I\nfind this analogy a little bit of a stretch, since even with timing and duration\nincluded, it's hard to argue that a single note event has semantic content in the way\nthat a word does.  A little more development of this idea, and some more concrete\nmotivation for the specific choices of which properties to include, would go a long\nway in strengthening the paper.\n\nSignificance\n------------\n\nThe significance of this work is difficult to assess without independent evaluation of the proposed novel components.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Melody Generation for Pop Music via Word Representation of Musical Properties","abstract":"Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors. Representation\nof multivariate property of notes has been one of the primary challenges.\nIt is also difficult to remain in the permissible spectrum of musical variety, outside\nof which would be perceived as a plain random play without auditory pleasantness.\nObserving the conventional structure of pop music poses further challenges.\nIn this paper, we propose to represent each note and its properties as a unique\n‘word,’ thus lessening the prospect of misalignments between the properties, as\nwell as reducing the complexity of learning. We also enforce regularization policies\non the range of notes, thus encouraging the generated melody to stay close\nto what humans would find easy to follow. Furthermore, we generate melody\nconditioned on song part information, thus replicating the overall structure of a\nfull song. Experimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models.","pdf":"/pdf/407c081b7c28c1f60b501cae992aefab2bf0e48f.pdf","TL;DR":"We propose a novel model to represent notes and their properties, which can enhance the automatic melody generation.","paperhash":"anonymous|melody_generation_for_pop_music_via_word_representation_of_musical_properties","_bibtex":"@article{\n  anonymous2018melody,\n  title={Melody Generation for Pop Music via Word Representation of Musical Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOoMSgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper249/Authors"],"keywords":["music","lstm","gan","generation","rnn","hmm"]}},{"tddate":null,"ddate":null,"tmdate":1512222599691,"tcdate":1511390819380,"number":1,"cdate":1511390819380,"id":"HkoD0O7xG","invitation":"ICLR.cc/2018/Conference/-/Paper249/Official_Review","forum":"ByOoMSgRW","replyto":"ByOoMSgRW","signatures":["ICLR.cc/2018/Conference/Paper249/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Insufficiently novel, results are not convincing","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a model for generating pop music melodies with a recurrent neural network conditioned on chord and part (song section) information.  They train their model on a small dataset and compare it to a few existing models in a human evaluation.\n\nI think this paper has many issues, which I describe below.  As a broad overview, the use of what the authors call \"word\" representations of notes is not novel (appearing first in BachBot and the PerformanceRNN); I suspect the model may be outputting sequences from the training set; and the dataset is heavily constrained in a way that will make producing pleasing melodies easily but heavily limits the possible outputs of the model.  The paper is also missing important references and is confusingly laid out (e.g. introducing a GAN model in a few paragraphs in the experiments).\n\nSpecific criticism:\n\n- \"often producing works that are indistinguishable from human works (Goodfellow et al. (2014); Radford et al. (2016); Potash et al. (2015)).\"  I would definitely not say that any of the cited papers produce anything that could be confused as \"real\"; e.g. the early GAN papers you cite were not even close (maybe somewhat close for images of bedrooms, which is a limited domain and certainly cannot be considered a \"work\").\n- There are many unsubstantiated claims in the second paragraph.  E.g. \"there is yet a certain aspect about it that makes it sound like (or not sound like) human-written music.\"  What is it?  What evidence do we have that this is true?  \" notes in the chorus part generally tend to be more high-pitched\" Really?  Where was this measured?  \"music is not merely a series of notes, but entails an overall structure of its own\" Sure, but natural images are not merely a series of pixels either, and they certainly have structure, but we are making lots of good progress modeling them.  Etc.\n- Your related work section is lacking.  For example, Eck & Schmidhuber in 2002 proposed using LSTMs for music composition, which is not much later than works from \"the early days\" despite having not \"employed rule or template based approach\".  Your note/time offset/duration representation is very similar to that of BachBot (by Liang) and Magenta's PerformanceRNN.  GANs were also previously applied to piano roll generation, see MuseGAN (Dong et al), MidiNet (Yang et al), etc.  Your critique of Jaques et al. is misleading; \"they defined a number of music-theory based rules to set up the reward function\" is the whole point - this is an optional step which improves results, and there is no reason a priori to think that hand-designing regularizers is better than hand-designing RL objectives.\n- The analogy to image captioning models is interesting, but this type of image captioning model is not only model which is effectively a conditional language model - any sequence-to-sequence model can be looked at this way.  I don't think that these image captioning models are even the most commonly known example, so I'm not sure why the proposed approach is being proposed in analogy to image captioning.\n- I don't think you need to reproduce the LSTM equations in your text, they are well-known.\n- You should define early on what you mean by \"part\", I think you mean the song's section (verse, chorus, etc) but I have also heard this used to refer to the different instruments in a song.  I don't think you should expect this term to be known outside of musical communities (e.g. the ICLR community).\n- It seems simpler (and more in keeping with the current zeigeist, e.g. the image captioning models you refer to) to replace your HMM with a model that \n- The regularization is interesting, but a simpler way to enforce this constraint would be to just only allow the model to produce notes within that predefined range.  Since you effectively constrain it to an octave, it would be simple to wrap all notes in your training data into this octave.  This baseline is probably worth comparing to since it is substantially simpler than your regularizer.\n- You write that the softmax cost should have \\frac{\\partial E}{\\partial p_i} \\mu added to it for the regularizer.  First, you don't define E anywhere, you only introduce it in its derivative (and of course you can't \"define\" the derivative of an expression, it's an analytically computed quantity).  Second, are you sure you mean that the partial derivative should be added, and not the cost C itself?\n- Your results showing that human raters preferred your models are impressive, but you have made the task easier for yourself in various ways: 1) Constraining the training data to pop music 2) Making all of the training data in a single (major) key 3) Effectively limiting the melody range to within a single octave.\n- It sounds very much like your model is repeating bars, e.g. it generates a melody of length N bars, then repeats this melody.  Is this something you hard-coded into the model?  It would be very surprising if it learned to exhibit this behavior on its own.  If you hard-coded it into the model, I would expect it to sound better to human raters, but this is a strong heuristic.\n- I'd suggest you provide example melodies from your model in isolation (more like the \"varying number of bars\" examples) rather than as part of a full music mix - this makes it easier to judge the quality of the model's output.\n- The GAN experiments are interesting but come as a big surprise and are largely orthogonal to the other model; why not include this in your model description section?  The model and training details are not adequately described and I don't think it adds much to the paper to include it.  Furthermore it's quite similar to the MidiNet and MuseGAN, so maybe it should be introduced as a baseline instead.\n- How did you order the notes for chords?  If three notes occur simultaneously (in a chord), there's no a priory correct way to list them sequentially (two with an interval of length zero between notes).\n- \"Generated instruments sound fairly in tune individually, confirming that our proposed model is applicable to other instruments as well\"Assuming you are still using C-major-only melodies, it's not surprising that the generations sound in tune!\n- It is not surprising that your model ends up overfitting because your dataset is very small, your model is very powerful, and your regularizer does not really limit the model's capacity much.  I suspect that your model is overfitting even earlier than you think. You should check that none of the sequences output by your model appear in the training set. You could easily compute n-gram overlap of the generated sequences vs. the training set.  At what point did you stop training before running the human evaluation?  If you let your model overfit, then of course it will generate very human-sounding melodies, but this is not a terribly interesting generative model.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Melody Generation for Pop Music via Word Representation of Musical Properties","abstract":"Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors. Representation\nof multivariate property of notes has been one of the primary challenges.\nIt is also difficult to remain in the permissible spectrum of musical variety, outside\nof which would be perceived as a plain random play without auditory pleasantness.\nObserving the conventional structure of pop music poses further challenges.\nIn this paper, we propose to represent each note and its properties as a unique\n‘word,’ thus lessening the prospect of misalignments between the properties, as\nwell as reducing the complexity of learning. We also enforce regularization policies\non the range of notes, thus encouraging the generated melody to stay close\nto what humans would find easy to follow. Furthermore, we generate melody\nconditioned on song part information, thus replicating the overall structure of a\nfull song. Experimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models.","pdf":"/pdf/407c081b7c28c1f60b501cae992aefab2bf0e48f.pdf","TL;DR":"We propose a novel model to represent notes and their properties, which can enhance the automatic melody generation.","paperhash":"anonymous|melody_generation_for_pop_music_via_word_representation_of_musical_properties","_bibtex":"@article{\n  anonymous2018melody,\n  title={Melody Generation for Pop Music via Word Representation of Musical Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOoMSgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper249/Authors"],"keywords":["music","lstm","gan","generation","rnn","hmm"]}},{"tddate":null,"ddate":null,"tmdate":1509739404736,"tcdate":1509081759792,"number":249,"cdate":1509739402074,"id":"ByOoMSgRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByOoMSgRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Melody Generation for Pop Music via Word Representation of Musical Properties","abstract":"Automatic melody generation for pop music has been a long-time aspiration for\nboth AI researchers and musicians. However, learning to generate euphonious\nmelody has turned out to be highly challenging due to a number of factors. Representation\nof multivariate property of notes has been one of the primary challenges.\nIt is also difficult to remain in the permissible spectrum of musical variety, outside\nof which would be perceived as a plain random play without auditory pleasantness.\nObserving the conventional structure of pop music poses further challenges.\nIn this paper, we propose to represent each note and its properties as a unique\n‘word,’ thus lessening the prospect of misalignments between the properties, as\nwell as reducing the complexity of learning. We also enforce regularization policies\non the range of notes, thus encouraging the generated melody to stay close\nto what humans would find easy to follow. Furthermore, we generate melody\nconditioned on song part information, thus replicating the overall structure of a\nfull song. Experimental results demonstrate that our model can generate auditorily\npleasant songs that are more indistinguishable from human-written ones than\nprevious models.","pdf":"/pdf/407c081b7c28c1f60b501cae992aefab2bf0e48f.pdf","TL;DR":"We propose a novel model to represent notes and their properties, which can enhance the automatic melody generation.","paperhash":"anonymous|melody_generation_for_pop_music_via_word_representation_of_musical_properties","_bibtex":"@article{\n  anonymous2018melody,\n  title={Melody Generation for Pop Music via Word Representation of Musical Properties},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOoMSgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper249/Authors"],"keywords":["music","lstm","gan","generation","rnn","hmm"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}