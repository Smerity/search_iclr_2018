{"notes":[{"tddate":null,"ddate":null,"tmdate":1515162469064,"tcdate":1515162469064,"number":4,"cdate":1515162469064,"id":"ByTDsb6Qf","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Comment","forum":"Hyp-JJJRW","replyto":"rkWU5vQxf","signatures":["ICLR.cc/2018/Conference/Paper109/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper109/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thank you for your feedback. We apologize for not clearly stating the motivation behind this work. Our main motivation was to design a classifier network that also has the capacity to be generative. We believe that a generative network would be less susceptible to being fooled by adversarial inputs since it would not be able to reconstruct nonsensical input. But before showing that the network is less vulnerable to adversarial examples, we want to investigate the properties of such a network. We have added text to our paper to clarify that this is our ultimate goal.\n\nHowever, in creating a classifier/generative network, we wanted to investigate the relationship between the classification part of the encoding, and the “style memory” part of the encoding. Much of this paper is devoted to understanding this relationship.\n\nWe have added experiments that we conducted on the Extended MNIST letter dataset which contains 145,600 samples, and where uppercase and lowercase letters are included in the same class (i.e. ‘A’ and ‘a’ are in the same class). This makes the dataset more challenging than MNIST. We also expanded our discussion of figure 7 (and figure 8, which was added). The discussion argues, in quantitative terms, that style memory contains a representation that augments, but is substantially different from, the character class. The figures also illustrate that the representation in style memory is very different from the original, image-space representation.\n\nWe have modified our paper to address your comments, and feel that the paper is much improved from its original form."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515161300461,"tcdate":1515161300461,"number":3,"cdate":1515161300461,"id":"SynAU-TmG","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Comment","forum":"Hyp-JJJRW","replyto":"H109AKKlM","signatures":["ICLR.cc/2018/Conference/Paper109/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper109/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thank you for your feedback. We apologize for not clearly stating the motivation behind this work. Our main motivation was to design a classifier network that also has the capacity to be generative. We believe that a generative network would be less susceptible to being fooled by adversarial inputs since it would not be able to reconstruct nonsensical input. But before showing that the network is less vulnerable to adversarial examples, we want to investigate the properties of such a network. We have added text to our paper to clarify that this is our ultimate goal.\n\nAlthough we have done preliminary experiments to show that the network is less vulnerable to adversarial examples, these results are not reported in this paper; we feel that further investigations and experimentations are warranted.\n\nThank you for also pointing out the paper “Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure” by Salakhutdinov and Hinton (2007). We agree that the work is relevant, and have added a discussion of the paper to our “Related Work” section.\n\nWe also changed our network to include convolutional layers, as you suggested. Lastly, we removed the tSNE visualization because we felt it did not service the main message of the paper.\n\nWe have modified our paper to address your comments, and feel that the paper is much improved. We hope that you agree."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515161203131,"tcdate":1515161203131,"number":2,"cdate":1515161203131,"id":"ryjOU-TQM","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Comment","forum":"Hyp-JJJRW","replyto":"S1yZxBslG","signatures":["ICLR.cc/2018/Conference/Paper109/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper109/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Thank you for your feedback. We apologize for not clearly stating the motivation behind this work. Our goal was to design a classifier network that also has the capacity to be generative. We believe that a generative network would be less susceptible to being fooled by adversarial inputs since it would not be able to reconstruct nonsensical input. But before showing that the network is less vulnerable to adversarial examples, we want to investigate the properties of such a network. However, results on adversarial inputs are not reported in this paper as we feel that further investigations and experiments are still needed.\n\nWe have modified our paper to address your feedback. We feel our paper is much better after implementing those changes."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515160839174,"tcdate":1515160839174,"number":1,"cdate":1515160839174,"id":"Sy1frWTQM","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Comment","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper109/Authors"],"content":{"title":"Updated based on valuable reviewer comments","comment":"We appreciate the constructive comments that the reviewers made on our paper, and have revised the manuscript accordingly. In particular, we have clarified the purpose of the research. This work is a necessary stepping-stone to our goal of investigating the possibility that generative networks are less susceptible to being fooled by ambiguous or adversarial inputs. The work outlined in this paper lays the foundation for how to create networks that simultaneously perform both classification and reconstruction. We have also included a more difficult dataset, EMNIST. We also altered the design of our network so that it now has two convolutional layers, and the resulting classification performance is much improved."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515642382615,"tcdate":1511899127066,"number":3,"cdate":1511899127066,"id":"S1yZxBslG","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Review","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper proposes augmenting classifier deep neural networks with 'style memory' features (along the lines of auto-encoders) and training the two at the same time.","rating":"4: Ok but not good enough - rejection","review":"The paper proposes combining classification-specific neural networks with auto-encoders. This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction. The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders). \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works. The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction). For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different. The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.\n\nWhile the experimental results are interesting they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders. Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515642382655,"tcdate":1511788182343,"number":2,"cdate":1511788182343,"id":"H109AKKlM","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Review","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/AnonReviewer3"],"readers":["everyone"],"content":{"title":"results are not convincing","rating":"3: Clear rejection","review":"The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \"style memory\", which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \"style\" and \"content\", is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see \"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515642382694,"tcdate":1511385673317,"number":1,"cdate":1511385673317,"id":"rkWU5vQxf","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Review","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lack of convincing motivation and results not particularly unimpressive","rating":"3: Clear rejection","review":"This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or \"style\" as used in this paper). This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer. Experiments on MNIST are provided to analyse what this approach learns.\n\nUnfortunately, I fail to see a significantly valuable contribution from this work. First, the paper could do a better job at motivating the problem being addressed. Why is it important to separate class from style? Should it allow better classification performance? If so, it's never measured in this work. If that's not the motivation, then what is it?\n\nSecond, all experiments were conducted on the MNIST dataset. In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method.\n\nFinally, the results are not particularly impressive. I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input). Also, that the \"style\" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result. And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully. The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great. But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective.\n\nFor all these reasons, I'm afraid I must recommend this paper be rejected.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1515161086146,"tcdate":1508990724774,"number":109,"cdate":1509739476537,"id":"Hyp-JJJRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.","pdf":"/pdf/49dd910c399e8623642078eeb2eb5eee5c072b0f.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}