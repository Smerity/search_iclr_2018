{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222554767,"tcdate":1511899127066,"number":3,"cdate":1511899127066,"id":"S1yZxBslG","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Review","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper proposes augmenting classifier deep neural networks with 'style memory' features (along the lines of auto-encoders) and training the two at the same time.","rating":"4: Ok but not good enough - rejection","review":"The paper proposes combining classification-specific neural networks with auto-encoders. This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction. The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders). \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works. The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction). For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different. The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.\n\nWhile the experimental results are interesting they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders. Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as stacked autoencoders, jointly minimizing both classification and reconstruction losses. The generative function of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs. We further investigate the nature of the style memory, and how it relates to composing digits from MNIST.","pdf":"/pdf/0ddadbced55c671122851c49e971a2b42d2d3e78.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1512222554812,"tcdate":1511788182343,"number":2,"cdate":1511788182343,"id":"H109AKKlM","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Review","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/AnonReviewer3"],"readers":["everyone"],"content":{"title":"results are not convincing","rating":"3: Clear rejection","review":"The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \"style memory\", which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \"style\" and \"content\", is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see \"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as stacked autoencoders, jointly minimizing both classification and reconstruction losses. The generative function of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs. We further investigate the nature of the style memory, and how it relates to composing digits from MNIST.","pdf":"/pdf/0ddadbced55c671122851c49e971a2b42d2d3e78.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1512222554852,"tcdate":1511385673317,"number":1,"cdate":1511385673317,"id":"rkWU5vQxf","invitation":"ICLR.cc/2018/Conference/-/Paper109/Official_Review","forum":"Hyp-JJJRW","replyto":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference/Paper109/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lack of convincing motivation and results not particularly unimpressive","rating":"3: Clear rejection","review":"This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or \"style\" as used in this paper). This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer. Experiments on MNIST are provided to analyse what this approach learns.\n\nUnfortunately, I fail to see a significantly valuable contribution from this work. First, the paper could do a better job at motivating the problem being addressed. Why is it important to separate class from style? Should it allow better classification performance? If so, it's never measured in this work. If that's not the motivation, then what is it?\n\nSecond, all experiments were conducted on the MNIST dataset. In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method.\n\nFinally, the results are not particularly impressive. I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input). Also, that the \"style\" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result. And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully. The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great. But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective.\n\nFor all these reasons, I'm afraid I must recommend this paper be rejected.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as stacked autoencoders, jointly minimizing both classification and reconstruction losses. The generative function of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs. We further investigate the nature of the style memory, and how it relates to composing digits from MNIST.","pdf":"/pdf/0ddadbced55c671122851c49e971a2b42d2d3e78.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]}},{"tddate":null,"ddate":null,"tmdate":1509739479191,"tcdate":1508990724774,"number":109,"cdate":1509739476537,"id":"Hyp-JJJRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hyp-JJJRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Style Memory: Making a Classifier Network Generative","abstract":"Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as stacked autoencoders, jointly minimizing both classification and reconstruction losses. The generative function of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs. We further investigate the nature of the style memory, and how it relates to composing digits from MNIST.","pdf":"/pdf/0ddadbced55c671122851c49e971a2b42d2d3e78.pdf","TL;DR":"Augmenting the top layer of a classifier network with a style memory enables it to be generative.","paperhash":"anonymous|style_memory_making_a_classifier_network_generative","_bibtex":"@article{\n  anonymous2018style,\n  title={Style Memory: Making a Classifier Network Generative},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyp-JJJRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper109/Authors"],"keywords":["neural networks","autoencoder","generative","feed-back"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}