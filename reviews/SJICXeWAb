{"notes":[{"ddate":null,"tddate":1512040451855,"tmdate":1512222692588,"tcdate":1512040426695,"number":3,"cdate":1512040426695,"id":"rJ7xdwpeM","invitation":"ICLR.cc/2018/Conference/-/Paper571/Official_Review","forum":"SJICXeWAb","replyto":"SJICXeWAb","signatures":["ICLR.cc/2018/Conference/Paper571/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Weak results; mostly already known to experts","rating":"3: Clear rejection","review":"The paper shows that there are functions that can be represented by depth 3 sigmoidal neural networks (with polynomial weights and polynomially many units), but sigmoidal networks of depth 2 with polynomially bounded weights require exponentially many units. There is nothing new technically in the paper and I find the results uninteresting given the spate of results of this kind. I don't share the authors enthusiasm about much more general distributions etc. The approximations the authors are shooting for are much stronger the kind that has been used by Eldan and Shamir (2016) or other such papers. The approximation used here is $\\ell_\\infty$ rather than $\\ell_2$. So a negative result for depth 2 is weaker; the earlier work (and almost trivially by using the work of Cybenko, Hornik, etc.) already shows that the depth -3 approximations are uniform approximators. \n\nThe fact that sigmoidal neural networks with bounded weights can be expressed as \"low\" degree polynomials is not new. Much stronger results including bounds on the weights of the polynomial (sum of squares of coefficients) appear implicitly in Zhang et al. (2014) and Goel et al. (2017). In fact, these last two papers go further and show that this has implications for learnability not just for representation as the current paper shows. \n\nAdditionally, I think the paper is a bit sloppy in the maths. For example, Theorem 7 does not specifiy what delta is. I'm sure they mean that there is a \"small enough \\delta\" (with possible dependence on d, B, etc.). But surely this statement is not true for all values of $\\delta$. For e.g. when $\\delta = 1$, sin(\\pi d^5 \\Vert x \\Vert^2) can rather trivially be expressed as a sigmoidal neural network of depth 2. \n\nOverall, I think this paper has a collection of results that are well-known to experts in the field and add little novelty. It's unlikely that having yet another paper separating depth 2 from depth 3 with some other set of conditions will move us towards further progress in the very important question of depth separation. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Depth separation and weight-width trade-offs for sigmoidal neural networks","abstract":"Some recent work has shown separation between the expressive power of depth-2 and depth-3 neural networks. These separation results are shown by constructing functions and input distributions, so that the function is well-approximable by a depth-3 neural network of polynomial size but it cannot be well-approximated under the chosen input distribution by any depth-2 neural network of polynomial size. These results are not robust and require carefully chosen functions as well as input distributions.\n\nWe show a similar separation between the expressive power of depth-2 and depth-3 sigmoidal neural networks over a large class of input distributions, as long as the weights are polynomially bounded. While doing so, we also show that depth-2 sigmoidal neural networks with small width and small weights can be well-approximated by low-degree multivariate polynomials.","pdf":"/pdf/5e2f106d798b0e1c4cf4f39a238fe2a7725041fd.pdf","TL;DR":"depth-2-vs-3 separation for sigmoidal neural networks over general distributions","paperhash":"anonymous|depth_separation_and_weightwidth_tradeoffs_for_sigmoidal_neural_networks","_bibtex":"@article{\n  anonymous2018depth,\n  title={Depth separation and weight-width trade-offs for sigmoidal neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJICXeWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper571/Authors"],"keywords":["depth separation","neural networks","weights-width trade-off"]}},{"tddate":null,"ddate":null,"tmdate":1512222692628,"tcdate":1511745890358,"number":2,"cdate":1511745890358,"id":"ry5vY1teG","invitation":"ICLR.cc/2018/Conference/-/Paper571/Official_Review","forum":"SJICXeWAb","replyto":"SJICXeWAb","signatures":["ICLR.cc/2018/Conference/Paper571/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"Depth separation and weight-width trade-offs for sigmoidal neural networks\"","rating":"5: Marginally below acceptance threshold","review":"This paper proves a new separation results from 3-layer neural networks to 2-layer neural networks. The core of the analysis is a proof that any 2-layer neural networks can be well approximated by a polynomial function with reasonably low degrees. Then the authors constructs a highly non-smooth function can be represented by a 3-layer network, but impossible to approximate by any polynomial-degree polynomial function.\n\nSimilar results about polynomial approximation can be found in [1] (Theorem 4). To me, the result proved in [1] is spiritually very similar to propositions 3-4. The authors need to justify the difference.\n\nThe main strength of the new separation result is that it holds for a larger class of input distributions. Comparing to Daniely (2017) which requires the input distribution to be spherically uniform, the new result only needs the distribution to be lower bounded by 1/poly(d) in a small ball of radius 1/poly(d). Conceptually I don't think this is a much weaker condition. For a \"truly\" non-uniform distribution, one should allow its density function to be very close to zero at certain regions of the ball. Nevertheless, the result is a step forward from Daniely (2017) and the paper is well written.\n\nI am still in doubt of the practical value of such kind of separation results. The paper proves the separation by constructing a very specific function that cannot be approximated by 2-layer networks. This function has a super large Lipschitz constant, which we don't expect to see in practice. Consider the function f(x)=cos(Nx). When N is chosen large enough, the function f can not be well approximated by any 2-layer network with polynomial size. Does it imply that the family of cosine functions is rich enough so that it is a better family to learn than 2-layer neural networks? I guess the answer would be negative. In addition, the paper doesn't show that any 2-layer network can be well approximated by a 3-layer network, which is a missing piece in justifying the richness of 3-layer nets.\n\nFinally, the constructed \"hard\" function has order d^5 Lipschitz constant, but Theorem 7 assumes that the 2-layer networks' weight must be bounded by O(d^2). This assumption is crucial to the proof but not well justified (especially considering the d^5 factor in the function definition).\n\n[1] On the Computational Efficiency of Training Neural Networks, Livni et al., NIPS'14","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Depth separation and weight-width trade-offs for sigmoidal neural networks","abstract":"Some recent work has shown separation between the expressive power of depth-2 and depth-3 neural networks. These separation results are shown by constructing functions and input distributions, so that the function is well-approximable by a depth-3 neural network of polynomial size but it cannot be well-approximated under the chosen input distribution by any depth-2 neural network of polynomial size. These results are not robust and require carefully chosen functions as well as input distributions.\n\nWe show a similar separation between the expressive power of depth-2 and depth-3 sigmoidal neural networks over a large class of input distributions, as long as the weights are polynomially bounded. While doing so, we also show that depth-2 sigmoidal neural networks with small width and small weights can be well-approximated by low-degree multivariate polynomials.","pdf":"/pdf/5e2f106d798b0e1c4cf4f39a238fe2a7725041fd.pdf","TL;DR":"depth-2-vs-3 separation for sigmoidal neural networks over general distributions","paperhash":"anonymous|depth_separation_and_weightwidth_tradeoffs_for_sigmoidal_neural_networks","_bibtex":"@article{\n  anonymous2018depth,\n  title={Depth separation and weight-width trade-offs for sigmoidal neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJICXeWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper571/Authors"],"keywords":["depth separation","neural networks","weights-width trade-off"]}},{"tddate":null,"ddate":null,"tmdate":1512222692673,"tcdate":1511198632880,"number":1,"cdate":1511198632880,"id":"rkb3kqxxf","invitation":"ICLR.cc/2018/Conference/-/Paper571/Official_Review","forum":"SJICXeWAb","replyto":"SJICXeWAb","signatures":["ICLR.cc/2018/Conference/Paper571/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Decent (but somewhat incremental) contribution to an interesting question","rating":"6: Marginally above acceptance threshold","review":"This paper contributes to the growing literature on depth separations in neural network, showing cases where depth is provably needed to express certain functions. Specifically, the paper shows that there are functions on R^d that can be approximated well by a depth-3 sigmoidal network with poly(d) weights, that cannot be approximated by a depth-2 sigmoidal network with poly(d) weights, and with respect to any input distributions with sufficiently large density in some part of the domain. The proof builds on ideas in Daniely (2017) and Shalev-Shwartz et al. (2011). \n\nCompared to previous works, the main novelty of the result is that it applies to a very large family of input distributions, as opposed to some specific distributions. On the flip side, it applies only to networks with sigmoids as activation functions, and the weights need to be polynomially bounded. Moreover, although the result is robust to the choice of input distribution, the function used to get the lower bound is still rather artificial ( x -> sin(N||x||^2) for some large N). In a sense, this is complementary to the separation result in Safran and Shamir (2017), mentioned by the authors, where the function is arguably \"natural\", but the distribution is not. Finally, the proof ideas appear to be not too different than those of Daniely (2017).\n\nOverall, I think this is a decent contribution to this topic, and would recommend accepting it given enough room. It's a bit incremental in light of existing work, but does contribute to the important question of whether we can prove depth separations which are also robust.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Depth separation and weight-width trade-offs for sigmoidal neural networks","abstract":"Some recent work has shown separation between the expressive power of depth-2 and depth-3 neural networks. These separation results are shown by constructing functions and input distributions, so that the function is well-approximable by a depth-3 neural network of polynomial size but it cannot be well-approximated under the chosen input distribution by any depth-2 neural network of polynomial size. These results are not robust and require carefully chosen functions as well as input distributions.\n\nWe show a similar separation between the expressive power of depth-2 and depth-3 sigmoidal neural networks over a large class of input distributions, as long as the weights are polynomially bounded. While doing so, we also show that depth-2 sigmoidal neural networks with small width and small weights can be well-approximated by low-degree multivariate polynomials.","pdf":"/pdf/5e2f106d798b0e1c4cf4f39a238fe2a7725041fd.pdf","TL;DR":"depth-2-vs-3 separation for sigmoidal neural networks over general distributions","paperhash":"anonymous|depth_separation_and_weightwidth_tradeoffs_for_sigmoidal_neural_networks","_bibtex":"@article{\n  anonymous2018depth,\n  title={Depth separation and weight-width trade-offs for sigmoidal neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJICXeWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper571/Authors"],"keywords":["depth separation","neural networks","weights-width trade-off"]}},{"tddate":null,"ddate":null,"tmdate":1509739229549,"tcdate":1509127118150,"number":571,"cdate":1509739226887,"id":"SJICXeWAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJICXeWAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Depth separation and weight-width trade-offs for sigmoidal neural networks","abstract":"Some recent work has shown separation between the expressive power of depth-2 and depth-3 neural networks. These separation results are shown by constructing functions and input distributions, so that the function is well-approximable by a depth-3 neural network of polynomial size but it cannot be well-approximated under the chosen input distribution by any depth-2 neural network of polynomial size. These results are not robust and require carefully chosen functions as well as input distributions.\n\nWe show a similar separation between the expressive power of depth-2 and depth-3 sigmoidal neural networks over a large class of input distributions, as long as the weights are polynomially bounded. While doing so, we also show that depth-2 sigmoidal neural networks with small width and small weights can be well-approximated by low-degree multivariate polynomials.","pdf":"/pdf/5e2f106d798b0e1c4cf4f39a238fe2a7725041fd.pdf","TL;DR":"depth-2-vs-3 separation for sigmoidal neural networks over general distributions","paperhash":"anonymous|depth_separation_and_weightwidth_tradeoffs_for_sigmoidal_neural_networks","_bibtex":"@article{\n  anonymous2018depth,\n  title={Depth separation and weight-width trade-offs for sigmoidal neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJICXeWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper571/Authors"],"keywords":["depth separation","neural networks","weights-width trade-off"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}