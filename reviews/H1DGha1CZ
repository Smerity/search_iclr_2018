{"notes":[{"tddate":null,"ddate":null,"tmdate":1512219444681,"tcdate":1512194682360,"number":1,"cdate":1512194682360,"id":"B1juMp1WM","invitation":"ICLR.cc/2018/Conference/-/Paper181/Official_Comment","forum":"H1DGha1CZ","replyto":"SJwceNsxG","signatures":["ICLR.cc/2018/Conference/Paper181/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper181/Authors"],"content":{"title":"Answers to comments of Reviewer","comment":"Regarding the first comment, we emphasize that the reduction of the mean activations produced by DReLU was not the only argument used to explain why DReLU works better than the other evaluated non-linearities. In fact, theoretical reasons were also provided. Furthermore, it was mentioned and mathematically expressed that DReLU probably implicates less damage to the normalization process as it extends the linear function into the third quadrant, which is a characteristic provided by neither LReLU nor ELU. Moreover, rigorous statistical tests and the vast amount of repetitions of the experiments also contributed from an experimental point of view to ensure DReLU improves the deep models presented.\n\nIn respect of the second comment, we argue that CIFAR10/100 are the standard and most frequently used datasets in deep learning computer vision research papers. Moreover, for each dataset, the paper presented consistent results for a range of standard and relevant models with a substantially different number of layers. Naturally, it is never possible to be sure the results presented on some datasets will repeat in other bases. Unfortunately, we will probably not have enough time to execute more 150 experiments needed to include the ImageNet in this study in the next few weeks.\n\nRegarding the third comment, we believe that one of the primary results of our work is precisely showing that without dropout, ReLU is likely to outperform all previously proposed activation functions. It is entirely consistent with the fact that all recently introduced models (VGG, ResNet, WideResNet, DenseNets, GANs, etc.) still use ReLU as default activation function. It is no surprise considering the results of our work.\n\nIt is also relevant to observe that the mentioned studies usually completely avoid dropout (ResNet, WideResNet, and GANs) or show that the variant without dropout clearly outperforms the one using it (DenseNets). Therefore, we emphasize that no dropout was used since we believe that adding it implies worst results than just use batch normalization, at least for convolutional neural networks. Certainly, since 2014 we have seen dropout increasingly less relevant.\n\nThe above mention arguments are indeed in agreement with the more than three hundred experiments and statistical tests we performed which shows that ReLU is a compelling option in strictly batch normalized ConvNets, which is, in our opinion, the best possible design from a regularization point of view to achieve higher performances. Indeed, the test accuracies presented by the paper are essentially state-of-the-art for the models and datasets considered. Besides, dropout slows the training.\n\nWe remember that, as mentioned in the paper, the vast majority of the previously proposed activation functions used experiments with dropout and almost always without batch normalization as many of them were designed before the advent of it. We believe that if the experiments with Inception V3 that you mentioned used dropout, it could explain the reason why ELU/LReLU/RReLU outperformed ReLU. If not enough executions were performed or statistical tests were not used, it could also be a statistical error. Finally, different from our study, we emphasize that the previous proposed nonlinearity works did not use standard models (but rather very hand-designed ones), perform statistical tests or at least execute many times the same experiment."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study","abstract":"In this paper, we turn our attention to the interworking between the activation functions and the batch normalization, which is a virtually mandatory technique to train deep networks currently. We propose the activation function Displaced Rectifier Linear Unit (DReLU) by conjecturing that extending the identity function of ReLU to the third quadrant enhances compatibility with batch normalization. Moreover, we used statistical tests to compare the impact of using distinct activation functions (ReLU, LReLU, PReLU, ELU, and DReLU) on the learning speed and test accuracy performance of standardized VGG and Residual Networks state-of-the-art models. These convolutional neural networks were trained on CIFAR-100 and CIFAR-10, the most commonly used deep learning computer vision datasets. The results showed DReLU speeded up learning in all models and datasets. Besides, statistical significant performance assessments (p<0.05) showed DReLU enhanced the test accuracy presented by ReLU in all scenarios. Furthermore, DReLU showed better test accuracy than any other tested activation function in all experiments with one exception, in which case it presented the second best performance. Therefore, this work demonstrates that it is possible to increase performance replacing ReLU by an enhanced activation function.","pdf":"/pdf/f7c6dac96f354c618f77134dc45427ad0c562939.pdf","TL;DR":"A new activation function called Displaced Rectifier Linear Unit is proposed. It is showed to enhance the training and inference performance of batch normalized convolutional neural networks.","paperhash":"anonymous|enhancing_batch_normalized_convolutional_networks_using_displaced_rectifier_linear_units_a_systematic_comparative_study","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DGha1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper181/Authors"],"keywords":["Batch Normalized","Convolutional Neural Networks","Displaced Rectifier Linear Unit","Comparative Study"]}},{"tddate":null,"ddate":null,"tmdate":1512222585764,"tcdate":1511895183510,"number":3,"cdate":1511895183510,"id":"SJwceNsxG","invitation":"ICLR.cc/2018/Conference/-/Paper181/Official_Review","forum":"H1DGha1CZ","replyto":"H1DGha1CZ","signatures":["ICLR.cc/2018/Conference/Paper181/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Reivew","rating":"5: Marginally below acceptance threshold","review":"This paper describes DReLU, a shift version of ReLU. DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). The author runs a few CIFAR-10/100 experiments with DReLU.\n\nComments:\n\n1. Using expectation to explain why DReLU works well is not sufficient and convincing. Although DReLU’s expectation is smaller than expectation of ReLU, but it doesn’t explain why DReLU is better than very leaky ReLU, ELU etc.\n2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc.\n3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious. I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU. \n\nOverall, I don’t think this paper meet ICLR’s novelty standard, although the authors present some good numbers, but they are not convincing. \n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study","abstract":"In this paper, we turn our attention to the interworking between the activation functions and the batch normalization, which is a virtually mandatory technique to train deep networks currently. We propose the activation function Displaced Rectifier Linear Unit (DReLU) by conjecturing that extending the identity function of ReLU to the third quadrant enhances compatibility with batch normalization. Moreover, we used statistical tests to compare the impact of using distinct activation functions (ReLU, LReLU, PReLU, ELU, and DReLU) on the learning speed and test accuracy performance of standardized VGG and Residual Networks state-of-the-art models. These convolutional neural networks were trained on CIFAR-100 and CIFAR-10, the most commonly used deep learning computer vision datasets. The results showed DReLU speeded up learning in all models and datasets. Besides, statistical significant performance assessments (p<0.05) showed DReLU enhanced the test accuracy presented by ReLU in all scenarios. Furthermore, DReLU showed better test accuracy than any other tested activation function in all experiments with one exception, in which case it presented the second best performance. Therefore, this work demonstrates that it is possible to increase performance replacing ReLU by an enhanced activation function.","pdf":"/pdf/f7c6dac96f354c618f77134dc45427ad0c562939.pdf","TL;DR":"A new activation function called Displaced Rectifier Linear Unit is proposed. It is showed to enhance the training and inference performance of batch normalized convolutional neural networks.","paperhash":"anonymous|enhancing_batch_normalized_convolutional_networks_using_displaced_rectifier_linear_units_a_systematic_comparative_study","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DGha1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper181/Authors"],"keywords":["Batch Normalized","Convolutional Neural Networks","Displaced Rectifier Linear Unit","Comparative Study"]}},{"tddate":null,"ddate":null,"tmdate":1512222585807,"tcdate":1511823609066,"number":2,"cdate":1511823609066,"id":"S1WbYz5gM","invitation":"ICLR.cc/2018/Conference/-/Paper181/Official_Review","forum":"H1DGha1CZ","replyto":"H1DGha1CZ","signatures":["ICLR.cc/2018/Conference/Paper181/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple idea, Need comparison in an uncontrolled setting","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero. As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. The DReLU is supposed to remedy the problem of covariate shift better. \n\nThe presentation of the paper is clear. The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed). Statistical tests are performed for many of the experimental results, which is solid.\n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? If it is done by hyperparameter tuning with cross-validation, the training cost may be too high.\n2) I believe the control experiments are encouraging, but I do not agree that other techniques like Dropouts are not useful. Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important. The arguments for skipping this experiments are respectful, but not convincing enough.  \n3) Batch normalization is popular, especially for the convolutional neural networks. However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway. \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study","abstract":"In this paper, we turn our attention to the interworking between the activation functions and the batch normalization, which is a virtually mandatory technique to train deep networks currently. We propose the activation function Displaced Rectifier Linear Unit (DReLU) by conjecturing that extending the identity function of ReLU to the third quadrant enhances compatibility with batch normalization. Moreover, we used statistical tests to compare the impact of using distinct activation functions (ReLU, LReLU, PReLU, ELU, and DReLU) on the learning speed and test accuracy performance of standardized VGG and Residual Networks state-of-the-art models. These convolutional neural networks were trained on CIFAR-100 and CIFAR-10, the most commonly used deep learning computer vision datasets. The results showed DReLU speeded up learning in all models and datasets. Besides, statistical significant performance assessments (p<0.05) showed DReLU enhanced the test accuracy presented by ReLU in all scenarios. Furthermore, DReLU showed better test accuracy than any other tested activation function in all experiments with one exception, in which case it presented the second best performance. Therefore, this work demonstrates that it is possible to increase performance replacing ReLU by an enhanced activation function.","pdf":"/pdf/f7c6dac96f354c618f77134dc45427ad0c562939.pdf","TL;DR":"A new activation function called Displaced Rectifier Linear Unit is proposed. It is showed to enhance the training and inference performance of batch normalized convolutional neural networks.","paperhash":"anonymous|enhancing_batch_normalized_convolutional_networks_using_displaced_rectifier_linear_units_a_systematic_comparative_study","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DGha1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper181/Authors"],"keywords":["Batch Normalized","Convolutional Neural Networks","Displaced Rectifier Linear Unit","Comparative Study"]}},{"tddate":null,"ddate":null,"tmdate":1512222585845,"tcdate":1511757479125,"number":1,"cdate":1511757479125,"id":"ByyhLzKgM","invitation":"ICLR.cc/2018/Conference/-/Paper181/Official_Review","forum":"H1DGha1CZ","replyto":"H1DGha1CZ","signatures":["ICLR.cc/2018/Conference/Paper181/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Compare against usage of BN after ReLU","rating":"3: Clear rejection","review":"The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations. Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance. The approach of using BN after non-linearity is termed \"standardization layer\" (https://arxiv.org/pdf/1301.4083.pdf). I encourage the authors to validate their claims against simple approach of using BN after non-linearity.  ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study","abstract":"In this paper, we turn our attention to the interworking between the activation functions and the batch normalization, which is a virtually mandatory technique to train deep networks currently. We propose the activation function Displaced Rectifier Linear Unit (DReLU) by conjecturing that extending the identity function of ReLU to the third quadrant enhances compatibility with batch normalization. Moreover, we used statistical tests to compare the impact of using distinct activation functions (ReLU, LReLU, PReLU, ELU, and DReLU) on the learning speed and test accuracy performance of standardized VGG and Residual Networks state-of-the-art models. These convolutional neural networks were trained on CIFAR-100 and CIFAR-10, the most commonly used deep learning computer vision datasets. The results showed DReLU speeded up learning in all models and datasets. Besides, statistical significant performance assessments (p<0.05) showed DReLU enhanced the test accuracy presented by ReLU in all scenarios. Furthermore, DReLU showed better test accuracy than any other tested activation function in all experiments with one exception, in which case it presented the second best performance. Therefore, this work demonstrates that it is possible to increase performance replacing ReLU by an enhanced activation function.","pdf":"/pdf/f7c6dac96f354c618f77134dc45427ad0c562939.pdf","TL;DR":"A new activation function called Displaced Rectifier Linear Unit is proposed. It is showed to enhance the training and inference performance of batch normalized convolutional neural networks.","paperhash":"anonymous|enhancing_batch_normalized_convolutional_networks_using_displaced_rectifier_linear_units_a_systematic_comparative_study","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DGha1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper181/Authors"],"keywords":["Batch Normalized","Convolutional Neural Networks","Displaced Rectifier Linear Unit","Comparative Study"]}},{"tddate":null,"ddate":null,"tmdate":1509739441324,"tcdate":1509051407248,"number":181,"cdate":1509739438668,"id":"H1DGha1CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1DGha1CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study","abstract":"In this paper, we turn our attention to the interworking between the activation functions and the batch normalization, which is a virtually mandatory technique to train deep networks currently. We propose the activation function Displaced Rectifier Linear Unit (DReLU) by conjecturing that extending the identity function of ReLU to the third quadrant enhances compatibility with batch normalization. Moreover, we used statistical tests to compare the impact of using distinct activation functions (ReLU, LReLU, PReLU, ELU, and DReLU) on the learning speed and test accuracy performance of standardized VGG and Residual Networks state-of-the-art models. These convolutional neural networks were trained on CIFAR-100 and CIFAR-10, the most commonly used deep learning computer vision datasets. The results showed DReLU speeded up learning in all models and datasets. Besides, statistical significant performance assessments (p<0.05) showed DReLU enhanced the test accuracy presented by ReLU in all scenarios. Furthermore, DReLU showed better test accuracy than any other tested activation function in all experiments with one exception, in which case it presented the second best performance. Therefore, this work demonstrates that it is possible to increase performance replacing ReLU by an enhanced activation function.","pdf":"/pdf/f7c6dac96f354c618f77134dc45427ad0c562939.pdf","TL;DR":"A new activation function called Displaced Rectifier Linear Unit is proposed. It is showed to enhance the training and inference performance of batch normalized convolutional neural networks.","paperhash":"anonymous|enhancing_batch_normalized_convolutional_networks_using_displaced_rectifier_linear_units_a_systematic_comparative_study","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing Batch Normalized Convolutional Networks using Displaced Rectifier Linear Units: A Systematic Comparative Study},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DGha1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper181/Authors"],"keywords":["Batch Normalized","Convolutional Neural Networks","Displaced Rectifier Linear Unit","Comparative Study"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}