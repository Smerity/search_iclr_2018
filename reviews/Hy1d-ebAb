{"notes":[{"tddate":null,"ddate":null,"tmdate":1515772165400,"tcdate":1515772165400,"number":8,"cdate":1515772165400,"id":"Sy6ZK8IEM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"HJH-GmaXz","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer4"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/AnonReviewer4"],"content":{"title":"Grammar VAE comparison","comment":"Thanks for adding this comparison against the Grammar VAE model. I think it certainly allows for a better placement of your proposed model w.r.t related work.\n\nWhile I hoped this comparison would tell a clear story about whether a) decoding with a grammar or b) decoding directly into a graph representation would work better, it seems that the results that you report raise a few more questions.\n\nIt is unclear to me why your proposed LSTM decoder baseline (without respecting SMILES grammar) should work so much better (in terms of number of valid molecules) than the recurrent decoder of the grammar VAE. You mention that you suspect that the lack of full auto-regressiveness (output is not fed back as input to the next time step) might be a distinguishing factor. Since there is such a significant difference in model performance, this would certainly have to be experimentally verified in order to be an acceptable explanation for this difference. What happens if your LSTM decoder is trained with teacher forcing instead (like the original SMILES VAE paper https://arxiv.org/pdf/1610.02415v1.pdf) or without any new input at each time step? Will it similarly degrade performance and explain the difference?\n\nAll in all, I stick to my original evaluation of the paper as I think the paper offers a promising approach for generating (small) graphs which certainly deserves attention by the community. The experimental evaluation is extensive while some points (see above) still require clarification. I hope the authors can address my last few questions should the paper be accepted (or for some later later venue)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515494645976,"tcdate":1515494645976,"number":7,"cdate":1515494645976,"id":"SJ0xpMM4f","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"S1crSKYgM","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/AnonReviewer2"],"content":{"title":"Rebuttal response","comment":"Thanks to the authors for the rebuttal and their modifications. In my opinion, the problem of generating large graphs remains. Also, the interplay between the intermediate node/graph representations and the generation process during learning remains unclear to me."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515168252928,"tcdate":1515168252928,"number":6,"cdate":1515168252928,"id":"HJH-GmaXz","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"Hk8ilROQG","signatures":["ICLR.cc/2018/Conference/Paper556/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/Authors"],"content":{"title":"We have added a new comment w.r.t. the grammar VAE","comment":"Thank you for your review and for suggesting a comparison against grammar VAE.\n\nWe have tried grammar VAE on our dataset.  Please see our latest comment above for more detail."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515167858182,"tcdate":1515167858182,"number":5,"cdate":1515167858182,"id":"Hk9Ol76mG","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/Authors"],"content":{"title":"New results comparing against the grammar VAE baseline","comment":"We have tried the grammar VAE on our dataset and did a comparison with the results reported in our paper.  The experiment was based on the published code for grammar VAE available here: https://github.com/mkusner/grammarVAE\n\nThe code did not work directly on our data, so we made a few tweaks:\n- The grammar they used was tailored to their dataset, and did not directly work on our dataset, i.e. many of our molecules cannot be generated by their grammar.  We have added a few more grammar rules to make it work on our dataset.\n- They used a variant of the VAE loss where the weighting of the reconstruction part and the KL part of the loss did not directly correspond to the standard ELBO bound, so we changed it to make the VAE bound comparable to our likelihood estimates.\n- No sampling code was provided in the codebase, so we added our own implementation based on their code.  The sampling process generates random latents from the prior N(0,1) and then pass them through the decoder with all the grammar handling described in the Algorithm 1 in the grammar VAE paper, this part was used for evaluation.\n\nAfter training, the grammar VAE achieves a negative ELBO of 11.98 on the test set, but out of 100,000 samples generated from the trained model, only 29.56% are valid SMILES strings.  Note the 11.98 ELBO bound is considerably better than reported in our paper with the best numbers around 20, but the fraction of valid SMILES strings is a lot worse than our results where it is easy to get over 90% valid, but this result is on par with the reported numbers in the grammar VAE paper where around 31(+/- 7) % are valid after Bayesian optimization.\n\nThese results are a bit surprising, we try to interpret these results with the following explanations:\n- Our graph model and the LSTM baseline are capable of modeling a wider class of molecules than the grammar VAE due to the limited capability of the grammar.  The grammar used in grammar VAE is a context-free grammar with a set of simple expansion rules, which is enough for modeling our datasets.  But our models would still assign some probability to more complicated graphs (those with nested loops for example) therefore leading to a lower likelihood number.\n- The grammar offers very strong domain knowledge that is very helpful for shaping the likelihood of a given string.  In the implementation the grammar is used to zero-out inapplicable expansion rules and renormalize the rest which can significantly boost the likelihood of a given sequence where our model and the LSTM baseline do not have access to any of these.\n- However, when sampling, the grammar is still quite brittle as it can generate many invalid strings, for example unpaired digits for rings, and invalid valence for certain atoms.  To capture these more complex behaviors more complicated grammars need to be used, which requires significant expert knowledge.  Our approach and the LSTM baseline does not use any such domain knowledge.  In our evaluation the quality of the generated samples from the grammar VAE model is considerably worse than both our model and the LSTM baseline.\n- The decoder of the grammar VAE model is not fully auto-regressive, i.e. the output of one step is not fed back to the model as the input to the next step, making it fully auto-regressive may improve performance.\n\nOverall, our approach offers a very generic and powerful solution to the graph generation problem without the need of domain expertise, while the grammar VAEs went the opposite route which relies on expert knowledge (the grammar).  Nevertheless, we can combine our graph generation model with domain knowledge including grammars to help us in the graph generation process to further improve performance."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1514885278462,"tcdate":1514885278462,"number":4,"cdate":1514885278462,"id":"Hk8ilROQG","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"HkCWGa0xM","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer4"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/AnonReviewer4"],"content":{"title":"Rebuttal response","comment":"I would like to thank the authors for their detailed response and for adding a model description section in appendix A that clarifies implementation details. \n\nAs pointed out in my initial review, I still feel that the paper misses a direct experimental comparison against some related established work, which is why I am not willing to change my review score at this point. As mentioned in my review, I think it would be best to compare (or at least comment on why such a comparison was left out) against work such as the Grammar VAE (M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017). "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513678444508,"tcdate":1513678345739,"number":3,"cdate":1513678345739,"id":"B1GMUvUGf","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"Bk1RrPUMz","signatures":["ICLR.cc/2018/Conference/Paper556/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/Authors"],"content":{"title":"More comments on other concerns raised in the reviews","comment":"In the following we clarify a few other concerns raised by the reviewers:\n\nReviewer 4: comparison against Pointer Networks\n\nPointer networks provide a way to select and output items from a set of candidates.  We used this pointer-style mechanism in our model in the node selection module f_nodes.  Standard pointer nets assume a set of candidates is given, e.g. the input sequence as a set of candidate tokens in a seq2seq framework.  In our model we learn to construct this set of candidates (a set of nodes in the graph) starting from an empty set, which is non-trivial.\n\nReviewer 2: model is too general, better focus on restricted classes of graphs\n\nThe primary goal of this work is to have a powerful generic model capable of generating any arbitrary graphs.  This is an important but not well-studied task as recognized by Reviewers 3 and 4.  For long we have specific models designed for restricted classes of graphs, e.g. models of trees, and models that capture some properties of graphs like the random graph models discussed in the related work, but to our knowledge our work is the first generic model that is capable of generating any type of graphs.  The model is powerful and can adapt its graph generating behavior by learning from data.  Comparing our proposed model to the previous graph generative models is in spirit analogous to the contrast between RNN language models and grammar-based or n-gram language models.\n\nReviewer 2: the model is tweaked for generating trees, this seems to be cheating\n\nIn the experiments in section 4.1, we used the exact same model to learn on three different datasets without any tweaking for each individual dataset, learning to generate cycles, trees and Barabasi-Albert graphs, and our proposed model can successfully adapt and generate graphs similar to each of these three datasets.  In the parsing experiment in section 4.3, we removed the inner loop and always generate one edge fore each new node.  This simplified the model and introduced a bit more structure into our model, which results in a performance improvement.   Note that the baselines we compared against also exploits the tree structure, in particular the sequentialized trees encode the tree structure with opening and closing brackets, which is very effective, and this information is not available to the graph model as we trained exclusively on the very generic graph generating sequences.\n\nReviewer 2: discussion on graph grammars not clear\n\nThe questions about decidability in graph grammars is a mostly orthogonal to the point of the paper. We included this discussion to provide context to the paper since graph grammars (of various classes) and automata have been widely used in attempts to formalize generative models of graphs. Corcelle’s undecidability/impossibility results are precisely why we are taking an alternative approach to modeling graphs in this work. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513678401942,"tcdate":1513678279114,"number":2,"cdate":1513678279114,"id":"Bk1RrPUMz","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/Authors"],"content":{"title":"We thank the reviewers for the reviews and have updated the paper","comment":"We thank the reviewers for the thoughtful reviews and suggestions, and for recognizing the significance and novelty of this work.  Graph generation is an important topic, and our work provides a generic framework that is capable of generating arbitrary graphs through learning from data.\n\nWe have updated the submission to address some of the comments we received so far, which help us making this paper better, in particular:\n\n- We have added an entire section B in the appendix to describe model implementation details which should help clarifying confusions, as all reviewers raised this concern.  In addition we have added detailed hyperparameter settings for all tasks in appendix C to make the results more reproducible.\n\n- We added a reference to “Learning Graph State Transitions” [1] and discussed the relationship and differences between our work and [1] at the end of section 2.  As reviewer 4 and the anonymous comment pointed out, both our work and [1] share some similarities.  However [1] mostly uses a graph as intermediate representation to help solving reasoning tasks, while we aim to learn an unconditional or conditional probabilistic model of a distribution of graphs from a sample of representative graphs.  As generative models of graphs, [1] assigns soft strengths for each node and edge, while in our generative model in a sample a node / edge either exists or does not exist.  [1] also made a few strong assumptions about the graph generation process, while we don’t make any such assumptions.  See the paper for more details.\n\n- We added a sentence at the end of the paragraph following equations (1)-(3) to address Reviewer 4’s comments on weight sharing, explaining that the parameters in different rounds of propagation don’t have to be tied, and in the experiments we always use different parameters in different propagation rounds which empirically is consistently better than tied weights.  Reviewer 4 also suggested we may drop weight sharing in the outer recurrent loop as well, but this is hard as the graph generating sequences are not fixed length sequences.  It is unclear how dropping weight sharing would work here.\n\n- We added some extra discussion on learning an ordering to the last paragraph of section 3.4.  We thank Reviewer 4 for pointing out the related work of [2].  [2] described a way to match a set of ground truths to a sequence of candidates generated by a model, which is related to learning an ordering.  We have added this reference in the paper.  Applying such a matching-based solution seems challenging in our setting though, as it is unclear how this can be used to learn a distribution over graphs, and we don’t have a clear distance metric between the generated graph components and the reference graph.  Learning such a distance metric by itself seems to be a nontrivial task.  We are aware of some other literature on learning an ordering / permutation, in particular from the learning to rank community, we have added a few other references in this direction and hope this can provide some alternative insights on this problem.\n\n- We modified figure 1 and added another possible graph generating sequence to figure 6, which Reviewer 3 suggested could make the presentation clearer.\n\n- We added a paragraph discussing the effect of fitting the fixed canonical ordering to the end of Appendix C.2.  As reviewer 3 pointed out, our model may overfit to a particular ordering if it is always trained with that ordering.  In the experiments we do observe that the model assigns higher probability to the canonical ordering it is being trained on, and much less probability to other orderings.  However, in some cases the canonical ordering does not have the highest probability under a trained model, as can be also seen from Table 3, where the likelihood under fixed ordering (the ordering being trained on) is not always the same as the likelihood under the best possible ordering.  This indicates there may be potential in learning an ordering improving the canonical one.\n\n- We changed the paragraph on the “dependence on T” to focus more on the challenges w.r.t. scalability, as Reviewer 2 mentioned this could make the paper clearer.\n\n- We changed a few numbers in Table 2 and 3 to reflect our latest results after the deadline, which does not change the overall conclusion on the comparison between different approaches.\n\nWe will try to add more experimental results to the paper when they are ready.\n\n[1] Learning Graph State Transitions.  Daniel D Johnson.  ICLR 2017.\n[2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes.  CVPR 2016\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642468148,"tcdate":1512129030161,"number":3,"cdate":1512129030161,"id":"HkCWGa0xM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Review","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Revisiting auto-regressive models for graph generation","rating":"6: Marginally above acceptance threshold","review":"The authors introduce a sequential/recurrent model for generation of small graphs. The recurrent model takes the form of a graph neural network. Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden state. \n\nThe paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field.\n\nThe proposed model has several interesting novelties (mainly in terms of new applications/experiments, and being fully auto-regressive), yet also shares many similarities with the generative component of the model introduced in [1] (not cited): Both models make use of (recurrent) graph neural networks to learn intermediate node representations, from which they predict whether new nodes/edges should be added or not. [1] speeds this process up by predicting multiple nodes and edges at once, whereas in this paper, such a multi-step process is left for future work. Training the generative model with fixed ground-truth ordering was similarly performed in [1] (“strong supervision”) and is thus not particularly novel.\n\nEqs.1-3: Why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop (h_v -> h_v’)? Have the authors experimented with other variants (dropping the weight sharing in either or both of these steps)?\n\nOrdering problem: A solution for the ordering problem was proposed in [2]: learning a matching function between the orderings of model output and ground truth. A short discussion of this result would make the paper stronger.\n\nFor chemical molecule generation, a direct comparison to some more recent work (e.g. the generator of the grammar VAE [3]) would be insightful.\n\nOther minor points:\n- In the definition of f_nodes: What is p(y)? It would be good to explicitly state that (boldface) s is a vector of scores s_u (or score vectors, in case of multiple edge types) for all u in V.  \n- The following statement is unclear to me: “but building a varying set of objects is challenging in the first place, and the graph model provides a way to do it.” Maybe this can be substantiated by experimental results (e.g. a comparison against Pointer Networks [4])?\n- Typos in this sentence: “Lastly, when compared using the genaric graph generation decision sequence, the Graph architecture outperforms LSTM in NLL as well.”\n\nOverall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well-written, well-presented and addresses an important problem.\n\n[1] D.D. Johnson, Learning Graphical State Transitions, ICLR 2017\n[2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016\n[3] M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017\n[4] O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, NIPS 2015","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642468186,"tcdate":1511816520874,"number":2,"cdate":1511816520874,"id":"ByZ8Tx9ez","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Review","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A promising generative model for graphs","rating":"6: Marginally above acceptance threshold","review":"The authors proposed a graph neural network based architecture for learning generative models of graphs. Compared with traditional learners such as LSTM, the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data. The representation is clear with detailed empirical studies. I support its acceptance.\n\nThe draft does need some improvements and here is my suggestions.\n1. Figure 1 could be improved using a concrete example like in Figure 6. If space allowed, an example of different ordering leads to the same graph will also help.\n\n2. More details on how node embedding vectors are initialized. How does different initializations affect results? Why is nodes at different stages with the same initialization problematic?\n\n3. More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation.\n\n4. The sequence ordering is important. While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment. I suggest the authors at least discuss the empirical over-fitting problem with respect to ordering.\n\n5. In Section 4.1, the choice of ER random graph as a baseline is too simplistic. It does not provide a meaningful comparison. A better generative model for cycles and trees could help.\n\n6. When comparing training curves with LSTM, it might be helpful to also include the complexity comparison of each iteration.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642468223,"tcdate":1511785793558,"number":1,"cdate":1511785793558,"id":"S1crSKYgM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Review","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference/Paper556/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Learning deep generative models for graphs","rating":"5: Marginally below acceptance threshold","review":"The paper introduces a generative model for graphs. The three main decision functions in the sequential process are computed with neural nets. The neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step T. The paper is well written but, in my opinion, a description of the learning framework should be given in the paper. Also, a summary of the hyperparameters used in the proposed system should be given. It is claimed that all possible types of graphs can be learned which seems rather optimistic. For instance, when learning trees, the system is tweaked for generating trees. Also, it is not clear whether models for large graphs can be learned. The paper contain many interesting contributions but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs. Therefore, I am not convinced that the paper is ready for publication at ICLR'18.\n\n* Introduction. I am not convinced by the discussion on graph grammars in the second paragraph. It is known that there does not exist a definition of regular grammars in graph (see Courcelle and Engelfriet, graph structure and monadic second-order logic ...). Moreover, many problems are known to be undecidable. For weighted automata, the reference Droste and Gastin considers weighted word automata and weighted logic for words. Therefore I does not seem pertinent here. A more complete reference is \"handbook of weighted automata\" by Droste. Also, many decision problems for wighted automata are known to be undecidable. I am not sure that the paragraph is useful for the paper. A discussion on learning as in footnote 1 shoud me more interesting.\n* Related work. I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs.\n* Section 3.1. Constraints can be introduced to impose structural properties of the generated graphs. This leads to the question of cheating in the learning process.\n* Section 3.2. The functions f_m and g_m for defining graph embedding are left undefined. As the graph embedding is used in the generating process and for learning, the functions must be defined and their choice explained and justified.\n* Section 3. As said before, a general description of the learning framework should be given. Also, it is not clear to me how the node and graph embeddings are initialized and how they evolve along the learning process. Therefore, it is not clear to me why the proposed updating framework for the embeddings allow to generate decision functions adapted to the graphs to be learned.  Consequently, it is difficult to see the influence of T. Also, it should be said whether the node embeddings and graph embeddings for the output graph can be useful.\n* Section 3. A summary of all the hyperparameters should be given.\n* Section 4.1. The number of steps is not given. Do you present the same graph multiple times. Why T=2 and not 1 or 10 ?\n* Section 4.2. From table 2, it seems that all permutations are used for training which is rather large for molecules of size 20. Do you use tweaks in the generation process.\n* Section 4.3. The generation process is adapted for generating trees which seems to be cheating. Again the choice of T seems ad hoc and based on computational burden.\n* Section 5 should contain a discussion on complexity issues because it is not clear how the model can learn large graphs.\n* Section 5. The discussion on the difficulty of training shoud be emphasized and connected to the --missing-- description of the model architecture and its hyperparameters.\n* acronyms should be expansed at their first use","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510837174661,"tcdate":1510837174661,"number":1,"cdate":1510837174661,"id":"r1kajbsyG","invitation":"ICLR.cc/2018/Conference/-/Paper556/Official_Comment","forum":"Hy1d-ebAb","replyto":"HkzAQO_kM","signatures":["ICLR.cc/2018/Conference/Paper556/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper556/Authors"],"content":{"title":"Reply","comment":"Thanks for the comment and bringing up this related paper.  We will update our paper with more discussion and citations to related work (we are not allowed to make changes to our submission at the moment).\n\nThe main difference between our work and Johnson (2017) is that our goal in this paper is to learn and represent unconditional or conditional densities on a space of graphs given a representative sample of graphs, whereas Johnson is primarily interested in using graphs as intermediate representations in reasoning tasks.  However, Johnson (2017) do offer a probabilistic semantics for their graphs (the soft, real-valued node and connectivity strengths).  But, as a generative model, Johnson (2017) did make a few strong assumptions for the generation process, e.g. a fixed number of nodes for each sentence, independent probability for edges given a batch of new nodes, etc.; while our model doesn't make any of these assumptions.\n\nOn the other side, as we are modeling graph structures, the samples from our model are graphs where an edge or node either exists or does not exist; whereas in Johnson (2017) all the graph components, e.g. existence of a node or edge, are all soft, and it is this form of soft node / edge connectivity that was been used for other reasoning tasks.  Dense and soft representation may be good for some applications, while the sparse discrete graph structures may be good for others.  Potentially, our graph generative model can also be used in an end-to-end pipeline to solve prediction problems as well, like Johnson (2017)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510667210034,"tcdate":1510667210034,"number":1,"cdate":1510667210034,"id":"HkzAQO_kM","invitation":"ICLR.cc/2018/Conference/-/Paper556/Public_Comment","forum":"Hy1d-ebAb","replyto":"Hy1d-ebAb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relation to Learning Graphical State Transitions","comment":"I've enjoyed reading this paper, but I'm wondering if the authors are aware of \"Learning Graphical State Transitions\" (Johnson, ICLR'17 oral). The work presented here feels like a generalization, but it shares many ideas with the earlier paper, and a discussion of the differences would definitely be very helpful."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513677938396,"tcdate":1509126502795,"number":556,"cdate":1509739235291,"id":"Hy1d-ebAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hy1d-ebAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Deep Generative Models of Graphs","abstract":"Graphs are fundamental data structures required to model many important real-world data, from knowledge graphs, physical and social interactions to molecules and proteins. In this paper, we study the problem of learning generative models of graphs from a dataset of graphs of interest. After learning, these models can be used to generate samples with similar properties as the ones in the dataset.  Such models can be useful in a lot of applications, e.g. drug discovery and knowledge graph construction. The task of learning generative models of graphs, however, has its unique challenges. In particular, how to handle symmetries in graphs and ordering of its elements during the generation process are important issues. We propose a generic graph neural net based model that is capable of generating any arbitrary graph.  We study its performance on a few graph generation tasks compared to baselines that exploit domain knowledge.  We discuss potential issues and open problems for such generative models going forward.","pdf":"/pdf/a5216c3df94438533331393c67a868d4bff54bfe.pdf","TL;DR":"We study the graph generation problem and propose a powerful deep generative model capable of generating arbitrary graphs.","paperhash":"anonymous|learning_deep_generative_models_of_graphs","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models of Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy1d-ebAb}\n}","keywords":["Generative Model of Graphs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper556/Authors"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}