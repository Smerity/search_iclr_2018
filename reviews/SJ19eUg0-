{"notes":[{"tddate":null,"ddate":null,"tmdate":1515595828564,"tcdate":1515595828564,"number":1,"cdate":1515595828564,"id":"ry6E_jXVz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Public_Comment","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"More detail study and experiments","comment":"1. Wall clock time plots:\n     I think it is very important that you include in the final paper the wall-clock time plots. Per iteration, improvement is not of that much interest from a practical perspective if there is significantly different time computation. In the optimization of any Machine Learning model, there are two things of interest - if the final result is significantly different and how long it takes to get there. In your reply to AnonReviwer2, you mentioned that it is between 5-10 times of ADAM. However, it would be nice to clear up if you have actually measured that or is this an eye-balling of the factor. Also is this on a CPU or on a GPU? The aim of such publication should be towards people adopting these methods and without this **very** relevant information it really does not tell us a lot.\n\n2. Batch size and model size\n   In your first 2 experiments, you either use very large batch sizes (6000 or 1000) or very tiny models (LSTM with 10 hidden units). These very unrepresentative of what modern Deep Learning workloads are. Nevertheless, this is not nessacarily an issue, as the method is new. However, failing to report what happens when you decrease the batch size (very standard batch size is ~ 100-200) or increase the model size in the LSTM example (the smallest LSTMs I've seen used in practical applications have at least a few hundred nodes)  is missfortunate. Especially, from some of the papers in the literature, we know that using too small batches for the curvature matrices (when you can not afford to store moving averages like in [1,2,3]) can lead to significantly detrimental effects, as the Monte-Carlo estimates of the Gauss-Newton and the Hessian become degenerate. I genuinely hope that the authors decide to include this comparison in the final version. The final experiment in the paper is indeed interesting. However, as we can see ADAM achieves lower training cost. Although they have same test accuracy, this is really not much meaningful, as it is highly unfair to compare optimizer based on their generalization performance, when these are methods built for optimizing the objective at hand.\n\n3. Hyperparameter optimization\n   In the comparison against ADAM, you use the default setting of the optimizer. Although that is used often in practice, it is again unfair, since given that you have presented only 3 experiments, it is highly likely that the hyperparameters selected for the BHF have been highly tuned to those specific datasets. In [3] they specifically state that they also do hyperparameter search for ADAM parameters, including the decay rate of the learning rate. Additionally, on some of the CNN in the literature is not uncommon to use Momentum rather than ADAM. Thus a more in-depth comparison against better fine-tuned first order methods would be quite desirable. \n\n[1] Optimizing Neural Networks with Kronecker-factored Approximate Curvature. ICML 2015\n[2] A Kronecker-factored Approximate Fisher Matrix for Convolution Layers. ICML 2016\n[3] Practical Gauss-Newton Optimisation for Deep Learning. ICML 2017. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1515004823013,"tcdate":1515004823013,"number":3,"cdate":1515004823013,"id":"H11iXicmz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Comment","forum":"SJ19eUg0-","replyto":"SyJaBw1eG","signatures":["ICLR.cc/2018/Conference/Paper268/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper268/Authors"],"content":{"title":"RE: There is nothing particularly wrong with the paper - it is a nice work, that is based a lot on previous attempts, like those in Martens. Coming from a more theoretical background, I would like to see more theory. Nevetheless this does not lessens the value of the paper. For the moment, weak accept until I also read the other reviews.","comment":"We would like to thank the reviewer for appreciating our contribution in this paper. \n\n------It is not clear why the deficiency of first-order methods on training NNs with big batches motivates us to turn into second-order methods. Is there a reasoning for this statement? Or is it just because second-order methods are kind-of the only other alternative we have?\n\nResponse: It have been shown that second-order methods worked well with big batch sizes and in fact small batch size will make the convergence of the second-order methods unstable and hurt their performances. On the contrary, the first-order methods on training NNs with big batches have problem on the speedup and generalization (Keshar et al. 2016; Takac et al. 2013; Dinh et al. 2017). These  deficiencies of first-order methods with large mini batch size motivate us to turn into second-order methods to handle big batch size.\n \n------Assuming we can perform a second-order method, like Newton's method, on a deep NN. Since originally Newton's method was designed to find solutions that have gradient equal to zero, and since NNs have saddle points (probably many more than local minima), even if we could perfectly perform second-order Newton motions, there is no guarantee whether we converge to a local minimum or a saddle point. However, since we perform Newton's method approximately in practice, this might help escaping saddle points. Any comment on this aspect (I'm not aware whether this is already commented in Schraudolph 2002, where the Gauss-Newton matrix was proposed instead of the Hessian)?\n \nResponse: The reviewer proposes an very interesting view of the possible advantage of the Gauss-Newton matrix and the approximate Newton over Newton’s method, which was not commented in (Schraudolph 2002). As far as we know the main problem of Newton’s method on trading  deep NN is that for nonlinear system, the Hessian matrix is not necessarily positive definite so Newton’s method may diverge, which is consistent with the unstable practical performance of Newton’s method in training deep NN. The Gauss-Newton matrix is an approximation of the local curvature with positive semidefinite property as long as the loss function with respect to  the output of the network is convex, which holds for most popular loss functions (MSE, cross entropy). Indeed, these approximations to the curvature matrix may act as noises which help to escape saddle points while the exact Newton’s method may fail. This perspective  requires further exploration.\n \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1515005381800,"tcdate":1515004590242,"number":2,"cdate":1515004590242,"id":"BkI3GiqXz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Comment","forum":"SJ19eUg0-","replyto":"BkCQ4Ywgz","signatures":["ICLR.cc/2018/Conference/Paper268/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper268/Authors"],"content":{"title":"RE: The paper proposes a block-diagonal second order method for training deep networks. The algorithm is not novel. Experimental results are good in training auto-encoder and LSTM (in terms of number of updates).","comment":"Thank the reviewer for the thoughtful feedback.\n\n------The block-diagonal approximation has been used in [1]. Although [1] is using Gauss-Newton(GN) matrix, the idea of \"block-diagonal\" approximation is similar.\n\nResponse: The work [1] and earlier works [2,3] uses block-diagonal approximation to the Gauss-Newton matrix or Fisher information matrix and requires explicit representation and inverse of each block of GN matrix or Fisher matrix. [1, 2] are only applied to feedforward networks. Further approximation and assumption are considered for working around convolutional neural networks [3]. It is not obvious how to generalize the algorithm to recurrent networks. The significant difference between ours and [1,2,3] is that we use block-diagonal approximation when evaluating the Hessian (Gauss-Newton) vector product and don’t require explicit inverse of the GN matrix and can work directly with convolutional and recurrent networks.\n \n------Is the computational time (per iteration) of the proposed method similar to SGD/Adam? All the figures are showing the comparison in terms of number of updates, but it is not clear whether this speedup can be reflected in the training time.\n\nResponse: The time consumption of block-diagonal Hessian-free (BHF) and that of the full Hessian-free (HF) are comparable. The time per iteration of BHF and HF is 5-10 times of the Adam method. However, the total number of iterations of BHF and HF are much smaller than Adam, which can offset the per-iteration cost.\n\n \n------Comparing block-diagonal approximation vs original HF method: It is not clear to me what's the benefit using block-diagonal approximation. Is the time cost per iteration similar or faster? Or the main benefit is to reduce #CG iterations? (but it seems #CG iterations are fixed for both methods in the experiments). Also, the paper mentioned that \"the HF method requires many hundreds of CG iterations for one update\". Is this true? Usually we can set a stopping condition for solving the Newton system.\n\nResponse: The BHF algorithm partitions the original HF method into a bunch of sub-problems and solves each sub-problem with the same CG iterations as the full HF method and hence may get more accurate solution. In practice given a fixed budgets of CGs the BHF takes slightly more time (15%) per update than the full HF method if without paralleling the sub-problems onto different workers but achieves better accuracy. Hence the main benefit is to reduce #CGs and achieve better accuracy. We note that the performance of HF cannot be improved by simply increasing the #CGs. It is true that we can set a stopping condition (fixed number of CGs) for solving the Newton system. How to achieve good accuracy given a number of CGs for solving the Newton system is not clear. Our BHF algorithm provides a way that easily achieve good accuracy with a small number of CG runs.\n\n\n \n[1] Practical Gauss-Newton Optimisation for Deep Learning. ICML 2017.\n[2] Optimizing Neural Networks with Kronecker-factored Approximate Curvature. ICML 2015\n[3] A Kronecker-factored Approximate Fisher Matrix for Convolution Layers. ICML 2016\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1515004721351,"tcdate":1515004264587,"number":1,"cdate":1515004264587,"id":"rJ-_ZscQz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Comment","forum":"SJ19eUg0-","replyto":"SJ1GzQKxz","signatures":["ICLR.cc/2018/Conference/Paper268/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper268/Authors"],"content":{"title":"RE: Nice paper, however, I would be happier if more experiments on larger datasets are presented","comment":"We thank the reviewer for appreciating our work.\nThe “x”-axis represents the number of updates and CG cost is not included.\nThe time consumption of block-diagonal Hessian-free (BHF) and that of the full Hessian-free (HF) are comparable. The time per iteration of BHF and HF is 5-10 times of the Adam method. However, the total number of iterations of BHF and HF are much smaller than Adam, which can offset the per-iteration cost.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1515642422483,"tcdate":1511760392544,"number":3,"cdate":1511760392544,"id":"SJ1GzQKxz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Review","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference/Paper268/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice paper, however, I would be happier if more experiments on larger datasets are presented","rating":"6: Marginally above acceptance threshold","review":"In this paper, authors discuss the use of block-diagonal hessian when computing the updates. The block-diagonal hessian makes it easier to solve the \"newton\" directions, as the CG can be run only on smaller blocks (and hence less CG iterations are needed).\n\nThe paper is nicely written and all was clear to me. In general, I agree that having larger batch-size is the way to go, for very large datasets and a pure SGD type of methods are having problems to efficiently utilize large clusters.\n\nThe only negative thing I find in the paper is the lack of more numerical results. Indeed, the paper is clearly not a theoretical paper, is proposing a new algorithm, hence there should be evidence that it works. For example, I would like to see how the choice of hyper-parameters influences the speed of the algorithm. Was \"CG\" cost included in the \"x\"-axis? i.e. if we put \"passes\" over the data as x-axis, then 1 update \\approx 30 CG + some more == 32 batch evaluation.\nSo please try to make the \"x\"-axis more fair.\n\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1515642422531,"tcdate":1511654437607,"number":2,"cdate":1511654437607,"id":"BkCQ4Ywgz","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Review","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference/Paper268/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper proposes a block-diagonal second order method for training deep networks. The algorithm is not  novel. Experimental results are good in training auto-encoder and LSTM (in terms of number of updates). ","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a block-diagonal hessian-free method for training deep networks. \n\n- The block-diagonal approximation has been used in [1]. Although [1] is using Gauss-Newton matrix, the idea of \"block-diagonal\" approximation is similar. \n\n- Is the computational time (per iteration) of the proposed method similar to SGD/Adam? All the figures are showing the comparison in terms of number of updates, but it is not clear whether this speedup can be reflected in the training time. \n\n- Comparing block-diagonal approximation vs original HF method: \nIt is not clear to me what's the benefit using block-diagonal approximation. Is the time cost per iteration similar or faster? \nOr the main benefit is to reduce #CG iterations? (but it seems #CG iterations are fixed for both methods in the experiments). \nAlso, the paper mentioned that \"the HF method requires many hundreds of CG iterations for one update\". Is this true?\n Usually we can set a stopping condition for solving the Newton system.\n\n- It seems the benefit of block-diagonal approximation is marginal in CNN. \n\n[1] Practical Gauss-Newton Optimisation for Deep Learning. ICML 2017. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1515642422568,"tcdate":1511122359313,"number":1,"cdate":1511122359313,"id":"SyJaBw1eG","invitation":"ICLR.cc/2018/Conference/-/Paper268/Official_Review","forum":"SJ19eUg0-","replyto":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference/Paper268/AnonReviewer3"],"readers":["everyone"],"content":{"title":"There is nothing particularly wrong with the paper - it is a nice work, that is based a lot on previous attempts, like those in Martens. Coming from a more theoretical background, I would like to see more theory. Nevetheless this does not lessens the value of the paper. For the moment, weak accept until I also read the other reviews. ","rating":"6: Marginally above acceptance threshold","review":"Summary: \nThe paper considers second-order optimization methods for training of neural networks.\nIn particular, the contribution of the paper is a Hessian-free method that works on blocks of parameters (this is a user defined splitting of the parameters in blocks, e.g., parameters of each layer is one block, or parameters in several layers could constitute a block). \nThis results into a block-diagonal approximation to the curvature matrix, in order to improve Hessian-free convergence properties: in the latter, a single step might require many CG steps, so the benefit from using second-order information is not apparent.\nThis is mainly an experimental work, where the authors show the merits of their approach on deep autoencoders, convolutional networks and LSTMs: results show favourable performance compared to the original Hessian-free approach and the Adam method.\n\nOriginality: \nThe paper is based on the works of Collobert (2004) and Le Roux et al. (2008), as well as the work of Martens: the twist is that each layer of the neural network is considered a parameter block, so that gradient interactions among weights in a single layer are more useful than those between weights in different layers. This increases the separability of the problem and reduces the complexity. \n\nImportance: \nUnderstanding the difference between first- and second-order methods for NN training is an important topic. Using second-order methods could be considered at its infancy, compared to the wide variety of first-order methods. Having new results on second-order methods with interesting results would definitely attract some attention at the conference. \n\nPresentation/Clarity: \nThe paper is well structured and well written. The authors clearly place their work w.r.t. state of the art and previous works, so that it is clear what is new and what is known.\n\nComments:\n1. It is not clear why the deficiency of first-order methods on training NNs with big batches motivates us to turn into second-order methods. Is there a reasoning for this statement? Or is it just because second-order methods are kind-of the only other alternative we have?\n\n2. Assuming we can perform a second-order method, like Newton's method, on a deep NN. Since originally Newton's method was designed to find solutions that have gradient equal to zero, and since NNs have saddle points (probably many more than local minima), even if we could perfectly perform second-order Newton motions, there is no guarantee whether we converge to a local minimum or a saddle point. However, since we perform Newton's method approximately in practice, this might help escaping saddle points. Any comment on this aspect (I'm not aware whether this is already commented in Schraudolph 2002, where the Gauss-Newton matrix was proposed instead of the Hessian)?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]}},{"tddate":null,"ddate":null,"tmdate":1513808817947,"tcdate":1509085318641,"number":268,"cdate":1509739392207,"id":"SJ19eUg0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ19eUg0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS","abstract":"Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.","pdf":"/pdf/24d0da99dcaf71bb338d27742f4a21fa1b547b6e.pdf","paperhash":"anonymous|blockdiagonal_hessianfree_optimization_for_training_neural_networks","_bibtex":"@article{\n  anonymous2018block-diagonal,\n  title={BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ19eUg0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper268/Authors"],"keywords":["deep learning","second-order optimization","hessian free"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}