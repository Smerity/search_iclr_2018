{"notes":[{"tddate":null,"ddate":null,"tmdate":1512397016023,"tcdate":1512397016023,"number":3,"cdate":1512397016023,"id":"HkeyY0M-z","invitation":"ICLR.cc/2018/Conference/-/Paper602/Official_Review","forum":"rJrTwxbCb","replyto":"rJrTwxbCb","signatures":["ICLR.cc/2018/Conference/Paper602/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A thought provoking tentative claim but exposition needs a lot of work.","rating":"5: Marginally below acceptance threshold","review":"This paper has at its core an interesting, novel, tentative claim, backed up by simple experiments, that small batch gradient descent and large batch gradient descent may converge to points in the same basin of attraction, contrary to the discussion (but not the actual experimental results) of Keskar et al. (2016). In general, there is a pressing need for insight into the qualitative behavior of gradient-based optimization and this area is of immense interest to many machine learning practitioners. Unfortunately the interesting tentative insights are surrounded by many unsubstantiated and only tangentially related theoretical discussions. Overall the paper has the appearance of lacking a sharp focus. This is a shame since I found the core of the paper very interesting and thought provoking.\n\nMajor comments:\n\nWhile the paper has some interesting tentative experimental insights, the relationship between theory and experiment is complicated. The theoretical claims are vague and wide ranging, and are not all individually well supported or even tested by the experiments. Rather than including lots of small potential insights which the authors have had about what may be going on during gradient-based optimization, I'd prefer to see a paper with much tighter focus with a small number of theoretical claims well supported by experiments (it's fine if the experiments are simplistic as here; that's still interesting).\n\nA large amount of the paper hinges on being able to ignore the second term in (6), and this fact is referred to many times, but the theoretical and experimental justification for this claim is very thin.\n\nThe authors mention overparameterization repeatedly, and it's in the title, but they never define it. It also doesn't appear to take center stage in their experimental investigations (if it is in fact critical to the experiments then it should be made clearer how).\n\nThroughout this paper there is not a clear distinction between eigenvalues being zero and eigenvalues being close to zero, or similarly between the Hessian being singular and ill-conditioned. This distinction is particularly important in the theoretical discussion.\n\nIt would be helpful to be clearer about the differences between this work and that presented in Sagun et al. (2016).\n\nMinor comments:\n\nThe assumption that the target y is real is at odds with many regression problems and practically all classification. It might be worth generalizing the discussion to multidimensional targets.\n\nIt would be good to have some citations to support the claim that often \"the number of parameters M is comparable to the number of examples N (if not much larger)\". With 1-dimensional targets as considered here, that sounds like a recipe for extreme overfitting and poor generalization. Generically based on counting constraints and free parameters one would expect to be able to fit exactly any dataset of N output values using a model with M free parameters. (With P-dimensional targets the relevant comparison would be M vs N P rather than M vs N).\n\nAt the end of intro to section 1, \"loss is non-degenerate\" should be \"Hessian of the loss is non-degenerate\"? Also, didn't the paper cited assume at least one negative eigenvalue at any saddle point, rather than non-degeneracy?\n\nIn section 1.1, it would be helpful to explain the precise sense in which \"overparameterized\" is being used. Hopefully it is in the sense that there are more parameters than needed for good performance at the true global minimum (the additional parameters helping with the process of *finding* a good minimum rather than its existence) or in the sense that M -> infinity for N \"equal to\" infinity. If it is in the sense that M >> N then I'm not sure of the relevance to practical machine learning.\n\nIt would be helpful to use a log scale for the plot in Figure 1. The claim that the Hessian is ill-conditioned depends on the condition number, which is impossible to estimate from the plot.\n\nThe fact that \"wide basins, as opposed to narrow ones, generalize better\" is not a new claim of the Keskar et al. paper. I'd argue it's well-known and part of the classical explanation of why maximum likelihood methods overfit and Bayesian ones don't. See for example MacKay, Information Theory Inference and Learning Algorithms.\n\n\"It turns out that the Hessian is degenerate at any given point\" makes it sound like the result is a theoretical one. As I understand it, the experimental investigation in Sagun et al. (2016) just shows that the Hessian may often be ill-conditioned. As above, more clarity is also needed about whether it is literally degenerate or just approximately so, in which case ill-conditioned is probably a more appropriate word. Ill-conditioned is also more appropriate than singular in \"slightly singular but extremely so\".\n\nHow much data was used for the simple experiments in Figure 1? Infinite data? What data was used?\n\nIt would be helpful to spell out the intuition in \"Intuitively, this kind of singularity...\".\n\nI don't think the decomposition (5) is required to \"explain why having more parameters than samples results in degenerate Hessian matrices\". Generically one would expect that with 1-dimensional targets, N datapoints and N + Q parameters, there would be a Q-dimensional submanifold of parameter space on which the loss would be zero. Of course there would be a few conditions needed to make this into a precise statement, but no need for assuming the second term is negligible.\n\nIs the conventional decomposition of the loss into l o f used for the generalized Gauss Newton that f is a function only of the input to the neural net and the model parameters, but not the target? I could be wrong, but that was always my interpretation.\n\nIt's not clear whether the phrase \"bottom of the landscape\" used several times in the paper refers to the neighborhood of local minima or of global minima.\n\nWhat is the justification for assuming l'(f(w)) and grad f(w) are not correlated? That seems unlikely to be true in general! Also spell out why this implies the second term can be ignored. I'm a bit skeptical of the claim in general. It's easy to come up with counterexamples. For example take l to be the identity (say f has a relu applied to it to ensure everything is well formed).\n\n\"Immediately, this implies that there are at least M - N trivial eigenvalues of the Hessian\". Make it clear that trivial here means approximately not exactly zero (in which case a good word would be \"small\"); this follows since the second term in (5) is only approximately zero. In fact it should be possible to prove there are M - N values which are exactly zero, but that doesn't follow from the argument presented. As above I'd argue this analysis is somewhat beside the point since N should be greater than M in practice to prevent severe overfitting.\n\nIn section 3.1, \"trivial eigenvalues\" should be \"non-trivial eigenvalues\".\n\nWhat's the relevance of using PCA on the data in Figure 2 when it comes to analyzing training neural nets? Also, is there any reason 2 classes breaks the trend?\n\nWhat size of data was used for the experiments to plot figure 2 and figure 3? Infinite?\n\nIt's not completely clear what the takeaway is from Figure 3. I presume this is supporting the point that the eigenvalues of the Hessian at convergence consist of a bulk and outliers. The could be stated explicitly. Is there any significance to the fact that the number of clusters is equal to the number of outliers? Is this supporting some broader claim of the paper?\n\nFigure 4, 5, 6 would benefit from being log plots, and make the claim that the bulk has the same shape independent of data much stronger.\n\nThe x-axis in Figure 5 is not \"ordered counts of eigenvalues\" but \"index of eigenvalues\", and in Figure 6 is not \"ratios of eigenvalues\" but ratio of the index. In the caption for Figure 6, \"scaled by their ratio\" is not clear.\n\nI don't follow why Figure 6 confirms that \"the effects of the ignored term in the decomposition is small\" for negative eigenvalues.\n\nIn section 3.3, when saying the variances of the steps are different but the means are similar, it may interesting to note that the variance is often the dominant term and much greater in magnitude than the mean when doing SGD (at least that's what I've experienced).\n\nWhat's the meaning of \"elbow at similar levels\"? What's the significance?\n\nIn section 4 it is claimed that overparameterization is what \"leads to flatness at the bottom of the landscape which is easy to optimize\". The bulk-outlier view suggests that adding extra parameters may just add extra dimensions to the flat region, but why is optimizing 100 values in a flat 100-dimensional space easier than optimizing 10 values in a flat 10-dimensional space?\n\nIn section 4.1, \"fair comparison\" is misleading since it depends on perspective. If one cares about compute time then certainly measuring steps rather than epochs would not be a fair comparison!\n\nWhat's the relevance of the fact that random initial points in high-dimensional spaces are almost always nearly orthogonal (N.B. the \"nearly\" should be added)? This seems to be assuming something about the mapping from initial point to basin of attraction.\n\nWhat's the meaning of \"extending away from either end points appear to be confirming the sharpness of [the] LB solution\"? Is this shown somewhere?\n\nIt would be helpful to highlight the key difference to Keskar et al. (which I believe is initializing SB training from LB point rather than from scratch). I presume the claim is that Keskar et al. only found their \"inverted camel hump\" linear interpolation results due to the random initialization, and that this would also often be observed for, say, two random LB-from-scratch trainings (which may randomly fall into different basins of attraction). If this is the intended point then it would be good to make this explicit.\n\nIn \"the first terms starts to dominate\", to dominate what? The gradient, or the second term in (5)? If the latter, what is the relevance of this?\n\nWhy \"even\" in \"Even when the weight space has large flat regions\"?\n\nIn the last paragraph of section 4.1, it might be worth spelling out that (as I understand it) the idea is that the small batch method finds itself in a poor region to begin with, since the average loss over an SB-noise-sized neighborhood of the LB point is actually not very good, and so there is a non-zero gradient through flat space to a place where the average loss over an SB-noise-sized neighborhood is good.\n\nIn section 5, \"we see that even large batch methods are able to get to the level where small batch methods go\" seems strange. Isn't this of training set loss? Isn't the \"level\" people care about the test set loss?\n\nIn appendix A, the meaning of consecutive in \"largest consecutive gap\" and \"largest consecutive ratio\" was not clear to me.\n\nAppendix B is only referred to in a footnote. What is its significance for the main theme of the paper? I'd suggest either making it more prominent or putting it in a separate paper.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Empirical Analysis of the Hessian of Over-Parametrized Neural Networks","abstract":"We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.","pdf":"/pdf/7e8c0c57b6e380ebc7db896ad7b0a794db065a2d.pdf","TL;DR":"The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.","paperhash":"anonymous|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJrTwxbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper602/Authors"],"keywords":["Deep Learning","Over-parametrization","Hessian","Eigenvalues","Flat minima","Large batch Small batch"]}},{"tddate":null,"ddate":null,"tmdate":1512222699961,"tcdate":1511832516657,"number":2,"cdate":1511832516657,"id":"rJT6jEcgz","invitation":"ICLR.cc/2018/Conference/-/Paper602/Official_Review","forum":"rJrTwxbCb","replyto":"rJrTwxbCb","signatures":["ICLR.cc/2018/Conference/Paper602/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The assumption for the numerical analysis is not sound and more experiments need to be conducted","rating":"4: Ok but not good enough - rejection","review":"This paper studies the spectrum of the Hessian matrix for neural networks. To explain the observation that the spectrum of Hessian is composed of a bulk of eigenvalues centered near zero and several outliers away from the bulk, it applies the generalized Gauss-Newton decomposition on the Hessian matrix and argues that the Hessian can be approximated by the average of N rank-1 matrices. It also studies the effects on the spectrum from the model size, input data distribution and the algorithm empirically. Finally, this paper revisits the issue that if SGD solutions with different batch sizes converge to the same basin. \n\nPros:\n1. The spectra of the Hessians with different model sizes, input data distributions and algorithms are empirically studied, which provides some insights into the behavior of over-parameterized neural networks. \n2. A decomposition of the Hessian is introduced to explain the degeneracy of the Hessian. Although no mathematical justification for the key approximation Eq. (6) is provided, the experiments in Sec. 3 and Sec. 4 seem to suggest the analysis and support the approximation. \n\nCons:\n1. The paper's contributions seem to be marginal. Many arguments in the paper have been first brought out in Sagun et. al.(2016) and Keskar et. al.(2016): the degeneracy of the Hessian, the bulk and outlier decomposition of the Hessian matrix and the flatness of loss surface at basins. The authors failed to show the significance of their results. For example, what further insights do the results in Sec. 3 provide to the community compared with Sagun et. al.(2016) and Keskar et. al.(2016).\n\n2. More mathematical justification is needed. For example, in the derivation of Eq (6), why can we assume l'(f) and the gradient of f to be uncorrelated? How does this lead to the vanishing of the second term in the decomposition? \n\n3.  More experiments are needed to support the arguments. For example, Sec. 4.1 shows that the solutions of SB SGD and LB SGD fall into the same basin, which is opposed to the results of Keskar et. al. (2016). However, this conclusion is not convincing. First, this result is drawn from one dataset. Second, the solution of SB SGD is initialized from the solution of LB SGD. As claimed in Keskar et. al. (2016), the solution of LB SGD may already get trapped at some bad minimum and it is not certain if SB SGD can escape from that. If it can't, then SB and LB can still be in the same basin as per the setting in this paper. So I'd like to suggest the author compare SB and LB when random initializations are conducted for both algorithms.\n\n4.  In general, this paper is easy to read. However, it is not well organized. In the introduction, the authors spent several paragraphs for line search and expensive computation of GD and the Hessian, which I don't think are very related to the main purpose of this paper. Besides, the connection between the analysis and the experimental results is very weak and should be better established. \n\nMinor points:\n1. Language is awkward in section 3.1 and 3.2: 'contains contains', 'more smaller than', 'very close zero'...\n2. More experimental details need to be included, such as the parameters used in training and generating the synthetic dataset.\n3. The author needs to provide an explanation for the disagreement between Figure (10) and the result of Keskar et. al.(2016). What's the key difference in experimental settings?\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Empirical Analysis of the Hessian of Over-Parametrized Neural Networks","abstract":"We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.","pdf":"/pdf/7e8c0c57b6e380ebc7db896ad7b0a794db065a2d.pdf","TL;DR":"The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.","paperhash":"anonymous|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJrTwxbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper602/Authors"],"keywords":["Deep Learning","Over-parametrization","Hessian","Eigenvalues","Flat minima","Large batch Small batch"]}},{"tddate":null,"ddate":null,"tmdate":1512222700003,"tcdate":1510944765613,"number":1,"cdate":1510944765613,"id":"ryIbx22yz","invitation":"ICLR.cc/2018/Conference/-/Paper602/Official_Review","forum":"rJrTwxbCb","replyto":"rJrTwxbCb","signatures":["ICLR.cc/2018/Conference/Paper602/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A worthwhile set of experiments, but not entirely convincing, and confusingly presented.","rating":"5: Marginally below acceptance threshold","review":"The authors perform a set of experiments in which they inspect the Hessian matrix of the loss of a neural network, and observe that most of the eigenvalues are very close to zero. This is a potentially important observation, and the experiments were well worth performing, but I don't find them fully convincing (partly because I was confused by the presentation).\n\nThey perform four sets of experiments:\n\n1) In section 3.1, they show on simulated data that for data drawn from k clusters, there are roughly k significant eigenvalues in the Hessian of the solution.\n\n2) In section 3.2, they show on MNIST that the solution contains few large eigenvalues, and also that there are negative eigenvalues.\n\n3) In section 3.3, they show (again on MNIST) that at their respective solutions, large batch and small batch methods find solutions with similar numbers of large eigenvalues, but that for the large batch method the magnitudes are larger.\n\n4) In section 4.1, they train (on CIFAR10) using a large batch method, and then transition to a small batch method, and argue that the second solution appears to be better than the first, but that they are a part of the same basin (since linearly while interpolating between them they don't run into any barriers).\n\nI'm not fully convinced by the second and third experiments, partly because I didn't fully understand the plots (more on this below), but also because it isn't clear to me what we should expect from the spectrum of a Hessian, so I don't know whether the observed specra have fewer large eigenvalues, or more large eigenvalues, then would be \"natural\". In other words, there isn't a *baseline*.\n\nFor the fourth experiment, it's unsurprising that the small batch method winds up in a different location in the same basin as the large batch method, since it was initialized to the large batch method's solution (and it doesn't appear to me, in figure 9, that the small batch solution is significantly different).\n\nSection 2.1 is said to contain an argument that the second term of equation 5 can be ignored, but only says that if \\ell' and \\nabla^2 of f are uncorrelated, then it can be ignored. I don't see any reason that these two quantities should be correlated, but this is not an argument that they are uncorrelated. Also, it isn't clear to me where this approximation was used--everywhere? In section 3.2, it sounds as if the exact Hessian is used, and at the end of this section the authors say that figure 6 demonstrates that the effect of this second term is small, but I don't see why this is, and it isn't explained.\n\nMy main complaint is that I had a great deal of difficulty interpreting the plots: it often wasn't clear to me what exactly was being plotted, and most of the language describing them was frustratingly vague. For example, figure 6 is captioned \"left edge of the spectrum, eigenvalues are scaled by their ratio\". The text explains that \"left edge of the spectrum\" means \"small but negative eigenvalues\" (this would be better in the caption), but what are the ratios? Ratio of what to what? I think it would greatly enhance clarity if every plot caption described exactly, and unambiguously, what quantities were plotted on the horizontal and vertical axes.\n\nSome minor notes:\n\nThere are a number of places where \"it's\" is used, where it should be \"its\".\n\nIn the introduction, the definition of \\mathcal{L}' is slightly confusing, since it's an expectation, but the use of \"'\" makes one expect a derivative. Perhaps use \\hat{\\mathcal{L}} for the empirical loss, and \\mathcal{L} for the expected one?\n\nOn the bottom of page 4, \"if \\ell' and \\nabla f are not correlated\": I think the \\nabla should be \\nabla^2.\n\nIt's \"principal components\", not \"principle components\".","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Empirical Analysis of the Hessian of Over-Parametrized Neural Networks","abstract":"We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.","pdf":"/pdf/7e8c0c57b6e380ebc7db896ad7b0a794db065a2d.pdf","TL;DR":"The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.","paperhash":"anonymous|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJrTwxbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper602/Authors"],"keywords":["Deep Learning","Over-parametrization","Hessian","Eigenvalues","Flat minima","Large batch Small batch"]}},{"tddate":null,"ddate":null,"tmdate":1509739207708,"tcdate":1509128124694,"number":602,"cdate":1509739205048,"id":"rJrTwxbCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJrTwxbCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Empirical Analysis of the Hessian of Over-Parametrized Neural Networks","abstract":"We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.","pdf":"/pdf/7e8c0c57b6e380ebc7db896ad7b0a794db065a2d.pdf","TL;DR":"The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.","paperhash":"anonymous|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks","_bibtex":"@article{\n  anonymous2018empirical,\n  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJrTwxbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper602/Authors"],"keywords":["Deep Learning","Over-parametrization","Hessian","Eigenvalues","Flat minima","Large batch Small batch"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}