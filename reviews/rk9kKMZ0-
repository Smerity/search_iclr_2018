{"notes":[{"tddate":null,"ddate":null,"tmdate":1512337874415,"tcdate":1512337874415,"number":3,"cdate":1512337874415,"id":"ry9RWezWM","invitation":"ICLR.cc/2018/Conference/-/Paper878/Official_Review","forum":"rk9kKMZ0-","replyto":"rk9kKMZ0-","signatures":["ICLR.cc/2018/Conference/Paper878/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The authors purpose a method for creating mini batches for a student network by using a second learned representation space to dynamically selecting  examples by their 'easiness and true diverseness'. The framework is detailed and results on MNIST, cifar10 and fashion-MNIST are presented. The work presented is novel but there are some notable omissions: \n - there are no specific numbers presented to back up the improvement claims; graphs are presented but not specific numeric results\n- there is limited discussion of the computational cost of the framework presented \n- there is no comparison to a baseline in which the additional learning cycles used for learning the embedding are used for training the student model.\n- only small data sets are evaluated. This is unfortunate because if there are to be large gains from this approach, it seems that they are more likely to be found in the domain of large scale problems, than toy data sets like mnist. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEAP: Learning Embeddings for Adaptive Pace","abstract":"The parameterization of mini-batches for training Deep Neural Networks (DNN) is a non-trivial problem. In this paper, we propose a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). Our method parameterizes mini-batches dynamically based on the easiness and true diverseness of the sample within a salient feature representation space. In LEAP, we train an embedding Convolutional Neural Network (CNN) to learn an expressive representation space by adaptive density discrimination using the Magnet Loss. The student CNN classifier dynamically selects samples to form a mini-batch based on the easiness from cross-entropy losses and true diverseness of examples from the representation space sculpted by the embedding CNN. We evaluate LEAP using deep CNN architectures for the task of supervised image classification on MNIST, FashionMNIST and CIFAR-10. We show that the LEAP framework can achieve a higher convergence w.r.t. the number of mini-batch updates required to achieve a comparable or better test performance on each of the datasets. Our framework is implemented in PyTorch and will be released as open-source on GitHub following review.","pdf":"/pdf/f4e80762d5c1fec1b5d8b82c8ea95b76476f740c.pdf","TL;DR":"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. ","paperhash":"anonymous|leap_learning_embeddings_for_adaptive_pace","_bibtex":"@article{\n  anonymous2018leap:,\n  title={LEAP: Learning Embeddings for Adaptive Pace},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk9kKMZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper878/Authors"],"keywords":["deep metric learning","self-paced learning","representation learning","cnn"]}},{"tddate":null,"ddate":null,"tmdate":1512222801799,"tcdate":1512160420961,"number":2,"cdate":1512160420961,"id":"Byjs3NyZz","invitation":"ICLR.cc/2018/Conference/-/Paper878/Official_Review","forum":"rk9kKMZ0-","replyto":"rk9kKMZ0-","signatures":["ICLR.cc/2018/Conference/Paper878/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors propose a method that uses an embedding network trained with magnet loss for adaptively sampling and feeding the student network that is being trained for the actual task","rating":"4: Ok but not good enough - rejection","review":"While the idea is novel and I do agree that I have not seen other works along these lines there are a few things that are missing and hinder this paper significantly.\n\n1. There are no quantitative numbers in terms of accuracy improvements, overhead in computation in having two networks.\n2. The experiments are still at the toy level, the authors can tackle more challenging datasets where sampling goes from easy to hard examples like birdsnap. MNIST, FashionMNIST and CIFAR-10 are all small datasets where the true utility of sampling is not realized. Authors should be motivated to run the large scale experiments.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEAP: Learning Embeddings for Adaptive Pace","abstract":"The parameterization of mini-batches for training Deep Neural Networks (DNN) is a non-trivial problem. In this paper, we propose a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). Our method parameterizes mini-batches dynamically based on the easiness and true diverseness of the sample within a salient feature representation space. In LEAP, we train an embedding Convolutional Neural Network (CNN) to learn an expressive representation space by adaptive density discrimination using the Magnet Loss. The student CNN classifier dynamically selects samples to form a mini-batch based on the easiness from cross-entropy losses and true diverseness of examples from the representation space sculpted by the embedding CNN. We evaluate LEAP using deep CNN architectures for the task of supervised image classification on MNIST, FashionMNIST and CIFAR-10. We show that the LEAP framework can achieve a higher convergence w.r.t. the number of mini-batch updates required to achieve a comparable or better test performance on each of the datasets. Our framework is implemented in PyTorch and will be released as open-source on GitHub following review.","pdf":"/pdf/f4e80762d5c1fec1b5d8b82c8ea95b76476f740c.pdf","TL;DR":"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. ","paperhash":"anonymous|leap_learning_embeddings_for_adaptive_pace","_bibtex":"@article{\n  anonymous2018leap:,\n  title={LEAP: Learning Embeddings for Adaptive Pace},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk9kKMZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper878/Authors"],"keywords":["deep metric learning","self-paced learning","representation learning","cnn"]}},{"tddate":null,"ddate":null,"tmdate":1512222801839,"tcdate":1511783764873,"number":1,"cdate":1511783764873,"id":"S1p86uteG","invitation":"ICLR.cc/2018/Conference/-/Paper878/Official_Review","forum":"rk9kKMZ0-","replyto":"rk9kKMZ0-","signatures":["ICLR.cc/2018/Conference/Paper878/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review ","rating":"3: Clear rejection","review":"(Summary)\nThis paper is about learning a representation with curriculum learning style minibatch selection in an end-to-end framework. The authors experiment the classification accuracy on MNIST, FashionMNIST, and CIFAR-10 datasets.\n\n(Pros)\nThe references to the deep metric learning methods seem up to date and nicely summarizes the recent literatures.\n\n(Cons)\n1. The method lacks algorithmic novelty and the exposition of the method severely inhibits the reader from understand the proposed idea. Essentially, the method is described in section 3. First of all, it's not clear what the actual loss the authors are trying to minimize. Also, \\min_v E(\\theta, v; \\lambda, \\gamma) is incorrect. It looks to me like it should be E \\ell (...) where \\ell is the loss function. \n\n2. The experiments show almost no discernable practical gains over 'random' baseline which is the baseline for random minibatch selection.\n\n(Assessment)\nClear rejection. The method is poorly written, severely lacks algorithmic novelty, and the proposed approach shows no empirical gains over random mini batch sampling.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEAP: Learning Embeddings for Adaptive Pace","abstract":"The parameterization of mini-batches for training Deep Neural Networks (DNN) is a non-trivial problem. In this paper, we propose a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). Our method parameterizes mini-batches dynamically based on the easiness and true diverseness of the sample within a salient feature representation space. In LEAP, we train an embedding Convolutional Neural Network (CNN) to learn an expressive representation space by adaptive density discrimination using the Magnet Loss. The student CNN classifier dynamically selects samples to form a mini-batch based on the easiness from cross-entropy losses and true diverseness of examples from the representation space sculpted by the embedding CNN. We evaluate LEAP using deep CNN architectures for the task of supervised image classification on MNIST, FashionMNIST and CIFAR-10. We show that the LEAP framework can achieve a higher convergence w.r.t. the number of mini-batch updates required to achieve a comparable or better test performance on each of the datasets. Our framework is implemented in PyTorch and will be released as open-source on GitHub following review.","pdf":"/pdf/f4e80762d5c1fec1b5d8b82c8ea95b76476f740c.pdf","TL;DR":"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. ","paperhash":"anonymous|leap_learning_embeddings_for_adaptive_pace","_bibtex":"@article{\n  anonymous2018leap:,\n  title={LEAP: Learning Embeddings for Adaptive Pace},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk9kKMZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper878/Authors"],"keywords":["deep metric learning","self-paced learning","representation learning","cnn"]}},{"tddate":null,"ddate":null,"tmdate":1510092386500,"tcdate":1509136610162,"number":878,"cdate":1510092362923,"id":"rk9kKMZ0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk9kKMZ0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LEAP: Learning Embeddings for Adaptive Pace","abstract":"The parameterization of mini-batches for training Deep Neural Networks (DNN) is a non-trivial problem. In this paper, we propose a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework, which we call Learning Embeddings for Adaptive Pace (LEAP). Our method parameterizes mini-batches dynamically based on the easiness and true diverseness of the sample within a salient feature representation space. In LEAP, we train an embedding Convolutional Neural Network (CNN) to learn an expressive representation space by adaptive density discrimination using the Magnet Loss. The student CNN classifier dynamically selects samples to form a mini-batch based on the easiness from cross-entropy losses and true diverseness of examples from the representation space sculpted by the embedding CNN. We evaluate LEAP using deep CNN architectures for the task of supervised image classification on MNIST, FashionMNIST and CIFAR-10. We show that the LEAP framework can achieve a higher convergence w.r.t. the number of mini-batch updates required to achieve a comparable or better test performance on each of the datasets. Our framework is implemented in PyTorch and will be released as open-source on GitHub following review.","pdf":"/pdf/f4e80762d5c1fec1b5d8b82c8ea95b76476f740c.pdf","TL;DR":"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. ","paperhash":"anonymous|leap_learning_embeddings_for_adaptive_pace","_bibtex":"@article{\n  anonymous2018leap:,\n  title={LEAP: Learning Embeddings for Adaptive Pace},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk9kKMZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper878/Authors"],"keywords":["deep metric learning","self-paced learning","representation learning","cnn"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}