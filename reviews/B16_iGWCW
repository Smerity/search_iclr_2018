{"notes":[{"tddate":null,"ddate":null,"tmdate":1513984616331,"tcdate":1513984616331,"number":2,"cdate":1513984616331,"id":"BJluMzjMf","invitation":"ICLR.cc/2018/Conference/-/Paper941/Public_Comment","forum":"B16_iGWCW","replyto":"ry00rINxM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response","comment":" We thank the reviewers for their comments. Individual points are addressed below. \n\n-The objective function of our algorithm is the weighted margin between the average correct classification probability and the average incorrect classification probability, as in Eq.(3) and Eq.(4). The first term of Eq.(4) is just the likelihood of positive samples. So, the loss can be seen as the opposite number of objective function Eq.(3). Maximizing the margin in Eq.(3) is equivalent to minimizing the loss. To update the weight distribution for different categories, we employ an exponential updating rule as in Eq.(7), which encourages focusing on the categories that are hard to classify.\n\n-For large-scale visual recognition, it is worth noting that every object class may contain large numbers of hard images due to huge intra-class visual diversity, thus weighting the sample errors may not be able to achieve the same effects as weighting the object classes according to their learning complexities, e.g., weighting the sample errors may not be able to improve the accuracy rates for the hard object classes. Because large numbers of object classes may have different learning complexities, the errors from the hard object classes and the easy ones may have significantly different effects on optimizing their joint objective function. Therefore, it is very attractive to invest new boosting algorithms that can train the deep networks for the hard object classes and the easy ones sequentially in an easy-to-hard way, such that the ensemble network can improve the accuracy rates for the hard object classes at certain degrees while effectively maintaining high accuracy rates for the easy ones.\n\n-Because large numbers of object classes may have different learning complexities, the sample errors from the hard object classes and the easy ones may have significantly different roles in optimizing their joint objective function on learning their joint deep network. Unfortunately, for large-scale visual recognition (i.e., recognizing large numbers of object classes), weighting the sample errors individually (like traditional deep boosting approaches) may not be able to achieve the same effects as weighting the object classes directly according to their learning complexities, e.g., treating the sample errors from the hard object classes and the easy ones to be equally important may easily distract their joint deep network on achieving higher accuracy rates on recognizing the easy object classes but paying less attentions on correcting the sample errors from the hard object classes. Therefore, it is very attractive to invest new boosting algorithms that can train the deep networks for the hard object classes and the easy ones sequentially in an easy-to-hard way, such that the ensemble network can improve the accuracy rates for the hard object classes at certain degrees while effectively maintaining high accuracy rates for the easy ones."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1513984331218,"tcdate":1513984331218,"number":1,"cdate":1513984331218,"id":"Skm8WGozM","invitation":"ICLR.cc/2018/Conference/-/Paper941/Public_Comment","forum":"B16_iGWCW","replyto":"SJR0TtHZG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response","comment":"We thank the reviewer for the comments. Individual points are addressed below. \n-In Eq. (3), \\tilde{D} is not defined.\nA:As described in the paragraph below Eq. (4),  \\tilde{D}_t(l) is the normalized importance score for class l in the tth base expert. Its initial value is 1/C. Then it will be updated iteratively as in Eq. (9).\n-Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$. \nA:In Section 3.3, we discuss the selection of $\\lambda$.   In the case $\\epsilon_t(l) > \\frac{1}{2\\lambda}$，it means that the l-th category is the hard category, and we decrease the hyper-parameter $\\lambda$ such that $0 < \\beta_t < 1$ holds.\n- How many layers is the DenseNet-BC used in this paper? Why the error rate reported here is higher than that in the original paper? \nA: A 100-layer DenseNet-BC model is used in this paper on CIFAR100 which is the same in the original paper (https://arxiv.org/pdf/1608.06993.pdf). The reason why the error rate here is higher is mainly due to that we do not use all 50,000 samples on training split and validation split at the final run, which is the training trick reported in the original paper. As mentioned in our paper, we only use the validation split for the update of the weights in the following iteration. Another minor factor may be that we only train the model once in the first iteration and do not run many times for selection of the best model since we care more about the effects of our boosting algorithm.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1515642533805,"tcdate":1512574422314,"number":3,"cdate":1512574422314,"id":"SJR0TtHZG","invitation":"ICLR.cc/2018/Conference/-/Paper941/Official_Review","forum":"B16_iGWCW","replyto":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference/Paper941/AnonReviewer4"],"readers":["everyone"],"content":{"title":"deep learning with the boosting trick","rating":"5: Marginally below acceptance threshold","review":"This paper applies the boosting trick to deep learning. The idea is quite straightforward, and the paper is relatively easy to follow. The proposed algorithm is validated on several image classification datasets.\n\nThe paper is its current form has the following issues:\n1. There is hardly any baseline compared in the paper. The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against.\n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise.\n3. In Eq. (3), \\tilde{D} is not defined.\n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$.  \n5. How many layers is the DenseNet-BC used in this paper? Why the error rate reported here is higher than that in the original paper?\nTypo: \nIn Session 3 Line 7, there is a missing reference.\nIn Session 3 Line 10, “1,00 object classes” should be “100 object classes”.\nIn Line 3 of the paragraph below Equation 5, “classe” should be “class”.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1515642533842,"tcdate":1511857031669,"number":2,"cdate":1511857031669,"id":"HJecicqxG","invitation":"ICLR.cc/2018/Conference/-/Paper941/Official_Review","forum":"B16_iGWCW","replyto":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference/Paper941/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposed a boosting method for learning an ensemble of neural networks","rating":"6: Marginally above acceptance threshold","review":"In conventional boosting methods, one puts a weight on each sample. The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right.  Thus the learned weak learner at this round will make different mistakes.\nThis idea however is difficult to be applied to deep learning with a large amount of data. This paper instead designed a new boosting method which puts large weights on the category with large error in this round.  In other words samples in the same category will have the same weight \n\nError bound is derived.  Experiments show its usefulness though experiments are limited\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1515642533878,"tcdate":1511445974007,"number":1,"cdate":1511445974007,"id":"ry00rINxM","invitation":"ICLR.cc/2018/Conference/-/Paper941/Official_Review","forum":"B16_iGWCW","replyto":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference/Paper941/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A boosting with CNNs approach with fixed class weights between iterations - Many fundamental questions and experiments haven't been addressed","rating":"2: Strong rejection","review":"This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks.\n\nWhile the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:\n\n- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners. It's not clear what kind of loss function is really being optimised here. It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end.\n- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup.\n- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners.\n- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method) 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST) 3) comparison to simply ensembling with random initialisations.\n\nOther comments:\n- Paper would benefit from writing improvements to make it read better.\n- \"simply use the weighted error function\": I don't think this is correct, AdaBoost loss function is an exponential loss. When you train the base learners, their loss functions will become weighted.\n-  \"to replace the softmax error function (used in deep learning)\": I don't think we have softmax error function","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1510092384661,"tcdate":1509137272664,"number":941,"cdate":1510092362148,"id":"B16_iGWCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}