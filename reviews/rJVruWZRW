{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222721157,"tcdate":1511826990032,"number":3,"cdate":1511826990032,"id":"SJLNIX9lG","invitation":"ICLR.cc/2018/Conference/-/Paper695/Official_Review","forum":"rJVruWZRW","replyto":"rJVruWZRW","signatures":["ICLR.cc/2018/Conference/Paper695/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not exciting","rating":"4: Ok but not good enough - rejection","review":"The authors propose an RNN that combines temporal shortcut connections from [Soltani & Jang, 2016] and Gated Recurrent Attention [Chung, 2014]. However, their justification about the novelty and efficacy of the model is not well demonstrated in the paper. The experiment part is modest with only one small dataset Penn Tree Bank is used. The results are not significant enough and no comparisons with models in [Soltani & Jang, 2016] and [Chung, 2014] are provided in the paper to show the effectiveness of the proposed combination. To conclude, this paper is an incremental work with limited contributions.\n\nSome writing issues:\n1. Lack of support in arguments,\n2. Lack of referencing to previous works. For example, the sentence “By selecting the same dropout mask for feedforward, recurrent connections, respectively, the dropout can apply to the RNN, which is called a variational dropout” mentions “variational dropout” with no citing. Or “NARX-RNN and HO-RNN increase the complexity by increasing recurrent depth. Gated feedback RNN has the fully connection between two consecutive timesteps” also mentions a lot of models without any references at all.\n3. Some related papers are not cited, e.g., Hierarchical Multiscale Recurrent Neural Networks [Chung, 2016]\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dense Recurrent Neural Network with Attention Gate","abstract":"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly. As the density of the connection increases, the number of paths through which the gradient flows can be increased. It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time. Larger gradients, however, can also cause exploding gradient problem. To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows. We describe the relation between the attention gate and the gradient flows by approximation. The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","pdf":"/pdf/e148ef63a8ee9edec7ee3e2903859ef4c10070c0.pdf","TL;DR":"Dense RNN that has fully connections from each hidden state to multiple preceding hidden states of all layers directly.","paperhash":"anonymous|dense_recurrent_neural_network_with_attention_gate","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Recurrent Neural Network with Attention Gate},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJVruWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper695/Authors"],"keywords":["recurrent neural network","language modeling","dense connection"]}},{"tddate":null,"ddate":null,"tmdate":1512222721195,"tcdate":1511823139882,"number":2,"cdate":1511823139882,"id":"Hk37PGqlz","invitation":"ICLR.cc/2018/Conference/-/Paper695/Official_Review","forum":"rJVruWZRW","replyto":"rJVruWZRW","signatures":["ICLR.cc/2018/Conference/Paper695/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Dense RNNs with Attention Gate","rating":"4: Ok but not good enough - rejection","review":"Summary: \n\nThis paper proposes a fully connected dense RNN architecture that has connections to every layer and the preceding connections of each layer. The connections are also gated by using a simple gating mechanism. The authors very briefly discusses about the effect of these on the dynamics of the learning. They report results on PTB character-level language modelling task.\n\n\nQuestions:\nWhat is the computational complexity of this approach compared to a vanilla RNN architecture?\nWhat is the implications of these skip connections in terms of memory consumption during BPTT?\nDid you use gradient clipping and have you used any specific type of initialization for the parameters?\nHow would this approach would compare against the Clockwork RNNs which has a block-diagonal weight matrices? [1]\nHow would dense-RNNs compare against to the MANNs [2]?\nHow would you implement this model efficiently?\n\nPros:\nInteresting idea.\nCons:\nLack of experiments and empirical results supporting the arguments.\nHand-wavy theory.\nLack of references to the relevant literature. \n\nGeneral Comments:\nIn general the paper is relatively well written despite having some minor typos. The idea is interesting, however the experiments in this paper is seriously lacking. The only results presented in this paper is on PTB. The results are quite behind the SOTA and PTB is a really tiny, toyish language modeling task. The theory is very hand-wavy, the connections to the previous attempts to come up with related properties of the recurrent models should be cited. The Figure 2 is very related to the Gersgorin circle theorem in [3]. The discussion about the skip-connections is very related to the results in [2]. \n\nOverall, I think this paper is rushed and not ready for the publication.\n\n[1] Koutnik, J., Greff, K., Gomez, F., & Schmidhuber, J. (2014, January). A clockwork rnn. In International Conference on Machine Learning (pp. 1863-1871).\n[2] Gulcehre, Caglar, Sarath Chandar, and Yoshua Bengio. \"Memory Augmented Neural Networks with Wormhole Connections.\" arXiv preprint arXiv:1701.08718 (2017).\n[3] Zilly, Julian Georg, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. \"Recurrent highway networks.\" arXiv preprint arXiv:1607.03474 (2016).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dense Recurrent Neural Network with Attention Gate","abstract":"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly. As the density of the connection increases, the number of paths through which the gradient flows can be increased. It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time. Larger gradients, however, can also cause exploding gradient problem. To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows. We describe the relation between the attention gate and the gradient flows by approximation. The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","pdf":"/pdf/e148ef63a8ee9edec7ee3e2903859ef4c10070c0.pdf","TL;DR":"Dense RNN that has fully connections from each hidden state to multiple preceding hidden states of all layers directly.","paperhash":"anonymous|dense_recurrent_neural_network_with_attention_gate","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Recurrent Neural Network with Attention Gate},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJVruWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper695/Authors"],"keywords":["recurrent neural network","language modeling","dense connection"]}},{"tddate":null,"ddate":null,"tmdate":1512222721238,"tcdate":1511817853809,"number":1,"cdate":1511817853809,"id":"SkLFMZ9gG","invitation":"ICLR.cc/2018/Conference/-/Paper695/Official_Review","forum":"rJVruWZRW","replyto":"rJVruWZRW","signatures":["ICLR.cc/2018/Conference/Paper695/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"2: Strong rejection","review":" This paper proposes a new type of RNN architectures called Dense RNNs. The authors combine several different RNN architectures and claim that their RNN can model long-term dependencies better, can learn multiscale representation of the sequential data, and can sidestep the exploding or vanishing gradients problem by using parametrized gating units.\n\nUnfortunately, this paper is hard to read, it is difficult to understand the intention of the authors. The authors make several claims without any supportive reference or experimental evidence. Both intuitive and theoretical justifications of the proposed architecture are not so convincing. The experiment is only done on PTB dataset, and the reported numbers are not that promising either. \n\nThis paper tries to combine three different features from previous works, and unfortunately, it is not so well conducted.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dense Recurrent Neural Network with Attention Gate","abstract":"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly. As the density of the connection increases, the number of paths through which the gradient flows can be increased. It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time. Larger gradients, however, can also cause exploding gradient problem. To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows. We describe the relation between the attention gate and the gradient flows by approximation. The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","pdf":"/pdf/e148ef63a8ee9edec7ee3e2903859ef4c10070c0.pdf","TL;DR":"Dense RNN that has fully connections from each hidden state to multiple preceding hidden states of all layers directly.","paperhash":"anonymous|dense_recurrent_neural_network_with_attention_gate","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Recurrent Neural Network with Attention Gate},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJVruWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper695/Authors"],"keywords":["recurrent neural network","language modeling","dense connection"]}},{"tddate":null,"ddate":null,"tmdate":1509739155499,"tcdate":1509132348238,"number":695,"cdate":1509739152842,"id":"rJVruWZRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJVruWZRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Dense Recurrent Neural Network with Attention Gate","abstract":"We propose the dense RNN, which has the fully connections from each hidden state to multiple preceding hidden states of all layers directly. As the density of the connection increases, the number of paths through which the gradient flows can be increased. It increases the magnitude of gradients, which help to prevent the vanishing gradient problem in time. Larger gradients, however, can also cause exploding gradient problem. To complement the trade-off between two problems, we propose an attention gate, which controls the amounts of gradient flows. We describe the relation between the attention gate and the gradient flows by approximation. The experiment on the language modeling using Penn Treebank corpus shows dense connections with the attention gate improve the model’s performance.","pdf":"/pdf/e148ef63a8ee9edec7ee3e2903859ef4c10070c0.pdf","TL;DR":"Dense RNN that has fully connections from each hidden state to multiple preceding hidden states of all layers directly.","paperhash":"anonymous|dense_recurrent_neural_network_with_attention_gate","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Recurrent Neural Network with Attention Gate},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJVruWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper695/Authors"],"keywords":["recurrent neural network","language modeling","dense connection"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}