{"notes":[{"tddate":null,"ddate":null,"tmdate":1515186995409,"tcdate":1515186995409,"number":3,"cdate":1515186995409,"id":"HysNsD6XM","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Comment","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1/Authors"],"content":{"title":"Review Response","comment":"We would like to thank each of the reviewers for their time and constructive feedback, which we have incorporated in this revision. Specifically:\n\n-We updated second and third parts of  Section 4.1 to more thoroughly investigate the correlation between SMASH scores and resulting validation scores by examining scores for a variety of HyperNet architectures and ratios of generated vs. static weights. We examine the strength and significance of the correlation between SMASH scores and validation scores using Pearson's R. We have moved the two original experiments addressing the breakdown of the correlation to the appendix, and updated them to properly reference the previously unreferenced figure.\n\n-We have slightly improved writing throughout, fixing the noted typos, and changing our wording to make clear that the memory bank view is a novel development which we are introducing in this work.\n\n-We have moved the Future Work section from the appendix into the main body of the paper at the suggestion of Reviewer 2. This section had previously been relegated to the appendix at the behest of a previous review cycle for an earlier revision of the paper.\n \n-We have added some simple runtime numbers in Table 2 for comparison to other architecture search methods\n\nThanks again for your reviews.\n\nBest,\n\nPaper1 Authors\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/b8fee6f10900b6b5c5b164a93f44415f9fdc895f.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1513971568760,"tcdate":1513971568760,"number":2,"cdate":1513971568760,"id":"ryK_yyjGM","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Comment","forum":"rydeCEhs-","replyto":"SkmGrjvlz","signatures":["ICLR.cc/2018/Conference/Paper1/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1/Authors"],"content":{"title":"Quick Clarification","comment":"Hi Reviewer2,\n\nThank you for your detailed and constructive feedback. We are preparing a revision and a complete response but would like a quick bit of clarification as to specifically which experimental results fall under the lackluster banner.\n\nThanks,\n\nPaper1 Authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/b8fee6f10900b6b5c5b164a93f44415f9fdc895f.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1515642375300,"tcdate":1512090386109,"number":3,"cdate":1512090386109,"id":"SycMimAgG","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Review","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper tackles an important problem on learning neural net architectures that outperforms comparable methods and is reasonably faster","rating":"6: Marginally above acceptance threshold","review":"This paper tackles the problem of finding an optimal architecture for deep neural nets . They propose to solve it by training an auxiliary HyperNet to generate the main model. The authors propose the so called \"SMASH\" algorithm that ranks the neural net architectures based on their validation error. The authors adopt a memory-bank view of the network configurations for exploring a varied collection of network configurations. It is not clear whether this is a new contribution of this paper or whether the authors merely adopt this idea.  A clearer note on this would be welcome. My key concern is with the results as described in 4.1.; the correlation structure breaks down completely for \"low-budget\" SMASH in Figure 5(a) as compared Figure (4). Doesn't this then entail an investigation of what is the optimal size of the hyper network? Also I couldn't quite follow the importance of figure 5(b) - is it referenced in the text? The authors also note that SMASH is saves a lot of computation time; some time-comparison numbers would probably be more helpful to drive home the point especially when other methods out-perform SMASH. \nOne final point, for the uninitiated reader- sections 3.1 and 3.2 could probably be written somewhat more lucidly for better access.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/b8fee6f10900b6b5c5b164a93f44415f9fdc895f.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1515642375357,"tcdate":1511821012458,"number":2,"cdate":1511821012458,"id":"rJ200-5ez","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Review","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An experimental framework for designing neural architectures","rating":"7: Good paper, accept","review":"This paper is about a new experimental technique for exploring different neural architectures. It is well-written in general, numerical experiments demonstrate the framework and its capabilities as well as its limitations. \n\nA disadvantage of the approach may be that the search for architectures is random. It would be interesting to develop a framework where the search for the architecture is done with a framework where the updates to the architecture is done using a data-driven approach. Nevertheless, there are so many different neural architectures in the literature and this paper is a step towards comparing various architectures efficiently. \n\nMinor comments:\n\n1) Page 7,  \".. moreso than domain specificity.\" \n\nIt may be better to spell the word \"moreso\" as \"more so\", please see: https://en.wiktionary.org/wiki/moreso","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/b8fee6f10900b6b5c5b164a93f44415f9fdc895f.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1515787896274,"tcdate":1511662859420,"number":1,"cdate":1511662859420,"id":"SkmGrjvlz","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Review","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well written paper that introduces and applies SMASH framework with some experimental success","rating":"7: Good paper, accept","review":"Summary of paper - This paper presents SMASH (or the one-Shot Model Architecture Search through Hypernetworks) which has two training phases (one to quickly train a random sample of network architectures and one to train the best architecture from the first stage). The paper presents a number of interesting experiments and discussions about those experiments, but offers more exciting ideas about training neural nets than experimental successes. \n\nReview - The paper is very well written with clear examples and an excellent contextualization of the work among current work in the field. The introduction and related work are excellently written providing both context for the paper and a preview of the rest of the paper. The clear writing make the paper easy to read, which also makes clear the various weaknesses and pitfalls of SMASH. \n\nThe SMASH framework appears to provide more interesting contributions to the theory of training Neural Nets than the application of said training. While in some experiments SMASH offers excellent results, in others the results are lackluster (which the authors admit, offering possible explanations). \n\nIt is a shame that the authors chose to push their section on future work to the appendices. The glimmers of future research directions (such as the end of the last paragraph in section 4.2) were some of the most intellectually exciting parts of the paper. This choice may be a reflection of preferring to highlight the experimental results over possible contributions to theory of neural nets.  \n\n\nPros - \n* Strong related work section that contextualizes this paper among current work\n* Very interesting idea to more efficiently find and train best architectures \n* Excellent and thought provoking discussions of middle steps and mediocre results on some experiments (i.e. last paragraph of section 4.1, and last paragraph of section 4.2)\n* Publicly available code \n\nCons - \n* Some very strong experimental results contrasted with some mediocre results\n* The balance of the paper seems off, using more text on experiments than the contributions to theory. \n* (Minor) - The citation style is inconsistent in places. \n\n=-=-=-= Response to the authors\n\nI thank the authors for their thoughtful responses and for the new draft of their paper. The new draft laid plain the contribution of the memory bank which I had missed in the first version. As expected, the addition of the future work section added further intellectual excitement to the paper. \n\nThe expansion of section 4.1 addressed and resolved my concerns about the balance of the paper by effortless intertwining theory and application. I do have one question from this section -  In table 1, the authors report p-values but fail to include them in their interpretation; what is purpose of including these p-values, especially noting that only one falls under the typical threshold for significance?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/b8fee6f10900b6b5c5b164a93f44415f9fdc895f.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1515186129377,"tcdate":1506721263693,"number":1,"cdate":1509739534531,"id":"rydeCEhs-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/b8fee6f10900b6b5c5b164a93f44415f9fdc895f.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}