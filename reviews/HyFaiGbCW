{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222827062,"tcdate":1512199020277,"number":3,"cdate":1512199020277,"id":"rJNd7A1bz","invitation":"ICLR.cc/2018/Conference/-/Paper950/Official_Review","forum":"HyFaiGbCW","replyto":"HyFaiGbCW","signatures":["ICLR.cc/2018/Conference/Paper950/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper has some potentially interesting ideas, but they are not well-enough explored to support the strong claims it makes about generalization.","rating":"4: Ok but not good enough - rejection","review":"The paper uses an echo state network to learn to classify image transformations (between pairs of images) into one of fives classes.  The image data is artificially represented as a time series, and the goal is generalization of classification ability to unseen image pairs.  The network dynamics are studied and are claimed to have explanatory power.\n\nThe paper is well-written and easy to follow, but I have concerns about the claims it makes relative to how convincing the results are.  The focus is on one simple, and frankly now-overused data set (MNIST).  Further, treating MNIST data as a time series is artificial and clunky.  Why does the series go from left to right rather than right to left or top to bottom or inside out or something else?  How do the results change if the data is \"temporalized\" in some other way?\n\nFor training in Section 2.4, is M the number of columns for a pair of images?  It's not clear how pairs are input in parallel--- one after the other? Concatenated? Interleaved columns?  Something else? What are k, i, j in computing $\\delta X_k$?  Later, in Section 3.2, it says, \"As in section 2.2, $xl(mn)$ is the differential reservoir state value of the $m$th reservoir node at time $n$ for input image $l$\", but nothing like this is discussed in Section 2.2; I'm confused.\n\nThe generalization results on this one simple data set seem pretty good.  But, how does this kind of approach do on other kinds of or more complex data?  I'm not sure that RC has historically had very good success scaling up to \"real-world\" problems to date.\n\nTable 1 doesn't really say anything.  Of course, the diagonals are higher than the off diagonals because these are dot products.  True, they are dot products of averages over different inputs (which is why they are less than 1), but still.  Also, what Table 1 really seems to say is that the off-diagonals really aren't all that different than the diagonals, and that especially the differences between same and different digits is not very different, suggesting that what is learned is pretty fragile and likely won't generalize to harder problems.  I like the idea of using dynamical systems theory to attempt to explain what is going on, but I wonder if it is not being used a bit simplistically or naively.\n\nWhy were the five transform classes chosen?  It seems like the \"transforms\" a (same) and e (different) are qualitatively different than transforms b-d (rotated, scaled, blurred).  This seems like it should talked about.\n\n\"Thus, we infer, that the reservoir is in fact, simply training these attractors as opposed to training the entire reservoir space.\"  What does this mean?  The reservoir isn't trained at all in ESNs (which is also stated explicitly for the model presented here)…\n\nFor 3.3, why did were those three classes chosen? Was this experiment tried with other subsets of three classes?  Why are results reported on only the one combination of rotated/blurred vs. rotated?  Were others tried?  If so, what were the results?  If not, why?  How does the network know when to take more than the highest output (so it can say that two transforms have been applied)?  In the case of combination, counting either transform as the correct output kind of seems like cheating a bit—it over states how well the model is doing.  Also, does the order in which the transforms are applied affect their relative representative strength in the reservoir?\n\nThe comparison with SNNs is kind of interesting, but I'm not sure that I'm (yet) convinced, as there is little detail on how the experiment was performed and what was done (or not) to try to get the SNN to generalize.  My suspicion is that with the proper approach, an SNN or similar non-dynamical system could generalize well on these tasks.  The need for a dynamical system could be argued to make sense for the camera task, perhaps, as video frames naturally form a time series; however, as already mentioned, for the MNIST data, this is not the case, and the fact that the SNN does not generalize here seems likely due to their under utilization rather than due to an inherent lack of capability.\n\nI don't believe that there is sufficient support for this statement in the conclusion, \"[ML/deep networks] do not work as well for generalization of learning. In generalized learning, RCNs outperform them, due to their ability to function as a dynamical system with ‘memory’.\"  First of all, ML is all about generalization, and there are lots and lots and lots of results showing that many ML systems generalize very well on a wide variety of problems, well beyond just classification, in fact.  And, I don't think the the paper has convincingly shown that a dynamical system 'memory' is doing something especially useful, given that the main task studied, that of character recognition (or classification of transformation or even transformation itself), does not require such a temporal ability.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalization of Learning using Reservoir Computing","abstract":"We investigate the methods by which a Reservoir Computing Network (RCN), trained to classify pairs of images as 'similar', 'transformed' or 'different', learns the relationships between the images and generalizes these concepts to previously unseen types of data. One of our motivations is to explore how biologically realistic features, like recurrent relationships between neuron-like units and resemblance to neural dynamics (key features of RCNs), contribute to the learning capabilities of Artificial Neural Networks (ANNs). Specifically, we show that an RCN trained to identify strong similarities or transformations between image pairs drawn from a subset of digits from the MNIST database generalizes the learned transformations to images of digits unseen during training. We observe that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship. Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples. The directions of the principal component of the time-projected reservoir states representing input image pairs with the same relationship are aligned closer together than those representing image pairs with different relationships. Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space. We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier. Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep siamese neural networks in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera. This helps bridge the gap between machine learning and the biological problem of human cognition, and points to new directions in the investigation of learning processes.","pdf":"/pdf/8bf870b2b07b44ac87e15008f27d94ed52297932.pdf","TL;DR":"Generalization of the relationships learnt between pairs of images using a small training data to previously unseen types of images using a biologically plausible model, Reservoir Computing.","paperhash":"anonymous|generalization_of_learning_using_reservoir_computing","_bibtex":"@article{\n  anonymous2018generalization,\n  title={Generalization of Learning using Reservoir Computing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyFaiGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper950/Authors"],"keywords":["Generalization","Reservoir Computing","dynamical system","Siamese Neural Network","image classification","similarity","dimensionality reduction"]}},{"tddate":null,"ddate":null,"tmdate":1512222827109,"tcdate":1511784442439,"number":2,"cdate":1511784442439,"id":"SkMZxYKgz","invitation":"ICLR.cc/2018/Conference/-/Paper950/Official_Review","forum":"HyFaiGbCW","replyto":"HyFaiGbCW","signatures":["ICLR.cc/2018/Conference/Paper950/AnonReviewer1"],"readers":["everyone"],"content":{"title":"OK but nothing really new","rating":"4: Ok but not good enough - rejection","review":"The technical part of the paper is a nice study for classification with Echo State Networks. The main novelty here is the task itself, classifying different distortions of MNIST data. The actual technique presented is not original, but an application of the standard ESN approach. The task is interesting but by itself I don't find it convincing enough. Moreover, the biological plausibility that is used as an argument at several places seems to be false advertising in my view. The mere presence of recurrent connections doesn't make the approach more biological plausible, in particular given that ridge regression is used for training of the output weights. If biological plausibility was the goal, a different approach should have been used altogether (e.g., what about local training of connections, unsupervised training, ...). Also there is no argument why biological plausibility is supposed to be an advantage. A small number of training examples would have been a more specific and better motivation, given that the number of \"training\" examples for humans is only discussed qualitatively and without a reference. \n\nThe analysis using the PCs is nice; the works by Jaeger on Conceptors (2014) make also use of the principal components of the reservoir states during presentation of patterns (introduction in https://arxiv.org/abs/1406.2671), so seem like relevant references to me. \n\nIn my view the paper would benefit a lot from more ambitious task (with good results), though even then I would probably miss some originality in the approach.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalization of Learning using Reservoir Computing","abstract":"We investigate the methods by which a Reservoir Computing Network (RCN), trained to classify pairs of images as 'similar', 'transformed' or 'different', learns the relationships between the images and generalizes these concepts to previously unseen types of data. One of our motivations is to explore how biologically realistic features, like recurrent relationships between neuron-like units and resemblance to neural dynamics (key features of RCNs), contribute to the learning capabilities of Artificial Neural Networks (ANNs). Specifically, we show that an RCN trained to identify strong similarities or transformations between image pairs drawn from a subset of digits from the MNIST database generalizes the learned transformations to images of digits unseen during training. We observe that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship. Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples. The directions of the principal component of the time-projected reservoir states representing input image pairs with the same relationship are aligned closer together than those representing image pairs with different relationships. Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space. We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier. Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep siamese neural networks in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera. This helps bridge the gap between machine learning and the biological problem of human cognition, and points to new directions in the investigation of learning processes.","pdf":"/pdf/8bf870b2b07b44ac87e15008f27d94ed52297932.pdf","TL;DR":"Generalization of the relationships learnt between pairs of images using a small training data to previously unseen types of images using a biologically plausible model, Reservoir Computing.","paperhash":"anonymous|generalization_of_learning_using_reservoir_computing","_bibtex":"@article{\n  anonymous2018generalization,\n  title={Generalization of Learning using Reservoir Computing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyFaiGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper950/Authors"],"keywords":["Generalization","Reservoir Computing","dynamical system","Siamese Neural Network","image classification","similarity","dimensionality reduction"]}},{"tddate":null,"ddate":null,"tmdate":1512222827152,"tcdate":1511740237570,"number":1,"cdate":1511740237570,"id":"r1IUXROxz","invitation":"ICLR.cc/2018/Conference/-/Paper950/Official_Review","forum":"HyFaiGbCW","replyto":"HyFaiGbCW","signatures":["ICLR.cc/2018/Conference/Paper950/AnonReviewer2"],"readers":["everyone"],"content":{"title":"limited novelty, questionable experiments","rating":"4: Ok but not good enough - rejection","review":"The claimed results of  \"combining transformations\" in the context of RC was done in the works of Herbert Jaeger on conceptors [1], which also should be cited here.\n\nThe argument of biological plausibility is not justified. The authors  use an echo-state neural network with standard tanh activations, which is as far away from real neuronal signal processing than  ordinary RNNs used in the field, with the difference that the recurrent weights are not trained.  If the authors want to make the case of biological plausibility, they should use spiking neural networks.\n\nThe experiment on MNIST seems artificial, in particular transforming the image into a time-series and thereby imposing an artificial temporal structure. The assumption that column_i is obtained  by information  of column_{i-k},..,column_{i-1} is not true for images. To make a point, the authors should use a datasets with related sets of time-series data, e.g EEG or NLP data.\n\nIn total this paper does not have enough novelty for acceptance and the experiments are not well chosen for this kind of work. Also the authors overstate the claim of biological plausibility (just because we don't train the recurrent weights does not make a method biologically plausible).\n\n[1] H. Jaeger (2014): Controlling Recurrent Neural Networks by Conceptors. Jacobs University technical report Nr 31 (195 pages) \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalization of Learning using Reservoir Computing","abstract":"We investigate the methods by which a Reservoir Computing Network (RCN), trained to classify pairs of images as 'similar', 'transformed' or 'different', learns the relationships between the images and generalizes these concepts to previously unseen types of data. One of our motivations is to explore how biologically realistic features, like recurrent relationships between neuron-like units and resemblance to neural dynamics (key features of RCNs), contribute to the learning capabilities of Artificial Neural Networks (ANNs). Specifically, we show that an RCN trained to identify strong similarities or transformations between image pairs drawn from a subset of digits from the MNIST database generalizes the learned transformations to images of digits unseen during training. We observe that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship. Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples. The directions of the principal component of the time-projected reservoir states representing input image pairs with the same relationship are aligned closer together than those representing image pairs with different relationships. Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space. We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier. Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep siamese neural networks in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera. This helps bridge the gap between machine learning and the biological problem of human cognition, and points to new directions in the investigation of learning processes.","pdf":"/pdf/8bf870b2b07b44ac87e15008f27d94ed52297932.pdf","TL;DR":"Generalization of the relationships learnt between pairs of images using a small training data to previously unseen types of images using a biologically plausible model, Reservoir Computing.","paperhash":"anonymous|generalization_of_learning_using_reservoir_computing","_bibtex":"@article{\n  anonymous2018generalization,\n  title={Generalization of Learning using Reservoir Computing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyFaiGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper950/Authors"],"keywords":["Generalization","Reservoir Computing","dynamical system","Siamese Neural Network","image classification","similarity","dimensionality reduction"]}},{"tddate":null,"ddate":null,"tmdate":1510092383683,"tcdate":1509137374642,"number":950,"cdate":1510092361997,"id":"HyFaiGbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyFaiGbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generalization of Learning using Reservoir Computing","abstract":"We investigate the methods by which a Reservoir Computing Network (RCN), trained to classify pairs of images as 'similar', 'transformed' or 'different', learns the relationships between the images and generalizes these concepts to previously unseen types of data. One of our motivations is to explore how biologically realistic features, like recurrent relationships between neuron-like units and resemblance to neural dynamics (key features of RCNs), contribute to the learning capabilities of Artificial Neural Networks (ANNs). Specifically, we show that an RCN trained to identify strong similarities or transformations between image pairs drawn from a subset of digits from the MNIST database generalizes the learned transformations to images of digits unseen during training. We observe that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship. Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples. The directions of the principal component of the time-projected reservoir states representing input image pairs with the same relationship are aligned closer together than those representing image pairs with different relationships. Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space. We find that RCNs can identify and generalize linear and non-linear transformations, and combinations of transformations, naturally and be a robust and effective image classifier. Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep siamese neural networks in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera. This helps bridge the gap between machine learning and the biological problem of human cognition, and points to new directions in the investigation of learning processes.","pdf":"/pdf/8bf870b2b07b44ac87e15008f27d94ed52297932.pdf","TL;DR":"Generalization of the relationships learnt between pairs of images using a small training data to previously unseen types of images using a biologically plausible model, Reservoir Computing.","paperhash":"anonymous|generalization_of_learning_using_reservoir_computing","_bibtex":"@article{\n  anonymous2018generalization,\n  title={Generalization of Learning using Reservoir Computing},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyFaiGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper950/Authors"],"keywords":["Generalization","Reservoir Computing","dynamical system","Siamese Neural Network","image classification","similarity","dimensionality reduction"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}