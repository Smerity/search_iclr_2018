{"notes":[{"tddate":null,"ddate":null,"tmdate":1515192945872,"tcdate":1515186670865,"number":5,"cdate":1515186670865,"id":"B1Px5vamf","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Comment","forum":"BkUp6GZRW","replyto":"BkUp6GZRW","signatures":["ICLR.cc/2018/Conference/Paper1017/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1017/Authors"],"content":{"title":"Revised manuscript","comment":"Thanks for the constructive reviews and comments!\n\nWe have submitted our updated manuscript with a few revisions and more experiments for clarity accordingly, including:\n\n1, Discussion about the parametrization effect w.r.t. the path regularization. \n\n2, More explanation on the benefits of the proposed several important extensions.\n\n3, More details for proofs in Appendix.\n\n4, More ablation experiments with different k = {1, 10, 50} on two more MuJoCo tasks, i.e., Swimmer-v1 and Hopper-v1, and more comparison with TRPO and PPO on Walker-v1. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515018167681,"tcdate":1515018167681,"number":4,"cdate":1515018167681,"id":"Byg6DR5QM","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Comment","forum":"BkUp6GZRW","replyto":"HkJ6DWtgf","signatures":["ICLR.cc/2018/Conference/Paper1017/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1017/Authors"],"content":{"title":"Response to Reviewer1","comment":"Thanks for the constructive suggestions. \n\nWe modified the stepsize decay form more concretely (line 6 of Alg 1). It is adjusted based on the theoretical requirement for convergence [2, 3]\n\nWe fixed the extra space after `Dual-AC'. \n\nWe added more discussion of the benefits and the necessity of the path-regularization and stochastic dual ascent in the updated version in the 2nd paragraph and 3rd paragraph in page 5, respectively. For better illustrating the necessity of path-regularization and stochastic dual ascent, we also added more empirical experiments in the ablation study part in Figure 1. \n\nFor the experiment parts, we picked the **best** implementation of the state-of-the-art TRPO and PPO as our baselines based on the recent comprehensive comparison [1]. With the best implementations of TRPO and PPO, these two algorithms consistently achieve the best performance in most of the MuJoCo tasks, beating other alternatives, e.g., DDPG and ACKTR, with significant margins in [1].  Despite such strong baselines, our Dual-AC algorithm still shows substantial gain in 5 out of 6 domains (Fig 2), with a tie in the Swimmer-v1 task.  In InvertedDoublePendulum-v2, Dual-AC achieves almost 3x reward of TRPO and 4x of PPO.\n\n[1], Deep reinforcement learning that matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger. AAAI 2018.\n[2], Robust stochastic approximation approach to stochastic programming. A Nemirovski, A Juditsky, G Lan, A Shapiro. SIAM Journal on optimization 19 (4), 1574-1609.\n[3], Stochastic first-and zeroth-order methods for nonconvex stochastic programming. S Ghadimi, G Lan. SIAM Journal on Optimization 23 (4), 2341-2368.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515018119629,"tcdate":1515018119629,"number":3,"cdate":1515018119629,"id":"Byg5DCqQM","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Comment","forum":"BkUp6GZRW","replyto":"Bysjjx5lG","signatures":["ICLR.cc/2018/Conference/Paper1017/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1017/Authors"],"content":{"title":"Response to Reviewer3","comment":"Thanks for the constructive comments.\n\nAs suggested by the reviewer, we provided further details to explain the benefits of several important extensions: path regularization (2nd paragraph on page 5), stochastic dual ascent (1st paragraph of section 4.3 on page 5), practical updates for policy (the paragraphs surrounding Eqns 16 & 17 on page 7) and critic (2nd last paragraph on page 6).  \n\nThe gaps between Figs 1 and 2 are indeed mainly due to the batch size used in the algorithm. As expected, the batch size affects the variance of the gradients, thereby affecting the convergence of the algorithm.  Such an effect is not unique to our algorithm and has been observed in the literature; see for example similar results for the TRPO baseline in a recent empirical study [1].\n\nFor the comparison between the TRPO and PPO, the recent empirical study [1] shows that different implementations will affect their performance a lot.  Based on the evaluation results in [1], we compared our algorithm with the **best** implementation of TRPO, i.e., the original implementation by Schulman, 2015. From Table 1 and Figure 26 in [1], we can see that the best implementation of TRPO may achieve comparable or even better results comparing to PPO on several tasks.  On the other hand, we used the same parametrization for all the algorithms, which may be preferable to TRPO. We follow [2] using the “iteration” in the experiments to illustrate  the policy behaviors along with the number of updates in the algorithm, rather than the number of data collected for a better understanding of the algorithms in terms of each update. \n\nRe gains of our algorithm: Since the major contribution of our paper is a new algorithm, rather than an alternative parametrization, we conduct the comparison with the baseline using the same parametrizations for fairness. We did not introduce any extra complexity in terms of parameterization. In terms of updates in algorithm, although the update rule for value function needs an extra sample reweighting,  the update rule for policy is much simpler than TRPO, which requires extra adjustments for policy and related parameters.  Therefore, the gains are **not** achieved by added complexity.\n\n[1], Deep reinforcement learning that matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger, AAAI 2018.\n[2], Towards generalization and simplicity in continuous control. Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, Sham Kakade, NIPS 2017."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515017987546,"tcdate":1515017987546,"number":2,"cdate":1515017987546,"id":"S1iWD09Qz","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Comment","forum":"BkUp6GZRW","replyto":"Hyu5lW5xf","signatures":["ICLR.cc/2018/Conference/Paper1017/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1017/Authors"],"content":{"title":"Response to Reviewer2","comment":"We appreciate the constructive comments on both theoretical and empirical aspects by the reviewer. \n\nWe first emphasize our contributions. The major contributions of this paper are (1) the **first** establishment of the competition between actor and critic in a **multi-step** setting; and (2) a novel algorithm that is make effective thanks to several critical components we introduce, including path-regularization and stochastic dual ascent, to deal with potential numerical issues that arise when one directly solves the zero-sum game. \n\nTheory Clarification: \n1, Novelty of Theorems: Theorem 2 (one-step dual form)  is indeed an extension of existing results to continuous state and action MDP.  Theorem 3 (multi-step dual form)  is one of our major contributions and is a novel result.  The claim in Theorem 3 may appear natural, but its proof is a highly nontrivial generalization of the one-step case, since the convex-concave structure breaks down in the multi-step setting.  We have made this clearer in the revision. \n\n2, Assumptions on value function class and choice of regularization parameter:  We tried to separate the justification of path-regularization (theory) from the parametrization of value function (practice). \n\ti), Theoretically, regarding Theorem 4 and its proof, we consider the entire value function space, i.e., the nonparametric limit, without taking into account of parametrization. Hence, as long as the regularization parameter (i.e., eta) is selected appropriately, this doesn’t affect the optimality. Note that an implicit condition of eta is provided on Page 18; however, finding an explicit condition for the regularization parameter seems to be rather difficult and is beyond the scope of this work.\n\tii), Practically, we always parametrize the value function (which affects the valid range of eta) and tune the regularization parameter to achieve the best performance. \n\n3, Minor gaps in proofs:  Yes, there should be a square in the proof on page 18. This does not jeopardize the rest of the proof as we only need boundedness of this term.  We have fixed the issue in our revision. For the first inequality on page 14 about ||V^*||_{2, .mu}^2, it comes from the inequality E[(X+Y)^2] <= 2 * (E[X^2] + E[Y^2]), a generalization of (a+b)^2<= 2(a^2 +b^2).  We have added more details to the proof. Thanks for pointing out these issues. \n\n\nExperiments Clarification: \n1, Performance comparisons with and without the improvements:  In the ablation experiment part, we compared the proposed dual-AC algorithm with/without the path-regularization, and with/without multi-step on several MuJoCo tasks, including InvertedDoublePendulum, Swimmer, and Hopper. The results suggest that using path-regularization and multi-step significantly improves the performances.   Detailed experimental results can be found in Figure 1.\n\n2, Effects of the length of multi-steps: We conducted additional experiments to investigate the effects of multi-step lengths. Specifically, we compared the performance with different k = {1, 10, 50}, and tested on three tasks. Better performances are observed with increasing k, which indicates that reducing the bias is indeed critical. Detailed experimental results can be found in Figure 1.  \n\n3, Computation overheads of using multi-step: In terms of the computational cost, assuming the length of the trajectories are m, with a simple moving sum algorithm, we calculate all the k length partial reward sums for a trajectory in O(m) with a O(1) amortized cost to calculate each sum of rewards. This method was used in our experiments for our algorithm as well as all competitors. Comparing to TRPO and PPO, the cost for summation is the same for all algorithms and the update costs are constant for each individual algorithm regardless of the choice of k.\n\n4, Local convexity in practice:  In general, using a positive eta_V coefficient with path-regularization always enhances the local convexity. For example, if V is parametrized in a linear form, as long as eta_V is not zero, local convexity will hold.  A larger eta_V will result in faster convergence, at the cost of extra bias. It is not easy to theoretically/empirically inspect the exact local convexity condition when a complicated parametrization of V is used. In practice, we suggest to simply tune the regularization parameter, and that’s what we have done in the experiments.  \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515642377065,"tcdate":1511817359635,"number":3,"cdate":1511817359635,"id":"Hyu5lW5xf","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Review","forum":"BkUp6GZRW","replyto":"BkUp6GZRW","signatures":["ICLR.cc/2018/Conference/Paper1017/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Overall a good paper, with a few details that need clarification","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method, Dual-AC, for optimizing the actor(policy) and critic(value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor. In order to achieve that, they take the linear programming approach of solving the bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. The discussion on the deficiencies of the naive LP approach is mostly well done. Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization. They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements. Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms. \n\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Given these clarifications in an author response, I would be willing to increase the score. \n\nFor the theory, there are a few steps that need clarification and further clarification on novelty. For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results. It looks like Theorem 2 has already been shown in \"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time”. There is a statement that “Chen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization”. Is you Theorem 2 somehow an extension? Is Theorem 3 completely new?\n\nThis is particularly called into question due to the lack of assumptions about the function class for value functions. It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function). This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*. Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. More about conditions on eta_v would be illuminating. \n\nThere is also one step in the theorem that I cannot verify. On Page 18, how is the squared removed for difference between U and Upi? The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2. \n\n\nFor the experiments, the following should be addressed.\n\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.\n\n2. The central contribution is extending the single step LP to a multi-step formulation. It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.\n\n3. Increasing k also comes at a computational cost. I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).\n\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization. It was also mentioned that increasing the regularization parameter size increases the convergence rate. Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned. Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity? A bit more discussion on these choices would be helpful. \n\nMinor comments:\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515642377106,"tcdate":1511816098599,"number":2,"cdate":1511816098599,"id":"Bysjjx5lG","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Review","forum":"BkUp6GZRW","replyto":"BkUp6GZRW","signatures":["ICLR.cc/2018/Conference/Paper1017/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good contribution","rating":"6: Marginally above acceptance threshold","review":"The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper. Each extension to the Dual Actor-Critic is well motivated and clear in context. Perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice; multi-step updates, regularized against MC returns, stochastic mirror descent. \n\nThe practical implementation section losses some of this clear organization, and could certainly be clarified each part tied into Algorithm 1, and this was itself made less high-level. But these are minor gripes overall.\n\nTurning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO. There were a few things that jumped out to me that I was surprised by. The difference in performance for Dual-AC between Figure 1 and Figure 2b is significant, but the only difference seems to be a reduce batch size, is this right? This suggests a fairly significant sensitivity to this hyperparameter if so.\n\nReproducibility in continuous control is particularly problematic. Nonetheless, in recent work PPO and TRPO performance on the same set of tasks seem to be substantively different than what the authors get in their experiments. I'm thinking in particular of:\n\nProximal Policy Optimization Algorithms (Schulman et. al., 2017)\nMulti-Batch Experience Replay for Fast Convergence of Continuous Action Control (Han and Sung, 2017)\n\nIn both these cases the results for PPO and TRPO vary pretty significantly from what we see here, and an important one to look at is the InvertedDoublePendulum-v1 task, which I would think PPO would get closer to 8000, and TRPO not get off the ground. Part of this could be the notion of an \"iteration\", which was not clear to me how this corresponded to actual time steps. Most likely, to my mind, is that the parameterization used (discussed in the appendix) is improving TRPO and hurting PPO.\n\nWith these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved, which may beg the question if the algorithmic contributions are buying much for their added complexity?\n\nPros:\nWell written, thorough treatment of the approaches\nImprovements on top of Dual-AC with ablation study show improvement\n\nCons:\nEmpirical gains might not be very large\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515642377148,"tcdate":1511753655320,"number":1,"cdate":1511753655320,"id":"HkJ6DWtgf","invitation":"ICLR.cc/2018/Conference/-/Paper1017/Official_Review","forum":"BkUp6GZRW","replyto":"BkUp6GZRW","signatures":["ICLR.cc/2018/Conference/Paper1017/AnonReviewer1"],"readers":["everyone"],"content":{"title":"an interesting paper","rating":"7: Good paper, accept","review":"This paper studies a new architecture DualAC. The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible).\n\nThere are several drawbacks of the current format of the paper:\n1. The algorithm is vague. Alg 1 line 5: 'closed form': there is no closed form in Eq(14). It is just an MC approximation.\nline 6: Decay O(1/t^\\beta). This is indeed vague albeit easy to understand. The algorithm requires that every step is crystal clear.\n\n2. Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract,'Dual-AC ' (an extra space). There are many format errors like this throughout the paper. The author is suggested to do a careful format check.\n\n3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief.\n\n4. The experimental part is ok to me, but not very impressive.\n\nOverall, this seems to be a nice paper to me.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]}},{"tddate":null,"ddate":null,"tmdate":1515181379905,"tcdate":1509137874222,"number":1017,"cdate":1510092360569,"id":"BkUp6GZRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkUp6GZRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Boosting the Actor with Dual Critic","abstract":"This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.","pdf":"/pdf/75ae045846f133c5ae65829317b124262974d050.pdf","TL;DR":"We propose Dual Actor-Critic algorithm, which is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.","paperhash":"anonymous|boosting_the_actor_with_dual_critic","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting the Actor with Dual Critic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkUp6GZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1017/Authors"],"keywords":["reinforcement learning","actor-critic algorithm","Lagrangian duality"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}