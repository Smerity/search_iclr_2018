{"notes":[{"tddate":null,"ddate":null,"tmdate":1515180630727,"tcdate":1515164888997,"number":3,"cdate":1515164888997,"id":"S1ZySzpQf","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Comment","forum":"HJ4YGZ-AW","replyto":"HknhJ_Exf","signatures":["ICLR.cc/2018/Conference/Paper660/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper660/Authors"],"content":{"title":"Authors' reply to Reviewer 3","comment":"\no Reviewer’s comment: Just call the pos tagging task \"POS tagging\". Talking about classification of the part of speech of *a* word makes it sound like you're tagging a single word in isolation. \n\n   Answer:  Thank you for the comment. This part has been removed.  \n\no Reviewer’s comment: The statement that language structures can't be integrated with DL is a hopeless misrepresentation of current practice in NLP, and misses a large body of existing work. There's obviously Richard Socher's work on integrating DL and the output of eg the Stanford parser, but also a current raft of work on trying to induce tree structures automatically via a DL framework and task-based objective. Two examples, by no means exhaustive: Learning to Compose Words into Sentences with Reinforcement Learning Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling. ICLR 2017. Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs Jean Maillard, Stephen Clark, Dani Yogatama. \n\n    Answer: Thank you for your comments.  Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically structured networks as mentioned by the reviewer. In our paper, the network\nis not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves — filler/role structure.\n\no Reviewer’s comment: It's not at all clear to me what the third task is - something like chunking. Identification of the phrase structure sounds like parsing, but you're not doing full parsing. This needs explaining fully. There are various standard NLP tasks related to identifying phrase structure - I would just do one of those, using one of the standard datasets, then there won't be any confusion. \n\n   Answer: Thank you for the comment.  We will conduct more comprehensive study and present the findings in future work. \n\no Reviewer’s comment: The description of tagging on p.2 mentions MEMMs too much - these were superseded by CRFs, which I think is what you mean to refer to. \n\n   Answer: Yes, you are right.  \n\no Reviewer’s comment: The reference to a maximum entropy language model is a little odd, since as far as I know these never became mainstream (assuming you mean Rosenfeld's whole sentence maxent language models). \n\n   Answer: In the literature, the maximum entropy approach is one type of language model as mentioned in the following paper:  \nDevlin, Jacob, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. \"Language models for image captioning: The quirks and what works.\" arXiv preprint arXiv:1505.01809 (2015).\n\no Reviewer’s comment: N is terrible name for a system! \n\n    Answer: It is a notation for the TPGN system.  This notation is from \"N\" in \"TPGN\".\n\no  Reviewer’s comment: Not sure about the 5-role schema example on p.4, since presumably these would still be generated one word at a time? So in what sense is the model encoding a schema? \n\n    Answer: 5-role schema corresponds to five role vectors r_1, r_2, r_3, r_4, r_5.  The model encodes 5-role schema with fillers by Eq. (1).\n\no Reviewer’s comment: Section 5 is the key section in the paper. Unfortunately I found it hard to follow. I guess the LSTM equations are needed for completeness, but what I really needed was a clear paragraph explaining how the LSTM is used to build the vectors and matrices used by the binding/unbinding network. \n\n    Answer: Thank you for your comment. We re-organized the paper to improve its clarity. \n\no Reviewer’s comment: There are various oddities in the POS tagging experiments. Why use the Stanford tagger? Are you using this to get the train/test data? Just use the Penn Treebank, or another standard pos tag dataset. \n\n    Answer: Thank you for good suggestion. we will perform more comprehensive study and present the results in future work.\n\no Reviewer’s comment: Why precision and recall for tagging? Doesn't each word get assigned a single tag? In which case we just need accuracy.\n\n   Answer: Thanks. we will perform more comprehensive study and present the results in future work.\n\no Reviewer’s comment:  There are a number of minor comments I could have made re. the presentation, eg funny refs with both names (Chen and Laurence Zitnick). \n\n    Answer: We fixed it in the revised paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515179758856,"tcdate":1515164594468,"number":2,"cdate":1515164594468,"id":"H1qn7f6QG","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Comment","forum":"HJ4YGZ-AW","replyto":"HJ2YETtxz","signatures":["ICLR.cc/2018/Conference/Paper660/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper660/Authors"],"content":{"title":"Authors' reply to Reviewer 2","comment":"\nI) Approach: \n1) Reviewer’s comment: It would be nice to explain clearly why the pretraining of the TPGN with the LSTM input is needed. Is the idea that one would want to feed the representation of the entire representation as input in order to infer what “S_n” should be? Why is it then justified to feed in the image input instead? Also, in the second stage, the image features need not correspond to the LSTM feature dimensions, which means that the pretraining seems unprincipled. A better solution would have been to learn a joint embedding of image captions and labels (say via. ranking), and then use the embedding for the caption as input to the TPGN. This would ensure that when we use images, the model sees input that is appropriately “aligned”. A discussion why this is not needed or implementing this seems important. \n\n    Answer:  Thanks for the suggestion.  We removed it from the paper.\n\nII) Minor Points: \n1) Reviewer’s comment: “There are mainly two approaches to natural language generation in image captioning. The first approach takes the words detected by a CNN as input, and uses a probabilistic model, such as a maximum entropy (ME) language model, to arrange the detected words into a sentence.” -- can cite Fang. et.al [A] \n\n    Answer: Thank you for the suggestion.  We cited it in the revised paper.\n\nIII) Baselines: \n1) Reviewer’s comment: 1. The arXiv version and the PAMI version of the Neural Image Captioning paper (Vinyals, 2015) does report numbers on METEOR and CIDEr metrics, so they should be used to populate Table. 1 for completeness. Also, it would be good to clarify which split of MSCOCO Table. 1 reports results on -- is it the 40K large validation split or the 5K validation/test split released by (Karpathy, 2015)? Clarifying this would be nice since the numbers seem a bit on the lower side. \n\n    Answer: Thank you for pointing it out.  We corrected the numbers in the revised paper.\n\n2) Reviewer’s comment: What are the relative number of parameters in the Vinyals et.al. baseline and the proposed TPGN model? Would having an LSTM with twice the number of layers (by stacking them) or twice the size of the hidden state do better? \n\n    Answer: The complexity of the TPGN is comparable to the NIC. The results from NIC and our re-implementation in the paper are optimized w.r.t. the model architecture on hold-out data, and we observed that the performance start to saturate when adding more layers/nodes in the CNN-LSTM re-implementation. \n\n3) Reviewer’s comment: What if we used the hidden state of a regular LSTM decoder to do POS tagging? How well would that do? Does the TPN capture any more syntactic structure than an LSTM decoder (Table. 2). This seems to be an important result to report. \n\n    Answer: The accuracy of using the hidden state of a regular LSTM decoder to do POS tagging is less than 70%, which is much poorer than the Stanford POS tagger and our TPR based POS tagger.   Thanks for the suggestion. We will perform this experiments in future work.\n\nIV) Clarity:\nI) Reviewer’s comment:  Page 7.: “We also implemented the latest ResNet feature” -- would be good be explicit which resnet model is used. \n\n   Answer: We take the output of the 2048-way pool5 layer from ResNet-152, pretrained on the\nImageNet dataset. This feature is used in the CNN-LSTM reimplementation and our TPGN.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515189769540,"tcdate":1515164317654,"number":1,"cdate":1515164317654,"id":"BJIjffT7G","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Comment","forum":"HJ4YGZ-AW","replyto":"SkTs5lAxf","signatures":["ICLR.cc/2018/Conference/Paper660/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper660/Authors"],"content":{"title":"Authors' reply to Reviewer 1","comment":"\n1) Reviewer’s comment: The paper claims that \"Deep Learning (DL) has not been able to explicitly represent and enforce grammatical structures\", which is false, see \"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\", \"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\", \"DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks\" or \"Deep Compositional Question Answering with Neural Module Networks\". \n\n     Answer: Thank you for your comments.  Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically structured networks as mentioned by the reviewer. In our paper, the network\nis not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves — filler/role structure.\n\n2) Reviewer’s comment: The Introduction triple challenge is confusing, not clear what are the challenges this paper tries to address. \n\n     Answer: Thank you for the comment.  We removed it in the revised paper.\n\n3) Reviewer’s comment: \"The representation learned in a crucial layer of the TPGN can be interpreted as encoding grammatical roles\" Doesn't refer to any specific kind of layer, or what it make it special. \n\n     Answer:  Thank you for your suggestion.  We changed it to \"The representation learned in the TPGN can be interpreted as encoding grammatical roles\".\n\n4) Reviewer’s comment: The idea of using outer product as a layer has already been explored in \"Multimodal compact bilinear pooling for visual question answering and visual grounding\" \n\n     Answer: The idea of using the outer product to construct internal representations was indeed explored in the paper mentioned by the reviewer.  In this paper, by contrast, the learned representations are not themselves constrained, but the global structure of the network\nis designed to display the somewhat abstract property of being TPR-capable: the architecture uses the TPR unbinding operation of the matrix-vector product to extract individual words for sequential output.\n\n5) Reviewer’s comment: The following paragraph in page 2 is not clear, very confusing: The work reported here .... their categories \n\n     Answer: Thank you for the comment.  We removed it and rewrote this section.\n\n6) Reviewer’s comment: In page 3 authors claim that the \"vectors are linearly independent\" but didn't specify how they enforce that. \n\n     Answer:  In this paper, we do not enforce the independence among the role vectors.  We removed this sentence.\n\n7) Reviewer’s comment: Figure 3 contradicts Figure 1, not clear what are the inputs for module S. \n\n     Answer: Fig. 1 is used for image captioning with an image as input.  Fig. 3 is used for a grammar analyzer such a POS tagger, phrase detector/classifier.  Fig. 3 is removed in the revised paper to make the presentation more focused.\n\n8) Reviewer’s comment: The experiments reported in Table1 are useless, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results Even the numbers reported for Vinyals et al. (2015) are much higher in the leaderboard. There is no comparison with other models that use attention or analysis of the impact of the increased number of parameters of the method proposed. \n\n     Answer: The main contribution of this work is the proposed TPGN, which is a new TPR-inspired architecture compared to the LSTM. The experimental results show that it outperform LSTM on the large scale image captioning tasks. In the evaluation, in addition to quote the results from Vinyals et al. (2015). We noticed that the LSTM-based captioning model has been improved since 2015, so we also re-implemented the CNN-LSTM model given recent findings as a stronger baseline. Compared to this stronger baseline, the TPGN still performs significantly better (e.g., by 1.3 BLEU-4 pt). Our focus in this paper is to improve the essential architecture of the LSTM using TPGN. We leave the extension of TPGN with attention to future work.\n\n9) Reviewer’s comment: The experiments about POS tagger and Phrase Classifier are reported on 5000 from the COCO test set, which is useful for comparisons. Should report numbers on PennTreeBank or other common POS dataset. \n\n     Answer: Thanks for suggestions. Due to time limit we will present the results in future work.\n\n10) Reviewer’s comment: The text is missing a lot of references, for example: - page 2 GSC - page 2 The first approach takes the detected by a CNN .... - page 3 previous work where TPRs are hand-crafted\n\n       Answer: Thank you for the comment.  We revised the paper accordingly.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515642487183,"tcdate":1512077988678,"number":3,"cdate":1512077988678,"id":"SkTs5lAxf","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Review","forum":"HJ4YGZ-AW","replyto":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference/Paper660/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Confusing paper, poor experiments and missing a lot of references.","rating":"4: Ok but not good enough - rejection","review":"The paper claims that \"Deep Learning (DL) has not been able to explicitly represent and enforce grammatical structures\", which is false, see \"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\", \"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\", \"DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks\" or \"Deep Compositional Question Answering with Neural Module Networks\".\n\nThe Introduction triple challenge is confusing, not clear what are the challenges this paper tries to address.\n\n\"The representation learned in a crucial layer of the TPGN can be interpreted as encoding grammatical roles\" Doesn't refer to any specific kind of layer, or what it make it special.\n\nThe idea of using outer product as a layer has already been explored in \"Multimodal compact bilinear pooling for visual question answering and visual grounding\"\n\nThe following paragraph in page 2 is not clear, very confusing:\nThe work reported here .... their categories\n\n\nIn page 3 authors claim that the \"vectors are linearly independent\" but didn't specify how they enforce that.\n\nFigure 3 contradicts Figure 1, not clear what are the inputs for module S.\n\nThe experiments reported in Table1 are useless, there a tons of previous work with much better results, see \nhttps://competitions.codalab.org/competitions/3221#results\n\nEven the numbers reported for Vinyals et al. (2015) are much higher in the leaderboard. \n\nThere is no comparison with other models that use attention or analysis of the impact of the increased number of parameters of the method proposed. \n\nThe experiments about POS tagger and Phrase Classifier are reported on 5000 from the COCO test set, which is useful for comparisons. Should report numbers on PennTreeBank or other common POS dataset.\n\nThe text is missing a lot of references, for example:\n - page 2 GSC\n - page 2 The first approach takes the detected by a CNN ....\n - page 3 previous work where TPRs are hand-crafted","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515642487226,"tcdate":1511801987685,"number":2,"cdate":1511801987685,"id":"HJ2YETtxz","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Review","forum":"HJ4YGZ-AW","replyto":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference/Paper660/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper adapts a classical idea of Tensor Product Representation (TPN) which basically talks about how connectionist models can capture syntax explicitly and adapts the TPN as an inductive bias in image caption generation, and shows the generality of the learnt representations to tasks such as POS tagging and phrase classification. ","rating":"5: Marginally below acceptance threshold","review":"**Strengths**\nThe approach to sentence generation makes a lot of sense -- and provides a potentially elegant manner to incorporate or provide the model with the inductive bias that language has syntax and semantics, by leveraging a classical idea called Tensor Product Representation and showing how to adapt it to modern deep learning architectures and “learn” syntax and semantics end to end. Results on image captioning models indicate that the proposed approach might be promising. As a by-product, the paper also evaluates the model on POS tagging and shows that one can do fairly well using the representations learned in the TPGN. \n\n**Weakness**\nMy main concerns are the lack of appropriate baselines to establish concretely the contribution of the TPGN. It would be good to address issues under “Baselines” below.\n\nApproach:\n1. It would be nice to explain clearly why the pretraining of the TPGN with the LSTM input is needed. Is the idea that one would want to feed the representation of the entire representation as input in order to infer what “S_n” should be? Why is it then justified to feed in the image input instead? Also, in the second stage, the image features need not correspond to the LSTM feature dimensions, which means that the pretraining seems unprincipled. A better solution would have been to learn a joint embedding of image captions and labels (say via. ranking), and then use the embedding for the caption as input to the TPGN. This would ensure that when we use images, the model sees input that is appropriately “aligned”. A discussion why this is not needed or implementing this seems important.\n\nMinor Points:\n1.  “There are mainly two approaches to natural language generation in image captioning. The first approach takes the words detected by a CNN as input, and uses a probabilistic model, such as a maximum entropy (ME) language model, to arrange the detected words into a sentence.” -- can cite Fang. et.al [A]\n\nBaselines:\n1. The arXiv version and the PAMI version of the Neural Image Captioning paper (Vinyals, 2015) does report numbers on METEOR and CIDEr metrics, so they should be used to populate Table. 1 for completeness. Also, it would be good to clarify which split of MSCOCO Table. 1 reports results on -- is it the 40K large validation split or the 5K validation/test split released by (Karpathy, 2015)? Clarifying this would be nice since the numbers seem a bit on the lower side.\n\n2. What are the relative number of parameters in the Vinyals et.al. baseline and the proposed TPGN model? Would having an LSTM with twice the number of layers (by stacking them) or twice the size of the hidden state do better?\n\n3. What if we used the hidden state of a regular LSTM decoder to do POS tagging? How well would that do? Does the TPN capture any more syntactic structure than an LSTM decoder (Table. 2). This seems to be an important result to report.\n\nClarity:\n1. Page 7.: “We also implemented the latest ResNet feature” -- would be good be explicit which resnet model is used.\n\nReferences:\n[A]: Fang, Hao, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, et al. 2014. “From Captions to Visual Concepts and Back.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1411.4952.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515642487262,"tcdate":1511452595994,"number":1,"cdate":1511452595994,"id":"HknhJ_Exf","invitation":"ICLR.cc/2018/Conference/-/Paper660/Official_Review","forum":"HJ4YGZ-AW","replyto":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference/Paper660/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Worthwhile goal of bringing together GSC and DL; really poor presentation.","rating":"4: Ok but not good enough - rejection","review":"I have a huge amount of sympathy for this work, and was really hoping to read a well-presented paper setting out how to cleanly integrate Smolensky's theory with deep learning, ideally (but not necessarily) with some decent empirical results. If that had been the case, I would certainly have been recommending acceptance, since ICLR would benefit from the alternative perspective that Smolensky's work provides, compared to the majority of work in Deep Learning. The empirical results are decent, but the presentation requires too much work to warrant acceptance at ICLR for this year. There are also some questionable decisions made regarding the NLP evaluations.\n\nMore detailed comments.\n\no Just call the pos tagging task \"POS tagging\". Talking about classification of the part of speech of *a* word makes it sound like you're tagging a single word in isolation.\n\no The statement that language structures can't be integrated with DL is a hopeless misrepresentation of current practice in NLP, and misses a large body of existing work. There's obviously Richard Socher's work on integrating DL and the output of eg the Stanford parser, but also a current raft of work on trying to induce tree structures automatically via a DL framework and task-based objective. Two examples, by no means exhaustive:\n\nLearning to Compose Words into Sentences with Reinforcement Learning\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling.\nICLR 2017.\n\nJointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs\nJean Maillard, Stephen Clark, Dani Yogatama.\n\no It's not at all clear to me what the third task is - something like chunking. Identification of the phrase structure sounds like parsing, but you're not doing full parsing. This needs explaining fully. There are various standard NLP tasks related to identifying phrase structure - I would just do one of those, using one of the standard datasets, then there won't be any confusion.\n\no The description of tagging on p.2 mentions MEMMs too much - these were superseded by CRFs, which I think is what you mean to refer to.\n\no The reference to a maximum entropy language model is a little odd, since as far as I know these never became mainstream (assuming you mean Rosenfeld's whole sentence maxent language models).\n\no N is terrible name for a system!\n\no Not sure about the 5-role schema example on p.4, since presumably these would still be generated one word at a time? So in what sense is the model encoding a schema?\n\no Section 5 is the key section in the paper. Unfortunately I found it hard to follow. I guess the LSTM equations are needed for completeness, but what I really needed was a clear paragraph explaining how the LSTM is used to build the vectors and matrices used by the binding/unbinding network.\n\no There are various oddities in the POS tagging experiments. Why use the Stanford tagger? Are you using this to get the train/test data? Just use the Penn Treebank, or another standard pos tag dataset.\n\no Why precision and recall for tagging? Doesn't each word get assigned a single tag? In which case we just need accuracy.\n\no There are a number of minor comments I could have made re. the presentation, eg funny refs with both names (Chen and Laurence Zitnick). ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515163324672,"tcdate":1509130876443,"number":660,"cdate":1509739172677,"id":"HJ4YGZ-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJ4YGZ-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Neural-Symbolic Approach to Natural Language Tasks","abstract":"Deep learning (DL) has in recent years been widely used in natural\nlanguage processing (NLP) applications due to its superior\nperformance. However, while natural languages are rich in\ngrammatical structure, DL has not been able to explicitly\nrepresent and enforce such structures. This paper proposes a new\narchitecture to bridge this gap by exploiting tensor product\nrepresentations (TPR), a structured neural-symbolic framework\ndeveloped in cognitive science over the past 20 years, with the\naim of integrating DL with explicit language structures and rules.\nWe call it the Tensor Product Generation Network\n(TPGN), and apply it to image captioning. The key\nideas of TPGN are: 1) unsupervised learning of\nrole-unbinding vectors of words via a TPR-based deep neural\nnetwork, and 2) integration of TPR with typical DL architectures\nincluding Long Short-Term Memory (LSTM) models. The novelty of our\napproach lies in its ability to generate a sentence and extract\npartial grammatical structure of the sentence by using\nrole-unbinding vectors, which are obtained in an unsupervised\nmanner. Experimental results demonstrate the effectiveness of the\nproposed approach.","pdf":"/pdf/c2e14b9fd2e078523b76148025d345f954bcfecb.pdf","TL;DR":"This paper is intended to develop a tensor product representation approach for deep-learning-based natural language processinig applications.","paperhash":"anonymous|a_neuralsymbolic_approach_to_natural_language_tasks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Neural-Symbolic Approach to Natural Language Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4YGZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper660/Authors"],"keywords":["Deep learning","tensor product representation","LSTM","image captioning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}