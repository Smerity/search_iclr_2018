{"notes":[{"tddate":null,"ddate":null,"tmdate":1512419978145,"tcdate":1512419978145,"number":5,"cdate":1512419978145,"id":"r1zqfN7Zf","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"rJ5a7tg-G","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Quick answer for your questions","comment":"I am sorry again for rush derivations, will revise the derivations and provide an official response to your review. But just to quickly explain Q1 and Q3 here:\n\nQ1:\nLet's assume h(x) output a scalar for a moment. Then, stein's identity can be proved using integration by parts:\n\n\\int q(x) [ h(x) * dlogq(x)/dx + dh(x)/dx] dx\n= \\int [h(x) * dq(x)/dx + q(x) * dh(x)/dx ] dx    // dlogx/dx = x^{-1}\n= \\int d[h(x)q(x)]/dx dx\n= h(x)q(x)|_{\\partial X} \n= 0    // assumed by the boundary condition\n\nNow write multi-dimension version h(x) = (h_1(x), ..., h_{d'}(x)). Then looking at eq (8), we notice that the ith row of the LHS matrix is actually\n\\int q(x) [ h_i(x) * dlogq(x)/dx + dh_i(x)/dx] dx,\nmeaning that if we assume boundary condition for h(x) (which also implies boundary condition for h_i(x)), then we can prove again that the LHS matrix is actually zero.\n\nQ3:\nHere I assume we can apply the reparameterisation trick for q (see the VAE papers). This says, \nsampling z ~ q_{\\phi}(z | x) is equivalent to 1) sample \\epsilon ~ \\pi(\\epsilon), then 2) compute z = f_{\\phi}(\\epsilon, x). \\pi(\\epsilon) is the distribution of the noise which is usually Gaussian. This means, we can rewrite the expectation in q to expectation in \\pi. Please see section 2.4 in the original VAE paper (Kingma and Welling 2013) for an example math derivation. \n\nThen I wanted to differentiate the variational lower-bound wrt \\phi. Especially, eq. (3) derived the gradient of the entropy term wrt \\phi. I will add in detailed derivations in revision, but for your quick reference please see eq (5-7) in (Roeder et al. 2017) for an example derivation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512244162294,"tcdate":1512244162294,"number":4,"cdate":1512244162294,"id":"rJ5a7tg-G","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"r19JQOTgG","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"content":{"title":"Not clear yet","comment":"I am still not following the details for 1 and 3 yet. Feel free to elaborate below, but ideally these should have appeared in the paper.\nFor 2, what is pi?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222617458,"tcdate":1512100382141,"number":3,"cdate":1512100382141,"id":"rJ8QfICez","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Review","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer1"],"readers":["everyone"],"content":{"title":"an interesting paper","rating":"6: Marginally above acceptance threshold","review":"This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.\n\nThis is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. \n\n- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term \"Stein identity\" is appropriate to express the equation (8). \n- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512043741483,"tcdate":1512043741483,"number":3,"cdate":1512043741483,"id":"HJS1ru6gG","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"SJCCQvTgz","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Thanks for your comments!","comment":"Thank you for your interest in this paper!\n\nFor your questions:\n1. Yes the original Stein (1981) paper only described the identity for a multivariate Gaussian distribution, and it assumed the test function to output scalars. However, the proof technique only used integration by parts, which, if the boundary condition is assumed, should be able to generalise to general distribution case.\n\nOur twist of the formula comes from the observation that we can pack multiple (scalar output) test functions into a vector. This has also been considered in e.g. Liu et al. (2016). \n\nWhat would you suggest to call (8) other than \"Stein's identity\"?\n\n2. Yes it would be nice to have an example, however in the Gaussian case since the tails decay exponentially, almost all functions satisfy the boundary condition. Maybe it would be helpful to do an example with long-tail distributions.\n\nHope this helps and thanks again!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512043234311,"tcdate":1512043234311,"number":2,"cdate":1512043234311,"id":"r19JQOTgG","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"SJxdjXBgM","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Thanks for your questions!","comment":"Thank you for your time on reviewing this paper!\n\nOn your questions:\n1. Yes the original Stein (1981) paper only described the identity for a multivariate Gaussian distribution, and it assumed the test function to output scalars. However, the proof technique only used integration by parts, which, if the boundary condition is assumed, should be able to generalise to general distribution case.\n\nOur twist of the formula comes from the observation that we can pack multiple (scalar output) test functions into a vector. This has also been considered in e.g. Liu et al. (2016). Will fix the descriptions -- thank you!\n\n2. Sorry for the rush of background introduction. Here q denotes the approximate posterior, and q_{\\phi} just explicitly writes the dependency of q to its parameter \\phi. \\epsilon is the noise variable used in the reparameterisation trick.\n\n3. Again sorry for the rush of the derivation -- will add a full equation in appendix. \nIn short the idea is to apply the reparameterisation trick, and notice that the gradient of \\phi contains the path gradient (the first term in (3)) and an expectation of the REINFORCE gradient (the second term). Using the log-derivative trick we can show that the second term is zero.\n\nHope this helps!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512039381978,"tcdate":1512039381978,"number":1,"cdate":1512039381978,"id":"SJCCQvTgz","invitation":"ICLR.cc/2018/Conference/-/Paper303/Public_Comment","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"an interesting paper","comment":"This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.\n\nThis is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. \n\n- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term \"Stein identity\" is appropriate to express the equation (8). \n- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222617499,"tcdate":1511947816296,"number":2,"cdate":1511947816296,"id":"H1xEAg3lM","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Review","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper with novel and significant contribution!","rating":"7: Good paper, accept","review":"In this paper, the authors proposed the Stein gradient estimator, which directly estimates the score function of the implicit distribution. Direct estimation of gradient is crucial in the context of GAN because it could potentially lead to more accurate updates. Motivated by the Stein’s identity, the authors proposed to estimate the gradient term by replacing expectation with the empirical counterpart and then turn the resulting formulation into a regularized regression problem. They also showed that the traditional score matching estimator (Hyvarinen 2005) can be obtained as a special case of their estimator. Moreover, they also showed that their estimator can be obtained by minimizing the kernelized Stein discrepancy (KSD) which has been used in goodness-of-fit test. In the experiments, the proposed method is evaluated on few tasks including Hamiltonian flow with approximate gradients, meta-learning of approximate posterior samplers, and GANs using entropy regularization. \n\nThe novelty of this work consists of an approach based on score matching and Stein’s identity to estimate the gradient directly and the empirical results of the proposed method on meta-learning for approximate inference and entropy regularized GANs. The proposed method is new and technically sound. The authors also demonstrated through several experiments that the proposed technique can be applied in a wide range of applications.\n\nNevertheless, I suspect that the drawback of this method compared to existing ones is computational cost. If it takes significantly longer to compute the gradient using proposed estimator compared to existing methods, the gain in terms of accuracy is questionable. By spending the same amount of time, we may obtain an equally accurate estimate using other methods. For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case. Hence, the comparison in terms of computational cost should also be included either in the text or in the experiment section.\n\nWhile the proposed Stein gradient estimator is technically interesting, the experimental results do not seem to evident that it significantly outperforms existing techniques. In Section 4.2, the authors only consider four datasets (out of six UCI datasets). Also, in Section 4.3, it is not clear what the point of this experiment is: whether to show that entropy regularization helps or the Stein gradient estimator outperforms other estimators.\n\nSome comments:\n\n- Perhaps, it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work, i.e., using Stein’s identity to derive an estimate of the gradient of the score function.\n- Stein gradient estimator vs KDE: What if the kernel is not translation invariant? \n- In Section 4.3, why did you consider the entropy regularizer? How does it help answer the main hypothesis of this paper?\n- The experiments in Section 4.3 seems to be a bit out of context.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222617542,"tcdate":1511718291032,"number":1,"cdate":1511718291032,"id":"ryjc6__ez","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Review","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Didn't follow all the details","rating":"5: Marginally below acceptance threshold","review":"Summary\n================================================================\nThe authors present a method for estimating the gradient of some training objective\nfor generative models used to sample data, such as GANs. The idea is that this can be\nused in a training procedure. The idea is based off the Stein's identity, for which the\nauthors propose a kernelized solution. The authors present applications in Bayesian NNs\nand GANs.\n\n\n\n\nDetailed Reviews\n================================================================\n\nMy main concern is what I raised via a comment, for which I have not received a response\nas yet. It seems that you want the gradients w.r.t the parameters phi in (3). But the\nline immediately after claims that you need the gradients w.r.t the domain of a random\nvariable z and the subsequent sections focus on the gradients of the log density with\nrespect to the domain. I am not quite following the connection here.\n\nAlso, it doesn't help that many of the symbols on page 2 which elucidates the set up\nhave not been defined. What are the quantities phi, q, q_phi, epsilon, and pi?\n\nPresentation\n- Bottom row in Figure 1 needs to be labeled. I eventually figured that the colors\n  correspond to the figures above, but a reader is easily confused.\n- As someone who is not familiar with BNNs, I found the description in Section 4.2\n  inadequate.\n \nSome practical concerns:\n- The fact that we need to construct a kernel matrix is concerning. Have you tried\n  batch verstions of these estimator which update the gradients with a few data points?\n- How is the parameter \\eta chosen in practice? Can you comment on the values that you\n  used and how it compared to the eigenvalues of the kernel matrix?\n\nMinor\n- What is the purpose behind sections 3.1 and 3.2? They don't seem pertinent to the rest\n  of the exposition. Same goes for section 3.5? I don't see the authors using the\n  gradient estimators for out-of-sample points?\n\nI am giving an indifferent score mostly because I did not follow most of the details.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511656511730,"tcdate":1511500648131,"number":1,"cdate":1511500648131,"id":"SJxdjXBgM","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"content":{"title":"Some Questions","comment":"1. Can you give a reference for Stein's multivariate identity - paper and theorem? The Stein 1981 paper only seems to discuss the univariate case.\n2. What are the following quantities on page 2: phi, q, q_phi, epsilon, pi\n3. It seems that you want the gradients w.r.t the parameters phi in (3). But the line immediately claims that you need the gradients w.r.t the domain of a random variable z and the subsequent sections focus on the gradients of the log density with respect to the domain. I am not quite following the connection here."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739375852,"tcdate":1509093778932,"number":303,"cdate":1509739373191,"id":"SJi9WOeRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/713e2678123e4ae44a10f98743d33ab86cf0c081.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}