{"notes":[{"tddate":null,"ddate":null,"tmdate":1513748472684,"tcdate":1513748472684,"number":9,"cdate":1513748472684,"id":"SJlbOODMG","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"ryjc6__ez","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"thank you for your updated review, and answer your further comments","comment":"Thank you for the positive review. We will think about how to revise the paper. Now on your further comments:\n\n1. consistency\nWe did not claim the proposed Stein gradient estimator is unbiased. This is because: 1) we used the V-statistics of KSD, 2) the fixed point of the MC approximated objective is not necessary the fixed point of the KSD. Similar things apply to the KDE and Score matching estimators. However, asymtotic consistency results have been proved for KDE, and Score matching the proof requires the kernel machine hypothesis set to contain the ground truth. This is not always the case, and our proposal might be prefered here because it is non-parametric. \n\nWe are currently working on establishing similar asymtotic consistency results for the Stein gradient estimator.\n\n2. preference of the RKHS story\nIndeed if we directly start to talk about kernels then I would rather prefer the derivation of section 3.2 (in the current version). However for people (like engineers) who are not familiar with the RKHS theory, the explanation of section 3.1 might be more intuitive, and that's why I decided to include both of them. This is in similar spirit as to derive linear regression equations in many statistics textbooks: we first write down the solutions, and then notice that we can use the kernel trick to address the d' >> K problem.\n\nThank you for your feedback again and do let us know what we can do to improve the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513326397921,"tcdate":1513326330875,"number":8,"cdate":1513326330875,"id":"r1XbwWZMG","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"ryjc6__ez","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"thank you for your review and answering your comments","comment":"(We have revised the paper to make the presentation clearer. Please consider it and we would welcome your feedback.)\n\nThank you for your time for reviewing the paper. Again we are sorry that the presentation is not very clear in the first version of the manuscript. We have revised the paper according to your comments and added a brief introduction to Bayesian neural networks in the appendix.\n\nWe believe that our paper is highly novel and contains significant contributions (as reviewer 3 commented). The paper is based on an important observation that an accurate gradient approximation method would be very helpful in many learning tasks that involve fitting an implicit distribution. As the other two reviewers pointed out, the proposed Stein gradient estimator is highly novel, and the experiments consider novel tasks that have not been considered in the literature, e.g. meta-learning for approximate inference, and entropy regularisation methods for GANs. \n\nNow for your detailed comments:\n\n1. notations of phi, pi, etc.\nWe are sorry again for unclear presentation in the first version. In the latest version of the manuscript, we have explicitly defined them and provided a detailed derivation of the entropy gradient in eq (3). Please let us know if it is still unclear.\n\n2. computing kernel matrix.\nIn section 4.3 we performed mini-batch training, and this means we only need to compute the gradient of log q on the mini-batch data. We found that with mini-batch size K=100 (which is typical for deep learning tasks) the computational cost is quite cheap, see the revised paper for a report of running time.\n\n3. choice of \\eta.\nIndeed for kernel methods, \\eta needs to be tuned. However, our empirical observation indicates that for better performance of the Stein approach, small \\eta is often preferred than large ones. Apparently, matrix inversion has numerical issues, so in our tests, we set \\eta to be some small value but large enough to ensure numerical stability.\n\n4. purpose of 3.1 and 3.2 (in the first version).\nSince the Stein gradient estimator is kernel-based, we need to compare to existing kernel-based gradient estimator. Therefore we introduce them in 3.1 and 3.2 (in the first version of the paper).\n\n5. purpose of 3.5 (in the first version).\nOur experiment 4.1 actually needs predictive estimators, since we want the particles of parallel chains to be independent of each other. The estimator derived in section 3.3 (of the first version) introduces correlations between the estimates of the score function at different locations.\n\nAlso in an on-going work, we apply the proposed Stein gradient estimator to training implicit generative models, which also requires predicting the gradient values. We already have some success on MNIST data, and now we are incorporating kernel learning techniques to scale it to massive data.\n\nThank you again for reading the feedback, and we look forward to hearing from you again."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513324420297,"tcdate":1513324420297,"number":7,"cdate":1513324420297,"id":"Sy3KkZWfz","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"H1xEAg3lM","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"thank you for the positive review, and answering your comments","comment":"(We have revised the paper to make the presentation clearer. Please consider it and we would welcome your feedback.)\n\nThank you for your time for reviewing the paper. We appreciate your positive comment that the paper contains significant contributions to the community. The diversity of the experimental tasks show that gradient estimation is fundamental to many machine learning tasks, so we believe the proposed estimator is widely applicable as you pointed out.\n\nAlso, we would like to thank you for the suggestions on making the paper clearer. We have re-organised the presentation to emphasise the contribution of the Stein gradient estimator. \n\nNow on your comments:\n\n1. Computation cost\nWe added two paragraphs in the manuscript for further discussions on this. In short, we discussed:\n\nComparisons between kernel methods and other ideas. It is also known that the denoising auto-encoder (DAE), when trained with infinitesimal noise, also provides a score function estimator. However, this requires training the DAE, and depending on the neural network architecture, it can take significantly much more time compared to the kernel-based estimators which often have analytical solutions.\n\nFor the three kernel-based methods mentioned in the paper, both Score and Stein method require inverting a K*K matrix (O(K^3) time). All three methods require computing the kernel matrix (O(K^2 * d) time). However in the BNN and GAN experiments, since d >> K, the cost is dominated by the kernel matrix computation, meaning that all three methods have similar computational costs. Indeed we reported the running times for the GAN experiments which are almost identical. Also adding the entropy regularisation only resulted in 1s/epoch more time compared to vanilla BEGAN, which is actually quite cheap.\n\n2. BNN experiment.\nWe have clearly shown that the Stein approach is significantly better than the other two gradient estimators. SGLD with small step-size is known to work well, and the Stein method works equally well in this case. To our knowledge, this is the first attempt of meta-learning for approximate samplers, and our results demonstrate that this direction is worth investigation. We strongly believe that with a better neural network structure our method can be improved.\n\nRegarding the scale of the experiment: UCI datasets are standard benchmarks for Bayesian neural networks (e.g. see the PBP paper, Hernandez-Lobato and Adams 2015), and for datasets of this scale, we know that point estimates work worse. The size of the network is of the same scale as reported in Fig 5 (left) of (Andrychowicz et al. 2016).\n\n3. the GAN experiment in 4.3\nThe purpose of section 4.3 is to show the application of gradient estimation methods to tasks other than approximate inference (4.1 and 4.2). Our goal here is to show: (i) by adding entropy regulariser it can help address the mode collapse problem, and (ii) the resulting diversity measure also reflects the approximation accuracy of the entropy gradient. In this experiment, we showed that the Stein approach works considerably better.\n\nIndeed our ultimate goal of developing gradient estimation methods is to use them for training implicit generative models, and if successful, it can serve as an alternative to GAN-like approaches. In an on-going work, we already have some success on MNIST data. We are now working on incorporating kernel learning techniques to scale it to massive data.\n\n4. non translation invariant kernel case.\nTo our knowledge, it is rare for KDE methods to use non translation invariant kernels. And we have never seen consistency results proved for KDE gradient estimator in this case. But indeed connections between Stein and KDE methods is still a research question when using non translation invariant kernels.\n\nThank you again for reading the feedback and we look forward to hearing from you again."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1514426897779,"tcdate":1513321360460,"number":6,"cdate":1513321360460,"id":"Sydc7eZMz","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"rJ8QfICez","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Thank you for your review, and answering your comments","comment":"(We have revised the paper to make the presentation clearer. Please consider it and we would welcome your feedback.)\n\nThank you for your time for reviewing the paper. We appreciate your comment that the proposed approach is interesting.\n\nWe would like to emphasise that our work is highly novel (as both reviewers 2 and 3 pointed out). \n\n1. The Stein gradient estimator is a novel score function estimator, which, as you mentioned, generalises the score matching estimator. To our knowledge, this is the first **non-parametric** direct estimator: the KDE method, although also non-parametric, is an **indirect** method as it first estimates the density then takes the gradient.\n\n2. We applied the gradient estimation methods to a wide range of novel applications. To our knowledge, before our development, no paper has considered meta-learning tasks for approximate inference. Also, the entropy regularisation idea for GANs is novel, which cannot be done without an efficient gradient estimation method. \n\nIn an on-going work, we have applied the Stein gradient estimator to training implicit generative models, and small-scale experiments have shown promising results.\n\nNow on your comments:\n\n1. Yes as you pointed out, the original Stein's identity (Stein 1972, 1981) are for Gaussian distributions. However, the identity has been generalised to more general case. In equation (6) of the revised manuscript, we explicitly write down the integration by part derivations with the boundary condition assumed. Indeed for distributions with Gaussian-like tails almost any test function will satisfy the boundary condition. \n\n2. If you would like to see an counterexample: if q(x) is Cauchy, then h(x) should be less or equal than order of x^2. But in practice, since the kernel in use often has decaying tails, it is generally the case that the boundary condition is satisfied.\n\nThank you again for reading the feedback and we look forward to hearing from you again.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512419978145,"tcdate":1512419978145,"number":5,"cdate":1512419978145,"id":"r1zqfN7Zf","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"rJ5a7tg-G","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Quick answer for your questions","comment":"I am sorry again for rush derivations, will revise the derivations and provide an official response to your review. But just to quickly explain Q1 and Q3 here:\n\nQ1:\nLet's assume h(x) output a scalar for a moment. Then, stein's identity can be proved using integration by parts:\n\n\\int q(x) [ h(x) * dlogq(x)/dx + dh(x)/dx] dx\n= \\int [h(x) * dq(x)/dx + q(x) * dh(x)/dx ] dx    // dlogx/dx = x^{-1}\n= \\int d[h(x)q(x)]/dx dx\n= h(x)q(x)|_{\\partial X} \n= 0    // assumed by the boundary condition\n\nNow write multi-dimension version h(x) = (h_1(x), ..., h_{d'}(x)). Then looking at eq (8), we notice that the ith row of the LHS matrix is actually\n\\int q(x) [ h_i(x) * dlogq(x)/dx + dh_i(x)/dx] dx,\nmeaning that if we assume boundary condition for h(x) (which also implies boundary condition for h_i(x)), then we can prove again that the LHS matrix is actually zero.\n\nQ3:\nHere I assume we can apply the reparameterisation trick for q (see the VAE papers). This says, \nsampling z ~ q_{\\phi}(z | x) is equivalent to 1) sample \\epsilon ~ \\pi(\\epsilon), then 2) compute z = f_{\\phi}(\\epsilon, x). \\pi(\\epsilon) is the distribution of the noise which is usually Gaussian. This means, we can rewrite the expectation in q to expectation in \\pi. Please see section 2.4 in the original VAE paper (Kingma and Welling 2013) for an example math derivation. \n\nThen I wanted to differentiate the variational lower-bound wrt \\phi. Especially, eq. (3) derived the gradient of the entropy term wrt \\phi. I will add in detailed derivations in revision, but for your quick reference please see eq (5-7) in (Roeder et al. 2017) for an example derivation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512244162294,"tcdate":1512244162294,"number":4,"cdate":1512244162294,"id":"rJ5a7tg-G","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"r19JQOTgG","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"content":{"title":"Not clear yet","comment":"I am still not following the details for 1 and 3 yet. Feel free to elaborate below, but ideally these should have appeared in the paper.\nFor 2, what is pi?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642428364,"tcdate":1512100382141,"number":3,"cdate":1512100382141,"id":"rJ8QfICez","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Review","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer1"],"readers":["everyone"],"content":{"title":"an interesting paper","rating":"6: Marginally above acceptance threshold","review":"This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.\n\nThis is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. \n\n- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term \"Stein identity\" is appropriate to express the equation (8). \n- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512043741483,"tcdate":1512043741483,"number":3,"cdate":1512043741483,"id":"HJS1ru6gG","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"SJCCQvTgz","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Thanks for your comments!","comment":"Thank you for your interest in this paper!\n\nFor your questions:\n1. Yes the original Stein (1981) paper only described the identity for a multivariate Gaussian distribution, and it assumed the test function to output scalars. However, the proof technique only used integration by parts, which, if the boundary condition is assumed, should be able to generalise to general distribution case.\n\nOur twist of the formula comes from the observation that we can pack multiple (scalar output) test functions into a vector. This has also been considered in e.g. Liu et al. (2016). \n\nWhat would you suggest to call (8) other than \"Stein's identity\"?\n\n2. Yes it would be nice to have an example, however in the Gaussian case since the tails decay exponentially, almost all functions satisfy the boundary condition. Maybe it would be helpful to do an example with long-tail distributions.\n\nHope this helps and thanks again!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512043234311,"tcdate":1512043234311,"number":2,"cdate":1512043234311,"id":"r19JQOTgG","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"SJxdjXBgM","signatures":["ICLR.cc/2018/Conference/Paper303/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/Authors"],"content":{"title":"Thanks for your questions!","comment":"Thank you for your time on reviewing this paper!\n\nOn your questions:\n1. Yes the original Stein (1981) paper only described the identity for a multivariate Gaussian distribution, and it assumed the test function to output scalars. However, the proof technique only used integration by parts, which, if the boundary condition is assumed, should be able to generalise to general distribution case.\n\nOur twist of the formula comes from the observation that we can pack multiple (scalar output) test functions into a vector. This has also been considered in e.g. Liu et al. (2016). Will fix the descriptions -- thank you!\n\n2. Sorry for the rush of background introduction. Here q denotes the approximate posterior, and q_{\\phi} just explicitly writes the dependency of q to its parameter \\phi. \\epsilon is the noise variable used in the reparameterisation trick.\n\n3. Again sorry for the rush of the derivation -- will add a full equation in appendix. \nIn short the idea is to apply the reparameterisation trick, and notice that the gradient of \\phi contains the path gradient (the first term in (3)) and an expectation of the REINFORCE gradient (the second term). Using the log-derivative trick we can show that the second term is zero.\n\nHope this helps!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512039381978,"tcdate":1512039381978,"number":1,"cdate":1512039381978,"id":"SJCCQvTgz","invitation":"ICLR.cc/2018/Conference/-/Paper303/Public_Comment","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"an interesting paper","comment":"This paper deals with the estimation of the score function, i.e., the derivative of the log likelihood. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. The method was used to several applications.\n\nThis is an interesting approach to estimate the score function for location models in a non-parametric way. I have a couple of minor comments below. \n\n- Stein identity is the formula that holds for the class of ellipsoidal distribution including Gaussian distribution. I'm not sure the term \"Stein identity\" is appropriate to express the equation (8). \n- Some boundary condition should be assumed to assure that integration by parts works properly. Describing an explicit boundary condition to guarantee the proper estimation would be nice. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642428410,"tcdate":1511947816296,"number":2,"cdate":1511947816296,"id":"H1xEAg3lM","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Review","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper with novel and significant contribution!","rating":"7: Good paper, accept","review":"In this paper, the authors proposed the Stein gradient estimator, which directly estimates the score function of the implicit distribution. Direct estimation of gradient is crucial in the context of GAN because it could potentially lead to more accurate updates. Motivated by the Stein’s identity, the authors proposed to estimate the gradient term by replacing expectation with the empirical counterpart and then turn the resulting formulation into a regularized regression problem. They also showed that the traditional score matching estimator (Hyvarinen 2005) can be obtained as a special case of their estimator. Moreover, they also showed that their estimator can be obtained by minimizing the kernelized Stein discrepancy (KSD) which has been used in goodness-of-fit test. In the experiments, the proposed method is evaluated on few tasks including Hamiltonian flow with approximate gradients, meta-learning of approximate posterior samplers, and GANs using entropy regularization. \n\nThe novelty of this work consists of an approach based on score matching and Stein’s identity to estimate the gradient directly and the empirical results of the proposed method on meta-learning for approximate inference and entropy regularized GANs. The proposed method is new and technically sound. The authors also demonstrated through several experiments that the proposed technique can be applied in a wide range of applications.\n\nNevertheless, I suspect that the drawback of this method compared to existing ones is computational cost. If it takes significantly longer to compute the gradient using proposed estimator compared to existing methods, the gain in terms of accuracy is questionable. By spending the same amount of time, we may obtain an equally accurate estimate using other methods. For example, the authors claimed in Section 4.3 that the Stein gradient estimator is faster than other methods, but it is not clear as to why this is the case. Hence, the comparison in terms of computational cost should also be included either in the text or in the experiment section.\n\nWhile the proposed Stein gradient estimator is technically interesting, the experimental results do not seem to evident that it significantly outperforms existing techniques. In Section 4.2, the authors only consider four datasets (out of six UCI datasets). Also, in Section 4.3, it is not clear what the point of this experiment is: whether to show that entropy regularization helps or the Stein gradient estimator outperforms other estimators.\n\nSome comments:\n\n- Perhaps, it is better to move Section 3.3 before Section 3.2 to emphasize the main contribution of this work, i.e., using Stein’s identity to derive an estimate of the gradient of the score function.\n- Stein gradient estimator vs KDE: What if the kernel is not translation invariant? \n- In Section 4.3, why did you consider the entropy regularizer? How does it help answer the main hypothesis of this paper?\n- The experiments in Section 4.3 seems to be a bit out of context.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642428454,"tcdate":1511718291032,"number":1,"cdate":1511718291032,"id":"ryjc6__ez","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Review","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea","rating":"7: Good paper, accept","review":"Post rebuttal phase (see below for original comments)\n================================================================================\nI thank the authors for revising the manuscript. The methods makes sense now, and I think its quite interesting. While I do have some concerns (e.g. choice of eta, batching may not produce a consistent gradient estimator etc.), I  think the paper should be accepted. I have revised my score accordingly.\n\nThat said, the presentation (esp in Section 2) needs to be improved. The main problem is that many symbols have been used without being defined. e.g. phi, q_phi, \\pi,  and a few more. While the authors might assume that this is obvious, it can be tricky to a reader - esp. someone like me who is not familiar with GANs. In addition, the derivation of the estimator in Section 3 was also sloppy. There are neater ways to derive this using RKHS theory without doing this on a d' dimensional space.\n\nRevised summary: The authors present a method for estimating the gradient of some training objective for generative models used to sample data, such as GANs. The idea is that this can be used in a training procedure. The idea is based off the Stein's identity, for which the authors propose a kernelized solution. The key insight comes from rewriting the variational lower bound so that we are left with having to compute the gradients w.r.t a random variable and then applying Stein's identity. The authors present applications in Bayesian NNs and GANs.\n\n\nSummary\n================================================================\nThe authors present a method for estimating the gradient of some training objective\nfor generative models used to sample data, such as GANs. The idea is that this can be\nused in a training procedure. The idea is based off the Stein's identity, for which the\nauthors propose a kernelized solution. The authors present applications in Bayesian NNs\nand GANs.\n\n\n\n\nDetailed Reviews\n================================================================\n\nMy main concern is what I raised via a comment, for which I have not received a response\nas yet. It seems that you want the gradients w.r.t the parameters phi in (3). But the\nline immediately after claims that you need the gradients w.r.t the domain of a random\nvariable z and the subsequent sections focus on the gradients of the log density with\nrespect to the domain. I am not quite following the connection here.\n\nAlso, it doesn't help that many of the symbols on page 2 which elucidates the set up\nhave not been defined. What are the quantities phi, q, q_phi, epsilon, and pi?\n\nPresentation\n- Bottom row in Figure 1 needs to be labeled. I eventually figured that the colors\n  correspond to the figures above, but a reader is easily confused.\n- As someone who is not familiar with BNNs, I found the description in Section 4.2\n  inadequate.\n \nSome practical concerns:\n- The fact that we need to construct a kernel matrix is concerning. Have you tried\n  batch verstions of these estimator which update the gradients with a few data points?\n- How is the parameter \\eta chosen in practice? Can you comment on the values that you\n  used and how it compared to the eigenvalues of the kernel matrix?\n\nMinor\n- What is the purpose behind sections 3.1 and 3.2? They don't seem pertinent to the rest\n  of the exposition. Same goes for section 3.5? I don't see the authors using the\n  gradient estimators for out-of-sample points?\n\nI am giving an indifferent score mostly because I did not follow most of the details.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511656511730,"tcdate":1511500648131,"number":1,"cdate":1511500648131,"id":"SJxdjXBgM","invitation":"ICLR.cc/2018/Conference/-/Paper303/Official_Comment","forum":"SJi9WOeRb","replyto":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper303/AnonReviewer2"],"content":{"title":"Some Questions","comment":"1. Can you give a reference for Stein's multivariate identity - paper and theorem? The Stein 1981 paper only seems to discuss the univariate case.\n2. What are the following quantities on page 2: phi, q, q_phi, epsilon, pi\n3. It seems that you want the gradients w.r.t the parameters phi in (3). But the line immediately claims that you need the gradients w.r.t the domain of a random variable z and the subsequent sections focus on the gradients of the log density with respect to the domain. I am not quite following the connection here."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513319961672,"tcdate":1509093778932,"number":303,"cdate":1509739373191,"id":"SJi9WOeRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJi9WOeRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gradient Estimators for Implicit Models","abstract":"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the \\emph{Stein gradient estimator}, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversities.","pdf":"/pdf/e2356a5376ebec03257ab86e28e3ffb8b9fd4224.pdf","TL;DR":"We introduced a novel gradient estimator using Stein's method, and compared with other methods on learning implicit models for approximate inference and image generation.","paperhash":"anonymous|gradient_estimators_for_implicit_models","_bibtex":"@article{\n  anonymous2018gradient,\n  title={Gradient Estimators for Implicit Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJi9WOeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper303/Authors"],"keywords":["Implicit Models","Approximate Inference","Deep Learning"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}