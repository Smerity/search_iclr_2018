{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222783760,"tcdate":1512066233228,"number":2,"cdate":1512066233228,"id":"Bk-6h6Txz","invitation":"ICLR.cc/2018/Conference/-/Paper836/Official_Review","forum":"HJUOHGWRb","replyto":"HJUOHGWRb","signatures":["ICLR.cc/2018/Conference/Paper836/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An approach for end-to-end learning of interpretable models with experimental support","rating":"6: Marginally above acceptance threshold","review":"The article \"Contextual Explanation Networks\" introduces the class of models which learn the intermediate explanations in order to make final predictions. The contexts can be learned by, in principle, any model including neural networks, while the final predictions are supposed to be made by some simple models like linear ones. The probabilistic model allows for the simultaneous training of explanation and prediction parts as opposed to some recent post-hoc methods.\n\nThe experimental part of the paper considers variety of experiments, including classification on MNIST, CIFAR-10, IMDB and also some experiments on survival analysis. I should note, that the quality of the algorithm is in general similar to other methods considered (as expected). However, while in some cases the CEN algorithm is slightly better, in other cases it appears to sufficiently loose, see for example left part of Figure 3(b) for MNIST data set. It would be interesting to know the explanation. Also, it would be interesting to have more examples of qualitative analysis to see, that the learned explanations are really useful. I am a bit worried, that while we have interpretability with respect to intermediate features, these features theirselves might be very hard to interpret.\n\nTo sum up, I think that the general idea looks very natural and the results are quite supportive. However, I don't feel myself confident enough in this area of research to make strong conclusion on the quality of the paper.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Contextual Explanation Networks","abstract":"We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations. CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly. Our approach offers two major advantages: (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings. We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations. Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.","pdf":"/pdf/8b2946ef6926e7ab8d48a58cabdf76004d1d66dd.pdf","TL;DR":"A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability.","paperhash":"anonymous|contextual_explanation_networks","_bibtex":"@article{\n  anonymous2018contextual,\n  title={Contextual Explanation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJUOHGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper836/Authors"],"keywords":["interpretability","regularization","deep learning","graphical models","model diagnostics","survival analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222783802,"tcdate":1511878303454,"number":1,"cdate":1511878303454,"id":"H1wsCJjez","invitation":"ICLR.cc/2018/Conference/-/Paper836/Official_Review","forum":"HJUOHGWRb","replyto":"HJUOHGWRb","signatures":["ICLR.cc/2018/Conference/Paper836/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting idea, maybe helpful to differentiate more from previous neural net+graphical models idea","rating":"6: Marginally above acceptance threshold","review":"the paper is clearly written; it works on a popular idea of combining graphical models and neural nets.\n\nthis work could benefit from differentiating more from previous literature.\n\none key component is interpretability, which comes from the use of graphical models.  the authors claim that the previous art directly integrate neural networks into the graphical models as components, which renders the models uninterpretable. however, it is unclear, following the same logic, why the proposed method has interpretability. after all, how to go from the context to the parameters of the graphical models is still uninterpretable. specifically, it is helpful to pinpoint what is special in this model that makes it interpretable, compared to works like Gao, Y., Archer, E. W., Paninski, L., & Cunningham, J. P. (2016). NIPS or Johnson, M., Duvenaud, D. K., Wiltschko, A., Adams, R. P., & Datta, S. R. (2016). NIPS. also, is there any methodological advancement essential to CENs? \n\nthe other idea is to go context specific. this idea has been present in language modeling, for example, amortized embedding models like M. Rudolph, F. Ruiz, S. Athey, and D. Blei (2017). NIPS and L. Liu, F. Ruiz, S. Athey, and D. Blei.  (2017). NIPS. application to medical data is interesting. but it could be helpful for the readers to understand if the idea in this work is fundamentally different from these previous ideas from amortized inference.\n\na final thing. a common challenge with composing graphical models and neural networks (in interpretable or uninterpretable ways) is that the neural networks will usually eat up all the representational power. the variance captured by graphical models becomes negligible. to this end, the power of graphical models for interpretability is limited. interpretability in this case is not much different from fitting only a neural network, taking the penultimate layer to the output as \"context specific features\" can claim that we are composing a linear model with a neural network, and the linear model is interpretable. it would be interesting to be clear about how the authors get around this issue.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Contextual Explanation Networks","abstract":"We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations. CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly. Our approach offers two major advantages: (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings. We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations. Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.","pdf":"/pdf/8b2946ef6926e7ab8d48a58cabdf76004d1d66dd.pdf","TL;DR":"A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability.","paperhash":"anonymous|contextual_explanation_networks","_bibtex":"@article{\n  anonymous2018contextual,\n  title={Contextual Explanation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJUOHGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper836/Authors"],"keywords":["interpretability","regularization","deep learning","graphical models","model diagnostics","survival analysis"]}},{"tddate":null,"ddate":null,"tmdate":1509739075123,"tcdate":1509135725906,"number":836,"cdate":1509739072466,"id":"HJUOHGWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJUOHGWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Contextual Explanation Networks","abstract":"We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations. CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly. Our approach offers two major advantages: (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings. We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations. Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.","pdf":"/pdf/8b2946ef6926e7ab8d48a58cabdf76004d1d66dd.pdf","TL;DR":"A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability.","paperhash":"anonymous|contextual_explanation_networks","_bibtex":"@article{\n  anonymous2018contextual,\n  title={Contextual Explanation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJUOHGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper836/Authors"],"keywords":["interpretability","regularization","deep learning","graphical models","model diagnostics","survival analysis"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}