{"notes":[{"tddate":null,"ddate":null,"tmdate":1512273366224,"tcdate":1512273366224,"number":3,"cdate":1512273366224,"id":"SJ0CHgbbM","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Review","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Worthy contribution","rating":"6: Marginally above acceptance threshold","review":"[Apologies for short review, I got called in late. Marking my review as \"educated guess\" since i didn't have time for a detailed review]\n\nThe paper proposes an algorithm to tune the momentum and learning rate for SGD. While the algorithm does not have a theory for general non-quadratic functions, experimental validation is extensive, making it a worthy contribution in my opinion. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/9b95efe4ab1d93abdf3bc631c067fe1753b42690.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1512222620641,"tcdate":1511808774284,"number":2,"cdate":1511808774284,"id":"B1RZJ1cxG","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Review","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Misleading and shaky theoretical motivation/approach","rating":"4: Ok but not good enough - rejection","review":"The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin). They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.\n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence. There are two major problems with this approach:\n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition. Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions.\n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading. This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case.\nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case.\n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).\nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions.\nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed.\nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me.\n\nIn contrast to the theoretical part, the experiments seems very encouraging. Showing YF to perform very well on several deep learning tasks without (or with very little) tuning. Again, this seems a bit magical or even too good to be truth. I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/9b95efe4ab1d93abdf3bc631c067fe1753b42690.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1512222620676,"tcdate":1511753391784,"number":1,"cdate":1511753391784,"id":"HyuhIWYez","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Review","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/AnonReviewer1"],"readers":["everyone"],"content":{"title":"YellowFin and the Art of Momentum Tuning","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm.\n\nAlthough the results are promising, I found the presentation of this paper almost inaccessible to me.\n\nFirst, though a minor point, but where does the name *YellowFin* come from?\n\nFor the presentation, the motivation in introduction is fine, but the following section about momentum operator is hard to follow. There are a lot of undefined notation. For example, what does the *convergence rate* mean (what is the measurement for convergence)? And is the *optimal accelerated rate* the same as *convergence rate* mentioned above? Also, what do you mean by *all directions* in the sentence below eq.2?\n\nThen the paper talks about robustness properties of the momentum operator. But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative?\n\nIn the following paragraph, what is *contraction*? Does it have anything to do with the paper as I didn't see it in the remaining text?\n\nLemma 2 seems to use the spectral radius of the momentum operator as the *robustness*. But how can it describe the robustness? More details are needed to understand this.\n\nWhat it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter. I got confused in this section because eq.9 defines f(x) as a quadratic function. Is this f(x) the original function (non quadratic) or just the local quadratic approximation? If it is the local quadratic approximation, how is it correlated to the original function? It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation? If what I think is correct, I think it would be important to show this.\n\nAlso, the objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning.\n\nOverall, I think this is an interesting paper, but the presentation is too fuzzy to get it evaluated.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/9b95efe4ab1d93abdf3bc631c067fe1753b42690.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1509739363117,"tcdate":1509097229019,"number":325,"cdate":1509739360458,"id":"SyrGJYlRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/9b95efe4ab1d93abdf3bc631c067fe1753b42690.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}