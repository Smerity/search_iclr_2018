{"notes":[{"tddate":null,"ddate":null,"tmdate":1515783698029,"tcdate":1515743869854,"number":3,"cdate":1515743869854,"id":"ryUK51INM","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Comment","forum":"SyrGJYlRZ","replyto":"BJsMnO6mG","signatures":["ICLR.cc/2018/Conference/Paper325/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper325/Authors"],"content":{"title":"YellowFin with non-zero momentum can demonstrate better and more stable performance","comment":"Dear Carlos,\n\nThanks for the clarification. Rescaling the learning rate as per your suggestion, on multiple experiments, has the following consequences:\n\n-- As expected, training is sped-up compared to using YF’s LR un-adjusted and just setting momentum to 0. In some cases, like the one you conduct experiment on, YF is only marginally better (or the same) than YF-minus-momentum-plus-rescaling. \n\n-- Signs of instability start showing. Essentially, trading momentum for higher learning rate when we do the suggested rescaling, makes the performance curves behave more unstable (figures below).\n\n\nOverall, in a number of examples, the original, momentum based YF tuner demonstrates better validation metrics and better stability than the suggested rescaling rule with zero momentum. \n\nE.g. YellowFin rule with non-zero momentum can demonstrate better validation perplexity in the constituency parsing model. (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/parsing_test_perp.pdf).\n\nIn the following example of ResNext on CIFAR10, YellowFin rule with non-zero momentum can also demonstrate more stable validation accuracy (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/cifar_smooth_test_acc.pdf). \n\nAs the third example, using the momentum-based tuner, we are able to boost the learning rate (as described in Appendix J4) to get even better performance. In this example, we use learning rate factor 3.0 to increase the learning rate on ResNext for CIFAR10 (as what we did in the experiments in appendix J4), YellowFin rule with non-zero momentum gives *both* observably higher and more stable validation accuracy than the suggested rescaling rule (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/cifar_smooth_test_acc_lr_fac_3.pdf ).\n\nWe appreciate the feedback on this important point! We will be adding this discussion to our manuscript and, would also be happy to add an acknowledgement for your suggestion.\n\nBest regards,\nThe authors\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515532629977,"tcdate":1515532351617,"number":2,"cdate":1515532351617,"id":"SkdSe3zNz","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Comment","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper325/Authors"],"content":{"title":"Updated manuscript","comment":"Dear readers and reviewers,\n\nwe have updated our manuscript during the rebuttal period in addition to our response to the official reviews. Specifically, we:\n\n1. performed a significant rewrite of Sections 2 and 3 to make exposition of our ideas much more clear.\n\n2. we added discussion in section 3.1 on how our tuning rule in equ(8) intuitively generalizes to multiple dimensions.\n\n3. we addressed a number of reviewers comments and suggestions on clarification. \n\nBest regards,\nThe authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515191315279,"tcdate":1515191315279,"number":11,"cdate":1515191315279,"id":"BJsMnO6mG","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"SJ6oqITQG","signatures":["~Carlos_Stein_Brito1"],"readers":["everyone"],"writers":["~Carlos_Stein_Brito1"],"content":{"title":"Needs to rescale learning rate","comment":"Notice that I turn momentum off, but also rescale the learning rate in order to have the same effective learning rate. In this case the learning rate is not constant anymore, but there is no momentum. In my code, the important change is\n\nself._optimizer = tf.train.GradientDescentOptimizer(1.0 * self._lr_var * self.lr_factor / (1.0 - self._mu_var))\n\nin https://github.com/cstein06/YellowFin/blob/no_momentum/tuner_utils/yellowfin.py\n\nIn this case there is no momentum, but the results should be very similar in general. The figure I posted is for the CIFAR + Resnet model in your paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515182756968,"tcdate":1515182756968,"number":10,"cdate":1515182756968,"id":"SJ6oqITQG","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"B1jF7rpQz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Adaptive momentum tuning is important in the YellowFin framework","comment":"Dear Carlos,\n\nthank you for the interest in our method! You pose a salient question and we have the experimental results to answer it in the appendix of our manuscript.\n\nIt is absolutely true that positive momentum is *not always* necessary to achieve the best performance possible in the SGD+Momentum framework. Your plot is a good example of that (we’d like to point out that momentum tuning does not hurt performance in this case, if anything it marginally improves things).\n\nHowever, most importantly, *in many cases, adaptively tuning momentum strictly improves performance over the prescribed constant momentum values (such as 0.0 or 0.9).*\n\nOur manuscript includes experiments in Figure 10 (Appendix J.2) in support of this point. Specifically, we performed the following experiment: fix the momentum value to either 0.0 or 0.9 and just use the tuned learning rate from YellowFin. The results show an example where momentum tuning makes a big difference throughout training (the CharRNN on the left) and another example (the CIFAR100 Resnet on the right) where fixed 0.0 momentum seems optimal in the beginning, but eventually loses out to the adaptive momentum curve (the red YF curve).\n\nIn summary, adaptively tuning momentum:\ndoes not seem to be hurting performance in your example (or in the problems we considered)\nstrictly improves performance in many cases we’ve seen (and included in our manuscript)\n\nBest regards,\nThe authors\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515189318135,"tcdate":1515176835025,"number":9,"cdate":1515176835025,"id":"B1jF7rpQz","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["~Carlos_Stein_Brito1"],"readers":["everyone"],"writers":["~Carlos_Stein_Brito1"],"content":{"title":"This method is a learning rate scheduler and it's unrelated with momentum","comment":"I read the paper carefully and I don't think the results are related with momentum at all. In Figure 5 one sees that the momentum value does not go above 0.8, which shouldn't make much difference (common values are 0.99 for example).\nI confirmed this by turning off momentum and rescaling the learning rate appropriately, getting the same results. Plot:\nhttps://github.com/cstein06/YellowFin/blob/no_momentum/cifar/results/compare_losses.pdf\n\nIt means that it is a learning rate scheduler in disguise."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1514489781927,"tcdate":1513812249554,"number":8,"cdate":1513812249554,"id":"ryz7Z_OzM","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"HyuhIWYez","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Part I: On convergence rate, contraction and robustness of momentum operator","comment":"We appreciate AnonReviewer1's helpful comments about improving clarity. *We will upload a new manuscripts, with the suggestions and comments on clarification incorporated, in the next couple of days.* We answer the questions below inline. If there are any further ones, we would be happy to discuss and use them to keep improving the manuscript's clarity. \n\nQ: Where does the name *YellowFin* come from?\n\nWe wanted a mnemonic for our ‘tuner’. Yellowfin happens to be one of the fastest species of tuna. \n\n\nQ: What does the *convergence rate* mean (what is the measurement for convergence)? And is the *optimal accelerated rate* the same as *convergence rate* mentioned above? Also, what do you mean by *all directions* in the sentence below eq.2?\n\n\nThe convergence rate is with respect to the distance to the optimum. Specifically, the rate is \\beta if and only if \\|x_t - x* \\| <= \\beta^t \\| x_0 - x*  \\|, where x* is the optimum and x_t is the point after t steps in an iterative optimization process. The *optimal accelerated rate* is also with respect to distance towards the optimum, i.e. the optimal \\beta in the above definition of convergence rate.\n\nOur statement on “all directions” is in the context of multidimensional quadratics with different curvature on different axes (i.e. \\kappa > 1). Specifically, with large enough momentum \\mu and proper learning rate, momentum gradient descent has the same convergence rate \\sqrt{\\mu} along all the axes, i.e. | x_{i, t} - x* | <= \\sqrt{\\mu}^t | x_{i, 0} - x*| where x_{i, t} is the coordinate on axis i after t steps. This holds even if the eigendirection of the quadratics are not axes-aligned. We will rephrase in the updated version to clarify.\n\n\nQ: Then the paper talks about robustness properties of the momentum operator. But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative?\n\nDefinition 1 is not to re-define derivative. Instead, we define generalized curvature h  for 1d functions and rewrite the derivative in terms of h. We will rephrase to clarify in the manuscript.\n\n\nQ: In the following paragraph, what is *contraction*? Does it have anything to do with the paper as I didn't see it in the remaining text?\n\nThe contraction actually refers to the multiplicative factor that describes how fast the distance to optimum decays. E.g. for 1d quadratics, gradient descent gives x_{t + 1} - x* = (1 - \\alpha h(x_t) ) (x_{t} - x*) with |1 - \\alpha h(x_t) | as the contraction. In the appendix of our upcoming new manuscript, this concept will be helpful in demonstrating examples on the motivation of generalized curvature.\n\n\nQ: Lemma 2 seems to use the spectral radius of the momentum operator as the *robustness*. But how can it describe the robustness? More details are needed to understand this.\n\n*Robustness* of momentum operator means that momentum GD can achieve asymptotic linear convergence rate, which is *robust* to 1) the variation of generalized curvature of the landscape. 2) a range of different learning rate. Specifically, from Equ (4), we see constant spectral radius \\sqrt{\\mu} of operator A_t can imply asymptotic linear convergence rate \\sqrt{\\mu}. Lemma 2 gives the *condition* to achieve this rate. As discussed in the two paragraphs following Lemma 2, given momentum \\mu is properly set based on the dynamic range of generalized curvature, the *condition* can be robustly satisfied 1) regardless of the variation of generalized curvature; 2) with a range of different value for learning rate.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1514489793978,"tcdate":1513812153855,"number":7,"cdate":1513812153855,"id":"Bkf6euOGM","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"HyuhIWYez","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Part II: On the noisy quadratic model and the objective function in SingleStep algorithm","comment":"Q: What it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter. I got confused in this section because eq.9 defines f(x) as a quadratic function. Is this f(x) the original function (non quadratic) or just the local quadratic approximation? If it is the local quadratic approximation, how is it correlated to the original function? It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation? If what I think is correct, I think it would be important to show this.\n\nf(x) is the quadratic approximation of the original function. As AnonReviewer1 pointed out, we measure h and C from the original function. The h and C measurements are used to construct the local quadratic approximation and fed into the tuning rule. We would rephrase the statement on f(x) to connect it to the original function.\n\n\nQ: The objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning.\n\nThe SingleStep objective is a generalization of Equ (13) from 1D quadratics to multidimensional local quadratic approximations. Specifically, the SingleStep objective is the expected squared distance to the optimum of multiple dimensional local quadratic approximation after a single iterative step. For a multidimensional quadratic aligned with the axes, as we use *a single global learning rate and a single global momentum for the whole model*, the objective can be decomposed into sum of expected squared distance along different axes (i.e. on 1d quadratic), which is the left-hand side of Equ (13) (with t = 1 in Equ (13)). Note if the quadratic function is not axes-aligned, we can still decompose along the eigendirections of the Hessian instead of the axes. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1514489711739,"tcdate":1513811753534,"number":6,"cdate":1513811753534,"id":"HJWNy__GM","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"B1RZJ1cxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Part I: Scope of our work, and generalizing from 1D to multidimensional analysis ","comment":"We appreciate AnonReviewer3’s helpful and detailed comments. *We will upload a new manuscript with the clarification incorporated in the next couple of days*. In the following, we address AnonReviewer3’s questions in detail. In summary, we:  \n\n-- elaborate that the goal of the present paper is not to provide theoretical guarantees for general convex functions, but to use our simple analysis from quadratics to design an optimizer that works well empirically.\n\n-- explain how 1D analysis can generalize to multidimensional analysis in our case.\n\n-- explain why there exists a meaningful definition of a generalized condition number (GCN) for 1D functions; we give simple examples where accelerated linear rate is possible for 1D cases when we use the GCN (instead of conventional condition number) to tune momentum. \n\n-- provide the requested demonstration of convergence behaviors on high dimensional quadratics, as well as anonymous repo for experiment replication.\n\n\nQ: I found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence. \n\nWe would like to clarify that our algorithm does not ensure convergence outside the class of quadratic functions for multiple dimensional cases. This difficulty to guarantee general results for (Polyak’s) momentum gradient descent has been documented by the existence of specific counter-examples (Lessard et. al, 2016). We cite this paper in section 2 in order to make it clear that we do not give general guarantees. \n\nInstead we focus on ideas inspired from quadratics; these drive the design of our tuner, our main contribution. We conducted extensive experiments on 8 different popular deep learning models, empirically demonstrating our tuner’s merit on non-quadratics. We will make this point more prominent in the manuscript.\n\n\nQ: There are two major problems with this approach: First: Exploring 1-dim functions is indeed a nice way to get some intuition. Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions.\n\nWe agree with the reviewer that this is not an obvious point and elaborate here. We are updating our manuscript with the following clarifying discussion. We start by discussing a blueprint for generalization that is exact for quadratics (by extending discussions in the paragraph above Lemma 4). In our answer to the next question, we extend some of the quantities and ideas---like (generalized) curvature and (generalized) condition number---to the non-quadratic case. Then we follow the same generalization blueprint to go from 1D to multidimensional. As noted, we do not aim to provide analytical guarantees for non-quadratics, but rather to design an adaptive optimization method that works empirically well.\n\nThe analysis of momentum dynamics on quadratic objectives decomposes exactly into independent scalar problems along the eigenvectors of the Hessian. For each scalar quadratic problem, the curvature is constant everywhere; we can think of this as the degenerate case where the extremal curvatures are equal, that is h_min=h_max. As the reviewer points out, the condition number for individual slices is 1, hence the optimal momentum value is 0.\n\nAll scalar components of the multi-dimensional quadratic can be tuned jointly using a single instance of SingleStep to yield a single learning rate and single momentum value. In that case extremal curvatures h_min and h_max are taken over all directions and their ratio yields the condition number; individual 1D gradient variances sum up to the total multi-dimensional gradient variance, C, and the squared distance from optimum, D^2, can be either calculated as the sum of squared distances on the scalar problems, or approximately estimated directly (the latter is what we do in our implementation of the Distance() function). In this rebuttal we include a sanity-check experiment on a synthetic quadratic. It shows that when using exact oracles as input to SingleStep, we achieve the optimal convergence rate for quadratics.\n\nNow that we have established a blueprint for going from 1D to multidimensional analysis, in our next answer, we give a detailed explanation and examples about why:\n(i) our definition of a generalized condition number is meaningful on 1D functions and;\n(ii) even on 1D, we can use it to tune non-zero values of momentum and achieve faster convergence that with 0 momentum.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1514489726153,"tcdate":1513811644293,"number":5,"cdate":1513811644293,"id":"BkE6RP_fG","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"B1RZJ1cxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Part II: Generalized condition number and examples of acceleration on 1D","comment":"Q: Second, accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration. Concretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading. This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case. However, $\\nu$ might be much larger than 1 even in the 1-dim case.\n\nIn this response, we give a detailed explanation and examples about why:\n(i) our definition of a generalized condition number (GCN) is meaningful on 1D functions and;\n(ii) even on 1D, we can use the GCN to tune non-zero values of momentum and achieve faster convergence that with 0 momentum.\n\nRegarding the motivation of generalized curvature, our main idea is that the condition number, which is defined based on a local definition of curvature, can be generalized to incorporate longer-range, non-local variations of curvature. Specifically, classic curvature at a given point is described by the eigenvalues of the local Hessian. On quadratic problems, this curvature plays a crucial role on the rate of convergence: if we use learning rate α, and no momentum, on a 1D quadratic of curvature h, the contraction (i.e. the multiplicative factor describing the stepwise shrinkage of the distance towards optimum) at every step is |1-αh|. The intuition is that when h is high, the gradient output exerts a strong ‘pull’ towards the optimum. Unfortunately, when we move to non-quadratic problems, this tight connection between curvature and convergence rate is lost.\n\nOur definition of ‘generalized curvature’ tackles this issue and maintains this tight connection for non-quadratic problems. We define it with respect to a specific local minimum and it describes how strong the ‘pull’ towards that minimum is. If h’(x) describes the generalized curvature at point x, the contraction for gradient descent with no momentum becomes |1-αh’(x)|. In this we regain the tight connection between our new definition of curvature and the convergence rate.\n\nAs the first simple example to show acceleration in 1D, let's consider the 1D function f(x)=|x|. Curvature for all x \\neq 0 is 0. Generalized curvature on the other hand is h’(x) = 1/|x|. Now if we restrict ourselves to x \\in [ε, 1], the generalized (i.e. long-range) condition number (GCN) (with optimum x* = 0) is 1/ε. That is, as we get closer to the optimum, the relative pull towards it grows and the contraction factor becomes |1-α/|x||. Assume we are aiming for an accuracy of ε, then in the absence of momentum, we need to set the learning rate as α=Ο(ε). This means that starting from x_0=1, our first steps are going to converge at a rate of ~|1-ε|, which can be extremely slow. On the other hand, if we use the GCN to tune our momentum and learning rate, we get a momentum μ = 1-2(sqrt(ε)/(1+sqrt(ε)) ~= 1-2sqrt(ε), and experience a constant rate of sqrt(μ)=sqrt(1-2sqrt(ε)). As shown in the following plot (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/contraction_abs_func.pdf), momentum is able to achieve stronger contraction, i.e. faster reduction of the distance to the optimum, during the first steps of optimization. \n\nFurthermore, the next figure (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/convergence_abs_func.pdf) uses the analytical results above over time and shows that for f(x)=|x| using momentum tuned according to our GCN, can yield acceleration over gradient descent without momentum on 1D functions. \n\nAs a second example to show acceleration in 1D case, we would like to point to the non-convex example of Figure 3(a) of our manuscript. In that case again, GCN=1000 which suggest a value of momentum of about 0.9, even though this is a 1D function. Plot 3(b) already shows that using this momentum allows for a constant linear convergence rate on this non-convex function. Assume in Figure 3(a), that the curvature of the top, flatter quadratic is 1 and the curvature of the bottom, steeper quadratic is 1000. The learning rate for gradient descent without momentum cannot exceed 1/500, otherwise it would always escape from the steep quadratic. Again, a similar analysis to the one we presented in the previous examples, as well an simple experiments, show that the optimal value for momentum again is not zero, even though this is a 1D function. \n\nThe reason we are able to achieve this acceleration, is because we are taking into account long-range variations in curvature.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1514489748102,"tcdate":1513811489757,"number":4,"cdate":1513811489757,"id":"rJ9XCPOMG","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"B1RZJ1cxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Part III: A single global learning rate and momentum; demonstration on high dimensional quadratics","comment":"Q: Regarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).\n\nAs mentioned in the abstract, YellowFin only auto-tunes two hyperparameters for the entire model: a single global momentum and a single global learning rate (i.e. YellowFin does not use per-dimension learning rates and per-dimension momentums). The main question in the intro, which motivates our paper, is “can we produce an adaptive optimizer that does not depend on per-variable learning rate adaptation?”. The Yellowfin tuning rule uses only one SingleStep instance in section 3.1 for the whole model (instead of using one instance for each variable). It operates on the high dimensional local quadratic approximation and uses estimates of extremal curvatures h_min and h_max over all possible directions. It solves for a single momentum and a single learning rate. Note the SingleStep problem is a direct generalization of our 1D analysis at the beginning of Section 3, by decomposing along the eigendirections of the high dimensional quadratic.\n\nWe will make Section 3 more precise in an upcoming revision and emphasize that we run a single instance of the SingleStep optimizer for the entire model with the estimators for h_min and h_max providing rough estimates of extremal curvatures along all directions.\n\n\nQ: And as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions. Thus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed. The authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me.\n In contrast to the theoretical part, the experiments seems very encouraging. Showing YF to perform very well on several deep learning tasks without (or with very little) tuning. Again, this seems a bit magical or even too good to be truth. I suggest the authors to perform a experiment with say a quadratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n\nFollowing up on AnonReviewer3’s suggestion, we demonstrate the convergence behavior on a high-dimensional quadratic which is not aligned with the axes. More specifically, we first generated a 1000d quadratic, with curvature 1, 2, 3, …, 1000 on the axes. Then we rotate the quadratic for 45 degrees on the planes defined by the axes with curvature i and 1001 - i (for i = 1, 2,3, …, 500) in a pairwise fashion. We assume SingleStep problem has access to the oracle to exactly measure the curvature range and the distance to optimum. As shown in Figure (https://github.com/AnonRepository/YellowFin_Pytorch/blob/master/plots/1000d_quadratics.pdf), YellowFin’s tuning rule can demonstrate linear convergence rate on this landscape using the full gradient.\n\nWe have also set up anonymized YellowFin repositories with PyTorch and TensorFlow implementations. Our experiments can be easily replicated with the code in the repository.\n[PyTorch repo] https://github.com/AnonRepository/YellowFin_Pytorch\n[Tensorflow repo] https://github.com/AnonRepository/YellowFin\n\n\nIn summary, we \n\n-- showed how 1D analysis can generalize to multidimensional cases on quadratics.\n\n-- showed that there exists a meaningful definition of a generalized condition number (GCN) for 1D functions; and gave simple 1D examples where acceleration is possible.\n\n-- provided requested demonstration on high dimensional quadratics, as well as anonymized code repositories to replicate results in our manuscript.\n\nWe appreciate AnonReviewer3’s time and detailed comments/suggestions to further clarify our contributions, and merit of our optimizer. That said, we are happy to provide further analysis, clarifications and experiments to resolve further questions. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1514489696208,"tcdate":1513809835904,"number":1,"cdate":1513809835904,"id":"Hy4hvwuff","invitation":"ICLR.cc/2018/Conference/-/Paper325/Public_Comment","forum":"SyrGJYlRZ","replyto":"SJ0CHgbbM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for supporting the validity of our experiments","comment":"We thank AnonReviewer2 for providing independent support on the accuracy of our experiments. The kind support on the validity of our experiments is a great encouragement to us. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515642432484,"tcdate":1512273366224,"number":3,"cdate":1512273366224,"id":"SJ0CHgbbM","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Review","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Worthy contribution","rating":"6: Marginally above acceptance threshold","review":"[Apologies for short review, I got called in late. Marking my review as \"educated guess\" since i didn't have time for a detailed review]\n\nThe paper proposes an algorithm to tune the momentum and learning rate for SGD. While the algorithm does not have a theory for general non-quadratic functions, experimental validation is extensive, making it a worthy contribution in my opinion. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515642432532,"tcdate":1511808774284,"number":2,"cdate":1511808774284,"id":"B1RZJ1cxG","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Review","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Misleading and shaky theoretical motivation/approach","rating":"4: Ok but not good enough - rejection","review":"The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin). They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.\n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence. There are two major problems with this approach:\n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition. Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions.\n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading. This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case.\nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case.\n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).\nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions.\nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed.\nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me.\n\nIn contrast to the theoretical part, the experiments seems very encouraging. Showing YF to perform very well on several deep learning tasks without (or with very little) tuning. Again, this seems a bit magical or even too good to be truth. I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515642432572,"tcdate":1511753391784,"number":1,"cdate":1511753391784,"id":"HyuhIWYez","invitation":"ICLR.cc/2018/Conference/-/Paper325/Official_Review","forum":"SyrGJYlRZ","replyto":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference/Paper325/AnonReviewer1"],"readers":["everyone"],"content":{"title":"YellowFin and the Art of Momentum Tuning","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm.\n\nAlthough the results are promising, I found the presentation of this paper almost inaccessible to me.\n\nFirst, though a minor point, but where does the name *YellowFin* come from?\n\nFor the presentation, the motivation in introduction is fine, but the following section about momentum operator is hard to follow. There are a lot of undefined notation. For example, what does the *convergence rate* mean (what is the measurement for convergence)? And is the *optimal accelerated rate* the same as *convergence rate* mentioned above? Also, what do you mean by *all directions* in the sentence below eq.2?\n\nThen the paper talks about robustness properties of the momentum operator. But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative?\n\nIn the following paragraph, what is *contraction*? Does it have anything to do with the paper as I didn't see it in the remaining text?\n\nLemma 2 seems to use the spectral radius of the momentum operator as the *robustness*. But how can it describe the robustness? More details are needed to understand this.\n\nWhat it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter. I got confused in this section because eq.9 defines f(x) as a quadratic function. Is this f(x) the original function (non quadratic) or just the local quadratic approximation? If it is the local quadratic approximation, how is it correlated to the original function? It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation? If what I think is correct, I think it would be important to show this.\n\nAlso, the objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning.\n\nOverall, I think this is an interesting paper, but the presentation is too fuzzy to get it evaluated.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515185197986,"tcdate":1509097229019,"number":325,"cdate":1509739360458,"id":"SyrGJYlRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyrGJYlRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"YellowFin and the Art of Momentum Tuning","abstract":"Hyperparameter tuning is one of the most time-consuming workloads in deep learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam, reduce this labor by adaptively tuning an individual learning rate for each variable. Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better results. Motivated by this trend, we ask: can simple adaptive methods, based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam. We then analyze its robustness to learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for momentum and learning rate in SGD. YellowFin optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly. We empirically show YellowFin can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing, with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.","pdf":"/pdf/63206ec464b15516301aaee544f5d0a365cc839c.pdf","TL;DR":"YellowFin is an SGD based optimizer with both momentum and learning rate adaptivity.","paperhash":"anonymous|yellowfin_and_the_art_of_momentum_tuning","_bibtex":"@article{\n  anonymous2018yellowfin,\n  title={YellowFin and the Art of Momentum Tuning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyrGJYlRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper325/Authors"],"keywords":["adaptive optimizer","momentum","hyperparameter tuning"]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}