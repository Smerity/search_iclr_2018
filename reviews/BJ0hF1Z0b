{"notes":[{"tddate":null,"ddate":null,"tmdate":1515677578121,"tcdate":1515677578121,"number":3,"cdate":1515677578121,"id":"rJG5vkH4z","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Comment","forum":"BJ0hF1Z0b","replyto":"HJRVC6-Gz","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper504/AnonReviewer1"],"content":{"title":"Thanks for your response!","comment":"\n1. Noise\n\nThanks for the reference. It might indeed be an LSTM issue!\n\n2. Clipping\n\nOh right, I didn't thought about the bias introduces, that is a good point!\n\n3.  Optimizers\n\"Certainly an interesting direction, but beyond the scope of the current work.\"\n\nIndeed!\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1513377333999,"tcdate":1513377333999,"number":2,"cdate":1513377333999,"id":"HJRVC6-Gz","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Comment","forum":"BJ0hF1Z0b","replyto":"Bkg5_kcxG","signatures":["ICLR.cc/2018/Conference/Paper504/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper504/Authors"],"content":{"title":"Author response","comment":"We thank the reviewer for the thoughtful review and good questions, which we address below:\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline.\n\nIndeed, we were hoping to see some regularization benefit from noise, but there does not appear to be a significant effect, at least for these models. In Figure 3, which isolates the noise addition, we do see a slight improvement with a modest amount of noise early in training (blue line, noise around 0.012), but otherwise the Gaussian noise we add does not appear to help. We did not do training set evaluation on these models, it is possible (and likely based on results from \"Deep learning with differential privacy\", Figs. 3 and 6) that the addition of noise decreases the gap between test and training accuracy. Other work has also observed that adding noise may not work well as a regularizer for LSTMs, see  the \"negative results\" paragraph in Sec 4 of https://openreview.net/pdf?id=rkjZ2Pcxe \n\n2. Clipping is supposed to help with the exploding gradients problem. Do you have an idea why a low threshold hurts the performances? Is it because it reduces the amplitude of the updates (and thus simply slows down the training)?\n\nThis is an important direction for future work, but we have some preliminary thoughts. First, to clarify, note we are clipping each user's update before averaging across users, whereas traditional clipping is applied to a single minibatch update after averaging over examples, and so it is possible that these two types of clipping behave differently. \n\nWe suspect two primary reasons for the drop in performance with over-aggressive clipping: (1) reduction in the amplitude of the updates, as you suggest; and (2) clipping introduces bias into the way updates from different users are weighted, essentially changing the loss function being optimized. Some preliminary subsequent experiments indicate that both effects are significant, and that the effect of (1) can be somewhat offset by rescaling the updates on the server. Nevertheless, we emphasize that our primary result is that despite these effects, it is possible to set the clipping parameter large enough that we can still train high-accuracy models.\n\n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)?\n\nThere are multiple ways these optimizers could be extended to the federated setting. Either algorithm could be applied locally on each client (that is, inside UserUpdateFedAvg) to compute the update, and our approach would work without modification. Running these algorithms across clients while combining them with the additional local computation done by FederatedAverging (which we found to be important for achieving DP) would essentially mean designing a new optimization procedure --- certainly an interesting direction, but beyond the scope of the current work.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1513377066659,"tcdate":1513377066659,"number":1,"cdate":1513377066659,"id":"BkMETpWMz","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Comment","forum":"BJ0hF1Z0b","replyto":"ryImKM5lG","signatures":["ICLR.cc/2018/Conference/Paper504/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper504/Authors"],"content":{"title":"Author response","comment":"We thank the reviewer for the thoughtful review, and will attempt to improve the presentation in the final version. While it will be difficult to fit a complete introduction of all the topics mentioned into the page limit, we will add additional coverage of this material."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1515642458025,"tcdate":1511823646098,"number":3,"cdate":1511823646098,"id":"ryImKM5lG","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Review","forum":"BJ0hF1Z0b","replyto":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice work","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models. It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.\n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. This work makes a pretty significant contribution to such topic. It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy. The experiment shows good privacy and utility.\n\nThe presentation of the paper can be improved a bit. For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper. Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1515642458064,"tcdate":1511811210010,"number":2,"cdate":1511811210010,"id":"Bkg5_kcxG","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Review","forum":"BJ0hF1Z0b","replyto":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer1"],"readers":["everyone"],"content":{"title":" Nice extensions to FederatedAveraging, with strong experimental setup.","rating":"7: Good paper, accept","review":"\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee. The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed.\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent\n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point.\n\nCorrectness: The paper is technically correct.\n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline.\n2. Clipping is supposed to help with the exploding gradients problem. Do you have an idea why a low threshold hurts the performances? Is it because it reduces the amplitude of the updates (and thus simply slows down the training)?\n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)?\n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee.\n2. Strong experimental setup that analyses in details the proposed extensions.\n3. Experiments performed on public datasets.\n\nCons\n-------\n\nNone\n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \"is given in Figure 1\" -> \"is given in Algorithm 1\"\n\nNote\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1515642458104,"tcdate":1511740950669,"number":1,"cdate":1511740950669,"id":"BJ1XIR_ef","invitation":"ICLR.cc/2018/Conference/-/Paper504/Official_Review","forum":"BJ0hF1Z0b","replyto":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference/Paper504/AnonReviewer2"],"readers":["everyone"],"content":{"title":"I like the experimental strength of the paper, but I have mild concerns about the new algorithmic ideas in the paper.","rating":"7: Good paper, accept","review":"Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy. The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example. The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework.\n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale. The paper uses the right composition tools like moments accountant to get strong privacy guarantees. The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm. Both these contributions are important in the effectiveness of the overall algorithm.\n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models. I did not find strong algorithmic ideas in the paper. I found the paper to be lacking in that respect.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]}},{"tddate":null,"ddate":null,"tmdate":1509739266766,"tcdate":1509124534502,"number":504,"cdate":1509739264101,"id":"BJ0hF1Z0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ0hF1Z0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Differentially Private Recurrent Language Models","abstract":"We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy.  Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.","pdf":"/pdf/feb7ab061a0bf1ab2d0c40fc6bd3e7a622c3e63d.pdf","TL;DR":"User-level differential privacy for recurrent neural network language models is possible with a sufficiently large dataset.","paperhash":"anonymous|learning_differentially_private_recurrent_language_models","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Differentially Private Recurrent Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ0hF1Z0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper504/Authors"],"keywords":["differential privacy","LSTMs","language models","privacy"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}