{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642473285,"tcdate":1511920068595,"number":3,"cdate":1511920068595,"id":"Sy66Z9sgG","invitation":"ICLR.cc/2018/Conference/-/Paper584/Official_Review","forum":"B1J_rgWRW","replyto":"B1J_rgWRW","signatures":["ICLR.cc/2018/Conference/Paper584/AnonReviewer2"],"readers":["everyone"],"content":{"title":"good paper, consider publishing","rating":"7: Good paper, accept","review":"The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of “hard” functions that are not easily representable by shallower networks.  It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss.  I think these contributions warrant publishing the paper at ICLR 2018.  The paper is also well written, a bit dense in places, but overall well organized and easy to follow. \n\nA key limitation of the paper in my opinion is that typically DNNs do not contain a linear final layer.  It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer.  I also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like D^nw, would like hearing authors’ comments in case I’m missing some simplification.\n\nOne minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL.  For example, adding PWL to Theorems and Corollaries in Section 3.1 will help.  Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Deep Neural Networks with Rectified Linear Units","abstract":"In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to {\\em global optimality} with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of ``hard'' functions, contrary to countable, discrete families known in the literature.  An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at least $\\frac12k^{k+1}-1$ total nodes. Finally, we construct a family of $\\R^n\\to \\R$ piecewise linear functions for $n\\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ``hard'' functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.","pdf":"/pdf/650a69892adef2bb5beff98d50479ddbc63262f7.pdf","TL;DR":"This paper 1) characterizes functions representable by ReLU DNNs, 2) formally studies the benefit of depth in such architectures,  3) gives an algorithm to implement empirical risk minimization to global optimality for two layer ReLU nets.","paperhash":"anonymous|understanding_deep_neural_networks_with_rectified_linear_units","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Neural Networks with Rectified Linear Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1J_rgWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper584/Authors"],"keywords":["expressive power","benefits of depth","empirical risk minimization","global optimality","computational hardness","combinatorial optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642473322,"tcdate":1511818922893,"number":2,"cdate":1511818922893,"id":"BkQ3IWcxM","invitation":"ICLR.cc/2018/Conference/-/Paper584/Official_Review","forum":"B1J_rgWRW","replyto":"B1J_rgWRW","signatures":["ICLR.cc/2018/Conference/Paper584/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper investigates the function classes representable by ReLU networks. It contributes a more detailed view on hard functions representable by deep networks, their parametrisation, and gaps between deep and shallow.  ","rating":"6: Marginally above acceptance threshold","review":"The paper presents a series of definitions and results elucidating details about the functions representable by ReLU networks, their parametrisation, and gaps between deep and shallower nets. \n\nThe paper is easy to read, although it does not seem to have a main focus (exponential gaps vs. optimisation vs. universal approximation). The paper makes a nice contribution to the details of deep neural networks with ReLUs, although I find the contributed results slightly overstated. The 1d results are not difficult to derive from previous results. The advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network. The optimisation method appears close to brute force and is limited to 2 layers. \n\nTheorem 3.1 appears to be easily deduced from the results from Montufar, Pascanu, Cho, Bengio, 2014. For 1d inputs, each layer will multiply the number of regions at most by the number of units in the layer, leading to the condition w’ \\geq w^{k/k’}. Theorem 3.2 is simply giving a parametrization of the functions, removing symmetries of the units in the layers. \n\nIn the list at the top of page 5. Note that, the function classes might be characterized in terms of countable properties, such as the number of linear regions as discussed in MPCB, but still they build a continuum of functions. Similarly, in page 5 ``Moreover, for fixed n,k,s, our functions are smoothly parameterized''. This should not be a surprise. \n\nIn the last paragraph of Section 3 ``m = w^k-1'' This is a very big first layer. This also seems to subsume the first condition, s\\geq  w^k-1 +w(k-1) for the network discussed in Theorem 3.9. In the last paragraph of Section 3 ``To the best of our knowledge''. In the construction presented here, the network’s size is essentially in the layer of size m. Under such conditions, Corollary 6 of MPCB also reads as s^n. Here it is irrelevant whether one artificially increases the depth of the network by additional, very narrow, layers, which do not contribute to the asymptotic number of units. \n\nThe function class Zonotope is a composition of two parts. It would be interesting to consider also a single construction, instead of the composition of two constructions. \n\nTheorem 3.9 (ii) it would be nice to have a construction where the size becomes 2m + wk when k’=k. \n\nSection 4, while interesting, appears to be somewhat disconnected from the rest of the paper. \n\nIn Theorem 2.3. explain why the two layer case is limited to n=1. \n\nAt some point in the first 4 pages it would be good to explain what is meant by ``hard’’ functions (e.g. functions that are hard to represent, as opposed to step functions, etc.) \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Deep Neural Networks with Rectified Linear Units","abstract":"In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to {\\em global optimality} with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of ``hard'' functions, contrary to countable, discrete families known in the literature.  An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at least $\\frac12k^{k+1}-1$ total nodes. Finally, we construct a family of $\\R^n\\to \\R$ piecewise linear functions for $n\\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ``hard'' functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.","pdf":"/pdf/650a69892adef2bb5beff98d50479ddbc63262f7.pdf","TL;DR":"This paper 1) characterizes functions representable by ReLU DNNs, 2) formally studies the benefit of depth in such architectures,  3) gives an algorithm to implement empirical risk minimization to global optimality for two layer ReLU nets.","paperhash":"anonymous|understanding_deep_neural_networks_with_rectified_linear_units","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Neural Networks with Rectified Linear Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1J_rgWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper584/Authors"],"keywords":["expressive power","benefits of depth","empirical risk minimization","global optimality","computational hardness","combinatorial optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642473358,"tcdate":1511666845564,"number":1,"cdate":1511666845564,"id":"rJUiN3DeM","invitation":"ICLR.cc/2018/Conference/-/Paper584/Official_Review","forum":"B1J_rgWRW","replyto":"B1J_rgWRW","signatures":["ICLR.cc/2018/Conference/Paper584/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"Understanding Deep Neural Networks with Rectified Linear Units\"","rating":"6: Marginally above acceptance threshold","review":"This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks. I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN. Any smooth function can be approximated by such networks.\n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.\n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces\n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.\n\nAmong these results (1), (2), (4) are sort of known in the literature. This paper extends the existing results in some subtle ways. For (1), the authors show that the DNN has a tighter bound on the depth. For (2), the \"hard\" functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger. For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum.\n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU. Other than that, I don't get much more insight from the theoretical result. When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient. Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough.\n\nResult (3) is more interesting as it is a new result. The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.  However, the construction seems artificial and these functions don't seem to be visually very complex.\n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks. The results might be of theoretical interest, but I doubt if a pragmatic ReLU network user will learn anything by reading this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Deep Neural Networks with Rectified Linear Units","abstract":"In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to {\\em global optimality} with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of ``hard'' functions, contrary to countable, discrete families known in the literature.  An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at least $\\frac12k^{k+1}-1$ total nodes. Finally, we construct a family of $\\R^n\\to \\R$ piecewise linear functions for $n\\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ``hard'' functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.","pdf":"/pdf/650a69892adef2bb5beff98d50479ddbc63262f7.pdf","TL;DR":"This paper 1) characterizes functions representable by ReLU DNNs, 2) formally studies the benefit of depth in such architectures,  3) gives an algorithm to implement empirical risk minimization to global optimality for two layer ReLU nets.","paperhash":"anonymous|understanding_deep_neural_networks_with_rectified_linear_units","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Neural Networks with Rectified Linear Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1J_rgWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper584/Authors"],"keywords":["expressive power","benefits of depth","empirical risk minimization","global optimality","computational hardness","combinatorial optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739217982,"tcdate":1509127527358,"number":584,"cdate":1509739215316,"id":"B1J_rgWRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1J_rgWRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Understanding Deep Neural Networks with Rectified Linear Units","abstract":"In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to {\\em global optimality} with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of ``hard'' functions, contrary to countable, discrete families known in the literature.  An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at least $\\frac12k^{k+1}-1$ total nodes. Finally, we construct a family of $\\R^n\\to \\R$ piecewise linear functions for $n\\geq 2$ (also smoothly parameterized), whose number of affine pieces scales exponentially with the dimension $n$ at any fixed size and depth. To the best of our knowledge, such a construction with exponential dependence on $n$ has not been achieved by previous families of ``hard'' functions in the neural nets literature. This construction utilizes the theory of zonotopes from polyhedral theory.","pdf":"/pdf/650a69892adef2bb5beff98d50479ddbc63262f7.pdf","TL;DR":"This paper 1) characterizes functions representable by ReLU DNNs, 2) formally studies the benefit of depth in such architectures,  3) gives an algorithm to implement empirical risk minimization to global optimality for two layer ReLU nets.","paperhash":"anonymous|understanding_deep_neural_networks_with_rectified_linear_units","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Neural Networks with Rectified Linear Units},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1J_rgWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper584/Authors"],"keywords":["expressive power","benefits of depth","empirical risk minimization","global optimality","computational hardness","combinatorial optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}