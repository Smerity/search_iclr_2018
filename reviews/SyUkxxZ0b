{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222682819,"tcdate":1512099840984,"number":3,"cdate":1512099840984,"id":"HkKWeUCef","invitation":"ICLR.cc/2018/Conference/-/Paper546/Official_Review","forum":"SyUkxxZ0b","replyto":"SyUkxxZ0b","signatures":["ICLR.cc/2018/Conference/Paper546/AnonReviewer3"],"readers":["everyone"],"content":{"title":"List of observations without insights.","rating":"3: Clear rejection","review":"Adversarial example is studied on one synthetic data.\nA neural networks classifier is trained on this synthetic data. \nAverage distances and norms of errorneous perturbations are computed. \nIt is observed that small perturbation (chosen in a right direction) is sufficient to cause misclassification. \n\nCONS:\nThe writing is bad and hard to follow, with typos: for example what is a period just before section 3.1 for? Another example is \"Red lines indicate the range of needed for perfect classification\", which does not make sense. Yet another example is the period at the end of Proposition 4.1.  Another example is \"One counter-intuitive property of adversarial examples is it that nearly \". \n\nIt looks as if the paper was written in a hurry, and it shows in the writing.   \n\nAt the beginning of Section 3, Figure 1 is discussed. It points out that there exists adversarial directions that are very bad. But I don't see how it is relevant to adversarial examples. If one was interested in studying adversarial examples, then one would have done the following. Under the setting of Figure 1, pick a test data randomly from the distribution (and one of the classes), and find an adversarial direction\n\nI do not see how Section 3.1 fits in with other parts of the paper. Is it related to any experiment? Why it defining a manifold attack?\n\nPutting a \"conjecture\" on a paper has to be accompanied by the depth of the insight that brought the conjecture. Having an unjustified conjecture 5.1 would poison the field of adversarial examples, and it must be removed.\n\nThis paper is a list of experiments and observations, that are not coherent and does not give much insight into the topics of \"adversarial examples\". The only main messages are that on ONE synthetic dataset, random perturbation does not cause misclassification and targeted classification can cause misclassification. And, expected loss is good while worst-case loss is bad. This, in my opinion, is not enough to be published at a conference. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Spheres","abstract":"We introduce a simple synthetic task in order to study the phenomenon of adversarial examples. For this task, the data manifold is mathematically defined and there is an analytic characterization of the model’s decision boundary.  We show that when the data is high dimensional, generalization error may be so low that model errors may only be found adversarially.  Despite these errors being extremely unlikely they appear to always be close to randomly sampled \"clean\" data. These adversarial examples exist on the data manifold and even for a model class which can provably obtain perfect accuracy.  We conclude by drawing connections to adversarial examples for other machine learning models.","pdf":"/pdf/00da22e7a657d5a347c011bb9bad6bbd9d0e7a4e.pdf","TL;DR":"We explore properties of adversarial examples on a synthetic task.","paperhash":"anonymous|adversarial_spheres","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Spheres},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyUkxxZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper546/Authors"],"keywords":["Adversarial Examples","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222682859,"tcdate":1511819936172,"number":2,"cdate":1511819936172,"id":"rJOiq-clf","invitation":"ICLR.cc/2018/Conference/-/Paper546/Official_Review","forum":"SyUkxxZ0b","replyto":"SyUkxxZ0b","signatures":["ICLR.cc/2018/Conference/Paper546/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Exploration of data perturbations in the synthetic problem of classifying 2 concentric spheres","rating":"5: Marginally below acceptance threshold","review":"The paper considers the synthetic problem setting of classifying two concentric high dimensional spheres and the worst case behavior of neural networks on this task, in the hope to gain insights about the vulnerability of deep networks to adversarial examples. The problem dimension is varied along with the class separation in order to control the difficulty of the problem.\n\nConsidering representative synthetic problems is a good idea, but it is not clear to me why this particular choice is useful for the purpose.\n\n2 kind is \"attacks are generated\" for this purpose, and the ReLU network is simplified to a single layer network with quadratic nonlinearity. This gives an ellipsoid decision boundary around the origin. It is observed that words case and average case empirical error estimates diverge when the input is high dimensional. A Gaussian tail bound is then used to estimate error rates analytically for this special case. It is conjectured that the observed behaviour has to do with high dimensional geometrie.\n\nThis is a very interesting conjecture, however unfortunately it is not studied further. Some empirical observations are made, but it is not discussed whether what is observed is surprising in any way, or just as expected? For instance that there is nearly no error when trying to categorise the two concentric spheres without adversarial examples seems to me expected, since there is a considerable margin between the classes. The results are presented in  rather descriptive rather than a quantitative way.\n\nOverall, this works seems somewhat too preliminary at this stage.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Spheres","abstract":"We introduce a simple synthetic task in order to study the phenomenon of adversarial examples. For this task, the data manifold is mathematically defined and there is an analytic characterization of the model’s decision boundary.  We show that when the data is high dimensional, generalization error may be so low that model errors may only be found adversarially.  Despite these errors being extremely unlikely they appear to always be close to randomly sampled \"clean\" data. These adversarial examples exist on the data manifold and even for a model class which can provably obtain perfect accuracy.  We conclude by drawing connections to adversarial examples for other machine learning models.","pdf":"/pdf/00da22e7a657d5a347c011bb9bad6bbd9d0e7a4e.pdf","TL;DR":"We explore properties of adversarial examples on a synthetic task.","paperhash":"anonymous|adversarial_spheres","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Spheres},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyUkxxZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper546/Authors"],"keywords":["Adversarial Examples","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222682895,"tcdate":1511819213679,"number":1,"cdate":1511819213679,"id":"r1LAwb9xz","invitation":"ICLR.cc/2018/Conference/-/Paper546/Official_Review","forum":"SyUkxxZ0b","replyto":"SyUkxxZ0b","signatures":["ICLR.cc/2018/Conference/Paper546/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Proposes and analyzes one very simple artificial data set, looking for insights about adversarial examples; Despite some good motivations, the significance of the results is not clearly established.","rating":"4: Ok but not good enough - rejection","review":"The idea of analyzing a simple synthetic data set to get insights into open issues about adversarial examples has merit. However, the results reported here are not sufficiently significant for ICLR.\n\nThe authors make a big deal throughout the paper about how close to training data the adversarial examples they can find on the data manifold are. E.g.: “Despite being extremely rare, these misclassifications appear close to randomly sampled points on the sphere.”  They report mean distance to nearest errors on the data manifold is 0.18 whereas mean distance between two random points on inner sphere is 1.41. However, distance between two random points on the sphere is not the right comparison. The mean distance between random nearest neighbors from the training samples would be much more appropriate.\n\nThey also stress in the Conclusions their Conjecture 5.1 that under some assumptions “the average distance to nearest error may decrease on the order of O(1 / d) as the input dimension grows large.” However, earlier they admitted that “Whether or not a similar conjecture holds for image manifolds is unclear and should be investigated in future work.” So, the practical significance of this conjecture is unclear.  Furthermore, it is well known that in high dimensions, the distances between pairs of training samples tends towards a large constant (e.g. making nearest neighbor search using triangular inequality pruning infeasible), so extreme care much be taken to not over generalize any results from these sorts of synthetic high dimensional experiments.\n\nAuthors note that for higher dimensional spheres, adversarial examples on the manifold (sphere shell) could found, but not smaller d:  “In our experiments the highest dimension we were able to train the ReLU net without adversarial examples seems to be around d = 60.”  Yet,in their later statement in that same paragraph  “We did not investigate if larger networks will work for larger d.”, it is unclear what is meant by “will work”; because, presumably, larger networks (with more weights) would be HARDER to avoid adversarial examples being found on the data manifold, so larger networks should be less likely “to work”, if “work” means avoid adversarial examples.  In any case, their apparent use of only h=1000 unit networks (for both ReLU and quadratic cases) is disappointing, because it is not clear whether the phenomena observed would be qualitatively similar for different fully-separable discriminants (e.g. different h values with different regularization costs even if all such networks had zero classification errors).\n\nThe authors repeat the following exact same phrase in both the Introduction and the Conclusion:\n“Our results highlight the fact that the epsilon norm ball adversarial examples often studied in defence papers are not the real problem but are rather a tractable research problem. “\nBut it is not clear exactly what the authors meant by this. Also, the term “epsilon norm ball” is not commonly used in adversarial literature, and the only reference to such papers is Madry et al, (2017), which is only on ArXiv and not widely known — if these types of adversarial examples are “often studied” as claimed, there should be other / more established references to cite here.\n\nIn short, this work addresses the important problem of better understanding adversarial examples, but the simple setup has a higher burden to establish significance, which this paper as written has not met.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Spheres","abstract":"We introduce a simple synthetic task in order to study the phenomenon of adversarial examples. For this task, the data manifold is mathematically defined and there is an analytic characterization of the model’s decision boundary.  We show that when the data is high dimensional, generalization error may be so low that model errors may only be found adversarially.  Despite these errors being extremely unlikely they appear to always be close to randomly sampled \"clean\" data. These adversarial examples exist on the data manifold and even for a model class which can provably obtain perfect accuracy.  We conclude by drawing connections to adversarial examples for other machine learning models.","pdf":"/pdf/00da22e7a657d5a347c011bb9bad6bbd9d0e7a4e.pdf","TL;DR":"We explore properties of adversarial examples on a synthetic task.","paperhash":"anonymous|adversarial_spheres","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Spheres},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyUkxxZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper546/Authors"],"keywords":["Adversarial Examples","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739243468,"tcdate":1509126110376,"number":546,"cdate":1509739240801,"id":"SyUkxxZ0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyUkxxZ0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adversarial Spheres","abstract":"We introduce a simple synthetic task in order to study the phenomenon of adversarial examples. For this task, the data manifold is mathematically defined and there is an analytic characterization of the model’s decision boundary.  We show that when the data is high dimensional, generalization error may be so low that model errors may only be found adversarially.  Despite these errors being extremely unlikely they appear to always be close to randomly sampled \"clean\" data. These adversarial examples exist on the data manifold and even for a model class which can provably obtain perfect accuracy.  We conclude by drawing connections to adversarial examples for other machine learning models.","pdf":"/pdf/00da22e7a657d5a347c011bb9bad6bbd9d0e7a4e.pdf","TL;DR":"We explore properties of adversarial examples on a synthetic task.","paperhash":"anonymous|adversarial_spheres","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Spheres},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyUkxxZ0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper546/Authors"],"keywords":["Adversarial Examples","Deep Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}