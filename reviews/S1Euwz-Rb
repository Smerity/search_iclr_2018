{"notes":[{"tddate":null,"ddate":null,"tmdate":1516141144279,"tcdate":1516141128581,"number":10,"cdate":1516141128581,"id":"H1-Icx3Vf","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"SyKUVctlM","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"Dear Reviewer1:","comment":"\nThank you very much for your review - we truly appreciate it! \nWe have uploaded a revision (by the rebuttal deadline, jan 5) that addresses all your comments:\n\n1. We have revised the description of the writing unit to make it more clear - we have experimented with several variants for this unit - the \"standard\" one (for which all the results are about), and 3 variants: a. with self-attention, b. with gating, and c. with both self-attention and gating. In the ablations study section we have included results for each of these for the whole dataset, 10% of the dataset and also showed training curves for each variant.\n\n2. We have trained the models without GloVE and added these results along with clarification to the experiments section.\n\n3. We have included ablation studies in order to justify the architecture design choices and elucidate their impact. We have also added visualizations of attention weights for several examples and discussed them.\n\nThanks a lot again for your review!\n- Paper858 Authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1516136847659,"tcdate":1516136829177,"number":9,"cdate":1516136829177,"id":"ByrYKk3VG","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"BJoJaojNz","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"Thank you!","comment":"Thank you very much for your review and for improving the rating! We will fix the typos and indeed are also working on making the writing a bit more concise to shorten the overall paper length!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1516125667165,"tcdate":1516121314807,"number":8,"cdate":1516121314807,"id":"BJoJaojNz","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"S1AP0Njxz","signatures":["ICLR.cc/2018/Conference/Paper858/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/AnonReviewer3"],"content":{"title":"Final recommendation","comment":"Thanks for the statistical significance analysis, ablation studies, qualitative results and other clarifications, I have increased my rating from 6 to 7. Please fix minor things such as 2 (different) plots for Network Length in Figure 7, short explanation for why the attention is significant on the word \"is\" instead of an important word \"shape\" in qualitative examples 1&2 in figure 8, etc. Also, the current paper length is 14 pages (after the addition of about 4 pages in rebuttal) which is almost double the recommended length of 8 pages, so I would suggest reducing the paper length for future. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515797929782,"tcdate":1515797851553,"number":7,"cdate":1515797851553,"id":"B1Ewp2LVG","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"Bkb9w8r4f","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"To Reviewer2: ","comment":"Alright. Thank you very much for your detailed review - it was very helpful to us and we truly appreciate it! We are currently working on Cornell nlvr and also text-based datasets as well as more qualitative experiments and while I believe I shouldn't upload revisions anymore we will definitely explore these directions further!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515706249300,"tcdate":1515706249300,"number":6,"cdate":1515706249300,"id":"Bkb9w8r4f","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"Sk0oVNYlM","signatures":["ICLR.cc/2018/Conference/Paper858/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/AnonReviewer2"],"content":{"title":"Response to rebuttal","comment":"Thanks for the ablations! My score remains the same."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515775164637,"tcdate":1515189465540,"number":5,"cdate":1515189465540,"id":"Hyf1B_p7f","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"S1Euwz-Rb","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"Revision","comment":"\nDear reviewers,\n\nThank you very much for your detailed suggestions and comments! \nWe have uploaded a revision that addresses the comments raised in the reviews. In particular:\n- We report the model performance with random initialization for the word vectors rather than GloVE embeddings. Using a uniform initialization, we were able to get equivalent results for CLEVR and 1-2% difference for CLEVR-humans in the new setting, compared to those we have achieved with GloVE.\n- We have performed statistical significance analysis, running each model multiple times (10) and updated the plots to show averages and confidence intervals. These experiments show that the results of our model are indeed statistically significant compared to the alternative models.\n- We have performed ablation studies that cover the control, reading, and writing units as well as additional aspects of our model. We have shown results for the standard CLEVR dataset and 10% subset of it along with training curves. The ablation study we have conducted quantifies the contribution of each of the model components to its overall performance and shows its relative significance. Based on the results, we can see the importance of using attention over both the question and the image in a coordinated manner through a dual structure of recurrent control and memory paths.   \n- We have looked into attention maps of the model for the question and image and put a few examples of these in the revised version of the paper that demonstrate the model interpretability.\n- For the gating mechanism of the writing unit, we have performed additional experiments showing that untied gate values for each entry of the state vector perform better than having one shared potentially-interpretable gate for the whole state and so have changed the description of that subsection accordingly.\n\nAdditional changes in the paper:\n- We fixed typos that were in the original submission.\n- We have clarified a few missing points that were mentioned in the reviews. \n- This includes in particular clarification about the writing unit mods of operation, and the several variants - with self-attention, with a gate, and with both. Each of these variants is accompanied with training curves and final accuracies as part of the ablation section.\n\nIn response to specific comments by reviewers:\n- The ablations study shows the importance of using both the question and the final memory state. As explained in the revision, since the memory holds information only from the image, it may not contain all required information to answer the question, since potentially crucial aspects of the question are not represented in the image. \nFor instance: given an image with one object, one question can ask about its size and another question about its color, but in both cases the memory will attend to the same one object and thus will not contain enough information to respond to the question correctly.\n- As supported by both the ablations studies and the new experiments that show model's performance on CLEVR-humans without using GloVE, we claim that the model owes its ability to handle diverse language and learn from small amounts of data to the use of attention over question. In particular, the attention allows the model to ignore varied words in the CLEVR-humans dataset that the model hasn't been necessarily trained on but aren't crucial to understand the question, and rather can focus only on the key words that refer to the objects and their properties. \n- As demonstrated by the qualitative results of attention maps over the image and question and explained in the model section, the model is indeed transparent by having access to the attention maps of each computation step. And indeed, examples of such attention maps show interpretable rationales behind the model's predictions.\n\nThank you very much!\n- Paper858 Authors\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515334657854,"tcdate":1513133034660,"number":4,"cdate":1513133034660,"id":"BJQeEzAWG","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"S1Euwz-Rb","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"Re: the reviews","comment":"\nDear reviewers, \n\nThank you very much for your detailed comments and insightful suggestions for further exploration. We completely agree that ablation studies, statistical significance measures and qualitative analysis such as visualizations are necessary to justify the performance of the model and elucidate its behavior. We are actively working on a revised version of the paper that will include these studies and address the other comments raised in the reviews, in particular regarding the use of GloVE, the model’s performance on smaller subsets (<10%) of CLEVR, and the necessity of predicting the answer using both the question and the memory.\n\nSeveral clarifications in response to questions from the reviews:\n\n1. For comparative experiments, all the other systems used the original publicly-available authors’ implementations. All the models were trained with an equal batch size of 64 (as in the original implementations) and on the same machine, using a single Titan X Maxwell GPU per model.\n\n2. As mentioned in section 3.2.4, our claims about the model’s robustness to linguistic variability and its ability to handle datasets more diverse than the standard CLEVR indeed refer to its performance on CLEVR-Humans. We believe that attention mechanisms used over the question are a key factor that allows that, as discussed in the supplementary material, section C.4, and supported by Lu et al. (2016) and Yang et al. (2016). We will address this matter further in the revised version of the paper.\n\nThank you!\n- Paper858 Authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1516120849031,"tcdate":1511898730531,"number":3,"cdate":1511898730531,"id":"S1AP0Njxz","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Review","forum":"S1Euwz-Rb","replyto":"S1Euwz-Rb","signatures":["ICLR.cc/2018/Conference/Paper858/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Needs more analysis and qualitative evaluation","rating":"7: Good paper, accept","review":"Summary: \nThe paper presents a new model called Compositional Attention Networks (CAN) for visual reasoning. The complete model consists of an input unit, a sequence of the proposed Memory, Attention and Composition (MAC) cell, and an output unit. Experiments on CLEVR dataset shows that the proposed model outperforms previous models.\n\nStrengths: \n— The idea of building a compositional model for visual reasoning and visual question answering makes a lot of sense, and, I think, is the correct direction to go forward in these fields.\n— The proposed model outperforms existing models pushing the state-of-the-art.\n— The proposed model is computationally cheaper and generalizes well with less training data as compared to existing models.\n— The proposed model has been described in detail in the paper.\n\nWeaknesses: \n— Given that the performance of state-on-art on CLEVR dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models.\n— It is not clear which part of the proposed model leads to how much improvement in performance. Ablations studies are needed to justify the motivations for each of the components of the proposed model.\n— Analysis of qualitative results (including attention maps, gate values, etc.) is needed to justify if the model is actually doing what the authors think it should do. For example, the authors mention an example on page 6 at the end of Section 3.2.2, but do not justify if this is actually what the model is doing.\n— Why is it necessary to use both question and memory information to answer the question even when the question was already used to compute the memory information? I would think that including the question information helps in learning the language priors in the dataset. Have the authors looked at some qualitative examples where the model which only uses memory information gives an incorrect answer but adding the question information results in a correct answer?\n— Details such as using Glove word embeddings are important and can affect the performance of models significantly. Therefore, they should be clearly mentioned in the main paper while comparing with other models which do not use them.\n— The comparisons of number of epochs required for training and the training time need fixed batch sizes and CPU/GPU configurations. Is that true? These should be reported in this section.\n— The authors claim that their model is robust to linguistic variations and diverse vocabulary, by which I am guessing they are referring to experiments on CLEVR-Humans dataset. What is there in the architecture of the proposed model which provides this ability? If it is the Glove vectors, it should be clearly mentioned since any other model using Glove vectors should have this ability.\n— On page 6, second paragraph, the authors mention that there are cases which necessitate the model to ignore current memories. Can the authors show some qualitative examples for such cases?\n— In the intro, the authors claim that their proposed cell encourages transparency. But, the design of their cell doesn’t seem to do so, nor it is justified in the paper.\n\nOverall: The performance reported in the paper is impressive and outperforms previous state-of-the-art, but without proper statistical significance analysis of performance, ablation studies, analysis of various attention maps, memory gates, etc. and qualitative results, I am not sure if this work would be directly useful for the research community.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515642521593,"tcdate":1511789648729,"number":2,"cdate":1511789648729,"id":"SyKUVctlM","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Review","forum":"S1Euwz-Rb","replyto":"S1Euwz-Rb","signatures":["ICLR.cc/2018/Conference/Paper858/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Looks good but needs clarification","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a recurrent neural network for visual question answering. The recurrent neural network is equipped with a carefully designed recurrent unit called MAC (Memory, Attention and Control) cell, which encourages sequential reasoning by restraining interaction between inputs and its hidden states. The proposed model shows the state-of-the-art performance on CLEVR and CLEVR-Humans dataset, which are standard benchmarks for visual reasoning problem. Additional experiments with limited training data shows the data efficiency of the model, which supports its strong generalization ability.\n\nThe proposed model in this paper is designed with reasonable motivations and shows strong experimental results in terms of overall accuracy and the data efficiency. However, an issue in the writing, usage of external component and lack of experimental justification of the design choices hinder the clear understanding of the proposed model.\n\nAn issue in the writing\nOverall, the paper is well written and easy to understand, but Section 3.2.3 (The Write Unit) has contradictory statements about their implementation. Specifically, they proposed three different ways to update the memory (simple update, self attention and memory gate), but it is not clear which method is used in the end.\n\nUsage of external component\nThe proposed model uses pretrained word vectors called GloVE, which has boosted the performance on visual question answering. This experimental setting makes fair comparison with the previous works difficult as the pre-trained word vectors are not used for the previous works. To isolate the strength of the proposed reasoning module, I ask to provide experiments without pretrained word vectors.\n\nLack of experimental justification of the design choices\nThe proposed recurrent unit contains various design choices such as separation of three different units (control unit, read unit and memory unit), attention based input processing and different memory updates stem from different motivations. However, these design choices are not justified well because there is neither ablation study nor visualization of internal states. Any analysis or empirical study on these design choices is necessary to understand the characteristics of the model. Here, I suggest to provide few visualizations of attention weights and ablation study that could support indispensability of the design choices.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515642521630,"tcdate":1511765157668,"number":1,"cdate":1511765157668,"id":"Sk0oVNYlM","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Review","forum":"S1Euwz-Rb","replyto":"S1Euwz-Rb","signatures":["ICLR.cc/2018/Conference/Paper858/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper describes a new model architecture for machine reasoning. In contrast\nto previous approaches that explicitly predict a question-specific module\nnetwork layout, the current paper introduces a monolithic feedforward network\nwith iterated rounds of attention and memory. On a few variants of the CLEVR\ndataset, it outperforms both discrete modular approaches, existing iterated\nattention models, and the conditional-normalization-based FiLM model. \n\nSo many models are close to perfect accuracy on the standard CLEVR dataset that\nI'm not sure how interesting these results are. In this respect I think the\ncurrent paper's results on CLEVR-Humans and smaller fractions of synthetic CLEVR\nare much more exciting.\n\nOn the whole I think this is a strong paper. I have two main concerns. The\nlargest is that this paper offers very little in the way of analysis. The model\nis structurally quite similar to a stacked attention network or a particular\nfixed arrangement of attentive N2NMN modules, and it's not at all clear based on\nthe limited set of experimental results where the improvements are actually\ncoming from. It's also possible that many of the proposed changes are\ncomplementary to NMN- or CBN-type models, and it would be nice to know if this\nis the case.\n\nSecondarily, the paper asserts that \"our architecture can handle\ndatasets more diverse than CLEVR\", but runs no experiments to validate this. It\nseems like once all the pieces are in place it should be very easy to get\nnumbers on VQA or even a more interesting synthetic dataset like NLVR.\n\nBased on a sibling comment, it seems that there may also be some problems with\nthe comparison to FiLM, and I would like to see this addressed.\n\nOn the whole, the results are probably strong enough on their own to justify\nadmitting this paper. But I will become much more enthusiastic about if if the\nauthors can provide results on other datasets (even if they're not\nstate-of-the-art!) as well as evidence for the following:\n\n1. Does the control mechanism attend to reasonable parts of the sentence?\n\nHere it's probably enough to generate a bunch of examples showing sentence\nattentions evolving over time.\n\n2. Do these induce reasonable attentions over regions of the image?\n\nAgain, examples are fine.\n\n3. Do the self-attention and gating mechanisms recover the right structure?\n\nIn addition to examples, here I think there are some useful qualitative\nmeasures. It should be possible to extract reasonable discretized \"reasoning\nmaps\" by running MST or just thesholding on the \"edge weights\" induced by\nattention and gating. Having extracted these from a bunch of examples, you can\ncompare them to the structural properties of the ground-truth CLEVR network\nlayouts by plotting a comparison of sizes, branching factors, etc.\n\n4. More on the left side of the dataset size / accuracy curve. What happens if\n   you only give the model 7000 examples? 700? 70?\n\nFussy typographical notes:\n\n- This paper makes use of a lot of multi-letter names in mathmode. These are\n  currently written like $KB$, which looks bad, and should instead be\n  $\\mathit{KB}$.\n\n- Variables with both superscripts and subscripts have the superscripts pushed\n  off to the right; I think you're writing these like $b_5 ^d$ but they should\n  just be $b_5^d$ (no space).\n\n- Number equations and then don't bother carrying subscripts like $W_3$, $W_4$\n  around across different parts of the model---this isn't helpful.\n\n- The superscripts indicating the dimensions of parameter matrices and vectors\n  are quite helpful, but don't seem to be explained anywhere in the text. I\n  think the notation $W^{(d \\times d)}$ is more standard than $W^{d, d}$.\n\n- Put the cell diagrams right next to the body text that describes them (maybe even\n  inline, rather than in figures). It's annoying to flip back and forth.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1511577570404,"tcdate":1511452177009,"number":3,"cdate":1511452177009,"id":"SytM0vNlf","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"H1gOE_mxM","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"Re: Questions about comparisons to competitors and why CAN works (Part 1)","comment":"Thank you very much for the kind words and for the detailed response! We truly appreciate it!\n\nAddressing the questions you have raised:\n\n1) GloVE Word Embeddings\n\nFor the CLEVR dataset, we have observed an improvement of 0.17% in the final validation accuracy when using GloVE compared to word vectors initialized randomly with standard normal distribution, and 0.24% improvement compared to uniform-distribution initialization with range [-1,1]. \n\nNotably, in the early stages of the training process, the models with learned-from-scratch word embeddings actually outperformed the model with pretrained GloVE word embeddings. Only by the end of the training this trend is reversed to a modest advantage for the GloVE-based model. These results, which we will be happy to add to the paper, suggest that randomly-initialized word vectors are in fact easier for the model to distinguish between initially, whereas the small advantage of some additional semantic notions embodied in the GloVE embeddings become noticeable only by the end of training, useful for a low fraction of the questions. In any case, we will be glad to stress the fact that we have used GloVE in the experiments section of the paper.  \n\nFor CLEVR-Humans, we so far have indeed used the pre-trained vectors and, as mentioned in the paper, haven’t trained them any further to prevent a drift in their semantic meaning. I agree that it will be both interesting and fair to check the model performance on CLEVR-Humans for randomly-initialized vectors as well. I will be running an experiment for that right now, and so we will report the scores as soon as they arrive and update the paper with these additional results accordingly. \n \n2) Comparison to other models\n\nFor the competitor models, we have used the original publicly available implementations as-is for both FiLM (https://github.com/ethanjperez/film) and PG+EE (https://github.com/facebookresearch/clevr-iep) with their default arguments, after closely following all the training procedures listed on the websites (image features extraction, question preprocessing etc.). \n\nFor figure 4 Right, while indeed there is some difference between the numbers you report and the numbers we have obtained by self-running the mentioned github version, when compared to performance of other approaches, the difference seems to be quite modest in relative terms. It may be a good idea to run FiLM and other models several times and measure both the average and variance in performance across these attempts. I think some variance between two different runs of the same model can be generally expected even given the equal settings, and this also depends on the model stability and robustness. In any case, as shown in figure 4, the gap in performance between CANs and other models is consistently significant throughout the training process, and it remains the case also for the results you mention.\n\nFor figure 4 Left, accuracy as a function of the training-set size, it may be the case that the discrepancy you claim arises from difference in the training time that has been allowed before collecting the results: with the aim of having a fair comparison, we have run all the models the same amount of time: until the validation score of all models has not shown further improvement for several consecutive iterations. It may be the case that allowing longer time for the training of FiLM and other models will lead to better scores, and we will be happy to mention them as well in the paper. Indeed, there are trade-offs between training time, dataset size and accuracy that are interesting to explore.\n\nAnother important aspect pertaining to the comparability of different models is their size - the number of parameters being used. We have noted that FiLM has a relatively high dimension of 4096 for the  question-processing GRU hidden states, especially when compared to competing approaches that use sizes of 128-512 (our model has hidden dimension of 512, achieving similar results for 256 after slightly longer training time). Since the size of the weight matrix used in the GRU is quadratic as a function of the hidden state size, this leads to O(4096*4096) parameters which is, by and large, an order-of-magnitude higher than O(512*512) or O(256*256). In order to have a more fair comparison, it seems that it may be interesting to test FiLM with a smaller state size, comparably to other models."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"ddate":null,"tddate":1511452217193,"tmdate":1511578169519,"tcdate":1511452131217,"number":2,"cdate":1511452131217,"id":"ByjkRDEgz","invitation":"ICLR.cc/2018/Conference/-/Paper858/Official_Comment","forum":"S1Euwz-Rb","replyto":"H1gOE_mxM","signatures":["ICLR.cc/2018/Conference/Paper858/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper858/Authors"],"content":{"title":"Re: Questions about comparisons to competitors and why CAN works (Part 2)","comment":"3) Qualitative Experiments\n\nWhile developing our idea, we have performed a large number of experiments that test ablations, modifications and variations to our architecture, the results of which will be presented in a few days in the revised version of paper (once it becomes possible to submit revisions). These experiments indeed demonstrate the relative importance of each of the model’s different aspects to its overall performance. For instance, they show that while optional components of the Write Unit such as memory gating and self-attention (discussed in section 3.2.2) improve the final performance and accelerate training, the model still achieves strong state-of-the-art results without them. Conversely, they quantitatively show the importance of using attention to decompose the question into a series of control states, in contrast to having only spatial attention layers over the image, as is the case for example in stacked attention networks (Yang et al., 2016; Johnson et al., 2017). In addition, we have examined the performance of the model across different network lengths (i.e. number of MAC cells), hidden state dimensions, and with several types of nonlinearities, all are establishing the robustness of our model to implementation details and model variances.\n\nWe are actively exploring our model behavior in qualitative terms and, in the paper revision, we will show gate-values and attention-map visualizations over the image, question and previous memories, following each reasoning step and for different types of questions. Indeed, soft-attention lies at the heart of our model, conferring several advantages, as discussed in the paper: (1) Robustness against both linguistic and visual variations (corroborated also by Lu et al (2016) and Yang et al (2016)). (2) Capacity for easy-to-train multi-modal translation between attended question words to the corresponding attended regions in the image. (3) Compositionality of reasoning steps by standardizing the content of the dual control and memory states to be selective summaries of the question and image correspondingly, and finally, as you alluded to, (4) Interpretability - Ideally, the model may be able to show a clear step-by-step rationale supporting the predicted answer. \n\nFurthermore, we are working on an error analysis for both CLEVR and CLEVR-Humans datasets to have indeed a better understanding of the nature of mistakes our model makes. Based on the quantitative results in the paper, it is already noticeable that many of the errors are for the counting questions, which is reasonable given the larger output space they have compared to other question types. We are currently looking into potential ways for further improving models counting accuracies, and will add a discussion about that either in a revision of the paper or in future work. \n\n--\n\nThank you very much for the thorough and insightful response and suggestions for further exploration! We will upload a revised version with the aforementioned additions once the option becomes available! :)\n- Paper858 Authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1511388320016,"tcdate":1511388264414,"number":1,"cdate":1511388264414,"id":"H1gOE_mxM","invitation":"ICLR.cc/2018/Conference/-/Paper858/Public_Comment","forum":"S1Euwz-Rb","replyto":"S1Euwz-Rb","signatures":["~Ethan_Perez1"],"readers":["everyone"],"writers":["~Ethan_Perez1"],"content":{"title":"Questions about comparisons to competitors and why CAN works","comment":"Congratulations on the impressive results. The proposed CAN model is quite interesting. We are also grateful that you represented our work with FiLM respectfully :)\n\nWe have a few questions:\n1) How does CAN perform when learning word embeddings from scratch, as competing models do, rather than using GloVE embeddings? It seems that using external knowledge via pre-trained embeddings would lead to unfair comparisons for:\n  --CLEVR-Humans performance: CLEVR-Humans has many new words, which competing methods learn from scratch from a small training set or else use <UNK> tokens.\n  --CLEVR training curves: External word knowledge frees a model from having to learn word meanings from scratch early in training, facilitating high performance early on.\n  --CLEVR performance (to a lesser extent): GloVE might encode helpful notions, not learnable from CLEVR, on how to reason about various words.\n  --It seems that results using GloVE should have an accompanying asterisk denoting so in tables and that GloVE’s use should be noted in the main paper body (not just appendix). Even better, would it be too difficult to re-run these CAN experiments with learned word embeddings?\n2) How did you run competitor models? There are discrepancies which make us hesitant to accept the self-run competitors results that the paper reports and compares against:\n  --“Training curve” figure: Here are the FiLM validation accuracies for the reported model for first 11 epochs (plotted in (Perez et al. 2017)), higher than your paper plots: [0.5164, 0.7251, 0.802, 0.8327, 0.8568, 0.8887, 0.9063, 0.9243, 0.9328, 0.9346, 0.942]\n  --“Accuracy / Dataset size (out of 700k)” figure: Trained on 100% of CLEVR, FiLM achieves 97.84% validation acc., as reported in our paper (Perez et al. 2017), compared to ~94% in your plot. We have also run incomplete FiLM experiments on 25% and 50% subsets of CLEVR and achieved higher numbers than those you report/plot. We can run these full experiments and comment numbers if that would be helpful.\n  --“Accuracy / Dataset size (out of 700k)” figure: You plot that PG+EE achieves ~95.5% validation acc., lower than the reported 96.9% test acc. (a likely lower bound for validation acc.).\n3) Do you have evidence supporting the reason(s) behind CAN’s success? The paper only gives 2 pages of experiments and analysis, all quantitative and related to outperforming other models, rather than qualitative, ablative, or analytical. Thus, it’s difficult to tell which of the paper’s several proposed intuitions on why CAN works so well are valid. CAN consists of many components and aspects (control/read/write units, memory gates, compositionality, sequential reasoning, parameter-sharing, spatial attention, self-attention, etc.), and it’s unclear from overall acc. numbers which ones are crucial. In particular:\n  --Is there evidence spatial attention allows CAN to reason about space more effectively? Intuitively, spatial attention should help, but (Johnson et al., 2016; Santoro et al., 2017) show that spatial attention models struggle with CLEVR/spatial reasoning; in the extreme, it’s possible that CAN performs well in spite of spatial attention. On the other hand, (Perez et al. 2017) show, with CLEVR acc. and activation visualizations, that an attention-free method (FiLM) can effectively reason about space. Can you run analytical experiments or visualizations to support that spatial attention actually helps CAN reason about space? Where is the model attending after each reasoning step? What is the CLEVR acc. after replacing spatial attention with another form of conditioning? What errors does CAN make and not make, especially to other models? If it would help, we can provide our reported FiLM model via GitHub.\n  --It seems possible to build high-level aspects of CAN into simpler reasoning architectures and achieve similar performance. I.e., if spatial attention is key, adding spatial attention layers to a FiLM-based model, as in [1], might recover CAN’s performance. We would be interested to see evidence (such as ablations) showing that the entire CAN model is necessary for successful reasoning, as claimed.\n  --How does question-attention vary across reasoning steps? Does CAN iteratively focus on the query’s various parts, as claimed? Or does CAN’s attention not hop around, entirely ignore some question parts, etc.? CAN’s attention lends itself to very neat analysis possibilities.\n  --Do simpler questions actually use the memory gate to shorten the number of reasoning steps as expected?\n\nOverall, given the strong numerical results, CAN seems to potentially be a quite promising model, pending the authors’ response to these questions and some further analysis and evidence.\n\nKind Regards,\nEthan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville\n\nReferences:\n[1] Modulating early visual processing by language. Harm de Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, Aaron Courville. NIPS 2017."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]}},{"tddate":null,"ddate":null,"tmdate":1515187659007,"tcdate":1509136236466,"number":858,"cdate":1509739061197,"id":"S1Euwz-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1Euwz-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Compositional Attention Networks for Machine Reasoning","abstract":"We present Compositional Attention Networks, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. While many types of neural networks are effective at learning and generalizing from massive quantities of data, this model moves away from monolithic black-box architectures towards a design that provides a strong prior for iterative reasoning, enabling it to support explainable and structured learning, as well as generalization from a modest amount of data. The model builds on the great success of existing recurrent cells such as LSTMs: It sequences a single recurrent Memory, Attention, and Control (MAC) cell, and by careful design imposes structural constraints on the operation of each cell and the interactions between them, incorporating explicit control and soft attention mechanisms into their interfaces. We demonstrate the model's strength and robustness on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the new model is more computationally efficient, data-efficient, and requires an order of magnitude less time and/or data to achieve good results.","pdf":"/pdf/6550122f58cb81be85bf8cad1a972715f70c8336.pdf","TL;DR":"We present a novel architecture, based on dynamic memory, attention and composition for the task of machine reasoning.","paperhash":"anonymous|compositional_attention_networks_for_machine_reasoning","_bibtex":"@article{\n  anonymous2018compositional,\n  title={Compositional Attention Networks for Machine Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Euwz-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper858/Authors"],"keywords":["Deep Learning","Reasoning","Memory","Attention","VQA","CLEVR","Recurrent Neural Networks","Module Networks","Compositionality"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}