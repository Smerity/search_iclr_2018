{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642391840,"tcdate":1511853100948,"number":3,"cdate":1511853100948,"id":"ryrN2K9gf","invitation":"ICLR.cc/2018/Conference/-/Paper1157/Official_Review","forum":"SJ71VXZAZ","replyto":"SJ71VXZAZ","signatures":["ICLR.cc/2018/Conference/Paper1157/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"This paper shows that an LSTM language model trained on a large corpus of Amazon product reviews can learn representations that are useful for sentiment analysis. \nGiven representations from the language model, a logistic regression classifier is trained with supervised data from the task of interest to produce the final model.\nThe authors evaluated their approach on six sentiment analysis datasets (MR, CR, SUBJ, MPQA, SST, and IMDB), and found that the proposed method is competitive with existing supervised methods. \nThe results are mixed, and they understandably are better for test datasets from similar domains to the Amazon product reviews dataset used to train the language model.\nAn interesting finding is that one of the neurons captures sentiment property and can be used to predict sentiment as a single unit.\n\nI think the main result of the paper is not surprising and does not show much beyond we can do pretraining on unlabeled datasets from a similar domain to the domain of interest. \nThis semi-supervised approach has been known to improve in the low data regime, and pretraining an expressive neural network model with a lot of unlabeled data has also been shown to help in the past.\nThere are a few unanswered questions in the paper:\n- What are the performance of the sentiment unit on other datasets (e.g., SST, MR, CR)? Is it also competitive with the full model?\n- How does this method compare to an approach that first pretrains a language model on the training set of each corpus without using the labels, and then trains a logistic regression while fixing the language model? Is the large amount of unlabeled data important to obtain good performance here? Or is similarity to the corpus of interest more important?\n- I assume that the reason to use byte LSTM is because it is cheaper than a word level LSTM. Is this correct or was there any performance issue with using the word directly?\n- More analysis on why the proposed method does well on the binary classification task of SST, but performs poorly on the fine-grained classification would be useful. If the model is capturing sentiment as is claimed by the authors, why does it only capture binary sentiment instead of a spectrum of sentiment level?\n\nThe paper is also poorly written. There are many typos (e.g., \"This advantage is also its difficulty\", \"Much previous work on language modeling has evaluated \", \"We focus in on the task\", and others) so the writing needs to be significantly improved for it to be a conference paper, preferably with some help from a native English speaker.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning To Generate Reviews and Discovering Sentiment","abstract":"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.","pdf":"/pdf/82eaeeca82af695721cc73403066982e93ef60d2.pdf","TL;DR":"Byte-level recurrent language models learn high-quality domain specific representations of text.","paperhash":"anonymous|learning_to_generate_reviews_and_discovering_sentiment","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning To Generate Reviews and Discovering Sentiment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ71VXZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1157/Authors"],"keywords":["unsupervised learning","representation learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642391880,"tcdate":1511851242853,"number":2,"cdate":1511851242853,"id":"Sk7lrK5ez","invitation":"ICLR.cc/2018/Conference/-/Paper1157/Official_Review","forum":"SJ71VXZAZ","replyto":"SJ71VXZAZ","signatures":["ICLR.cc/2018/Conference/Paper1157/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper needs more serious work","rating":"2: Strong rejection","review":"First of all, I don't think I fully understand this paper, because it is difficult for me to find answers from this paper to the following questions:\n1) what is the hypothesis in this paper? Section 1 talks about lots of things, which I don't think is relevant to the central topic of this paper. But it misses the most important thing: what is THIS paper (not some other deep learning/representation problems)\n2) about section 2, regardless whether this is right place to talk about datasets, I don't understand why these two datasets. Since this paper is about generating reviews and discovering sentiment (as indicated in the paper)\n3) I got completely confused about the content in section 3 and lost my courage to read the following sections. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning To Generate Reviews and Discovering Sentiment","abstract":"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.","pdf":"/pdf/82eaeeca82af695721cc73403066982e93ef60d2.pdf","TL;DR":"Byte-level recurrent language models learn high-quality domain specific representations of text.","paperhash":"anonymous|learning_to_generate_reviews_and_discovering_sentiment","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning To Generate Reviews and Discovering Sentiment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ71VXZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1157/Authors"],"keywords":["unsupervised learning","representation learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642391924,"tcdate":1511801835308,"number":1,"cdate":1511801835308,"id":"SJXeNaYlM","invitation":"ICLR.cc/2018/Conference/-/Paper1157/Official_Review","forum":"SJ71VXZAZ","replyto":"SJ71VXZAZ","signatures":["ICLR.cc/2018/Conference/Paper1157/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting experiments but lack of model description","rating":"4: Ok but not good enough - rejection","review":"The authors propose to use a byte level RNN to classify reviews. In the meantime, they learn to generate reviews. The authors rely on the multiplicative LSTM proposed by Krause et al. 2016, a generative model predicting the next byte. They apply this architecture on the same task as the original article: document classification; they use a logistic regression on the extracted representation. The authors propose an evaluation on classical datasets and compare themselves to the state of the art.\nThe authors obtain interesting results on several datasets. They also explore the core of the unsupervised architecture and discover a neuron which activation matches the sentiment target very accurately. A deeper analyze shows that this neuron is more efficient on small datasets than on larger.\nExploiting the generative capacity of the network, they play with the \"sentiment neuron\" to deform a review. Qualitative results are interesting.\n\n\n\n\nThe authors do not propose an original model and they do not describe the used model inside this publication.\n\nNor the model neither the optimized criterion is detailled: the authors present some curve mentioning \"bits per character\" but we do not know what is measured. In fact, we do not know what is given as input and what is expected at the output -some clues are given in the experimental setup, but not in the model description-.\n\nFigure 2 is very interesting: it is a very relevant way to compare authors model with the literature.\n\nUnfortunately, the unsupervised abilities of the network are not really explained: we are a little bit frustrated by section 5.\n\n==\n\nThis article is very interesting and well documented. However, according to me, the fact that it provides no model description, no model analysis, no modification of the model to improve the sentiment discovery, prevents this article from being publicized at ICLR.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning To Generate Reviews and Discovering Sentiment","abstract":"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.","pdf":"/pdf/82eaeeca82af695721cc73403066982e93ef60d2.pdf","TL;DR":"Byte-level recurrent language models learn high-quality domain specific representations of text.","paperhash":"anonymous|learning_to_generate_reviews_and_discovering_sentiment","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning To Generate Reviews and Discovering Sentiment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ71VXZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1157/Authors"],"keywords":["unsupervised learning","representation learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510116678204,"tcdate":1510116678204,"number":1,"cdate":1510116678204,"id":"HJABp-xyM","invitation":"ICLR.cc/2018/Conference/-/Paper1157/Public_Comment","forum":"SJ71VXZAZ","replyto":"SJ71VXZAZ","signatures":["~Cuong_Hoang1"],"readers":["everyone"],"writers":["~Cuong_Hoang1"],"content":{"title":"Cool work, there are several very interesting findings but the paper could be improved","comment":"I am not an expert by any means. So I simply put my comments on the paper here, and hope someone would correct me if I was wrong at some points.\n\nI found this paper interesting because of two things. First, several things from the paper are new to me. Second, I like the way the authors make a very good story from their experiments.\n\nUnsupervised representation learning is very promising since unlabeled data are every where. But to date, supervised learning models still outperform unsupervised models. This may be explained because \"supervised approaches have clear objectives that can be directly optimized\". Meanwhile, \"unsupervised approaches rely on proxy tasks such as reconstruction, density estimation, or generation, which do not directly encourage useful representations for specific tasks.\" The paper exploits other perspectives: distributional issue and the limited capacity of current unsupervised representation learning models. Specifically, \"current generic distributed sentence representations may be very lossy - good at capturing the gist, but poor with the precise semantic or syntactic details which are critical for applications.\" This combines with the limited capacity may be the root of devil, and the authors investigate into details this point.\n\nHow? The authors first attempts to learn an unsupervised representation by training byte (character) level language modelling. Then we can use the outputs to train a sentiment analysis classifier. The authors trained their model on a very large dataset (Amazon review dataset) (the training took 1 month!)\n\nGiven a new text (paragraph, article or whatever), we simply perform some pre-processing and then feed the text into the mLSTM. Here is the interesting thing: we then get the outputs of all the output units (there are 4,096 units) and consider them as a feature vector representing the string read by the model. We turned the model into a sentiment classifier by taking a linear combination of these units, learning the weights of the combination via the available supervised data. This is new to me, indeed.\n\nWhat is next? By inspecting the relative contributions of features, they discovered a single unit within the LSTM that directly corresponds to sentiment. This is a very surprising finding, as remember that the mLSTM model is trained only to predict the next character in text.\n\nBut why is it the case? It is indeed an open question why the model recovers the concept of sentiment in such a precise way. It is pity, however, that the authors don't dig into details to have a satisfied answer!\n\nOverall I like the paper and like their interesting findings. This is a very cool work!\n\nBut I think the paper could be significantly improved in two ways:\n\n- I don't think the story written in the paper is really coherent.\n\n- The findings are interesting but a deeper investigation would satisfy readers more. So far everything is still as \"I read a very cool paper which shows that there exists a neural sentiment neuron by simply training language modeling, but I don't know why!\". "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning To Generate Reviews and Discovering Sentiment","abstract":"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.","pdf":"/pdf/82eaeeca82af695721cc73403066982e93ef60d2.pdf","TL;DR":"Byte-level recurrent language models learn high-quality domain specific representations of text.","paperhash":"anonymous|learning_to_generate_reviews_and_discovering_sentiment","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning To Generate Reviews and Discovering Sentiment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ71VXZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1157/Authors"],"keywords":["unsupervised learning","representation learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092379563,"tcdate":1509139423045,"number":1157,"cdate":1510092359380,"id":"SJ71VXZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ71VXZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning To Generate Reviews and Discovering Sentiment","abstract":"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.","pdf":"/pdf/82eaeeca82af695721cc73403066982e93ef60d2.pdf","TL;DR":"Byte-level recurrent language models learn high-quality domain specific representations of text.","paperhash":"anonymous|learning_to_generate_reviews_and_discovering_sentiment","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning To Generate Reviews and Discovering Sentiment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ71VXZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1157/Authors"],"keywords":["unsupervised learning","representation learning","deep learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}