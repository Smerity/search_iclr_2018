{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222795352,"tcdate":1511947914929,"number":3,"cdate":1511947914929,"id":"r1QqAe3lG","invitation":"ICLR.cc/2018/Conference/-/Paper843/Official_Review","forum":"ryCM8zWRb","replyto":"ryCM8zWRb","signatures":["ICLR.cc/2018/Conference/Paper843/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper discussed the issues for optimizing the loss functions in GRU4Rec and proposed tricks for optimize the loss functions, and also proposed enhanced version of the loss functions for GRU4Rec. Good performance improvements have been reported for the several datasets to show the effectiveness of the proposed methods.","rating":"6: Marginally above acceptance threshold","review":"This paper discussed the issues for optimizing the loss functions in GRU4Rec and proposed tricks for optimize the loss functions, and also proposed enhanced version of the loss functions for GRU4Rec. Good performance improvements have been reported for the several datasets to show the effectiveness of the proposed methods.\n\nThe good point of this work is to show that the loss function is important to train a better classifier for the session-based recommendation. This work is of value to the session-based recommendations.\n\nSome minor points:\nI think it may be better if the authors could put the results of RSC15 from Tan (2016) and Chatzis ect. (2017) into table 2 as well.\nAs these work has already been published and should be compared with and reported in the formal table. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Neural Networks with Top-k Gains for Session-based Recommendations","abstract":"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly.","pdf":"/pdf/a085c68037ead4424b2835cf557057c430ea6051.pdf","TL;DR":"Improving session-based recommendations with RNNs (GRU4Rec) by 35% using newly designed loss functions and sampling.","paperhash":"anonymous|recurrent_neural_networks_with_topk_gains_for_sessionbased_recommendations","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Neural Networks with Top-k Gains for Session-based Recommendations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryCM8zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper843/Authors"],"keywords":["gru4rec","session-based recommendations","recommender systems","recurrent neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222795391,"tcdate":1511825376401,"number":2,"cdate":1511825376401,"id":"S1d1eXqlM","invitation":"ICLR.cc/2018/Conference/-/Paper843/Official_Review","forum":"ryCM8zWRb","replyto":"ryCM8zWRb","signatures":["ICLR.cc/2018/Conference/Paper843/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Some interesting insights, but no substantial contributions and probably not well-suited for ICLR","rating":"4: Ok but not good enough - rejection","review":"This paper presents a few modifications on top of some earlier work (GRU4Rec, Hidasi et al. 2016) for session-based recommendation using RNN. The first one is to include additional negative samples based on popularity raised to some power between 0 and 1. The second one is to mitigate the vanishing gradient problem for pairwise ranking loss, especially with the increased number of negative samples from the first modification. The basic idea is to weight all the negative examples by their “relevance”, since for the irrelevant negatives the gradients are vanishingly small. Experimentally these modifications prove to be effective compared with the original GRU4Rec paper. \n\nThe writing could have been more clear, especially in terms of notations and definitions. I found myself sometimes having to infer the missing bits. For example, in Eq (4) and (5), and many that follow, the index i and j are not defined (I can infer it from the later part), as well as N_s (which I take it as the number of negative examples). This is just one example, but I hope the authors could carefully check the paper and make sure all the notations/terminologies are properly defined or referred with a citation when first introduced (e.g., pointwise, pairwise, and listwise loss functions). I consider myself very familiar with the RecSys literature, and yet sometimes I cannot follow the paper very well, not to mention the general ICLR audience. \n\nRegarding the two main modifications, I found the negative sampling rather trivial (and I am surprised in Hidasi et al. (2016) the negatives are only from the same batch, which seems a huge computational compromise) with many existing work on related topic: Steck (Item popularity and recommendation accuracy, 2011) used the same “popularity to the power between 0 and 1” strategy (they weighted the positive by the inverse popularity to the power). More closely, the negative sampling distribution in word2vec is in fact a unigram raised to the power of 0.75, which is the same as the proposed strategy here. As for the gradient vanishing problem for pairwise ranking loss, it has been previously observed in Rendle & Freudenthaler (Improving Pairwise Learning for Item Recommendation from Implicit Feedback, 2014) for BPR and they proposed an adaptive negative sampling strategy (trying to sample more relevant negatives while still keeping the computational cost low), which is closely related to the ranking-max loss function proposed in this paper. Overall, I don’t think this paper adds much on top of the previous work, and I think a more RecSys-oriented venue might benefit more from the insights presented in this paper.   \n\nI also have some high-level comments regarding using RNN for session-based recommendation (this was also my initial reaction after reading Hidasi et al. 2016). As mentioned in this paper, when applying RNN on RecSys datasets with longer time-span (which means there can be more temporal dynamics in users’ preference and item popularity), the results are not striking (e.g., Wu et al. 2017) with the proposed methods barely outperforming standard matrix factorization methods. It is puzzling that how RNN can work better for session-based case where a user’s preference can hardly change within such a short period of time. I wonder how a simple matrix factorization approach would work for session-based recommendation (which is an important baseline that is missing): regarding the claim that MF is not suited for session-based because of the absence of the concept of a user, each session can simply be considered as a pseudo-user and approaches like asymmetric matrix factorization (Paterek 2007, Improving regularized singular value decomposition for collaborative filtering) can even eliminate the need for learning user factors. ItemKNN is a pretty weak baseline and I wonder if a scalable version of the SLIM (Ning & Karypis 2011, SLIM: Sparse Linear Methods for Top-N Recommender Systems) would give better results. Finally, my general experience with BPR-type of pairwise ranking loss is that it is good at optimizing AUC, but not very well-suited for head-heavy metrics (MRR, Recall, etc.) I wonder how the propose loss would perform comparing with more competitive baselines. \n\nRegarding the page limit, given currently the paper is quite long (12 pages excluding references), I suggest the authors cutting down some space. For example, the part about fixing the cross entropy is not very relevant and can totally be put in the appendix. \n\nMinor comment:\n\n1. Section 3.3.1, “Part of the reasons lies in the rare occurrence…”, should r_j >> r_i be the other way around?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Neural Networks with Top-k Gains for Session-based Recommendations","abstract":"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly.","pdf":"/pdf/a085c68037ead4424b2835cf557057c430ea6051.pdf","TL;DR":"Improving session-based recommendations with RNNs (GRU4Rec) by 35% using newly designed loss functions and sampling.","paperhash":"anonymous|recurrent_neural_networks_with_topk_gains_for_sessionbased_recommendations","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Neural Networks with Top-k Gains for Session-based Recommendations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryCM8zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper843/Authors"],"keywords":["gru4rec","session-based recommendations","recommender systems","recurrent neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222795438,"tcdate":1511816498131,"number":1,"cdate":1511816498131,"id":"ryqETl9gG","invitation":"ICLR.cc/2018/Conference/-/Paper843/Official_Review","forum":"ryCM8zWRb","replyto":"ryCM8zWRb","signatures":["ICLR.cc/2018/Conference/Paper843/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Insightful analysis of existing loss functions, new losses are proposed and provide strong empirical results. Good paper.","rating":"8: Top 50% of accepted papers, clear accept","review":"This is an interesting paper that analyzes existing loss functions for session-based recommendations. Based on the result of these analysis the authors propose two novel losses functions which add a weighting to existing ranking-based loss functions. These novelties are meant to improve issues related to vanishing gradients of current loss functions. The empirical results on two large-scale datasets are pretty impressive. \n\nI found this paper to be well-written and easy to read.  It also provides a nice introduction to some of the recent literature on RNNs for session-based recommendations. \n\nIn terms of impact, while it studies a fairly applied (and narrow) question, it seems like it would be of interest to researchers and practitioners in recommender systems.\n\n\nI have a few comments and questions: \n\n- The results in Figure 3 show that both a good loss function and sampling strategy are required to perform well. This is interesting in the sense that doing the \"right thing\" according to the model (optimizing using all samples) isn't optimal. This is a very empirical observation and it would be insightful to better understand exactly the objective that is being optimized.\n\n- While BPR-max seems to be the strongest performer (Table 2), cross-entropy (XE) with additional samples is close. This further outlines the importance of the sampling method over the exact form of the loss function. \n\n- In ranking-max losses, it seems like \"outliers\" could have a bigger impact. I don't know how useful it is to think about (and it is a bit unclear what an \"outlier\" means in this implicit feedback setting).\n\n\nMinor comments: \n\n- Around Eq. 4 it may be worth being more explicit about the meaning of i and j. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Neural Networks with Top-k Gains for Session-based Recommendations","abstract":"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly.","pdf":"/pdf/a085c68037ead4424b2835cf557057c430ea6051.pdf","TL;DR":"Improving session-based recommendations with RNNs (GRU4Rec) by 35% using newly designed loss functions and sampling.","paperhash":"anonymous|recurrent_neural_networks_with_topk_gains_for_sessionbased_recommendations","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Neural Networks with Top-k Gains for Session-based Recommendations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryCM8zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper843/Authors"],"keywords":["gru4rec","session-based recommendations","recommender systems","recurrent neural network"]}},{"tddate":null,"ddate":null,"tmdate":1509739070463,"tcdate":1509135893944,"number":843,"cdate":1509739067801,"id":"ryCM8zWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryCM8zWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Recurrent Neural Networks with Top-k Gains for Session-based Recommendations","abstract":"RNNs have been shown to be excellent models for sequential data and in particular for session-based user behavior. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce a novel ranking loss function tailored for RNNs in recommendation settings. The better performance of such loss over alternatives, along with further tricks and improvements described in this work, allow to achieve an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 51% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly.","pdf":"/pdf/a085c68037ead4424b2835cf557057c430ea6051.pdf","TL;DR":"Improving session-based recommendations with RNNs (GRU4Rec) by 35% using newly designed loss functions and sampling.","paperhash":"anonymous|recurrent_neural_networks_with_topk_gains_for_sessionbased_recommendations","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Neural Networks with Top-k Gains for Session-based Recommendations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryCM8zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper843/Authors"],"keywords":["gru4rec","session-based recommendations","recommender systems","recurrent neural network"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}