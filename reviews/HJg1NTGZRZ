{"notes":[{"tddate":null,"ddate":null,"tmdate":1515576802950,"tcdate":1515576802950,"number":9,"cdate":1515576802950,"id":"B1jyA87EG","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"Syi6-muxf","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer1"],"content":{"title":"Experimental results are still too weak.","comment":"1. The state-of-the-art result on MNIST is clearly above 99.5% (top1 accuracy).  Actually by simply using LeNet, one can achieve top 1 accuracy higher than 99%. [1] However, in the paper the best result is less than 97%.\n2. The state-of-the-art result on CIFAR is based on deep residual nets (eg.  wide residual nets can achieve less than 4% top 1 error rate).  And in the paper the best result is less than 90%.\nSo, the current version is still too weak to demonstrate the effectiveness of the proposed method.\n\n[1] http://yann.lecun.com/exdb/mnist/\n[2] https://github.com/szagoruyko/wide-residual-networks"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1516547658726,"tcdate":1515189377882,"number":7,"cdate":1515189377882,"id":"B1qKE_Tmz","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"Syi6-muxf","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"Rebuttal for AnonReviewer 1","comment":"1- Only small networks on relatively small datasets are tested. \n>The results on VGG networks (larger networks) is being computed and will be included in the camera ready submission. \n\n2-The results on MNIST and CIFAR-10 are not good enough...\n>We found that our low performance on MNIST was caused by using 4X4 pooling layers. We have changed this experiment to use a the standard neural net architecture as a baseline and results on MNIST are shown in (Figure 1a) and  CIFAR-10 (Figure 1b)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1516547646212,"tcdate":1515189308840,"number":6,"cdate":1515189308840,"id":"HkrrVdTQG","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"Sy4v3Bolz","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"Rebuttal for AnonReviewer3","comment":"1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong. \n>The gradient update rule is correct, we added explanation that the number of bits is treated as a real number for the purpose of calculating gradients. The update rule ensures integrality. The next post discusses the details.\n\n2-a. End-to-end trained quantized networks have been studied in various previous works... None of these works have been compared with.    \n>We cite these papers in our related work section. We would like the reviewer to note that the networks, in the papers referred to, are trained with a fixed (hand coded) number of bits. The research question we are answering is: What is the optimal number of bits? The research question these references answer which is: What is the performance given a certain fixed number of bits.\n\n2-b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc.   \n>We have added baseline experiments using 4 and 16 bits (Figure 1 a&b). We have to fix the number of bits for these algorithms.    \n\n2-c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these.    \n>Most other approaches on low precision training such as (BinaryConnect by Courbariaux et al NIPS 2015) and (Soft weight sharing for NN Compression by Ullrich et. al. 2017) only compare on these simple datasets. \n\n2-d. No wall-time and real memory numbers are reported.\n>We are unclear whether this is regarding training or inference time. There is about 4X savings. Re inference time: a meaningful comparison requires hardware support for low precision operations. This is currently unavailable for arbitrary precision."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1516547635044,"tcdate":1515189212929,"number":5,"cdate":1515189212929,"id":"B1S1NdT7G","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"Bk1-V2plG","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"Rebuttal for AnonReviewer2","comment":"1- It's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss)... it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit. \n>We have clarified this in the current revision. It is reasonable to assume that the classification accuracy is similar for similar values of the parameters. The error due to the local linear approximation drops at a rate of 1/2^B. In the worst case, we use a fine grained approximation using 32 bits. This is clearly not needed as we show in the experiments, that a 5-6 bit local linear approximation gives good accuracy. \n\n2- It's unclear to me how effectively accuracy and precision are balanced by this training strategy.  \n>We have clarified this in the current revision. As we show in our experiments, the accuracy and precision trade-off varies with different values for \\lambda_1 and \\lambda_2. \n\n3-The results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful.   \n>We believe the reviewer is referring to Table 1. These are after 30 epochs only for MNIST. Our final error rate was 3% on MNIST. Please note that we are using a small learning rate (1e-3) in order to show the big impact of bit regularization. In Figure 3(a)(right panel), we showed that increasing the learning rate does indeed improve the performance to about a 2% error rate. \n\n>In the initial submission, we showed LeNet32 and BitNet with about 4% test error rate. We found that our low performance on MNIST was also caused by using 4X4 pooling layers. We have changed this experiment to use a 2X2 pooling and now show error rates of 2-3% on MNIST (Figure 1a) and 27-29% on CIFAR-10 (Figure 1b).  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1516547618280,"tcdate":1515189114094,"number":4,"cdate":1515189114094,"id":"HkGFQdT7z","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"General Rebuttal","comment":"We thank the reviewers for their feedback.  AnonReviewer1 clearly understood the paper saying \"The idea is introduced clearly and rather straightforward.\" AnonReviewer2 also seemed to fully understand the paper and its contributions saying \"the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful.\" AnonReviewer3 missed a couple of key points in our approach, which made the reviewer think the formulation is incorrect.  \n\nThis revision contains the following modifications and we reference these modifications accordingly in each of the reviewer's rebuttal:\n1- All three reviewers raised the following concerns about the experiments:\nThe performance on MNIST and CIFAR doesn't match the state-of-the-art. \n>We found that our performance can be improved by using 2X2 pooling instead of 4X4 pooling. Please note that our experiments use a small learning rate (1e-3) because it better distinguishes BitNet from LeNet. We showed in Figure 3(a)(right panel) that increasing the learning rate does lead to better accuracy. We have changed this experiment and now show error rates of 2-3% on MNIST (Figure 1a) and 27-29% on CIFAR-10 (Figure 1b).\n\n2- AnonReviewer3 mentioned that there was no variation in base line experiments with regards the number of bits ( only focusing on 8bits)\n>We added baseline experiments using 4 and 6 bits (Figure 1a and 1b).  \n\n3- Overall clarity of the text and formulation\n>We added overall text and formulation clarification \n1- Added explanation that the number of bits is treated as a real number for the purpose of calculating gradients. \n2- Added the closed form of the gradients of the quantization error wrt the number of bits and the weights. \n3- Expanded gradients in Eq (5) to show gradients wrt the terms of the proposed loss function.\n4- We added some clarifying text towards the end of Section 4."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512406433422,"tcdate":1512406397879,"number":2,"cdate":1512406397879,"id":"B1IKaxXbf","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"Sy4v3Bolz","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"Re: The update rule seems to be clearly wrong.","comment":"Re: 1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. \nThis seems to be mistaken. Internally B(i) is a real number that is restricted to take integral values through this update rule using the sign function. \n\nIt is easy to see how the gradient of L(tilde_W) is computed wrt B(i), ie. the gradient of q(W,B(i)) wrt B(i).  (n.b. q(.) is continuous and piecewise differentiable). To differentiate q wrt B(i) we need to differentiate tilde_W wrt B(i).\n\nFor a fixed W, you can write tilde_W as a case expression with outputs \\alpha+1\\delta, \\alpha+2\\delta etc for different conditions for w \\in W.\ndtilde_W/db is differentiated piecewise, and same as d\\delta/db, \\delta is a function of 1/(2**B(i)).\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642375908,"tcdate":1512059895268,"number":3,"cdate":1512059895268,"id":"Bk1-V2plG","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Review","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea; insufficient analysis of training methodology and concerning empirical work","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used). Updates are made to the parameter representing the # of bits via the sign of its gradient. Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10.\n\nOverall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. I have a few concerns: First, I find the discussion around the training methodology insufficient. Inherently, the objective is discontinuous since # of bits is a discrete parameter. This is worked around by updating the parameter using the sign of its gradient. This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume.\n\nIt's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss). We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either.\n\nI would be less concerned about the above points if I found the experiments compelling. Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful. Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad. Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve.\n\nOverall, while I like the and think the goal is good, I think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning. I can't recommend acceptance. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642375963,"tcdate":1511902300091,"number":2,"cdate":1511902300091,"id":"Sy4v3Bolz","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Review","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The learning procedure seems to be wrong; experiments not comprehensive.","rating":"3: Clear rejection","review":"This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4.\n\n1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong.\n2. The experimental section of this paper needs improvement.\n   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with.\n   b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc.\n   c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these.\n   d. No wall-time and real memory numbers are reported.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642376020,"tcdate":1511694787385,"number":1,"cdate":1511694787385,"id":"Syi6-muxf","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Review","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper focuses on how to train low-bit nets directly which is important. However, more experiments on large datasets are need to demonstrate the effectiveness of the proposed method.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a direct way to learn low-bit neural nets. The idea is introduced clearly and rather straightforward.\n\npros:\n(1) The idea is introduced clearly and rather straightforward.\n(2) The introduction and related work are well written.\n\ncons:\nThe provided experiments are weak to demonstrate the effectiveness of the proposed method.\n(1) only small networks on relatively small datasets are tested.\n(2) the results on MNIST and CIFAR 10 are not good enough for practical deployment.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512075405523,"tcdate":1511464708321,"number":1,"cdate":1511464708321,"id":"H12bJi4eM","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"BkchXu6yG","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"Re: implementation","comment":"Thank you for your comment. We have not open sourced our implementation yet. We will get back to you after the review period."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510994866353,"tcdate":1510994866353,"number":1,"cdate":1510994866353,"id":"BkchXu6yG","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Public_Comment","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Source code & implementation details","comment":"Hello, I am working on reproducing your work for the ICLR 2018 Reproducibility Challenge. I was wondering if/when you plan to open source the code used to perform your experiments. \nI would also appreciate if you could provide more details on the model architectures used by you. \n \nThanks and best regards"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515189482604,"tcdate":1509137724175,"number":1000,"cdate":1510092360768,"id":"HJg1NTGZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/44f6c137d8fae448054ca18d348ebcfa249a4c32.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}