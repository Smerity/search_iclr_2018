{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222536243,"tcdate":1512059895268,"number":3,"cdate":1512059895268,"id":"Bk1-V2plG","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Review","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea; insufficient analysis of training methodology and concerning empirical work","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion. The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used). Updates are made to the parameter representing the # of bits via the sign of its gradient. Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10.\n\nOverall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful. I have a few concerns: First, I find the discussion around the training methodology insufficient. Inherently, the objective is discontinuous since # of bits is a discrete parameter. This is worked around by updating the parameter using the sign of its gradient. This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume.\n\nIt's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss). We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision. But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term). This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either.\n\nI would be less concerned about the above points if I found the experiments compelling. Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful. Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad. Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve.\n\nOverall, while I like the and think the goal is good, I think the motivation and discussion for the training methodology is insufficient, and the empirical work is concerning. I can't recommend acceptance. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/535907a85d12776d0535edc83adf5509084b2f86.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222536295,"tcdate":1511902300091,"number":2,"cdate":1511902300091,"id":"Sy4v3Bolz","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Review","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The learning procedure seems to be wrong; experiments not comprehensive.","rating":"3: Clear rejection","review":"This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits. While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4.\n\n1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete. The update rule seems to be clearly wrong.\n2. The experimental section of this paper needs improvement.\n   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc. None of these works have been compared with.\n   b. All the baseline methods use 8 bits per value. This choice is quite ad-hoc.\n   c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment. I find the findings not conclusive based on these.\n   d. No wall-time and real memory numbers are reported.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/535907a85d12776d0535edc83adf5509084b2f86.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222536336,"tcdate":1511694787385,"number":1,"cdate":1511694787385,"id":"Syi6-muxf","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Review","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference/Paper1000/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper focuses on how to train low-bit nets directly which is important. However, more experiments on large datasets are need to demonstrate the effectiveness of the proposed method.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a direct way to learn low-bit neural nets. The idea is introduced clearly and rather straightforward.\n\npros:\n(1) The idea is introduced clearly and rather straightforward.\n(2) The introduction and related work are well written.\n\ncons:\nThe provided experiments are weak to demonstrate the effectiveness of the proposed method.\n(1) only small networks on relatively small datasets are tested.\n(2) the results on MNIST and CIFAR 10 are not good enough for practical deployment.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/535907a85d12776d0535edc83adf5509084b2f86.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512075405523,"tcdate":1511464708321,"number":1,"cdate":1511464708321,"id":"H12bJi4eM","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Official_Comment","forum":"HJg1NTGZRZ","replyto":"BkchXu6yG","signatures":["ICLR.cc/2018/Conference/Paper1000/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1000/Authors"],"content":{"title":"Re: implementation","comment":"Thank you for your comment. We have not open sourced our implementation yet. We will get back to you after the review period."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/535907a85d12776d0535edc83adf5509084b2f86.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510994866353,"tcdate":1510994866353,"number":1,"cdate":1510994866353,"id":"BkchXu6yG","invitation":"ICLR.cc/2018/Conference/-/Paper1000/Public_Comment","forum":"HJg1NTGZRZ","replyto":"HJg1NTGZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Source code & implementation details","comment":"Hello, I am working on reproducing your work for the ICLR 2018 Reproducibility Challenge. I was wondering if/when you plan to open source the code used to perform your experiments. \nI would also appreciate if you could provide more details on the model architectures used by you. \n \nThanks and best regards"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/535907a85d12776d0535edc83adf5509084b2f86.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092382636,"tcdate":1509137724175,"number":1000,"cdate":1510092360768,"id":"HJg1NTGZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJg1NTGZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bit-Regularized Optimization of Neural Nets","abstract":"We present a novel regularization strategy for training neural networks which we call ``BitNet''. The parameters of neural networks are usually unconstrained and have a dynamic range dispersed over a real valued range. Our key idea is to control the expressive power of the network by dynamically quantizing the range and set of values that the parameters can take. We formulate this idea using a novel end-to-end approach  that regularizes a typical classification loss function. Our regularizer is inspired by the Minimum Description Length (MDL) principle. For each layer of the network, our approach optimizes a translation and scaling factor along with integer-valued parameters. We empirically compare BitNet to an equivalent unregularized model on the MNIST and CIFAR-10 datasets. We show that BitNet converges faster to a superior quality solution. Additionally, the resulting model is significantly smaller in size due to the use of integer instead of floating-point parameters.","pdf":"/pdf/535907a85d12776d0535edc83adf5509084b2f86.pdf","paperhash":"anonymous|bitregularized_optimization_of_neural_nets","_bibtex":"@article{\n  anonymous2018bit-regularized,\n  title={Bit-Regularized Optimization of Neural Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJg1NTGZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1000/Authors"],"keywords":[]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}