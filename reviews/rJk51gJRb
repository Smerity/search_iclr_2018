{"notes":[{"ddate":null,"tddate":1512326806563,"tmdate":1512326908211,"tcdate":1512326774808,"number":2,"cdate":1512326774808,"id":"rkJtIabWM","invitation":"ICLR.cc/2018/Conference/-/Paper113/Official_Comment","forum":"rJk51gJRb","replyto":"ByzeYntef","signatures":["ICLR.cc/2018/Conference/Paper113/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper113/Authors"],"content":{"title":"computation cost","comment":"The reviewer is concerned about the computation cost for each training. Actually, in our experiments, training is very fast, took only a few hours on 9x9 and 11x11 Hex. Since all training/evaluation are conducted on the same computer with a single GTX 1080 GPU. We briefly list the detailed training time for each method here: \n  \n9x9 Hex:  total time usage for 400 iterations training: \nAMCPG-A: k=1: about 1 h 40 m,  k=3: about 2.5 h,  k=6: 4h 10 minutes, k=9: about 6h\nAMCPG-B: k=1:  similar as above\nREINFORCE-B:  similar as above\nREINFORCE-A: 1 hour 15 minutes\nREINFORCE-V: 1 hour 20 minutes\n\n11x11 Hex: \nAMCPG-A: k=1: about 3h15 minutes, k=3: about 5h, k=6: about 9h, k=9: about 12h\nAMCPG-B: k=1:  similar as above\nREINFORCE-B:  similar as above,\nREINFORCE-A: 2.5 hours\nREINFORCE-V: 2.5 hours\n\nIn fact, most of our time was not spending on training the neural net, but for evaluation the neural net model by playing against Wolve, as Wolve's search is orders of magnitude slower than a pure neural net player. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Policy Gradient for Alternating Markov Games","abstract":"Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, e.g., in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of its two-agent nature. We exploit the difference in associated Bellman-equations, which leads to different policy iteration algorithms. In principle, as policy gradient method is a kind of generalized policy iteration, we show how these differences in policy iteration are reflected in policy gradient for AMGs. We reformulate an adversarial policy gradient and discuss potential possibilities for developing better policy gradient methods other than self-play REINFORCE. Specifically, the core idea is to estimate the minimum rather than the mean for the “critic”. We show by experimental results that our newly developed Monte-Carlo policy gradient methods are better than the former methods.","pdf":"/pdf/da9aed7d4eb8db623335244c0acb08fd3a256acb.pdf","paperhash":"anonymous|adversarial_policy_gradient_for_alternating_markov_games","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Policy Gradient for Alternating Markov Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJk51gJRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper113/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512343370725,"tcdate":1512326718530,"number":1,"cdate":1512326718530,"id":"SkLrUaZWG","invitation":"ICLR.cc/2018/Conference/-/Paper113/Official_Comment","forum":"rJk51gJRb","replyto":"ByzeYntef","signatures":["ICLR.cc/2018/Conference/Paper113/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper113/Authors"],"content":{"title":"By HexIt, you mean ExIt?","comment":"Thanks for your comment. \n\nI will first response the reviewer's comment about \"Hexit\" (if you mean ExIt) and AlphaGo Zero. \nI want to clarify that all our experiments were carried out on a Desktop computer with one GTX 1080 GPU, i7 6700 CPU and 32 GB RAM.\n\nThe methods described in ExIt paper and AlphaGo Zero are similar. They works well but one significant problem is the computation cost is very high. \n\nIndeed, with only one GPU, since Alphago Zero came out, there has been a continuing discussion of the difficulty of replicating Zero's result in computer Go group. Given the fact that TPUs are orders of magnitude faster than GPUs for inference, it can be estimated that reproducing the full-strength Alphago Zero on a single GPU computer may take > 1000 years. \n\nhttp://computer-go.org/pipermail/computer-go/2017-October/010307.html\n\n-------\nFor ExIt: \nBy the time our paper is submitted, only first version is available on arxiv, though we are aware their work has been accepted in NIPS 2017. \n https://arxiv.org/abs/1705.08439\n\nThey did all experiments on 9x9 Hex. In the first version, they claim that that their final MCTS+NN player achieves better playing result than REINFORCE. This is, however, unsurprising since their approach follows the same ''heavy self-play” of a tree-search (as AlphaGo Zero did). Also, the result is obtained from a NN augmented  MCTS player, not pure neural net player. \n\nHowever, we do notice that in the recent updated version, after adding a “value net”, there is an additional claim that their final MCTS+policy_value_net player achieves better playing strength than the “state-of-the-art player MoHex”. \n\nThis new results looks impressive. However, the claim is not entirely true. \nFirst. The MoHex they were using is the 2011 version (see page 16 footnote, also for people who know recent MoHex, the first White move in Figure 6 of Page 16 looks pretty strange.).\n\nThe 2011 MoHex is a straightforward implementation of MCTS to Hex. The rollout policy in MoHex 2011 is mostly uniform random playout. \n \nSee paper “Monte Carlo  Tree Search in Hex” for a full description of MoHex 2011. \nhttps://webdocs.cs.ualberta.ca/~hayward/papers/mcts-hex.pdf\n\nAfter 2011, there have bee several significant improvements of MoHex, most notably:\n1. Stronger Virtual Connection is developed.\nhttps://webdocs.cs.ualberta.ca/~hayward/papers/strongervcs.pdf\n\n2. Parallel DFPN solver is developed.\nhttps://webdocs.cs.ualberta.ca/~hayward/papers/pawlhayw.pdf\n --In this paper, it is reported that all 9x9 Hex openings have been solved. In some sense, cheap and perfect play on 9x9 Hex might be achieved by following the discovered proof tree (graph). \n\n3. MoHex 2.0 is developed.\nhttps://webdocs.cs.ualberta.ca/~hayward/papers/m2.pdf\n --In MoHex 2.0 paper, see Section 5. MoHex 2.0 winrates against MoHex 2011 is >80%. \n\nMoHex 2011 is far inferior to later MoHex 2.0 for playing, let alone that 9x9 Hex has been solved by the solver that has been embedded to MoHex 2.0. \n\n-----We are very familiar with MoHex. All the above has also been directly forwarded to the authors of ExIt. Our email contact with the authors of “ExIt” confirmed that they are indeed using the 2011 code. They expressed that they will alter the claim. \n\nOne excusable reason for this is that the documentation of MoHex is relatively poor. People who are not familiar with the progress of Computer Hex development might not be able to easily discern that repository is out-of-dated.\n\nGo back to the difference between our approach and ExIt.  As said, ExIt works similarly as AlphaGo Zero. Each iteration, it uses MCTS to generate training data, and optimize the neural on those data.\n\nSimilarly, ExIt suffers from the same problem as AlphaGo Zero. As one can verify this from their paper (Section 3.2). \nExIt only conducts experiments on 9x9 Hex. It is not very clear how much time could be used to produce significant results on larger board size, such as 11x11, presumably, this is not a easy task with only one GPU computer. \n\nBy contrast, our AMCPG-A or AMCPG-B follows traditional “light self-play”. No tree was built. To estimate the “minimum” critic, extra roll-outs are conducted. But it is very much due to the Monte-Calro nature of the method, and that is why we mention an actor-critic style might be more efficient.\n\nBut it is unfair to say that our better results compared to classic REINFORCE is merely due to extra roll-outs.  One can see that in REINFORCE-B, extra roll-outs are also conducted the same way as AMCPG-A and AMCPG-B.  Their extra computation costs due to extra roll-out are the same.  However, the results in Figure 2 suggests that REINFORCE-B has similar performance as REINFORCE-A and REINFORCE-V.  \n\nAlso, the final ExIt is a MCTS+NN player, not a pure neural net.  As a comparison, in the Zero paper, Figure 6 suggests that raw NN without MCTS is about 2000 Elo weaker than the NN+MCTS. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Policy Gradient for Alternating Markov Games","abstract":"Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, e.g., in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of its two-agent nature. We exploit the difference in associated Bellman-equations, which leads to different policy iteration algorithms. In principle, as policy gradient method is a kind of generalized policy iteration, we show how these differences in policy iteration are reflected in policy gradient for AMGs. We reformulate an adversarial policy gradient and discuss potential possibilities for developing better policy gradient methods other than self-play REINFORCE. Specifically, the core idea is to estimate the minimum rather than the mean for the “critic”. We show by experimental results that our newly developed Monte-Carlo policy gradient methods are better than the former methods.","pdf":"/pdf/da9aed7d4eb8db623335244c0acb08fd3a256acb.pdf","paperhash":"anonymous|adversarial_policy_gradient_for_alternating_markov_games","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Policy Gradient for Alternating Markov Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJk51gJRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper113/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222558084,"tcdate":1511821099855,"number":3,"cdate":1511821099855,"id":"SkNEyzcxG","invitation":"ICLR.cc/2018/Conference/-/Paper113/Official_Review","forum":"rJk51gJRb","replyto":"rJk51gJRb","signatures":["ICLR.cc/2018/Conference/Paper113/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A nice but somewhat minimal paper addressing caveats of existing adversarial RL attempts","rating":"5: Marginally below acceptance threshold","review":"The paper makes the simple but important observation that (deep) reinforcement learning in alternating Markov games requires a min-max formulation of the Bellman equation as well as careful attention to the way in which one alternates solving for both players' policies in a policy iteration setting.\n\nWhile some of the core algorithmic insights regarding Algorithms 3 & 4 in the paper stem from previous work (Condon, 1990; Hoffman & Karp, 1966), I was not actually aware of these previous results until I reviewed this paper.\n\nA nice corollary of Algorithms 3 & 4 is that they make for a straightforward adaptation of policy gradient algorithms since when optimizing one policy, the other is fixed to the greedy policy.\n\nIn general, it would be nice to have the algorithms specified as formal algorithms as opposed to text-based outlines.  I found myself reading and re-reading descriptions to make sure I understood what math was being implied by the descriptions.\n\nSection 6\n\n> Hex is simpler than Go in the sense that perfect play can \n> often be achieved whenever virtual connections are found \n> by H-Search\n\nIt is not clear here what virtual connections are, what H-Search is, and how these imply perfect play, if perfect play as previously discussed is unknown.\n\nOverall, the results on Hex for AMCPG-A and AMCPG-B vs. standard REINFORCE variants currently used are very encouraging.  That said, empirically it is always a question of whether these results are specific to Hex.  Because this paper is not proposing the best Hex player (i.e., the winning rate against Wolve never exceeds 0.5), I think it is quite reasonable to request the authors to compare AMCPG-A and AMCPG-B to standard REINFORCE variants on other games (they do not need to be as difficult as Hex).\n\nFinally, assuming that the results do generalize to other games, I am left wondering about the significance of the contribution.  On one hand, the authors have introduced me to literature I was not aware of, but on the other hand, their actual novel contribution is a rather straightforward adaptation of ideas in the literature to policy gradients (that could be formalized in a more technically precise way) with an evaluation on a single type of game.  This is a useful contribution no doubt, but I am concerned with whether it meets the significance level that I am used to with accepted ICLR papers in previous years.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Policy Gradient for Alternating Markov Games","abstract":"Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, e.g., in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of its two-agent nature. We exploit the difference in associated Bellman-equations, which leads to different policy iteration algorithms. In principle, as policy gradient method is a kind of generalized policy iteration, we show how these differences in policy iteration are reflected in policy gradient for AMGs. We reformulate an adversarial policy gradient and discuss potential possibilities for developing better policy gradient methods other than self-play REINFORCE. Specifically, the core idea is to estimate the minimum rather than the mean for the “critic”. We show by experimental results that our newly developed Monte-Carlo policy gradient methods are better than the former methods.","pdf":"/pdf/da9aed7d4eb8db623335244c0acb08fd3a256acb.pdf","paperhash":"anonymous|adversarial_policy_gradient_for_alternating_markov_games","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Policy Gradient for Alternating Markov Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJk51gJRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper113/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222558131,"tcdate":1511799018060,"number":2,"cdate":1511799018060,"id":"ByzeYntef","invitation":"ICLR.cc/2018/Conference/-/Paper113/Official_Review","forum":"rJk51gJRb","replyto":"rJk51gJRb","signatures":["ICLR.cc/2018/Conference/Paper113/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A bit verbose on existing methods + notations and low on experiments","rating":"5: Marginally below acceptance threshold","review":"This paper introduces a variation over existing policy gradient methods for two players zero sum games, in which instead of using the outcome of a single policy network rollout as the return, they use the minimum outcome among a few rollouts either from the original position or where the first action from that position is selected uniformly among the top k policy outputs. \n\nThe proposed method supposedly provides slightly stronger targets, due to the extra lookahead / rollouts. Experiments show that this provides faster progress per iteration on the game of Hex against a fixed third party opponent.\n\nThere is no comparison against state of the art methods like AlphaGo Zero which uses MCTS root move distribution and MCTS rollouts outcome to train policy and value network, even though the author do cite this work. There is also no comparison with Hexit which also trains policy net on MCTS move distribution, and was also applied to Hex.\n\nThe actual proposed method is actually a one liner change, which could be introduced much sooner in the paper to save the reader some time. While the idea is interesting, the paper felt quite verbose on introducing notations and related work, and a bit lacking on actual change that is being proposed and the experiment to back it up.\n\nFor example,  was it really necessary to introduce state transition probabilities p(s’, a, s) when all the experiments are done in the deterministic game of Hex ?\n\nAlso the experiment seems not fully fair to the reinforce baseline. My understand is that the proposed method is much more costly due to extra rollouts that are needed. It would be interesting to see the same learning curves as in Figure 2, but the x axis would be some computational budget (total number of network forward, or wall clock time). It is conceivable that the vanilla reinforce would do just as well as the proposed method if the plots were aligned this way. It would also be good to know the asymptotic behavior.\n\nSo even though the idea is interesting, it seems that much stronger methods AlphaGo Zero / Hexit are now available, and the experimental section is a bit weak. I would recommend to accept for a workshop paper but not sure about the main track.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Policy Gradient for Alternating Markov Games","abstract":"Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, e.g., in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of its two-agent nature. We exploit the difference in associated Bellman-equations, which leads to different policy iteration algorithms. In principle, as policy gradient method is a kind of generalized policy iteration, we show how these differences in policy iteration are reflected in policy gradient for AMGs. We reformulate an adversarial policy gradient and discuss potential possibilities for developing better policy gradient methods other than self-play REINFORCE. Specifically, the core idea is to estimate the minimum rather than the mean for the “critic”. We show by experimental results that our newly developed Monte-Carlo policy gradient methods are better than the former methods.","pdf":"/pdf/da9aed7d4eb8db623335244c0acb08fd3a256acb.pdf","paperhash":"anonymous|adversarial_policy_gradient_for_alternating_markov_games","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Policy Gradient for Alternating Markov Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJk51gJRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper113/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222558170,"tcdate":1511452817325,"number":1,"cdate":1511452817325,"id":"rJFql_Nxz","invitation":"ICLR.cc/2018/Conference/-/Paper113/Official_Review","forum":"rJk51gJRb","replyto":"rJk51gJRb","signatures":["ICLR.cc/2018/Conference/Paper113/AnonReviewer1"],"readers":["everyone"],"content":{"title":"n/a","rating":"5: Marginally below acceptance threshold","review":"This paper is outside of my area of expertise, so I'll just provide a light review:\n\n- the idea of assuming that the opponent will take the worst possible action is reasonable in widely used in classic search, so making value functions follow this intuition seems sensible,\n- but somehow I wonder if this is really novel? Isn't there a whole body of literature on fictitious self-play, including need RL variants (e.g. Heinrich&Silver, 2016) that approaches things in a similar way?\n- the results on Hex have some signal, but I don’t know how to calibrate them w.r.t. The state of the art on that game? A 40% win rate seems low, what do other published papers based on RL or search achieve?\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Policy Gradient for Alternating Markov Games","abstract":"Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, e.g., in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of its two-agent nature. We exploit the difference in associated Bellman-equations, which leads to different policy iteration algorithms. In principle, as policy gradient method is a kind of generalized policy iteration, we show how these differences in policy iteration are reflected in policy gradient for AMGs. We reformulate an adversarial policy gradient and discuss potential possibilities for developing better policy gradient methods other than self-play REINFORCE. Specifically, the core idea is to estimate the minimum rather than the mean for the “critic”. We show by experimental results that our newly developed Monte-Carlo policy gradient methods are better than the former methods.","pdf":"/pdf/da9aed7d4eb8db623335244c0acb08fd3a256acb.pdf","paperhash":"anonymous|adversarial_policy_gradient_for_alternating_markov_games","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Policy Gradient for Alternating Markov Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJk51gJRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper113/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739477020,"tcdate":1508994950579,"number":113,"cdate":1509739474362,"id":"rJk51gJRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJk51gJRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adversarial Policy Gradient for Alternating Markov Games","abstract":"Policy gradient reinforcement learning has been applied to two-player alternate-turn zero-sum games, e.g., in AlphaGo, self-play REINFORCE was used to improve the neural net model after supervised learning. In this paper, we emphasize that two-player zero-sum games with alternating turns, which have been previously formulated as Alternating Markov Games (AMGs), are different from standard MDP because of its two-agent nature. We exploit the difference in associated Bellman-equations, which leads to different policy iteration algorithms. In principle, as policy gradient method is a kind of generalized policy iteration, we show how these differences in policy iteration are reflected in policy gradient for AMGs. We reformulate an adversarial policy gradient and discuss potential possibilities for developing better policy gradient methods other than self-play REINFORCE. Specifically, the core idea is to estimate the minimum rather than the mean for the “critic”. We show by experimental results that our newly developed Monte-Carlo policy gradient methods are better than the former methods.","pdf":"/pdf/da9aed7d4eb8db623335244c0acb08fd3a256acb.pdf","paperhash":"anonymous|adversarial_policy_gradient_for_alternating_markov_games","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Policy Gradient for Alternating Markov Games},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJk51gJRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper113/Authors"],"keywords":[]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}