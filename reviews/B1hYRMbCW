{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222547948,"tcdate":1512041111253,"number":3,"cdate":1512041111253,"id":"ry1j9wpgz","invitation":"ICLR.cc/2018/Conference/-/Paper1066/Official_Review","forum":"B1hYRMbCW","replyto":"B1hYRMbCW","signatures":["ICLR.cc/2018/Conference/Paper1066/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A too preliminary discussion of ways of regularizing Gans with the L_1 Wasserstein metric","rating":"6: Marginally above acceptance threshold","review":"The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric. Basics on mass transportation are briefly recalled in section 2, while section 3 formulate the GANs approach in the Wasserstein context. Taking into account the Lipschitz constraint and (non-) differentiability of optimal critic functions f are discussed in section 4 and Section 5 proposes a way to penalize candidate functions f that do not satisfy the Lipschitz condition using a tuning parameter lambda, ruling a trade-off between marginal fitting and gradient control. The approach is illustrated by numerical experiments. Such results are hardly convincing, since the tuning of the parameter lambda plays a crucial role in the performance of the method. More importantly, The heuristic proposed in the paper is interesting and promising in some respects but there is a real lack of theoretical guarantees motivating the penalty form chosen, such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the regularization of Wasserstein GANs","abstract":"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets.","pdf":"/pdf/5322df3532cfaefb2e27f5e75ac8f9784b494363.pdf","TL;DR":"A new regularization term can improve your training of wasserstein gans","paperhash":"anonymous|on_the_regularization_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018on,\n  title={On the regularization of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hYRMbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1066/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222547984,"tcdate":1511770115774,"number":2,"cdate":1511770115774,"id":"ry2WdrtgM","invitation":"ICLR.cc/2018/Conference/-/Paper1066/Official_Review","forum":"B1hYRMbCW","replyto":"B1hYRMbCW","signatures":["ICLR.cc/2018/Conference/Paper1066/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice regularization term for WGAN with strong relation to regularized OT ","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1. The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss. The reasons for this choice are discussed and linked to theoretical properties of OT. Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset.\n\nThe paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea. Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport.\n\nDiscussion:\n\n+ The paper spends a lot of time justifying the proposed method by discussing the limits of the \"Improved training of Wasserstein GAN\" from Gulrajani et al. (2017). The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix. The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next).\n\n+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem. As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss. This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture.\n\n+ The paper is rather vague on the reason to go from Eq. (6) to Eq. (7). (gradient approximation between samples to gradient on samples). Does it lead to better stability to choose one or the other? \n How is it implemented in practice? recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix. Numerical experiments comparing the two implementation or at least a discussion is necessary.\n\n+ The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization. I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve. A discussion of the links is necessary and will clearly bring more theoretical ground to the method. Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization. In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that.\n\n+ The numerical experiments are encouraging but a bit short. The 2D example seem to work very well and the convergence curves are far better with the proposed regularization. But the real data CIFAR experiments are much less detailed with only a final inception score (very similar to the competing method) and no images even in appendix. The authors should also define (maybe in appendix) the conditional and unconditional inception scores and why they are important (and why only some of them are computed in Table 1).\n\n+ This is more of a suggestion. The comparison of the dual critic to the true Wasserstein distance is very interesting. It would be nice to see the behavior for different values of lambda.\n\n\n[1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the regularization of Wasserstein GANs","abstract":"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets.","pdf":"/pdf/5322df3532cfaefb2e27f5e75ac8f9784b494363.pdf","TL;DR":"A new regularization term can improve your training of wasserstein gans","paperhash":"anonymous|on_the_regularization_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018on,\n  title={On the regularization of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hYRMbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1066/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222548028,"tcdate":1511709653003,"number":1,"cdate":1511709653003,"id":"r1aAjU_xG","invitation":"ICLR.cc/2018/Conference/-/Paper1066/Official_Review","forum":"B1hYRMbCW","replyto":"B1hYRMbCW","signatures":["ICLR.cc/2018/Conference/Paper1066/AnonReviewer2"],"readers":["everyone"],"content":{"title":"On the regularization of Wasserstein GANs","rating":"2: Strong rejection","review":"This paper is proposing a new formulation for regularization of Wasserstein Generative Adversarial models (WGAN). The original min/max formulation of the WGAN aim at minimizing over all measures, the maximal dispersion of expectation for 1-Lipschitz with the one provided by the empirical measure. This problem is often regularized by adding a \"gradient penalty\", \\ie a  penalty of the form \"\\lambda E_{z~\\tau}}(||\\grad f (z)||-1)^2\" where \\tau is the distribution of (tx+(1-x)y) where x is drawn according to the empirical measure and y is drawn according to the target measure. In this work the authors consider substituting the previous penalty by \"\\lambda E_{z~\\tau}}(max( ||\\grad f (z)||-1,0)^2\".\n\nOverall the paper is too vague on the mathematical part, and the experiments provided are not particularly convincing in assessing the benefit of the new penalty.\nThe authors have tried to use mathematical formulations to motivate their choice, but they lack rigorous definitions/developments to make their point convincing.\nThey should also present early their model and their mathematical motivation: in what sense is their new penalty \"preferable\"?\n\n\n\nPresentation issues:\n- in printed black and white versions most figures are meaningless.\n- red and green should be avoided on the same plots, as colorblind people will not perceived any difference...\n- format for images should be vectorial (eps or pdf), not jpg or png...\n- legend/sizes are not readable (especially in printed version).\n\nReferences issues:\n- harmonize citations: if you add first name for some authors add them for all of them: why writing Harold W. Kuhn and C. Vilani for instance?\n- cramer->Cramer\n- wasserstein->Wasserstein (2x)\n- gans-> GANs\n- Salimans et al. is provided twice, and the second is wrong anyway.\n\n\n\nSpecific comments:\n\npage 1:\n- \"different more recent contributions\" -> more recent contributions\n- avoid double brackets \"))\"\n\npage 2:\n- Please rewrite the first sentence below Definition 1 in a meaningful way.\n- Section 3: if \\mu is an empirical distribution, it is customary to write it \\mu_n or \\hat \\mu_n (in a way that emphasizes the number of observations available).\n- d is used as a discriminator and then as a distance. This is confusing...\n\npage 3:\n- \"f that plays the role of an appraiser (or critic)...\": this paragraph could be extended and possibly elements of the appendix could be added here.\n- Section 4: the way clipping is presented is totally unclear and vague. This should be improved.\n- Eq (5): as written the distribution of \\tilde{x}=tx+(1-t)y is meaningless: What is x and y in this context? please can you describe the distributions in a more precise way?\n- Proof of Proposition 5 (cf. page 13): this is a sketch of proof to me. Please state precise results using mathematical formulation.\n- \"Observation 1\": real and generated data points are not introduced at this stage... data points are not even introduced neither!\n\npage 5:\n- the examples are hard to understand. It would be helpful to add the value of \\pi^* and f^* for both models, and explaining in details how they fit the authors model.\n- in Figure 2 the left example is useless to me. It could be removed to focus more extensively on the continuous case (right example).\n- the the -> the\n\npage 6:\n- deterministic coupling could be discussed/motivated when introduced. Observation 3 states some property of non non-deterministic coupling but the concept itself seems somehow to appear out of the blue.\n\npage 10:\n- Figure 6: this example should be more carefully described in terms of distribution, f*, etc.\n\npage 14:\n- Proposition 1: the proof could be shorten by simply stating in the proposition that f and g are distribution...\n\npage 15:\n- \"we wish to compute\"-> we aim at showing?\n- f_1 is not defined sot the paragraph \"the latter equation...\" showing that almost surely x \\leq y is unclear to me, so is the result then.\nIt could be also interesting to (geometrically) interpret the coupling proposed. The would help understanding the proof, and possibly reuse the same idea in different context.\n\npage 16:\n- proof of Proposition 2 : key idea here is using the positive and negative part of (f-g). This could simplify the proof.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the regularization of Wasserstein GANs","abstract":"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets.","pdf":"/pdf/5322df3532cfaefb2e27f5e75ac8f9784b494363.pdf","TL;DR":"A new regularization term can improve your training of wasserstein gans","paperhash":"anonymous|on_the_regularization_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018on,\n  title={On the regularization of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hYRMbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1066/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092381463,"tcdate":1509138072271,"number":1066,"cdate":1510092360269,"id":"B1hYRMbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1hYRMbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the regularization of Wasserstein GANs","abstract":"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. Augmenting the loss by a regularization term that penalizes the deviation of the gradient norm of the critic (as a function of the network's input) from one, was proposed as an alternative that improves training. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets.","pdf":"/pdf/5322df3532cfaefb2e27f5e75ac8f9784b494363.pdf","TL;DR":"A new regularization term can improve your training of wasserstein gans","paperhash":"anonymous|on_the_regularization_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018on,\n  title={On the regularization of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hYRMbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1066/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}