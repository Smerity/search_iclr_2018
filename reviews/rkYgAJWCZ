{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222680893,"tcdate":1511851529032,"number":3,"cdate":1511851529032,"id":"Sybz8F9eG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Review","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference/Paper532/AnonReviewer3"],"readers":["everyone"],"content":{"title":"5: Marginally below the acceptance threshold.","rating":"5: Marginally below acceptance threshold","review":"Paper Summary\n\nFrom just seeing a word used in a sentence, humans can infer a lot about this word by leveraging the surrounding words. Based on this idea, this work tries to obtain a better understanding of words in the one-shot or few-shot setting by leveraging surrounding word. They do this by language modeling sentences which contain rarely seen or never seen words. They evaluated their model using percent change in perplexity on test sentences containing new word by varying the number of training sentences containing this word. 3 Proposed Methods to model few-shot words: (1) beginning with random embedding, (2) beginning with zero embedding (3) beginning with the centroid of other words in the sentence. They compare to 2 Baseline Methods: (1) centroid of other words in the sentence, and (2) full training including the sparse words. Their results show that learning from centroids of other words can outperform full training on the new words. \n\nExplanation\nThe paper is well written, and the experiments are well explained.  It is an interesting paper, and a research topic which is not well studied. The experiments are really good. The method seems to work well. \n\nHowever, the method provides a very marginal difference between the previous method in Lazaridou et al. (2017). They just use backdrop to learn from this starting position. The main contribution of this work is the evaluation section. \n\nWhy only use the PTB language modeling task. Why not use the task in Gauthier & Mordatch or Hermann et al. The one task of language modeling shows promising results, but it’s not totally convincing. \n\nOne of the biggest caveats is that the experiments are only done in a few words. I’m not sure why more couldn’t have been done. This is discussed in section 4.1, but I think some of these differences could have been alleviated if there were more experiments done. Regardless, the experiments on the 8 words that they did chose were well done. \n\nI don’t think that section 3.3 (embedding similarity) is particularly useful. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1512222680931,"tcdate":1511806019750,"number":2,"cdate":1511806019750,"id":"HJnrNAtlG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Review","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference/Paper532/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"The paper proposes a technique for exploiting prior knowledge to learn embedding representations for new words with minimal data. The authors provide a good motivation for the task and it is also a nice step in the general direction of learning deep nets and other systems with minimal supervision. \n\nThe problem is useful and very relevant to natural language applications, especially considering the widespread use of word embeddings within NLP systems. However, the demonstrated experimental results do not match the claims which seems a little grand. Overall, the empirical results is unsatisfactory. The authors pick a few example words and provide a detailed analysis. This is useful to understand how the test perplexity varies with #training examples for these individual settings. However, it is hardly enough to draw conclusion about the general applicability of the technique or effectiveness of the results. Why were these specific words chosen? If the reason is due to some statistical property (e.g., frequency) observed in the corpus, then why not generalize this idea and demonstrate empirical results for a class of words exhibiting the property. Such an analysis would be useful to understand the effectiveness of the overall approach. Another idea would be to use the one/few-shot learning to learn embeddings and evaluate their quality on a semantic task (as suggested in Section 3.3), but on a larger scale.\n\nThe technical contributions are also not novel. Coupled with the narrow experimentation protocol, it does not make the paper’s contributions or proposed claims convincing.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1512222680973,"tcdate":1511601521811,"number":1,"cdate":1511601521811,"id":"Sk9dBhUlG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Review","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference/Paper532/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Worthwile goal of using backpropogation and weight-clamping to perform few-shot learning; questionable evaluation","rating":"4: Ok but not good enough - rejection","review":"I am highly sympathetic to the goals of this paper, and the authors do a good job of contrasting human learning with current deep learning systems, arguing that the lack of a mechanism for few-shot learning in such systems is a barrier to applying them in realistic scenarios. However, the main evaluation only considers four words - \"bonuses\", \"explained\", \"marketers\", \"strategist\" - with no explanation of how these words were chosen. Can I really draw any meaningful conclusions from such an experimental setup? Even the authors acknowledge, in footnote 1, that, for one of the tests, getting lower perplexity in three out of the four casess \"may just be chance variation, of course\". I wonder why we can't arrive at a similar conclusion for the other results in the paper. At the very least I need convincing that this is a reasonable experimental paradigm.\n\nI don't understand the first method for initializing the word embeddings. How can we use the \"current\" embedding for a word if it's never been seen before? What does \"current\" mean in this context?\n\nI also didn't understand the Latin square setup. Training on ten different permutations of the ten sentences suggests that all ten sentences are being used, so I don't see how this can lead to a few-shot or one-shot scenario.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1511338047185,"tcdate":1511338047185,"number":2,"cdate":1511338047185,"id":"ByDSg2Ggz","invitation":"ICLR.cc/2018/Conference/-/Paper532/Public_Comment","forum":"rkYgAJWCZ","replyto":"rJEcBKfxG","signatures":["~Marco_Baroni1"],"readers":["everyone"],"writers":["~Marco_Baroni1"],"content":{"title":"Thanks for the clarification","comment":"Thanks for the clarification. I fully agree that, while the papers ask similar questions and propose similar approaches, yours is going further in various empirical ways."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1511327116286,"tcdate":1511327116286,"number":1,"cdate":1511327116286,"id":"rJEcBKfxG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Comment","forum":"rkYgAJWCZ","replyto":"rJcKVpa1f","signatures":["ICLR.cc/2018/Conference/Paper532/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper532/Authors"],"content":{"title":"Thanks, and clarification of the contribution of our research","comment":"Thank you for the helpful reference! We had not encountered this work previously, and we agree that they share some similar features and will reference it in our final version of the paper. However, we think there are several features that distinguish our work:\n\n* First, their work only showed benefits of learning from definitional sentences, whereas ours demonstrates the ability to benefit from sentences which are not so clearly informative about the target word. This is important, because in practice when a sentence containing a new word is encountered, it is unlikely to conveniently be a definition of that word.\n\n* Furthermore, the only metric on which their approach shows improvement is the similarity of the produced embeddings to the \"true\" embedding. This may or may not be meaningful, since our representational similarity analyses suggest that there are dissimilar word embeddings that nevertheless produce similar performance in a complex task.\n\n* We demonstrate behaviorally relevant improvements (that is, our model's ability to do its task in the context of the new word improves).  We view this as an important part of exploring whether the representation learned is actually of real use in a language processing task.\n\n* Finally, we conducted more detailed analyses of the behavior and errors produced by our approach, such as the impact on prediction of other words and how this is affected by replay. We think these analyses provide important insights and caveats about our approach that will make it easier to refine and generalize."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1511015553598,"tcdate":1511015553598,"number":1,"cdate":1511015553598,"id":"rJcKVpa1f","invitation":"ICLR.cc/2018/Conference/-/Paper532/Public_Comment","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["~Marco_Baroni1"],"readers":["everyone"],"writers":["~Marco_Baroni1"],"content":{"title":"Missing comparison?","comment":"Nice paper, but motivation and methodology are very similar to the ones presented in:\n\nHerbelot, A. and Baroni, M. 2017. High-risk learning: acquiring new word vectors from tiny data. In proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2017), Copenhagen, Denmark.\n\nhttp://aclweb.org/anthology/D/D17/D17-1030.pdf\n\nPerhaps, you could discuss how your proposal is different?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1509739251191,"tcdate":1509125617117,"number":532,"cdate":1509739248526,"id":"rkYgAJWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/dfff93b481635f774351e995bd6ede228874e62e.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}