{"notes":[{"tddate":null,"ddate":null,"tmdate":1514911469247,"tcdate":1514911469247,"number":4,"cdate":1514911469247,"id":"B1rxDNtmG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Comment","forum":"rkYgAJWCZ","replyto":"Sk9dBhUlG","signatures":["ICLR.cc/2018/Conference/Paper532/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper532/Authors"],"content":{"title":"Added an analysis of more words, and clarification of the smaller points raised.","comment":"All reviewers highlighted the small number of words we used. Briefly, we made this choice initially a) to allow us to explore the variation among different training sentences and different numbers of training sentences for the same word in more detail, and b) because our original experiments required training a new language model from scratch for each new word, which meant running many new words would require thousands of hours of compute time. However, it is useful to evaluate our approach across many words, and so we have added an experiment to the paper which does so while still maintaining computational feasibility.\n\nIn this experiment, we selected 100 of the ~150 words that appear exactly 20 times in the PTB train corpus (omitting the words we used in prior experiments). Instead of training separate models without each word as we had previously, we trained a single model with NONE of these words. We then tested our one-shot learning technique and the centroid technique on these sentences, and compared to results obtained from ``full training with all words'' -- a model trained with the train sentences for each of the hundred words and the rest of the PTB training corpus. Notice that this comparison is not as precise as the earlier ones -- the ``full training with all words'' model receives about 2.5% more training data over all than any of the one-shot learning models, which means it will both perform better even on other words and will thus have more relevant linguistic structure to learn the new words from. Nevertheless, the comparisons between our technique and the centroid technique are still valid, and the comparison to the full training with all words gives a *worst-case* bound on how poorly one-shot methods will do compared to full training. In these experiments, we saw that our method performed both relatively consistently across different words, and performed consistently better than the centroid method. On average, it performed about as well as full training with the word (see the revised paper for full results).\n\nIn order to partially compensate for the added material, we moved the embedding similairty analyses to the supplementary material, per reviewer 3's comment that they were unnecessary.\n\nClarification of some minor details:\n\nInitializing from current embedding: You can imagine that a <new-word> token would be included in the softmax from the beginning, which could then be used as an initialization for any new words encountered. This would likely help the new word embeddings to be well separated from old embeddings (though it ultimately proves to be detrimental). Thanks for pointing out that this was not explained clearly, we have clarified this in the article as well.\n\nLatin square: We actually performed 100 training runs for each word, 10 runs corresponding to taking the first sentence from each of the 10 permutations, 10 runs corresponding to taking the first two sentences, etc. We've added a sentence to the paper that we hope will clarify this.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1514911347539,"tcdate":1514911347539,"number":3,"cdate":1514911347539,"id":"r1i_IEtmG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Comment","forum":"rkYgAJWCZ","replyto":"HJnrNAtlG","signatures":["ICLR.cc/2018/Conference/Paper532/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper532/Authors"],"content":{"title":"Added an analysis of more words to demonstrate the reliability of our approach","comment":"All reviewers highlighted the small number of words we used. Briefly, we made this choice initially a) to allow us to explore the variation among different training sentences and different numbers of training sentences for the same word in more detail, and b) because our original experiments required training a new language model from scratch for each new word, which meant running many new words would require thousands of hours of compute time. However, it is useful to evaluate our approach across many words, and so we have added an experiment to the paper which does so while still maintaining computational feasibility.\n\nIn this experiment, we selected 100 of the ~150 words that appear exactly 20 times in the PTB train corpus (omitting the words we used in prior experiments). Instead of training separate models without each word as we had previously, we trained a single model with NONE of these words. We then tested our one-shot learning technique and the centroid technique on these sentences, and compared to results obtained from ``full training with all words'' -- a model trained with the train sentences for each of the hundred words and the rest of the PTB training corpus. Notice that this comparison is not as precise as the earlier ones -- the ``full training with all words'' model receives about 2.5% more training data over all than any of the one-shot learning models, which means it will both perform better even on other words and will thus have more relevant linguistic structure to learn the new words from. Nevertheless, the comparisons between our technique and the centroid technique are still valid, and the comparison to the full training with all words gives a *worst-case* bound on how poorly one-shot methods will do compared to full training. In these experiments, we saw that our method performed both relatively consistently across different words, and performed consistently better than the centroid method. On average, it performed about as well as full training with the word (see the revised paper for full results).\n\nIn order to partially compensate for the added material, we moved the embedding similairty analyses to the supplementary material, per reviewer 3's comment that they were unnecessary.\n\nWe agree that it would be exciting to see these methods applied to richer semantic tasks like the grounded tasks that we mentioned in the article, as several reviewers commented. However, it seems to us that our results are a useful starting place to demonstrate the method, and we are already straining the limits of the length of this paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1514911257689,"tcdate":1514911257689,"number":2,"cdate":1514911257689,"id":"S1fm8Vt7z","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Comment","forum":"rkYgAJWCZ","replyto":"Sybz8F9eG","signatures":["ICLR.cc/2018/Conference/Paper532/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper532/Authors"],"content":{"title":"Added an analysis of more words to demonstrate the consistency of our technique.","comment":"All reviewers highlighted the small number of words we used. Briefly, we made this choice initially a) to allow us to explore the variation among different training sentences and different numbers of training sentences for the same word in more detail, and b) because our original experiments required training a new language model from scratch for each new word, which meant running many new words would require thousands of hours of compute time. However, it is useful to evaluate our approach across many words, and so we have added an experiment to the paper which does so while still maintaining computational feasibility.\n\nIn this experiment, we selected 100 of the ~150 words that appear exactly 20 times in the PTB train corpus (omitting the words we used in prior experiments). Instead of training separate models without each word as we had previously, we trained a single model with NONE of these words. We then tested our one-shot learning technique and the centroid technique on these sentences, and compared to results obtained from ``full training with all words'' -- a model trained with the train sentences for each of the hundred words and the rest of the PTB training corpus. Notice that this comparison is not as precise as the earlier ones -- the ``full training with all words'' model receives about 2.5% more training data over all than any of the one-shot learning models, which means it will both perform better even on other words and will thus have more relevant linguistic structure to learn the new words from. Nevertheless, the comparisons between our technique and the centroid technique are still valid, and the comparison to the full training with all words gives a *worst-case* bound on how poorly one-shot methods will do compared to full training. In these experiments, we saw that our method performed both relatively consistently across different words, and performed consistently better than the centroid method. On average, it performed about as well as full training with the word (see the revised paper for full results), even though this is a worst-case bound. We think these results are quite encouraging, and hope they will address some of the concerns raised here.\n\nIn order to partially compensate for the added material, we moved the embedding similairty analyses to the supplementary material, per your comment\n\nWe agree that it would be exciting to see these methods applied to richer tasks like the grounded tasks that we mentioned in the article, as several reviewers commented. However, it seems to us that our results are a useful starting place to demonstrate the method, and we are already straining the limits of the length of this paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1515642463314,"tcdate":1511851529032,"number":3,"cdate":1511851529032,"id":"Sybz8F9eG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Review","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference/Paper532/AnonReviewer3"],"readers":["everyone"],"content":{"title":"5: Marginally below the acceptance threshold.","rating":"4: Ok but not good enough - rejection","review":"Paper Summary\n\nFrom just seeing a word used in a sentence, humans can infer a lot about this word by leveraging the surrounding words. Based on this idea, this work tries to obtain a better understanding of words in the one-shot or few-shot setting by leveraging surrounding word. They do this by language modeling sentences which contain rarely seen or never seen words. They evaluated their model using percent change in perplexity on test sentences containing new word by varying the number of training sentences containing this word. 3 Proposed Methods to model few-shot words: (1) beginning with random embedding, (2) beginning with zero embedding (3) beginning with the centroid of other words in the sentence. They compare to 2 Baseline Methods: (1) centroid of other words in the sentence, and (2) full training including the sparse words. Their results show that learning from centroids of other words can outperform full training on the new words. \n\nExplanation\nThe paper is well written, and the experiments are well explained.  It is an interesting paper, and a research topic which is not well studied. The experiments are reasonable. The method seems to work well. \n\nHowever, the method provides a very marginal difference between the previous method in Lazaridou et al. (2017). They just use backdrop to learn from this starting position. The main contribution of this work is the evaluation section. \n\nWhy only use the PTB language modeling task. Why not use the task in Gauthier & Mordatch or Hermann et al. The one task of language modeling shows promising results, but it’s not totally convincing. \n\nOne of the biggest caveats is that the experiments are only done in a few words. I’m not sure why more couldn’t have been done. This is discussed in section 4.1, but I think some of these differences could have been alleviated if there were more experiments done. Regardless, the experiments on the 8 words that they did chose were well done. \n\nI don’t think that section 3.3 (embedding similarity) is particularly useful. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1515642463354,"tcdate":1511806019750,"number":2,"cdate":1511806019750,"id":"HJnrNAtlG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Review","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference/Paper532/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"The paper proposes a technique for exploiting prior knowledge to learn embedding representations for new words with minimal data. The authors provide a good motivation for the task and it is also a nice step in the general direction of learning deep nets and other systems with minimal supervision. \n\nThe problem is useful and very relevant to natural language applications, especially considering the widespread use of word embeddings within NLP systems. However, the demonstrated experimental results do not match the claims which seems a little grand. Overall, the empirical results is unsatisfactory. The authors pick a few example words and provide a detailed analysis. This is useful to understand how the test perplexity varies with #training examples for these individual settings. However, it is hardly enough to draw conclusion about the general applicability of the technique or effectiveness of the results. Why were these specific words chosen? If the reason is due to some statistical property (e.g., frequency) observed in the corpus, then why not generalize this idea and demonstrate empirical results for a class of words exhibiting the property. Such an analysis would be useful to understand the effectiveness of the overall approach. Another idea would be to use the one/few-shot learning to learn embeddings and evaluate their quality on a semantic task (as suggested in Section 3.3), but on a larger scale.\n\nThe technical contributions are also not novel. Coupled with the narrow experimentation protocol, it does not make the paper’s contributions or proposed claims convincing.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1515642463391,"tcdate":1511601521811,"number":1,"cdate":1511601521811,"id":"Sk9dBhUlG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Review","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference/Paper532/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Worthwile goal of using backpropogation and weight-clamping to perform few-shot learning; questionable evaluation","rating":"4: Ok but not good enough - rejection","review":"I am highly sympathetic to the goals of this paper, and the authors do a good job of contrasting human learning with current deep learning systems, arguing that the lack of a mechanism for few-shot learning in such systems is a barrier to applying them in realistic scenarios. However, the main evaluation only considers four words - \"bonuses\", \"explained\", \"marketers\", \"strategist\" - with no explanation of how these words were chosen. Can I really draw any meaningful conclusions from such an experimental setup? Even the authors acknowledge, in footnote 1, that, for one of the tests, getting lower perplexity in three out of the four casess \"may just be chance variation, of course\". I wonder why we can't arrive at a similar conclusion for the other results in the paper. At the very least I need convincing that this is a reasonable experimental paradigm.\n\nI don't understand the first method for initializing the word embeddings. How can we use the \"current\" embedding for a word if it's never been seen before? What does \"current\" mean in this context?\n\nI also didn't understand the Latin square setup. Training on ten different permutations of the ten sentences suggests that all ten sentences are being used, so I don't see how this can lead to a few-shot or one-shot scenario.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1511338047185,"tcdate":1511338047185,"number":2,"cdate":1511338047185,"id":"ByDSg2Ggz","invitation":"ICLR.cc/2018/Conference/-/Paper532/Public_Comment","forum":"rkYgAJWCZ","replyto":"rJEcBKfxG","signatures":["~Marco_Baroni1"],"readers":["everyone"],"writers":["~Marco_Baroni1"],"content":{"title":"Thanks for the clarification","comment":"Thanks for the clarification. I fully agree that, while the papers ask similar questions and propose similar approaches, yours is going further in various empirical ways."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1511327116286,"tcdate":1511327116286,"number":1,"cdate":1511327116286,"id":"rJEcBKfxG","invitation":"ICLR.cc/2018/Conference/-/Paper532/Official_Comment","forum":"rkYgAJWCZ","replyto":"rJcKVpa1f","signatures":["ICLR.cc/2018/Conference/Paper532/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper532/Authors"],"content":{"title":"Thanks, and clarification of the contribution of our research","comment":"Thank you for the helpful reference! We had not encountered this work previously, and we agree that they share some similar features and will reference it in our final version of the paper. However, we think there are several features that distinguish our work:\n\n* First, their work only showed benefits of learning from definitional sentences, whereas ours demonstrates the ability to benefit from sentences which are not so clearly informative about the target word. This is important, because in practice when a sentence containing a new word is encountered, it is unlikely to conveniently be a definition of that word.\n\n* Furthermore, the only metric on which their approach shows improvement is the similarity of the produced embeddings to the \"true\" embedding. This may or may not be meaningful, since our representational similarity analyses suggest that there are dissimilar word embeddings that nevertheless produce similar performance in a complex task.\n\n* We demonstrate behaviorally relevant improvements (that is, our model's ability to do its task in the context of the new word improves).  We view this as an important part of exploring whether the representation learned is actually of real use in a language processing task.\n\n* Finally, we conducted more detailed analyses of the behavior and errors produced by our approach, such as the impact on prediction of other words and how this is affected by replay. We think these analyses provide important insights and caveats about our approach that will make it easier to refine and generalize."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1511015553598,"tcdate":1511015553598,"number":1,"cdate":1511015553598,"id":"rJcKVpa1f","invitation":"ICLR.cc/2018/Conference/-/Paper532/Public_Comment","forum":"rkYgAJWCZ","replyto":"rkYgAJWCZ","signatures":["~Marco_Baroni1"],"readers":["everyone"],"writers":["~Marco_Baroni1"],"content":{"title":"Missing comparison?","comment":"Nice paper, but motivation and methodology are very similar to the ones presented in:\n\nHerbelot, A. and Baroni, M. 2017. High-risk learning: acquiring new word vectors from tiny data. In proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2017), Copenhagen, Denmark.\n\nhttp://aclweb.org/anthology/D/D17/D17-1030.pdf\n\nPerhaps, you could discuss how your proposal is different?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]}},{"tddate":null,"ddate":null,"tmdate":1515083993458,"tcdate":1509125617117,"number":532,"cdate":1509739248526,"id":"rkYgAJWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkYgAJWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"One-shot and few-shot learning of word embeddings","abstract":"Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter.","pdf":"/pdf/c9ade40c8f59a1058b5d9f15b7ef14eed2a4bf09.pdf","TL;DR":"We highlight a technique by which natural language processing systems can learn a new word from context, allowing them to be much more flexible.","paperhash":"anonymous|oneshot_and_fewshot_learning_of_word_embeddings","_bibtex":"@article{\n  anonymous2018one-shot,\n  title={One-shot and few-shot learning of word embeddings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYgAJWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper532/Authors"],"keywords":["One-shot learning","embeddings","word embeddings","natural language processing","NLP"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}