{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222655485,"tcdate":1511855870601,"number":3,"cdate":1511855870601,"id":"BkDbv9qlM","invitation":"ICLR.cc/2018/Conference/-/Paper454/Official_Review","forum":"HyPpD0g0Z","replyto":"HyPpD0g0Z","signatures":["ICLR.cc/2018/Conference/Paper454/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The proposed methods seems useful but novelty seems limited","rating":"5: Marginally below acceptance threshold","review":"This paper aims at robust image classification against adversarial domain shifts. In the used model, there are two types of latent features, \"core\" features and \"style\" features, and the goal is to achieved by avoiding using the changing style features. The proposed method, which makes use of grouping information, seems reasonable and useful. \n\nIt is nice that the authors use \"counterfactual regularization\". But I failed to see a clear, new contribution of using this causal regularization, compared to some of the previous methods to achieve invariance (e.g., relative to translation or rotation). For examples of such methods, one may see the paper \"Transform Invariant Auto-encoder\" (by Matsuo et al.) and references therein.\n\nThe data-generating process for the considered model, given in Figure 2, seems to be consistent with Figure 1 of the paper \"Domain Adaptation with Conditional Transferable Components\" (by Gong et al.). Perhaps the authors can draw the connection between their work and Gong et al.'s work and the related work discussed in that paper.\n\nBelow are some more detailed comments. In Introduction, it would be nice if the authors made it clear that \"Their high predictive accuracy might suggest that the extracted latent features and learned representations resemble the characteristics our human cognition uses for the task at hand.\" Why do the features human cognition uses give an optimal predictive accuracy? On page 2, the authors claimed that \"These are arguably one reason why deep learning requires large sample sizes as large sample size is clearly not per se a guarantee that the confounding effect will become weaker.\" Could the authors give more detail on this? A reference would be appreciated. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Guarding Against Adversarial Domain Shifts with Counterfactual Regularization","abstract":"When training a deep neural network for supervised image classification, one can broadly distinguish between two types of latent features of images that will drive the classification: (i) \"immutable\" or \"core\" features that are really inherent to the object in question and do not change substantially from one instance of the object to another and (ii) \"mutable\" or \"style\" features such as position, rotation, image quality or brightness but also more complex ones like hair color or posture for images of persons. The distribution of the style features can change in the future. While transfer learning and domain adaptation would try to adapt to a shift in the distribution(s), we here want to protect against future adversarial domain shifts, arising through changing style features, by ideally not using the mutable style features altogether. \n\nThere are two broad scenarios and we show how exploiting grouping information in the data helps in both. (a) If the style features are known explicitly (such as translation, rotation, etc.) one usually proceeds by using data augmentation. By exploiting the grouping information about which original image an augmented sample belongs to, we can reduce the sample size required to achieve invariance to the style feature in question. (b) Sometimes the style features are not known explicitly but we still have information about samples that belong to the same underlying object (such as pictures of the same person in different circumstances). By constraining the classification to give the same forecast for all instances that belong to the same object,  we show how using this grouping information leads to invariance to such implicit style features  and helps to protect against adversarial domain shifts. \n\nWe provide a causal framework for the problem and treat groups of instances of the same object as counterfactuals under different  interventions on the mutable style features. We show links to questions of interpretability, fairness, transfer learning and adversarial examples. ","pdf":"/pdf/b948d03538efab29ffd771057c56de99109b4e8b.pdf","TL;DR":"We propose counterfactual regularization to guard against adversarial domain shifts arising through shifts in the distribution of latent \"style features\" of images.","paperhash":"anonymous|guarding_against_adversarial_domain_shifts_with_counterfactual_regularization","_bibtex":"@article{\n  anonymous2018guarding,\n  title={Guarding Against Adversarial Domain Shifts with Counterfactual Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyPpD0g0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper454/Authors"],"keywords":["supervised representation learning","causality","interpretability","transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222655522,"tcdate":1511811889853,"number":2,"cdate":1511811889853,"id":"rJqVoyclf","invitation":"ICLR.cc/2018/Conference/-/Paper454/Official_Review","forum":"HyPpD0g0Z","replyto":"HyPpD0g0Z","signatures":["ICLR.cc/2018/Conference/Paper454/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Potentially interesting, but fails to mention very related work","rating":"5: Marginally below acceptance threshold","review":"The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization. The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or “style” features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy. The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings.\n\nAlthough I really wanted to like the paper, I have several concerns. First and most importantly, the paper is not citing several important related work. Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change. Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).\nBesides, there are also other less related papers, e.g. http://proceedings.mlr.press/v28/zhang13d.pdf, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052/0, https://arxiv.org/abs/1707.09724, (or potentially https://arxiv.org/abs/1507.05333 and https://arxiv.org/abs/1707.06422), that I think may be mentioned for a more complete picture. Since there is some related work, it may be also worth to compare with it, or use the same datasets.\n\nI’m also not very happy with the term “counterfactual”. As the authors mention in footnote, this is not the correct use of the term, since counterfactual means “against the fact”. For example, a counterfactual query is “we gave the patient a drug and the patient died, what would have happened if we didn’t give the drug?” In this case, these are just different interventions on possibly the same object. I’m not sure that in the practical applications one can assure that the noise variables stay the same, which, as the authors correctly mention, would make it a bit closer to counterfactuals. It may sound pedantic, but I don’t understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn’t vary too much in the different images of the single object.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Guarding Against Adversarial Domain Shifts with Counterfactual Regularization","abstract":"When training a deep neural network for supervised image classification, one can broadly distinguish between two types of latent features of images that will drive the classification: (i) \"immutable\" or \"core\" features that are really inherent to the object in question and do not change substantially from one instance of the object to another and (ii) \"mutable\" or \"style\" features such as position, rotation, image quality or brightness but also more complex ones like hair color or posture for images of persons. The distribution of the style features can change in the future. While transfer learning and domain adaptation would try to adapt to a shift in the distribution(s), we here want to protect against future adversarial domain shifts, arising through changing style features, by ideally not using the mutable style features altogether. \n\nThere are two broad scenarios and we show how exploiting grouping information in the data helps in both. (a) If the style features are known explicitly (such as translation, rotation, etc.) one usually proceeds by using data augmentation. By exploiting the grouping information about which original image an augmented sample belongs to, we can reduce the sample size required to achieve invariance to the style feature in question. (b) Sometimes the style features are not known explicitly but we still have information about samples that belong to the same underlying object (such as pictures of the same person in different circumstances). By constraining the classification to give the same forecast for all instances that belong to the same object,  we show how using this grouping information leads to invariance to such implicit style features  and helps to protect against adversarial domain shifts. \n\nWe provide a causal framework for the problem and treat groups of instances of the same object as counterfactuals under different  interventions on the mutable style features. We show links to questions of interpretability, fairness, transfer learning and adversarial examples. ","pdf":"/pdf/b948d03538efab29ffd771057c56de99109b4e8b.pdf","TL;DR":"We propose counterfactual regularization to guard against adversarial domain shifts arising through shifts in the distribution of latent \"style features\" of images.","paperhash":"anonymous|guarding_against_adversarial_domain_shifts_with_counterfactual_regularization","_bibtex":"@article{\n  anonymous2018guarding,\n  title={Guarding Against Adversarial Domain Shifts with Counterfactual Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyPpD0g0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper454/Authors"],"keywords":["supervised representation learning","causality","interpretability","transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222655563,"tcdate":1511754353202,"number":1,"cdate":1511754353202,"id":"SkKuc-Kef","invitation":"ICLR.cc/2018/Conference/-/Paper454/Official_Review","forum":"HyPpD0g0Z","replyto":"HyPpD0g0Z","signatures":["ICLR.cc/2018/Conference/Paper454/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Lacks sufficient comparisons to other similar regularizers","rating":"4: Ok but not good enough - rejection","review":"Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same. As authors note, this constraint can be converted into a KKT style penalty with KKT multiplier lambda.  Thus this is very  similar to other regularizers that increase smoothness of the function, such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group, as well as manifold regularization (see e.g. Belkin, Niyogi et al. JMLR).  Heck, in practie ridge regularization will also do something similar for many function classes. \n\nExperiments didn't compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples). It's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results.   That made it hard to get excited about the results in a vacuum. \n\nWould this proposed strategy have thwarted the Russian tank legend problem? Would it have fixed the Google gorilla problem? Why or why not?\n\nOverall, I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples. \n\nPage 2: calling additional instances of the same person “counterfactual observations” didn’t seem consistent with the usual definition of that term… maybe I am just missing the semantic link here, but this isn't how we usually use the term counterfactual in my corner of the field.\n\nRe: “one creates additional samples by modifying…” be nice to quote more of the early work doing this, I believe the first work of this sort was Scholkopf’s, he called it “virtual examples” and I’m pretty sure he specifically did it for rotation MNIST images (and if not exactly that, it was implied).  I think the right citation is “Incorporating invariances in support vector learning machines\n“ Scholkopf, Burges, Vapnik 1996, but also see Decoste * Scholkopf 2002 “Training invariant support vector machines.” ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Guarding Against Adversarial Domain Shifts with Counterfactual Regularization","abstract":"When training a deep neural network for supervised image classification, one can broadly distinguish between two types of latent features of images that will drive the classification: (i) \"immutable\" or \"core\" features that are really inherent to the object in question and do not change substantially from one instance of the object to another and (ii) \"mutable\" or \"style\" features such as position, rotation, image quality or brightness but also more complex ones like hair color or posture for images of persons. The distribution of the style features can change in the future. While transfer learning and domain adaptation would try to adapt to a shift in the distribution(s), we here want to protect against future adversarial domain shifts, arising through changing style features, by ideally not using the mutable style features altogether. \n\nThere are two broad scenarios and we show how exploiting grouping information in the data helps in both. (a) If the style features are known explicitly (such as translation, rotation, etc.) one usually proceeds by using data augmentation. By exploiting the grouping information about which original image an augmented sample belongs to, we can reduce the sample size required to achieve invariance to the style feature in question. (b) Sometimes the style features are not known explicitly but we still have information about samples that belong to the same underlying object (such as pictures of the same person in different circumstances). By constraining the classification to give the same forecast for all instances that belong to the same object,  we show how using this grouping information leads to invariance to such implicit style features  and helps to protect against adversarial domain shifts. \n\nWe provide a causal framework for the problem and treat groups of instances of the same object as counterfactuals under different  interventions on the mutable style features. We show links to questions of interpretability, fairness, transfer learning and adversarial examples. ","pdf":"/pdf/b948d03538efab29ffd771057c56de99109b4e8b.pdf","TL;DR":"We propose counterfactual regularization to guard against adversarial domain shifts arising through shifts in the distribution of latent \"style features\" of images.","paperhash":"anonymous|guarding_against_adversarial_domain_shifts_with_counterfactual_regularization","_bibtex":"@article{\n  anonymous2018guarding,\n  title={Guarding Against Adversarial Domain Shifts with Counterfactual Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyPpD0g0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper454/Authors"],"keywords":["supervised representation learning","causality","interpretability","transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739295788,"tcdate":1509119935525,"number":454,"cdate":1509739293134,"id":"HyPpD0g0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyPpD0g0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Guarding Against Adversarial Domain Shifts with Counterfactual Regularization","abstract":"When training a deep neural network for supervised image classification, one can broadly distinguish between two types of latent features of images that will drive the classification: (i) \"immutable\" or \"core\" features that are really inherent to the object in question and do not change substantially from one instance of the object to another and (ii) \"mutable\" or \"style\" features such as position, rotation, image quality or brightness but also more complex ones like hair color or posture for images of persons. The distribution of the style features can change in the future. While transfer learning and domain adaptation would try to adapt to a shift in the distribution(s), we here want to protect against future adversarial domain shifts, arising through changing style features, by ideally not using the mutable style features altogether. \n\nThere are two broad scenarios and we show how exploiting grouping information in the data helps in both. (a) If the style features are known explicitly (such as translation, rotation, etc.) one usually proceeds by using data augmentation. By exploiting the grouping information about which original image an augmented sample belongs to, we can reduce the sample size required to achieve invariance to the style feature in question. (b) Sometimes the style features are not known explicitly but we still have information about samples that belong to the same underlying object (such as pictures of the same person in different circumstances). By constraining the classification to give the same forecast for all instances that belong to the same object,  we show how using this grouping information leads to invariance to such implicit style features  and helps to protect against adversarial domain shifts. \n\nWe provide a causal framework for the problem and treat groups of instances of the same object as counterfactuals under different  interventions on the mutable style features. We show links to questions of interpretability, fairness, transfer learning and adversarial examples. ","pdf":"/pdf/b948d03538efab29ffd771057c56de99109b4e8b.pdf","TL;DR":"We propose counterfactual regularization to guard against adversarial domain shifts arising through shifts in the distribution of latent \"style features\" of images.","paperhash":"anonymous|guarding_against_adversarial_domain_shifts_with_counterfactual_regularization","_bibtex":"@article{\n  anonymous2018guarding,\n  title={Guarding Against Adversarial Domain Shifts with Counterfactual Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyPpD0g0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper454/Authors"],"keywords":["supervised representation learning","causality","interpretability","transfer learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}