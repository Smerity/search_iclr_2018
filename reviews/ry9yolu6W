{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222615116,"tcdate":1511787212550,"number":3,"cdate":1511787212550,"id":"SyH0cKtlz","invitation":"ICLR.cc/2018/Conference/-/Paper29/Official_Review","forum":"ry9yolu6W","replyto":"ry9yolu6W","signatures":["ICLR.cc/2018/Conference/Paper29/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Mediocre paper without novelty or sufficient experimental validation. ","rating":"3: Clear rejection","review":"The authors propose to apply the Spatial Transformer Network (STN) on the dense prediction tasks, including semantic segmentation and microscopy image segmentation. However, the reason to apply STN on dense prediction tasks is not well justified by this work, and the technical details are insufficient to support the experimental results.\n\nDetailed comments:\n\n1. Why use STN for segmentation? In existing works, STNs are used to normalize the image content for classification tasks, e.g. digits, faces. However, the intuition to apply STN for segmentation is unclear. Does the proposed network learn any specific form of alignment for segmentation? We suggest the authors show the predicted grid points on some example images along with warped images to clarify what are the learned transforms, and how can it help the segmentation tasks.\n\n2. Missing prediction points. According to (6) and (7), it is possible to have some pixel Unm that does not match any (xi,yi), resulting zero features. In other words, the network does not see some pixels after applying the proposed STN, resulting in missing prediction points.\n\n3. Inconsistent coordinate system. According to 2.1, the predicted fiducial points are constrained between -1 and 1. However, in the following equations (4)-(10), the coordinate system seems to be integer-based.\n\n4. Doubtful baseline choice/No comparison to existing works. The proposed STN should be able to apply on different FCN baselines, but the authors propose to use U-Net as the baseline method without valid justification. The authors also make a statement “Experiments in prior work show that residual connections are important while different up-sampling methods lead to similar results” without any support number, while the performance on the PASCAL dataset is not comparable to any existing FCN baselines. Also, for the SNEMI3D dataset, the authors only report AUC instead of rand error, which is the standard metric for microscopy image segmentation, and the authors do not compare the results with any existing works.\n\n5. The proposed method achieves ~11% performance gain on the PASCAL dataset, which is unusually large for a single module. Do the authors use the same hyperparameters to train the baseline network and the proposed network?\n\nOverall, the paper presents a method to apply STN on segmentation tasks. However, the paper is not well-written with unclear statements and insufficient technical details. Though the performance gain is significant, the baseline performance is far away state-of-the-art methods. Taking all factors into account, the reviewer does not recommend this paper be accepted in ICLR.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dense Transformer Networks","abstract":"The key idea of current deep learning methods for dense prediction\nis to apply a model on a regular patch centered on each pixel to\nmake pixel-wise predictions. These methods are limited in the sense\nthat the patches are determined by network architecture instead of\nlearned from data. In this work, we propose the dense transformer\nnetworks, which can learn the shapes and sizes of patches from data.\nThe dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted\ninto each of the encoder and decoder paths. The novelty of this work\nis that we provide technical solutions for learning the shapes and\nsizes of patches from data and efficiently restoring the spatial\ncorrespondence required for dense prediction. The proposed dense\ntransformer modules are differentiable, thus the entire network can\nbe trained. We apply the proposed networks on natural and biological\nimage segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.","pdf":"/pdf/9acfdcd51fdaa1c53a7a6594a7f2aef6e6c4fcfd.pdf","paperhash":"anonymous|dense_transformer_networks","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9yolu6W}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper29/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222615158,"tcdate":1511747841483,"number":2,"cdate":1511747841483,"id":"SJF--gFlG","invitation":"ICLR.cc/2018/Conference/-/Paper29/Official_Review","forum":"ry9yolu6W","replyto":"ry9yolu6W","signatures":["ICLR.cc/2018/Conference/Paper29/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review for Dense Transformer Networks ","rating":"4: Ok but not good enough - rejection","review":"In this paper, the authors propose a version of Transformer Networks to deal with convolutional neural network architectures that contains upsampling (eg U-Net).  The original Transformer Network does not take in consideration pixel level alignment, as it was designed for classification task. The authors propose a layer (to be plugged on the decoder) to restore the spatial correspondence lost in the encoder transformer network. The paper is well written and easy to follow.\n\nThis work can be seen as an application of Transformer network on U-net type of architectures. I do not believe this work has enough innovation to be accepted to this conference.\n\nMoreover, I believe the paper lacks many experiences to validate the proposed method. The method should be compared to other semantic segmentation state-of-the-art approaches (eg pspnet, deeplab, fcn, etc). I also believe some more ablation studies is missing. Why using only one transformation network on enconder and one in decoder? How the performance changes by changing the number of transformer layers and where they are placed? What about how it scales to the amount of data? It would be nice to see experiments in more challenging segmentation datasets (eg COCO, CityScapes datasets)\n\nI would recommend the authors to improve the experimental results (to the point it empirically validates the usefulness of the method) and submit to a computer vision conference.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dense Transformer Networks","abstract":"The key idea of current deep learning methods for dense prediction\nis to apply a model on a regular patch centered on each pixel to\nmake pixel-wise predictions. These methods are limited in the sense\nthat the patches are determined by network architecture instead of\nlearned from data. In this work, we propose the dense transformer\nnetworks, which can learn the shapes and sizes of patches from data.\nThe dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted\ninto each of the encoder and decoder paths. The novelty of this work\nis that we provide technical solutions for learning the shapes and\nsizes of patches from data and efficiently restoring the spatial\ncorrespondence required for dense prediction. The proposed dense\ntransformer modules are differentiable, thus the entire network can\nbe trained. We apply the proposed networks on natural and biological\nimage segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.","pdf":"/pdf/9acfdcd51fdaa1c53a7a6594a7f2aef6e6c4fcfd.pdf","paperhash":"anonymous|dense_transformer_networks","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9yolu6W}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper29/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222615195,"tcdate":1511428706052,"number":1,"cdate":1511428706052,"id":"BkqvzfNgf","invitation":"ICLR.cc/2018/Conference/-/Paper29/Official_Review","forum":"ry9yolu6W","replyto":"ry9yolu6W","signatures":["ICLR.cc/2018/Conference/Paper29/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This work adds the spatial transformer module to U-Net, but not much technical novelty is presented.","rating":"4: Ok but not good enough - rejection","review":"This submission proposes to learn shapes and sizes of patches of networks for semantic segmentation of images. The method adds the spatial transformer module of Jaderberg et al. (2015) two times to the U-Net architecture, once during encoding and once during decoding. While the paper is very well written, I have major concerns regarding the experimental evaluation and the amount of novelty. \n\nThe authors just add the spatial transformer module of Jaderberg et al. (2015) to the U-Net architecture. Two entire pages are spend for describing the work of Jaderberg et al. (2015) in section 2. Although the description is very good and allows the reader to fully grasp the details, it seems unnecessary and a mere repetition of Jaderberg's work. Figure 1 shows a good overview of the proposed approach, but also reveils that there is not much technical novelty beyond combining two already existing approaches into one (U-Net + Spatial Transformer Module). The only technically novel part seems the gridding operator in the decoder part as described on P5, fourth paragraph, and dense decoder sampler on P6. In general, there is no clear statement of novelty or contribution until the very end of the paper. Only in the conclusion section, finally a statement is made \"…we develop the dense transformer decoder layer to restore the spatial correspondence\". Although this is interesting and it seems non-trivial to get this running in an efficient, end-to-end learnable way, I fear this is simply not enough technical novelty for an ICL18 paper. \n\nOnly two small experiments are shown. The authors compare the performance of the original U-Net to \"U-Net + Dense Spatial Transformer\" for Pascal 2012 and SNEMI3D where numbers slightly improve with the proposed approach. Neither comparisons to other networks are shown, nor any further tests on additional benchmarks. This is basically not convincing because it is unknown whether adding the spatial transformer module only slightly improves U-Net results on those particular data sets or if this is generally advantageous. To prove that adding spatial transformer modules to dense networks helps I would expect experiments with additional network architectures and on additional data sets.\n\nI summary, I am not fully convinced because technical novelty seems very limited and experiments do not sufficiently prove the claims. I thus recommend rejection of this submission from ICLR18. \n    \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dense Transformer Networks","abstract":"The key idea of current deep learning methods for dense prediction\nis to apply a model on a regular patch centered on each pixel to\nmake pixel-wise predictions. These methods are limited in the sense\nthat the patches are determined by network architecture instead of\nlearned from data. In this work, we propose the dense transformer\nnetworks, which can learn the shapes and sizes of patches from data.\nThe dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted\ninto each of the encoder and decoder paths. The novelty of this work\nis that we provide technical solutions for learning the shapes and\nsizes of patches from data and efficiently restoring the spatial\ncorrespondence required for dense prediction. The proposed dense\ntransformer modules are differentiable, thus the entire network can\nbe trained. We apply the proposed networks on natural and biological\nimage segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.","pdf":"/pdf/9acfdcd51fdaa1c53a7a6594a7f2aef6e6c4fcfd.pdf","paperhash":"anonymous|dense_transformer_networks","_bibtex":"@article{\n  anonymous2018dense,\n  title={Dense Transformer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9yolu6W}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper29/Authors"],"keywords":[]}}],"limit":2000,"offset":0}