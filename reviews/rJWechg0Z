{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222635075,"tcdate":1512073619015,"number":3,"cdate":1512073619015,"id":"SkjcYkCgf","invitation":"ICLR.cc/2018/Conference/-/Paper399/Official_Review","forum":"rJWechg0Z","replyto":"rJWechg0Z","signatures":["ICLR.cc/2018/Conference/Paper399/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper proposes a principled connection between correlation alignment and entropy minimization to achieve a more robust domain adaptation. The authors show the connection between the two approaches within a unified framework. The experimental results support the claims in the paper, and show the benefits over state-of-the-art methods such as DeepCoral. ","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors propose a novel deep learning approach which leverages on our finding that entropy minimization\nis induced by the optimal alignment of second order statistics between source and target domains. Instead of relying on Euclidean distances when performing the alignment, the authors use geodesic distances which preserve the geometry of the manifolds. Among others, the authors also propose a handy way to cross-validate the model parameters on target data using the entropy criterion. The experimental validation is performed on benchmark datasets for image classification. Comparisons with the state-of-the-art approaches show that the proposed marginally improves the results. The paper is well written and easy to understand.\n\nAs a main difference from DeepCORAL method, this approach relies on the use of geodesic distances when doing the alignment of the distribution statistics, which turns out to be beneficial for improving the network performance on the target tasks. While I don't see this as substantial contribution to the field, I think that using the notion of geodesic distance in this context is novel.  The experiments show the benefit over the Euclidean distance when applied to the datasets used in the paper. \n\nA lot of emphasis in the paper is put on the methodology part. The experiments could have been done more extensively, by also providing some visual examples of the aligned distributions and image features. This would allow the readers to further understand why the proposed alignment approach performs better than e.g. Deep Coral.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation","abstract":"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","pdf":"/pdf/e842248ec9d8953f531f68f2eaac6b5bfa90def0.pdf","TL;DR":"A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization","paperhash":"anonymous|minimalentropy_correlation_alignment_for_unsupervised_deep_domain_adaptation","_bibtex":"@article{\n  anonymous2018minimal-entropy,\n  title={Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJWechg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper399/Authors"],"keywords":["unsupervised domain adaptation","entropy minimization","image classification","deep transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222635116,"tcdate":1511819698954,"number":2,"cdate":1511819698954,"id":"r15hYW5gM","invitation":"ICLR.cc/2018/Conference/-/Paper399/Official_Review","forum":"rJWechg0Z","replyto":"rJWechg0Z","signatures":["ICLR.cc/2018/Conference/Paper399/AnonReviewer2"],"readers":["everyone"],"content":{"title":"New correlation alignment based domain adaptation method which results in minimal target entropy","rating":"7: Good paper, accept","review":"Summary:\nThis paper proposes minimal-entropy correlation alignment, an unsupervised domain adaptation algorithm which links together two prior class of methods: entropy minimization and correlation alignment. Interesting new idea. Make a simple change in the distance function and now can perform adaptation which aligns with minimal entropy on target domain and thus can allow for removal of hyperparameter (or automatic validation of correct one).\n\nStrengths\n-  The paper is clearly written and effectively makes a simple claim that geodesic distance minimization is better aligned to final performance than euclidean distance minimization between source and target. \n- Figures 1 and 2 (right side) are particularly useful for fast understanding of the concept and main result.\n\n\nQuestions/Concerns:\n- Can entropy minimization on target be used with other methods for DA param tuning? Does it require that the model was trained to minimize the geodesic correlation distance between source and target?\n- It would be helpful to have a longer discussion on the connection with Geodesic flow kernel [1] and other unsupervised manifold based alignment methods [2]. Is this proposed approach an extension of this prior work to the case of non-fixed representations in the same way that Deep CORAL generalized CORAL?\n- Why does performance suffer compared to TRIPLE on the SYN->SVHN task? Is there some benefit to the TRIPLE method which may be combined with the MECA approach?\n\n\t\t\t\t\t\n[1] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In CVPR, 2012.\n\t\t\t\t\t\n[2] Raghuraman Gopalan and Ruonan Li. Domain adaptation for object recognition: An unsupervised approach. In ICCV, 2011. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation","abstract":"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","pdf":"/pdf/e842248ec9d8953f531f68f2eaac6b5bfa90def0.pdf","TL;DR":"A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization","paperhash":"anonymous|minimalentropy_correlation_alignment_for_unsupervised_deep_domain_adaptation","_bibtex":"@article{\n  anonymous2018minimal-entropy,\n  title={Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJWechg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper399/Authors"],"keywords":["unsupervised domain adaptation","entropy minimization","image classification","deep transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222635159,"tcdate":1511731683141,"number":1,"cdate":1511731683141,"id":"BkiyM2dgG","invitation":"ICLR.cc/2018/Conference/-/Paper399/Official_Review","forum":"rJWechg0Z","replyto":"rJWechg0Z","signatures":["ICLR.cc/2018/Conference/Paper399/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Need further exploration for the use of entropy to select free parameters; geodesic correlation alignment is a reasonable improvement","rating":"6: Marginally above acceptance threshold","review":"This paper improves the correlation alignment approach to domain adaptation from two aspects. One is to replace the Euclidean distance by the geodesic Log-Euclidean distance between two covariance matrices. The other is to automatically select the balancing cost by the entropy on the target domain. Experiments are conducted from SVHN to MNIST and from SYN MNIST to SVHN. Additional experiments on cross-modality recognition are reported from RGB to depth.\n\nStrengths:\n+ It is a sensible idea to improve the Euclidean distance by the geodesic Log-Euclidean distance to better explore the manifold structure of the PSD matrices. \n+ It is also interesting to choose the balancing cost using the entropy on the target. However, this point is worth further exploring (please see below for more detailed comments).\n+ The experiments show that the geodesic correlation alignment outperforms the original alignment method. \n\nWeaknesses: \n- It is certainly interesting to have a scheme to automatically choose the hyper-parameters in unsupervised domain adaptation, and the entropy over the target seems like a reasonable choice. This point is worth further exploring for the following reasons. \n1. The theoretical result is not convincing given it relies on many unrealistic assumptions, such as the null performance degradation under perfect correlation alignment, the Diracâ€™s delta function as the predictions over the target, etc.\n2. The theorem actually does not favor the correlation alignment over the geodesic alignment. It does not explain that, in Figure 2, the entropy is able to find the best balancing cost \\lamba for geodesic alignment but not for the Euclidean alignment.\n3. The entropy alignment seems an interesting criterion to explore in general. Could it be used to find fairly good hyper-parameters for the other methods? Could it be used to determine the other hyper-parameters (e..g, learning rate, early stopping) for the geodesic alignment? \n4. If one leaves a subset of the target domain out and use its labels for validation, how different would the selected balancing cost \\lambda differ from that by the entropy? \n\n- The cross-modality setup (from RGB to depth) is often not considered as domain adaptation. It would be better to replace it by another benchmark dataset. The Office-31 dataset is still a good benchmark to compare different methods and for the study in Section 5.1, though it is not necessary to reach state-of-the-art results on this dataset because, as the authors noted, it is almost saturated. \n\nQuestion:\n- I am not sure how the gradients were computed after the eigendecomposition in equation (8).\n\n\nI like the idea of automatically choosing free parameters using the entropy over the target domain. However, instead of justifying this point by the theorem that relies on many assumptions, it is better to further test it using experiments (e.g., on Office31 and for other adaptation methods). The geodesic correlation alignment is a reasonable improvement over the Euclidean alignment.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation","abstract":"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","pdf":"/pdf/e842248ec9d8953f531f68f2eaac6b5bfa90def0.pdf","TL;DR":"A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization","paperhash":"anonymous|minimalentropy_correlation_alignment_for_unsupervised_deep_domain_adaptation","_bibtex":"@article{\n  anonymous2018minimal-entropy,\n  title={Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJWechg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper399/Authors"],"keywords":["unsupervised domain adaptation","entropy minimization","image classification","deep transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739323992,"tcdate":1509112297109,"number":399,"cdate":1509739321333,"id":"rJWechg0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJWechg0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation","abstract":"In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks.","pdf":"/pdf/e842248ec9d8953f531f68f2eaac6b5bfa90def0.pdf","TL;DR":"A new unsupervised deep domain adaptation technique which efficiently unifies correlation alignment and entropy minimization","paperhash":"anonymous|minimalentropy_correlation_alignment_for_unsupervised_deep_domain_adaptation","_bibtex":"@article{\n  anonymous2018minimal-entropy,\n  title={Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJWechg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper399/Authors"],"keywords":["unsupervised domain adaptation","entropy minimization","image classification","deep transfer learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}