{"notes":[{"tddate":null,"ddate":null,"tmdate":1512289443931,"tcdate":1512289443931,"number":3,"cdate":1512289443931,"id":"HknoNEWZz","invitation":"ICLR.cc/2018/Conference/-/Paper91/Official_Review","forum":"SkRsFSRpb","replyto":"SkRsFSRpb","signatures":["ICLR.cc/2018/Conference/Paper91/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Incremental contribution and detail missing.","rating":"5: Marginally below acceptance threshold","review":"In this paper, the authors propose to integrate the Fisher information metric with the  Seq2Seq network, which abridges the gap between deep recurrent neural networks and information geometry. By considering of the information geometry of the latent embedding, the authors propose to encode the RNN feature as a Fisher kernel of a parametric Gaussian Mixture Model, which demonstrate an experimental improvements compared with the non-probabilistic embedding. \n\nThe idea is interesting. However, the technical contribution is rather incremental. The authors seem to integrate some well-explored techniques, with little consideration of the specific challenges. Moreover, the experimental section is rather insufficient. The results on road network graph is not a strong support for the Seq2Seq model application. \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks","abstract":"The Fisher information metric is an important foundation of information geometry, wherein it allows us to approximate the local geometry of a probability distribution. Recurrent neural networks such as the Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield state-of-the-art performance on speech translation or image captioning have so far ignored the geometry of the latent embedding, that they iteratively learn. We propose the information geometric Seq2Seq network which abridges the gap between deep recurrent neural networks and information geometry. Specifically, the latent embedding offered by a recurrent network is encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism common in computer vision. We utilise such a network to predict the shortest routes between two nodes of a graph by learning the adjacency matrix using the information geometric Seq2Seq model; our results show that for such a problem the probabilistic representation of the latent embedding supersedes the non-probabilistic embedding by 10-15\\%.","pdf":"/pdf/e64dd36d8ba3bb60a613733e5249f277cbd17315.pdf","paperhash":"anonymous|geoseq2seq_information_geometric_sequencetosequence_networks","_bibtex":"@article{\n  anonymous2018geoseq2seq:,\n  title={GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkRsFSRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper91/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222812886,"tcdate":1511765479588,"number":2,"cdate":1511765479588,"id":"HJlgLNYxf","invitation":"ICLR.cc/2018/Conference/-/Paper91/Official_Review","forum":"SkRsFSRpb","replyto":"SkRsFSRpb","signatures":["ICLR.cc/2018/Conference/Paper91/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Missing some connections","rating":"5: Marginally below acceptance threshold","review":"In this work, the authors propose an approach to adapt latent representations to account for local geometry in the embedding space. They show modest improvement compared to reasonable baselines.\n\nWhile I find the idea of incorporating information geometry into embeddings very promising, the current work omits a number of key details that would allow the reader to draw deeper connections between the two (specific comments below). Additionally, the experiments are not particularly insightful.\n\nI believe a substantially revised version of the paper could address most of my concerns; still, I find the current version too preliminary for publication.\n\n=== Major comments / questions\n\nThe transformation from context vectors into Fisher vectors is not clear. Presumably, shortest paths in the training data have different lengths, and thus produce different numbers of context vectors. Does the GMM treat all of these independently (regardless of sample)? or is a separate GMM somehow trained for each training sequence? The same question applies to the VLAD-based approach.\n\nIn a related vein, it is not clear to what extent this method depends on the sequential nature of the considered networks. In particular, could a similar approach be applied to latent space embeddings from non-sequential models?\n\nIt is not clear if the geometric encoding operations are differentiable, or more generally, the entire training algorithm is not clear.\n\nThe choice to limit the road network graph feels quite arbitrary. Why was this done?\n\nDeep models are known to be sensitive to the choice of hyperparameters. How were these chosen? was a validation set used in addtion to the training and testing sets?\n\nThe target for training is very unclear. Throughout Sections 1 and 2, the aim of the paper appears to be to learn shortest paths; however, Section 3 states that the “network is capable of learning the adjacency matrix”, and the caption for Figure 2 suggests that “[t]he adjacency matrix is iteratively learnt (sic)....” However, calculating such training error for back-propagation/optimization would seem to rely on *already knowing* the adjacency matrix.\n\nThe performed experiments are minimal and offer very little insight into what is learned. For example, does the model predict “short” shortest paths better than longer ones? what do the “valid but not optimal” paths look like? are they close to optimal? what do the invalid paths look like? does it seem to learn parts of the road network better than others? sparse parts of the network? dense parts?\n\n=== Minor comments / questions\n\nThe term “context vector” is not explicitly defined or described. Based on the second paragraph in the “Fisher encoding” section, I assume these are the latent states for each element in the shortest path sequences.\n\nIs the graph directed? weighted? by Euclidean distance? (Roads are not necessarily straight, so the Euclidean distance from intersection to intersection may not accurately reflect the distance in some cases.)\n\nAre the nodes sampled uniformly at random for creating the training data?\n\nIs the choice to use a diagonal covariance matrix (as opposed to some more flexible one) a computational choice? or does the theory justify this choice?\n\nRoughly, what are the computational resources required for training?\n\nThe discussion should explain “condition number” in more detail.\n\nDo the “more precise” results for the Fisher encoding somehow rely on an infinite mixture? or, how much does using only a single component in the GMM affect the results?\n\nIt is not clear what “features” and “dictionary elements” are in the context of VLAD.\n\nWhat value of k was used for K-means clustering for VLAD?\n\nIt is not possible to assess the statistical significance of the presented experimental results. More datasets (or different parts of the road network) or cross-validation should be used to provide an indication of the variance of each method.\n\n=== Typos, etc.\n\nThe paper includes a number of runon sentences and other small grammatical mistakes. I have included some below.\n\nThe first paragraph in Section 2.2 in particular needs to be edited.\n\nThe references are inconsistently and improperly (e.g., “Turing” should be capitalized) formatted.\n\nIt seems like that $q_{ik} \\in \\{0,1\\}$ for the hard assignments in clustering.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks","abstract":"The Fisher information metric is an important foundation of information geometry, wherein it allows us to approximate the local geometry of a probability distribution. Recurrent neural networks such as the Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield state-of-the-art performance on speech translation or image captioning have so far ignored the geometry of the latent embedding, that they iteratively learn. We propose the information geometric Seq2Seq network which abridges the gap between deep recurrent neural networks and information geometry. Specifically, the latent embedding offered by a recurrent network is encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism common in computer vision. We utilise such a network to predict the shortest routes between two nodes of a graph by learning the adjacency matrix using the information geometric Seq2Seq model; our results show that for such a problem the probabilistic representation of the latent embedding supersedes the non-probabilistic embedding by 10-15\\%.","pdf":"/pdf/e64dd36d8ba3bb60a613733e5249f277cbd17315.pdf","paperhash":"anonymous|geoseq2seq_information_geometric_sequencetosequence_networks","_bibtex":"@article{\n  anonymous2018geoseq2seq:,\n  title={GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkRsFSRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper91/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222812932,"tcdate":1511643310519,"number":1,"cdate":1511643310519,"id":"rkU2dUDlf","invitation":"ICLR.cc/2018/Conference/-/Paper91/Official_Review","forum":"SkRsFSRpb","replyto":"SkRsFSRpb","signatures":["ICLR.cc/2018/Conference/Paper91/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Missing references and insufficient experiments","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a method for augmenting sequence-to-sequence (seq2seq) methods with Fisher vector encodings, allowing the decoder to better model the geometric structure of the embedding space. Experiments are performed on a shortest-route problem, where augmenting standard seq2seq architectures with Fisher vectors improves performance.\n\nPros:\n- Combining deep learning with methods from information geometry is an interesting direction for research\n- Method is a generic drop-in replacement for improving any seq2seq architecture\n- Experimental results show modest performance improvements over vanilla seq2seq methods\n\nCons:\n- Missing references for prior work combining information geometry and deep learning\n- Insufficient explanation of the method\n- Only experimental results are a nonstandard route-finding task\n- Missing references and baselines for prior work on deep learning on graphs\n\nThe general research direction of combining deep learning with methods from information geometry is an exciting and fertile area for interesting work. Unfortunately this paper fails to cite or discuss much recent work in this area; for example natural gradient methods in deep learning have recently been explored in [1, 2, 3]; more closely related to the topic of this paper, [4] and [5] have combined Fisher vector encodings and deep networks for image classification tasks. Although these prior methods do not consider the use of recurrent networks, the authors should discuss how their method compares to the approaches of [4] and [5].\n\nThe method is not described in sufficient detail. How exactly is the Fisher encoding combined with the recurrent neural network? In particular, how is GMM fitting interleaved with learning the RNN? Do you backpropagate through the GMM fitting procedure in order to jointly learn the RNN parameters and the GMM for computing Fisher encodings? Or is GMM fitting an offline step done once, after which the RNN decoder is learned on top of the Fisher encodings? The paper should clarify along these points. As a side note, it also feels a little disingenuous to describe the method in terms of GMMs, but to perform all experiments with K=1 mixture components; in this setting the GMM degrades to a simple Gaussian distribution.\n\nThe proposed method could in theory be used as a drop-in replacement for seq2seq on any task. Given its generality, I am surprised at the nonstandard choice of route-finding in a graph of Minnesota roads as the only task on which the method is tested; as a minimum the method should have been tested on more than one graph.\n\nMore generally, I would have liked to see the method evaluated on multiple tasks, and on more well-established seq2seq tasks so that the method could be more easily compared with previously published work. Strong results on machine translation would be particularly convincing; the authors might also consider algorithmic tasks such as copying, repeat copying, sorting, etc. similar to those on which Neural Turing Machines were evaluated.\n\nI am not sure that seq2seq is the best approach for the route-finding task. In particular, since the input is encoded as a [source, destination] tuple it has a fixed length; this means that you could use a feedforward rather than recurrent encoder.\n\nThe paper also fails to cite or discuss recent work involving deep learning on graphs. For example Pointer Networks [6] use a seq2seq model with attention to solve convex hull, Delaunnay Triangulation, and traveling salesman problems; however Pointer Networks assume that the entire graph is provided as input to the model, while in this paper the network learns to specialize to a single graph. In that case, the authors might consider embedding the nodes of the graph using methods such as DeepWalk [7], LINE [8], or node2vec [9] as a preprocessing step rather than learning these embeddings from scratch.\n\nFrom Table 1, seq2seq + VLAD significantly outperforms seq2seq + FV. Given these results, are there any reasons why one should use seq2seq + FV instead of seq2seq + VLAD?\n\nOverall I think that this paper has some interesting ideas. However, due to a number of missing references, unclear description of the method, and limited experimental results I feel that the paper is not ready for publication in its current form.\n\n\nReferences\n\n[1] Grosse and Salakhutdinov, “Scaling Up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix”, ICML 2015\n\n[2] Grosse and Martens, “A Kronecker-factored approximate Fisher matrix for convolution layers”, ICML 2016\n\n[3] Desjardins et al, “Natural Neural Networks”, NIPS 2015\n\n[4] Simonyan et al, “Deep Fisher Networks for Large-Scale Image Classification”, NIPS 2013\n\n[5] Sydorov et al, “Deep Fisher Kernels - End to End Learning of the Fisher Kernel GMM Parameters”, CVPR 2014\n\n[6] Vinyals et al, “Pointer Networks”, NIPS 2015\n\n[7] Perozzi et al, “DeepWalk: Online Learning of Social Representations”, KDD 2014\n\n[8] Tang et al, “LINE: Large-scale Information Network Embedding”, WWW 2015\n\n[9] Grover and Leskovec, “node2vec: Scalable Feature Learning for Networks”, KDD 2016","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks","abstract":"The Fisher information metric is an important foundation of information geometry, wherein it allows us to approximate the local geometry of a probability distribution. Recurrent neural networks such as the Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield state-of-the-art performance on speech translation or image captioning have so far ignored the geometry of the latent embedding, that they iteratively learn. We propose the information geometric Seq2Seq network which abridges the gap between deep recurrent neural networks and information geometry. Specifically, the latent embedding offered by a recurrent network is encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism common in computer vision. We utilise such a network to predict the shortest routes between two nodes of a graph by learning the adjacency matrix using the information geometric Seq2Seq model; our results show that for such a problem the probabilistic representation of the latent embedding supersedes the non-probabilistic embedding by 10-15\\%.","pdf":"/pdf/e64dd36d8ba3bb60a613733e5249f277cbd17315.pdf","paperhash":"anonymous|geoseq2seq_information_geometric_sequencetosequence_networks","_bibtex":"@article{\n  anonymous2018geoseq2seq:,\n  title={GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkRsFSRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper91/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739491054,"tcdate":1508952486067,"number":91,"cdate":1509739488395,"id":"SkRsFSRpb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkRsFSRpb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks","abstract":"The Fisher information metric is an important foundation of information geometry, wherein it allows us to approximate the local geometry of a probability distribution. Recurrent neural networks such as the Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield state-of-the-art performance on speech translation or image captioning have so far ignored the geometry of the latent embedding, that they iteratively learn. We propose the information geometric Seq2Seq network which abridges the gap between deep recurrent neural networks and information geometry. Specifically, the latent embedding offered by a recurrent network is encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism common in computer vision. We utilise such a network to predict the shortest routes between two nodes of a graph by learning the adjacency matrix using the information geometric Seq2Seq model; our results show that for such a problem the probabilistic representation of the latent embedding supersedes the non-probabilistic embedding by 10-15\\%.","pdf":"/pdf/e64dd36d8ba3bb60a613733e5249f277cbd17315.pdf","paperhash":"anonymous|geoseq2seq_information_geometric_sequencetosequence_networks","_bibtex":"@article{\n  anonymous2018geoseq2seq:,\n  title={GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkRsFSRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper91/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}