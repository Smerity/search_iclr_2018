{"notes":[{"tddate":null,"ddate":null,"tmdate":1514028398314,"tcdate":1514028398314,"number":7,"cdate":1514028398314,"id":"rJLdp2jfz","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"ryeCOusGf","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"Recall rate","comment":"Thanks for your question. \nOn the CIFAR-10 dataset against the Opt L2 attack, our LID-based detector achieved: AUC: 98.94%, Accuracy: 95.49%, Precision: 92.98%, Recall: 93.54% (AUC score was reported in Table 1). "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1514010906802,"tcdate":1514010824342,"number":5,"cdate":1514010824342,"id":"ryeCOusGf","invitation":"ICLR.cc/2018/Conference/-/Paper59/Public_Comment","forum":"B1gJ1L2aW","replyto":"rka1XhcGf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Great stuff!","comment":"Thanks for your responses. They do shed some light on the results. More questions out of curiosity: What is the recall rate? That is, what fraction of test/train examples are detected as adversarial when you learn from the mini-batch and generalize? I suspect that from the plots you've shown, most test examples are identified as non-adversarial but it looks like there are some adversarials that are very close to the tests (or even lower LID than some of the tests). If you try to get these removed, maybe you end up removing some tests. I'm wondering what the test-set rejection rate would be corresponding to the numbers in the table...."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513961224973,"tcdate":1513961224973,"number":6,"cdate":1513961224973,"id":"HyZMDhcfG","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"Hk9lgX_Mz","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"Re: BIM-b in Table 2?","comment":"Agree. We do find some interesting results and will address them in the next version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1514006169194,"tcdate":1513960164577,"number":5,"cdate":1513960164577,"id":"rka1XhcGf","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"ByIbHZKGG","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"white-box attacking LID detector","comment":"Good question. In Section \"Robustness to Adaptive Attack\", we have shown that simply attacking the LID score (towards decreasing the LID scores of adversarial examples) is not effective. Attacking the logistic regression model itself can be expected to have similar results if directly integrate the detection model into the adversarial objective as we did for the LID score. In this paper, we show that adversarial examples tend to transit from a low dimensional submanifold to a more \"complex\" submanifold. A more interesting question is to what extent an attack strategy relies on such transition to find a valid solution. Our proposed LID is not a perfect solution for adversarial detection, after all, it is not 100% accurate in the detection of adversarial examples. In the future, we will investigate more forms of adapted Opt attacks against our LID detector and develop a more in-depth understanding of the detected/escaped adversarial examples. We would very much like to see Nicholas's response to this."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513857737749,"tcdate":1513850110061,"number":4,"cdate":1513850110061,"id":"ByIbHZKGG","invitation":"ICLR.cc/2018/Conference/-/Paper59/Public_Comment","forum":"B1gJ1L2aW","replyto":"Hk9lgX_Mz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"What happens if the adversary has knowledge of the logistic regression model?","comment":"Do you have insights on what might happen if the adversary actually has full knowledge of the logistic regression model trained to distinguish based on the LID score (or he/she could train one himself/herself, too)?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513791474385,"tcdate":1513791474385,"number":3,"cdate":1513791474385,"id":"Hk9lgX_Mz","invitation":"ICLR.cc/2018/Conference/-/Paper59/Public_Comment","forum":"B1gJ1L2aW","replyto":"SkjsvgdMG","signatures":["~Nicholas_Carlini1"],"readers":["everyone"],"writers":["~Nicholas_Carlini1"],"content":{"title":"Re: BIM-b in Table 2?","comment":"I agree with the comment that it looks like there is something interesting going on here (it's not immediately clear why training on FGSM will make it do better on optimization methods). However: it does look like the authors properly evaluate the defense given equation (5).\n\nPerforming the adaptive attack is what is missing from most prior work (and, indeed, even from many of the papers submitted here). I would much rather see a paper with a proper evaluation and some followup questions on the results than one that omits the evaluation entirely and therefore has no questions. I hope the authors are not penalized for this."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513781154856,"tcdate":1513781154856,"number":4,"cdate":1513781154856,"id":"SkjsvgdMG","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"ByhCuVUff","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"Re: BIM-b in Table 2?","comment":"Thank you for these comments. We have uploaded a new version to address them.\n\n-- Understanding of Table 2:\nWe would like to clarify that the detectors used in Table 2 to detect other attacks, in the previous version, were trained with 20% more data than those used in Table 1 --- that is, the detectors in Table 2 were trained on the training set (80%) plus the test set (20%), whereas those in Table 1 were trained only on the training set (80%). In the latest version of the paper, we have fixed this inconsistency and provided updated results for Table 2, using exactly the same amount of training data (80%) as was used for Table 1. Moreover, we have also provided additional explanations about the exceptionally poor performance of BU measure transferring from FGM to BIM-b.\n \nMeanwhile, we would like to point out that the Opt (or CW) attack we used in this paper is its general L2 version, different to many of its variants designed to attack specific defenses. Code on GitHub by the authors of Opt: https://github.com/carlini/nn_robust_attacks/blob/master/l2_attack.py\nThe reason is that we are more interested in the understanding of the shared properties across different types of attack strategies including Opt and also many others, so that to motivate defense against adversarial attacks in general (not limited to Opt attack). You are very welcome to check the consistency of the code --- we appreciate your interest in this.\n \nThe following responses are related to two papers: 1) the original defense paper of KD and BU (Feinman et al. (2017)), and 2) the latest attack paper (Carlini&Wagner (2017a)).\n \n-- KD works on CIFAR-10?\nYes, our result in Table 1 indicates this. This contradicts the statement in Paper 2 Sect. 5.2, where it says that KD cannot work on CIFAR, as 80% of the time the adversarial example has a higher likelihood score than the original image. However, our result is consistent with the result in the original Feinman paper (Table 2, Paper 1). We found not only that Opt plus KD can detect Opt, but also that the simpler attack FGM plus KD can detect Opt (Table 2). The deeper reasons behind this require further exploration.\n \n--‘Opt’ seems to fail against the KD detector?\nYes. But it is the general Opt L2 attack that failed the KD detector, not the KD-adapted version of Opt. In Paper 2, the KD detector has been attacked by a variant of Opt specifically adapted to target KD measure. But we did not use this version of Opt, as we are more interested in the general Opt attack rather than a specially-adapted version of it."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513706566946,"tcdate":1513666772377,"number":2,"cdate":1513666772377,"id":"ByhCuVUff","invitation":"ICLR.cc/2018/Conference/-/Paper59/Public_Comment","forum":"B1gJ1L2aW","replyto":"rJ1LK7zfM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"BIM-b in Table 2?","comment":"Table 2: What happens in the case of BIM-b? \n\nAlso, it seems very strange that for many cases in Table 2, training with FGM seems to give better detection rates that training with the actual attack used? (In 7/15 cases).\n\nAlso, the 'opt' approach seems to fail on KD as well, while it has been shown to break KD easily earlier in literature (as mentioned in intro). Why does 'opt' fail on KD in the current setting? Was something changed with KD?\n\nSection 5.2 of Carlini & Wagner 2017a indicates an approach to make these attacks successful in the MNIST setting. And, they suggest that KD completely breaks down in the CIFAR-10 setting whilst you report a 91% accuracy. I suspect there might be some issue with implementing the CW attack, since the numbers are in complete contrast to that reported in 2017a, to be sure I would just check with the implementation at https://github.com/carlini/nn_breaking_detection/blob/master/density_estimation.py."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513400845229,"tcdate":1513400845229,"number":3,"cdate":1513400845229,"id":"r1BGcmzzG","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"rkARQJwez","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"Typo fixed.","comment":"We are glad that you like our work and would like to thank you for the summary. The typo has been fixed in the updated version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513400768726,"tcdate":1513400768726,"number":2,"cdate":1513400768726,"id":"HJFaKQGfG","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"H1wVDrtgM","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"Thanks for your reply.","comment":"We appreciate your candor about this research topic. At a high level, although deep neural networks have demonstrated superior performance for many tasks, certain properties which can affect their behavior (such as subspaces, manifold properties) are still not well understood.   A better understanding of these properties can motivate more robust/efficient/effective deep learning models, which can in turn lead to further improving their performance. Adversarial vulnerability is one such property that jeopardizes the reliability of deep neural network learning models, as very small changes on inputs can sometimes lead to completely incorrect predictions (such changed inputs are called adversarial inputs). In this paper, we investigate the expansion dimensional property of the subspaces surrounding such adversarial inputs and show that it can be used as an effective characteristic for detecting such inputs. We hope our work can provide some new insights into adversarial subspaces and their detection."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513400646982,"tcdate":1513400646982,"number":1,"cdate":1513400646982,"id":"rJ1LK7zfM","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Comment","forum":"B1gJ1L2aW","replyto":"S1tVnWqxM","signatures":["ICLR.cc/2018/Conference/Paper59/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper59/Authors"],"content":{"title":"Comments have been addressed in updated paper.","comment":"Thank you very much for these comments. We address them in detail below.\nQ1: The adversarial attack technique needs to be available for training.\nA1: Thank you for highlighting this. The ability to detect unseen adversarial attacks is an interesting issue.   We have conducted some additional experiments to evaluate the generalizability of our LID based detector, see “Generalizability Analysis”, Section 5.3. The result illustrates that our LID-based detector generalizes well to detect previously unseen adversarial attacks.    \n \nQ2: (Minor) Page 3, Eq 1: The expansion dimension cares more about the probability mass.\nA2: Yes, we agree. The suggested explanation has been added to Paragraph 1, Section 3.\n \nQ3: Alg 1, L3: is this where the adversarial attacks are applied?\nA3: Yes. We have updated the description of the algorithm to clarify this (see Paragraph 2 in \"Using LID to Characterize Adversarial Examples\", Section 4).\n \nQ4: Alg 1, L12 & Section 5, Experimental Setup: leave-one-out estimate?\nA4: Yes, the query point x is \"left out\". We have added extra explanations of how Eq (4) (as used in L12-14, Alg 1) works in the last paragraph of Section 3.\n \nQ5: Section 4 and Alg 1: assuming training data is free of adversarial examples?\nA5: Yes. This is a reasonable assumption and is the one which has been made in previous work. We have highlighted this in the 2nd paragraph of \"Using LID to Characterize Adversarial Examples\", Section 4."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642475482,"tcdate":1511820336600,"number":3,"cdate":1511820336600,"id":"S1tVnWqxM","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Review","forum":"B1gJ1L2aW","replyto":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference/Paper59/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Intuitive solution to important problem; well written ","rating":"7: Good paper, accept","review":"The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly. The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility. My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector. Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique. However, this does not invalidate the contributions of this manuscript.\n\nSpecific comments/questions:\n- (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting.\n- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described.\n- Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique? \n- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate? Otherwise, r_1(.) for each point is 0, leading to a somewhat \"under-estimate\" of the true LID of the normal points in the training set. I understand that it is not an issue in the test set.\n- Section 4 and Alg 1: S we do not really care about the \"labels/targets\" of the examples. All examples in the dataset are considered \"normal\" to start with. Is this assuming that the \"initial training set\" which is used to obtain the \"pre-trained DNN\" free of adversarial examples?\n- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the \"leave-one-out\" version of the estimation.\n- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642475555,"tcdate":1511769903415,"number":2,"cdate":1511769903415,"id":"H1wVDrtgM","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Review","forum":"B1gJ1L2aW","replyto":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference/Paper59/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not familiar with this research topic.","rating":"6: Marginally above acceptance threshold","review":"This paper tried to analyze the subspaces of the adversarial examples neighborhood. More specifically, the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces. The characteristics and theoretical analysis of the proposed method are discussed and explained. This paper helps others to better understand the vulnerabilities of DNNs.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642475599,"tcdate":1511613398400,"number":1,"cdate":1511613398400,"id":"rkARQJwez","invitation":"ICLR.cc/2018/Conference/-/Paper59/Official_Review","forum":"B1gJ1L2aW","replyto":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference/Paper59/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Intrinsic dimensionality around the adversarial example is very different from the one of normal or noisy data","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper considers a problem of adversarial examples applied to the deep neural networks. The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples. More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4).  Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time. In other words, the paper proposes a particular approach for the adversarial defence.\n\nIt turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) . Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours. The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples. For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN. These values are finally used as features in classifying adversarial examples from normal and noisy ones. \n\nThe authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature. The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1).\n\nInterestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design. Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time. In other words, the proposed technique is rather stable and can not be easily exploited.\n\nI really enjoyed reading this paper. All the statements are very clear, the structure is transparent and easy to follow. The writing is excellent. I found only one typo (page 8, \"We also NOTE that...\"), otherwise I don't actually have any comments on the text.\n\nUnfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method. However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513780674763,"tcdate":1508822744305,"number":59,"cdate":1509739506101,"id":"B1gJ1L2aW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1gJ1L2aW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality","abstract":"Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. In particular, effective measures are required to discriminate adversarial examples from normal examples in such regions. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the detection of adversarial examples generated using the state-of-the-art attacks. We show that when applied for adversarial detection, an LID-based method can outperform several state-of-the-art detection measures by large margins for five attack strategies across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.","pdf":"/pdf/b701e527bc22fb29434e7fb6f207076c11ee1c9c.pdf","TL;DR":"We characterize the dimensional properties of adversarial subspaces in the neighborhood of adversarial examples via the use of Local Intrinsic Dimensionality (LID), and empirically show that such characteristics can discriminate adversarial examples effectively.","paperhash":"anonymous|characterizing_adversarial_subspaces_using_local_intrinsic_dimensionality","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1gJ1L2aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper59/Authors"],"keywords":["Adversarial Subspace","Local Intrinsic Dimensionality","Adversarial Defense","Deep Neural Networks"]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}