{"notes":[{"tddate":null,"ddate":null,"tmdate":1512073387789,"tcdate":1512072747325,"number":4,"cdate":1512072747325,"id":"S1mE8yRxz","invitation":"ICLR.cc/2018/Conference/-/Paper761/Public_Comment","forum":"B18WgG-CZ","replyto":"B18WgG-CZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"size of embeddings","comment":"In the Appendix, it is said \"In tables 3 and 5 we do not concatenate the representations of multiple models.\", which is a bit confusing. Are the embeddings a concatenation of separate encoders for the results in Table 2, and from a shared encoder in table 3 and 5?\nIt would be nice to include the size of the embeddings in Table 2 for a clearer and fair comparison to other methods: in particular, what is the size of the sentence embeddings of \"+STN +Fr +De +NLI\" and variants?\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222745005,"tcdate":1511912786109,"number":3,"cdate":1511912786109,"id":"H1qLBusxz","invitation":"ICLR.cc/2018/Conference/-/Paper761/Official_Review","forum":"B18WgG-CZ","replyto":"B18WgG-CZ","signatures":["ICLR.cc/2018/Conference/Paper761/AnonReviewer2"],"readers":["everyone"],"content":{"title":"compelling empirical results; a few oddities that can be resolved in time","rating":"7: Good paper, accept","review":"This paper is about learning sentence embeddings by combining a bunch of training signals: predicting the next & previous sentences (skip-thought), predicting the sentence's translation, classifying entailment relationships between two sentences, and predicting the constituent parse of a sentence. This is a simple idea that combines a bunch of things from prior work into one framework and yields strong results, outperforming most prior work on most tasks. \n\nI think this paper is impressive in how it scales up training to use so many tasks and such large training sets for each task.  That and its strong experimental results make it worthy of publication. It's not very surprising that adding more tasks and data improves performance on average across downstream tasks, but it is nice to see the experimental results in detail. While many people would think of this idea, few would have the resources and expertise necessary to do it justice.  I also like how the authors move beyond the standard sentence tasks to evaluate also on the Quora question duplicate task with different amounts of training data and also consider the sentence characteristic / syntactic property tasks.  It would be great if the authors could release their pretrained sentence representation model so that other researchers could use it. \n\nI do have some nitpicks here and there with the presentation and exposition, and I am concerned that at times the paper appears to be minimizing its weaknesses, but I think these are things that can be addressed in the next revision. I understand that sometimes it's tempting to minimize one's weaknesses in order to get a paper accepted because the reviewers may not understand the area very well and may get hung up on the wrong things. I understand the area well and so all the feedback I offer below comes from a place of desiring this paper's publication while also desiring it to be as accurate and helpful for the community as possible. \n\nBelow I'll discuss my concerns with the experiments and description of the results.\n\nRegarding the results in Table 2:\n\nThe results in Table 2 seem a little bit unstable, as it is unclear which setting to use for the classification tasks; maybe it depends on the kind of classification being performed. One model seems best for the sentiment tasks (\"+2L +STP\") while other models seem best for SUBJ and MPQA.  Adding parsing as a training task hurts performance on the sentence classification tasks while helping performance on the semantic tasks, as the authors note.  It is unclear which is the best general model.  In particular, when others write papers comparing to the results in this paper, which setting should they compare to?  It would be nice if the authors could discuss this. \n\nThe results reported for the CNN-LSTM of Gan et al. do not exactly match those of any single row from Gan et al, either v1 or v2 on arxiv or the published EMNLP version. How were those specific numbers selected? \n\nThe caption of Table 2 states \"All results except ours are taken from Conneau et al. (2017).\" However, Conneau et al (neither the latest arxiv version nor the published EMNLP version) does not include many of the results in the table, such as CNN-LSTM and DiscSent mentioned in the following sentence in the caption. Did the authors replicate the results of those methods themselves, or report them from other papers?\n\nWhat does bold and underlining indicate in Table 2?  I couldn't find this explained anywhere. \n\nAt the bottom of Table 2, in the section with approaches trained from scratch on these tasks, I'd suggest including the 89.7 SST result of Munkhdalai and Yu (2017) and the 96.1 TREC result of Zhou et al. (2016) (as well as potentially other results from Zhou et al, since they report results on others of these datasets). The reason this is important is because readers may observe that the paper's new method achieves higher accuracies on SST and TREC than all other reported results and mistakenly think that the new method is SOTA on those tasks.  I'd also suggest adding the results from Radford et al. (2017) who report 86.9 on MR and 91.4 on CR.  For other results on these datasets, including stronger results in non-fixed-dimensional-sentence-embedding transfer settings, see results and references in McCann et al. (2017).  While the methods presented in this paper are better than prior work in learning general purpose, fixed-dimensional sentence embeddings, they still do not produce state-of-the-art results on that many of these tasks, if any.  I think this is important to note. \n\nFor all tasks for which there is additional training, there's a confound due to the dimensionality of the sentence embeddings across papers. Using higher-dimensional sentence embeddings leads to more parameters in the linear model being trained on the task data. So it is unclear if the increase in hidden units in rows with \"+L\" is improving the results because of providing more weights for the linear model or whether it is learning a better sentence representation. \n\nThe main sentence embedding results are in Table 2, and use the SentEval framework. However, not all tasks are included. The STS Benchmark results are included, which use an additional layer trained on the STS Benchmark training data just like the SICK tasks. But the other STS results, which use cosine similarity on the embedding space directly without any retraining, are only included in the appendix (in Table 7). The new approach does not do very well on those unsupervised tasks. On two years of data it is better than InferSent and on two years it is worse. Both are always worse than the charagram-phrase results of Wieting et al (2016a), which has 66.1 on 2012, 57.2 on 2013, 74.7 on 2014, and 76.1 on 2015.  Charagram-phrase trains on automatically-generated paraphrase phrase pairs, but these are generated automatically from parallel text, the same type of resource used in the \"+Fr\" and \"+De\" models proposed in this submission, so I think it should be considered as a comparable model. \n\nThe results in the bottom section of Table 7, reported from Arora et al (2016), were in turn copied from Wieting et al (2016b), so I think it would make sense to also cite Wieting et al (2016b) if those results are to be included. Also, it doesn't seem appropriate to designate those as \"Supervised Approaches\" as they only require parallel text, which is a subset of the resources required by the new model. \n\nThere are some other details in the appendix that I find concerning:\n\nSection 8 describes how there is some task-specific tuning of which function to compute on the encoder to produce the sentence representation for the task.  This means that part of the improvement over prior work (especially skip-thought and InferSent) is likely due to this additional tuning. So I suppose to use these sentence representations in other tasks, this same kind of tuning would have to be done on a validation set for each task?  Doesn't that slightly weaken the point about having \"general purpose\" sentence representations?\n\nSection 9 provides details about how the representations are created for different training settings. I am confused by the language here. For example, the first setting (\"+STN +Fr +De\") is described as \"A concatenation of the representations trained on these tasks with a unidirectional and bidirectional GRU with 1500 hidden units each.\" I'm not able to parse this. I think the authors mean \"The sentence representation h_x is the concatenation of the final hidden vectors from a forward GRU (with 1500-dimensional hidden vectors) and a bidirectional GRU (also with 1500-dimensional hidden vectors)\". Is this correct? \n\nAlso in Sec 9: I found it surprising how each setting that adds a training task uses the concatenation of a representation with that task and one without that task. What is the motivation for doing this? This seems to me to be an important point that should be discussed in Section 3 or 4. And when doing this, are the concatenated representations always trained jointly from scratch with the special task only updating a subset of the parameters, or do you use the fixed pretrained sentence representation from the previous row and just concatenate it with the new one?  To be more concrete, if I want to get the encoder for the second setting (\"+STN +Fr +De +NLI\"), do I have to train two times or can I just train once?  That is, the train-once setting would correspond to only updating the NLI-specific representation parameters when training on NLI data; on other data, all parameters would be updated. The train-twice setting would first train a representation on \"+STN +Fr +De\", then set it aside, then train a separate representation on \"+STN +Fr +De +NLI\", then finally concatenate the two representations as my sentence representation.  Do you use train-once or train-twice? \n\nRegarding the results in Table 3:\n\nWhat do bold and underline indicate?\n\nWhat are the embeddings corresponding to the row labeled \"Multilingual\"?\n\nIn the caption, I can't find footnote 4. \n\nThe caption includes the sentence \"our embeddings have 1040 pairs out of 2034 for which atleast one of the words is OOV, so a comparison with other embeddings isn't fair on RW.\"  How were those pairs handled?  If they were excluded, then I think the authors should not report results on RW.  I suspect that most of the embeddings included in the table also have many OOVs in the RW dataset but still compute results on it using either an unknown word embedding or some baseline similarity of zero for pairs with an OOV. I think the authors should find some way (like one of those mentioned, or some other way) of computing similarity of those pairs with OOVs. It doesn't make much sense to me to omit pairs with OOVs. \n\nThere are much better embeddings on SimLex than the embeddings whose results are reported in the table. Wieting et al. (2016a) report SimLex correlation of 0.706 and Mrkšić et al. (2017) report 0.751.  I'd suggest adding the results of some stronger embeddings to better contextualize the embeddings obtained by the new method.  Some readers may mistakenly think that the embeddings are SOTA on SimLex since no stronger results are provided in the table. \n\n\nThe points below are more minor/specific:\n\nSec. 2:\n\nIn Sec. 2, the paper discusses its focus on fixed-length sentence representations to distinguish itself from other work that produces sentence representations that are not fixed-length. I feel the motivation for this is lacking. Why should we prefer a fixed-length representation of a sentence? For certain downstream applications, it might actually be easier for practitioners to use a representation that provides a representation for each position in a sentence (Melamud et al., 2016; Peters et al., 2017; McCann et al., 2017) rather than an opaque sentence representation. Some might argue that since sentences have different lengths, it would be appropriate for a sentence representation to have a length proportional to the length of the sentence.  I would suggest adding some motivation for the focus on fixed-length representations. \n\nSec. 4.1:\n\n\"We take a simpler approach and pick a new task to train on after every parameter update sampled uniformly. An NLI minibatch is interspersed after every ten parameter updates on sequence-to-sequence tasks\"\nThese two sentences seem contradictory. Maybe in the first sentence \"pick a new task\" should be changed to \"pick a new sequence-to-sequence task\"?\n\nSec. 5.1:\n\ntypo: \"updating the parameters our sentence\" --> \"updating the parameters of our sentence\"\n\nSec. 5.2:\n\ntypo in Table 4 caption: \"and The\" --> \". The\"\n\ntypo: \"parsing improvements performance\" --> \"parsing improves performance\"\n\n\nIn general, there are many missing citations for the tasks, datasets, and prior work on them. I understand that the authors are pasting in numbers from many places and just providing pointers to papers that provide more citation info, but I think this can lead to mis-attribution of methods. I would suggest including citations for all datasets/tasks and methods whose results are being reported. \n\n\nReferences:\n\nMcCann, Bryan, James Bradbury, Caiming Xiong, and Richard Socher. \"Learned in translation: Contextualized word vectors.\" CoRR 2017.\n\nMelamud, Oren, Jacob Goldberger, and Ido Dagan. \"context2vec: Learning Generic Context Embedding with Bidirectional LSTM.\" CoNLL 2016.\n\nMrkšić, Nikola, Ivan Vulić, Diarmuid Ó. Séaghdha, Ira Leviant, Roi Reichart, Milica Gašić, Anna Korhonen, and Steve Young. \"Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints.\" TACL 2017.\n\nMunkhdalai, Tsendsuren, and Hong Yu. \"Neural semantic encoders.\" EACL 2017. \n\nPagliardini, Matteo, Prakhar Gupta, and Martin Jaggi. \"Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features.\" arXiv preprint arXiv:1703.02507 (2017).\n\nPeters, Matthew E., Waleed Ammar, Chandra Bhagavatula, and Russell Power. \"Semi-supervised sequence tagging with bidirectional language models.\" ACL 2017.\n\nRadford, Alec, Rafal Jozefowicz, and Ilya Sutskever. \"Learning to generate reviews and discovering sentiment.\" arXiv preprint arXiv:1704.01444 2017.\n\nWieting, John, Mohit Bansal, Kevin Gimpel, and Karen Livescu. \"Charagram: Embedding words and sentences via character n-grams.\" EMNLP 2016a.\n\nWieting, John, Mohit Bansal, Kevin Gimpel, and Karen Livescu. \"Towards universal paraphrastic sentence embeddings.\" ICLR 2016b.\n\nZhou, Peng, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. \"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling.\" COLING 2016.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1511894759685,"tcdate":1511894759685,"number":3,"cdate":1511894759685,"id":"SJlxJ4ixG","invitation":"ICLR.cc/2018/Conference/-/Paper761/Public_Comment","forum":"B18WgG-CZ","replyto":"B18WgG-CZ","signatures":["~Kyunghyun_Cho1"],"readers":["everyone"],"writers":["~Kyunghyun_Cho1"],"content":{"title":"how do you know when to end learning?","comment":"the models in this paper were trained for some arbitrary duration of 7 days. how was this duration selected, and how stable are the reported results w.r.t. different # of training days? if the training was terminated after 5 days (or 8 days or whatever that is not 7 days), would the results stay as they are reported in the submission?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222745054,"tcdate":1511855084621,"number":2,"cdate":1511855084621,"id":"BkSxEc5xz","invitation":"ICLR.cc/2018/Conference/-/Paper761/Official_Review","forum":"B18WgG-CZ","replyto":"B18WgG-CZ","signatures":["ICLR.cc/2018/Conference/Paper761/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"This paper shows that learning sentence representations from a diverse set of tasks (skip-thought objective, MT, constituency parsing, and natural language inference) produces .\nThe main contribution of the paper is to show learning from multiple tasks improves the quality of the learned representations.\nExperiments on various text classification and sentiment analysis datasets show that the proposed method is competitive with existing approaches.\nThere is an impressive number of experiments presented in the paper, but the results are a bit mixed, and it is not always clear that adding more tasks help.\n\nI think this paper addresses an important problem of learning general purpose sentence representations. \nHowever, I am unable to draw a definitive conclusion from the paper. \nFrom Table 2, the best performing model is not always the one with more tasks. \nFor example, adding a parsing objective can either improve or lower the performance quite significantly.\nCould it be that datasets such as MRPC, SICK, and STSB require more understanding of syntax?\nEven if this is the case, why adding this objective hurt performance for other datasets?\nImportantly, it is also not clear whether the performance improvement comes from having more unlabeled data (even if it is trained with the same training objective) or having multiple training objectives.\nAnother question I have is that if there is any specific reason that language modeling is not included as one of the training objectives to learn sentence representations, given that it seems to be the easiest one to collect training data for.\n\nThe results for transfer learning and low resource settings are more positive.\nHowever, it is not surprising that pretraining parts of the model on a large amount of unlabeled data helps when there is not a lot of labeled examples.\n\nOverall, while the main contribution of the paper is that having multiple training objectives help learning better sentence, I am not yet convinced by the experiments that this is indeed the case.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1511505933509,"tcdate":1511505933509,"number":2,"cdate":1511505933509,"id":"HkHMeSHez","invitation":"ICLR.cc/2018/Conference/-/Paper761/Public_Comment","forum":"B18WgG-CZ","replyto":"Syi8hpvCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Clarifications","comment":"Hi,\n\nThank you for your questions!\n\n1. We did not consider starting our ablations with just SkipThought or NMT since they’ve been explored individually by Kiros et al 2015 and Hill et al 2016 respectively. We however, ran an experiment with just a large skip-thought next model (+STN + L) (4096 dimensions). Our results of this new ablation on the set of tasks presented in Table 2 (in the same column order) are - 78.9|85.8|93.7|87.2|80.4|84.2|72.4/81.6|0.840|82.1|72.9/72.4 , which indicates that there is some improvement with the addition of NMT even on a smaller model. We'll include these results in our first paper revision.\n\n2. We did use SentEval in all evaluations except on the Quora dataset and Table 5 since they aren’t a part of SentEval (hence ‘largely’). We’ll make this clearer.\n\n3. The following are the results for our model (+STN +Fr +De +NLI +L +STP) using vocabulary expansion with the glove 840B vectors versus using our learned <unk> token for every OOV on the set of tasks presented in Table 2. The results are in the same column order and are in format (vocab expansion/<unk> token). 82.2/81.83| 87.8/87.3| 93.8/93.7| 91.3/91.1| 84.5/84.0| 92.4/91.8| (78.0/83.8)/(78.4/84.1)| 0.885/0.884| 86.8/86.9| (79.2/78.8)/(79.0/78.6)|. These results indicate that there is a small benefit to performing vocabulary expansion."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222745093,"tcdate":1511125631610,"number":1,"cdate":1511125631610,"id":"BJOtf_JlG","invitation":"ICLR.cc/2018/Conference/-/Paper761/Official_Review","forum":"B18WgG-CZ","replyto":"B18WgG-CZ","signatures":["ICLR.cc/2018/Conference/Paper761/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Generally positive review","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper explores a variety of tasks for the pretraining of a bidirectional GRU sentence encoder for use in data-poor downstream tasks. The authors find that the combination of supervised training with NLI, MT, and parsing, plus unsupervised training on the SkipThought objective yields a model that robustly outperforms the best prior method on every task included in the standard SentEval suite, and several others.\n\nThis paper isn't especially novel. The main results of the paper stem from a combination of a few ideas that were ripe for combination (SkipThought from Kiros, BiLSTM-max and S/MNLI from Conneau, MT from McCann, parsing following Luong, etc.). However, the problem that the paper addresses is a major open issue within NLP, and the paper is very well done, so it would be in the best interest of all involved to make sure that the results are published promptly. I strongly support acceptance.\n\nMy one major request would be a more complete ablation analysis. It would be valuable for researchers working on other languages (among others) to know which labeled or unlabeled datasets contributed the most. Your ablation does not offer enough evidence to one to infer this---among other things, NLI and MT are never presented in isolation, and parsing is never presented without those two. Minimally, this should involve presenting results for models trained separately on each of the pretraining tasks.\n\nI'll also echo another question from Samuel's comment: Could you say more about how you conducted the evaluation on the SentEval tasks? Did your task-specific model (or the training/tuning procedure for that model) differ much from prior work?\n\nDetails:\n\nThe paragraph starting \"we take a simpler approach\" is a bit confusing. If task batches are sampled *uniformly*, how is NLI be sampled less often than the other tasks?\n\nGiven how many model runs are presented, and that the results don't uniformly favor your largest/last model, it'd be helpful to include some kind of average of performance across tasks that can be used as a single-number metric for comparison. This also applies to the word representation evaluation table.\n\nWhen comparing word embeddings, it would be helpful to include the 840B-word release of GloVe embeddings. Impressionistically, that is much more widely used than the older 6B-word release for which Faruqui reports numbers. This isn't essential to the paper, but it would make your argument in that section more compelling.\n\n\"glove\" => GloVe; \"fasttext\" => fastText","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1509575762753,"tcdate":1509575762753,"number":1,"cdate":1509575762753,"id":"Syi8hpvCZ","invitation":"ICLR.cc/2018/Conference/-/Paper761/Public_Comment","forum":"B18WgG-CZ","replyto":"B18WgG-CZ","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Neat! A few questions...","comment":"Cool results. I very much appreciated the analysis (and Adi et al. results)!\n\n– Do you have any results (formal or informal) on either SkipThought alone or MT alone using your implementation? It's a bit odd that the ablation starts with three tasks.\n– Do you actually use SentEval (Conneau's evaluation software toolkit), or your own implementation of the same set of evaluations? You claim that you borrowed your evaluation 'largely' from them.\n– Do you have any impression of how important it was to perform vocabulary expansion (relative to using an UNK token and/or raw w2v vectors)?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739117470,"tcdate":1509134334288,"number":761,"cdate":1509739114804,"id":"B18WgG-CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B18WgG-CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning","abstract":"A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. \nWe train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.","pdf":"/pdf/90476aaceb535085aad9de286cc46cb257e05924.pdf","TL;DR":"A large-scale multi-task learning framework with diverse training objectives to learn fixed-length sentence representations","paperhash":"anonymous|learning_general_purpose_distributed_sentence_representations_via_large_scale_multitask_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B18WgG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper761/Authors"],"keywords":["distributed sentence representations","multi-task learning"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}