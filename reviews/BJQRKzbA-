{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222809372,"tcdate":1512062090439,"number":3,"cdate":1512062090439,"id":"HkM52nagG","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Review","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper on searching space of network design","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors present a novel evolution scheme applied to neural network architecture search. It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm. To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells. Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure. To speed up the search, they focus on finding cells instead of an entire network. In evaluation time, they insert these cells between layers of a network comparable in size to known networks. They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance.\n\nThe method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. It could lead to new insight on automating design of neural networks for given problems. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The paper presents a good work and is well articulated. However, it could benefit from additional details and a deeper analysis of the results.\n\nThe key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. The method is also appealing for its use of some kind of emergence between two levels of hierarchy. In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules. Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm. Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy? Are the cells found interpretable? The authors should try to give their opinion about the design obtained.\n\nThe implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description. During evaluation, what is a step? A batch or an epoch or other?\n\nThe method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. It raises questions on the usability of the method for small labs. At some point, we will have to use insights from this search to stop early, when no improvement is expected. Also, authors claim that their method consume less computation time than reinforcement learning. This should be supported by some quantitative results.\n\nThe paper would greatly benefit from a deeper comparison over other techniques. For instance, it could describe more the advantages over reinforcement learning. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. It could have taken more spaces in the paper.\n\nI am also concerned the computational efficiency of the results obtained with this method on current processors. Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand. Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand. Does the solution obtained with the optimization can be run as efficiently? A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper. This is a general comment over this kind of approach, but I think it should be addressed. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1512222809419,"tcdate":1511848728924,"number":2,"cdate":1511848728924,"id":"SkbQs_cgz","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Review","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Great results, needlessly overstated.","rating":"6: Marginally above acceptance threshold","review":"This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL. The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly.\n\nThe biggest two nitpicks:\n\n> In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs)\n\nThis is a gross overstatement. The architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution, separable convolution, and pooling layers. Only the topology of the cells themselves are designed. The work is still great, but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste. As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network. It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the \"[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search.\"\n\n> Table 1\n\nWhy is the second best method on CIFAR (“Hier. repr-n, random search (7000 samples)”) never tested on ImageNet? The omission is conspicuous. Just test it and report.\n\nSmaller nitpicks:\n\n> “New state of the art for evolutionary strategies on this task”\n\n“Evolutionary Strategies”, at least as used in Salimans 2017, has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do. It may be more clear to change this phrase to “evolutionary methods” or similar.\n\n> Our evolution algorithm is similar but more generic than the binary tournament selection (K = 2) used in a recent large-scale evolutionary method (Real et al., 2017).\n\nA K=5% tournament does not seem more generic than a binary K=2 tournament. They’re just different.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1512222809463,"tcdate":1511727484392,"number":1,"cdate":1511727484392,"id":"BJNK-sdxz","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Review","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/AnonReviewer2"],"readers":["everyone"],"content":{"title":"In this paper, the authors propose a novel evolutionary algorithm for neural architecture search. ","rating":"6: Marginally above acceptance threshold","review":"The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy.\n\nManually designing novel neural architectures is a laborious, time-consuming process. Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field. \n\nOverall, the paper is well-written, clear in its exposition and technically sound. While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible. \n\nThe fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. \n\nNevertheless, while the idea behind the proposed approach is definitely interesting, I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications. In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small. Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3.\n\nAnother concern I have is regarding the definition of the mutation operators in Section 3.1. While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it). If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A.\n\nFinally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations. I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values.\n\nReferences:\n\n[1] Grosse, Roger, et al. \"Exploiting compositionality to explore a large space of model structures.\" UAI (2012).\n[2] Duvenaud, David, et al. \"Structure discovery in nonparametric regression through compositional kernel search.\" ICML (2013).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1510092386284,"tcdate":1509136843575,"number":893,"cdate":1510092362723,"id":"BJQRKzbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}