{"notes":[{"tddate":null,"ddate":null,"tmdate":1515168945891,"tcdate":1515168945891,"number":2,"cdate":1515168945891,"id":"HJ9nV7amG","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Comment","forum":"BJQRKzbA-","replyto":"H1yO14fzz","signatures":["ICLR.cc/2018/Conference/Paper893/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper893/Authors"],"content":{"title":"Clarifications for reproducing the results","comment":"Thank you for taking the effort to implement our algorithm. Your detailed comments are valuable for us to improve the paper further. We provide the clarifications below, which would hopefully be useful for reproducing our results.\n\n* “it would be impossible for the depthwise convolution operation to actually have a constant number of output filters as described.”\nWe did not require depthwise convolution operations to have a constant number of output filters (we’ll remove “of C channels” in the the 2nd bullet point in Sect. 2.3, which was a typo). This is not an issue because the way that we merge the nodes (depthwise concatenation) does not require the input nodes to have the same number of filters.\n\n* “separable convolution would no longer be valid as a primitive as it could be produced by a stacked depthwise and 1x1 convolution”\nIn our case, each convolutional operation comes with batch normalization (BN) and ReLU, hence the separable convolution\n3x3_depthwise_conv->1x1_conv->BN->ReLU\nis not exactly the same as the stack of \n3x3_depthwise_conv->BN->ReLU and 1x1_conv->BN->ReLU.\nWe also note that in general the algorithm remains valid if one primitive operation can be expressed in terms of the others.\n\n* “the initialization routine seems to imply that the identity operation is available at every level”\nNo, the identity operation is only available at the motif level: a motif is initialised as a chain of identity operations, and a cell is initialised as a chain of motifs (note that a chain of identity chains is also an identity chain).\n\n* “The probability distributions used for random sampling in mutation are not given”\nWe always use uniform distributions in all of our experiments. That being said, no hyperparameters were involved or tuned for mutation operations.\n\n* About the number of mutations during initialization.\nWe would like to point out that a large number of mutations is necessary to produce a diverse initial population of architectures. In our case we used 1000."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1515168778864,"tcdate":1515168778864,"number":1,"cdate":1515168778864,"id":"r1mfNm6mG","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Comment","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper893/Authors"],"content":{"title":"Responses to Reviewers","comment":"We thank all reviewers for their comments. We will incorporate the suggested revisions into the new version of the paper. Our responses below focus on the major points.\n\n* About comparing computation time with RL-based approaches (reviewer 1)\nOur approach is faster than some published RL-based methods (e.g. 2000 GPU days in Zoph et al. (2017) vs 300 GPU days in our case). Having said that, we do not claim that evolution is more efficient than RL-based approaches in general.\n\n* Efficiency of architectures found using architecture search (reviewer 1)\nIn terms of the number of parameters, our ImageNet model is comparable to Inception-Resnet-v2 but larger than NASNet-A. Although identifying fast/compact architectures was not the primary focus of this work, an interesting future direction is to include FLOPS or wall clock time as a part of the evolution fitness, letting the architecture search algorithm to discover architectures that are both accurate and computationally efficient.\n\n* “During evaluation, what is a step?” (reviewer 1)\nAn evolution step refers to training and evaluation of a single architecture. We will make the definition more explicit in the revised paper.\n\n* “The authors should try to give their opinion about the design obtained” (reviewer 1)\nOur visualisation in appendix A shows that architecture search discovers a number of skip connections. For example, the cell contains a direct skip connection between input and output: nodes 1 and 5 are connected by Motif 4, which in turn contains a direct connection between input and output. The cell also contains several internal skip connections, through Motif 5 (which again comes with an input-to-output skip connection similar to Motif 4).\n\n* “the paper spins the actual experiments and results a too strongly.” (reviewers 2 and 3)\nThank you for the suggested improvements. We will revise our writing and soften the claims.\n\n* Missing ImageNet results for certain methods in Table 1 (reviewer 3)\nImageNet experiments under those two settings were still running at the time of the submission deadline. Their results are as follows:\nFlat repr-n, parameter-constrained, evolution (7000 samples): 21.2 / 5.8\nHier. repr-n, random search (7000 samples): 21.0 / 5.5.\nThe latter result is due to the fact that the evolution fitness computed on CIFAR is a proxy for ImageNet performance. Computationally efficient architecture search directly on ImageNet is an interesting direction for future research.\n\n* Mutation is biased towards adding edges (reviewer 2)\nIndeed, in our implementation we don’t ensure an equal probability of adding and deleting edges. We think inferring the mutation bias along with evolution is an interesting direction for future work.\n\n* Regarding a large number of hyperparameters specifying the architecture (reviewers 2 and 3)\nWe note that some hyperparameters can be adaptively tuned by evolution. Namely, M_l and |G^{(l)}| affect only the upper bounds on effective hyperparameters, since the algorithm may learn to not use a particular motif (hence the effective number of motifs becomes smaller than M_l), or to shortcut two nodes using an identity op (hence the effective number of nodes becomes smaller than |G^{(l)}|). Both behaviors have been empirically observed in our visualization (see Figure 5 & Figure 10 in Appendix A)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1513632070292,"tcdate":1513402214687,"number":1,"cdate":1513402214687,"id":"H1yO14fzz","invitation":"ICLR.cc/2018/Conference/-/Paper893/Public_Comment","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["~Gabriel_Meyer-Lee1"],"readers":["everyone"],"writers":["~Gabriel_Meyer-Lee1"],"content":{"title":"Reproducibility of ICLR2018 Conference Paper893","comment":"We implemented the deep neural network representation described in this paper as a part of the ICLR 2018 Reproducibility challenge and performed small-scale testing of the representation on the CIFAR-10 benchmark utilizing the described search methods.\nOur implementation of the hierarchical encoding of a deep convolutional network was written in Python utilizing Keras with a TensorFlow backend. In the process of writing this implementation, we noticed several key omissions. We presumed that “depthwise” and “separable” convolutions refer to the definition in [Chollet 2017]. In this case, it would be impossible for the depthwise convolution operation to actually have a constant number of output filters as described. Furthermore, separable convolution would no longer be valid as a primitive as it could be produced by a stacked depthwise and 1x1 convolution. As such our implementation replaced depthwise with “standard” convolution. The described merging using depthwise concatenation requires padding the pooling operations, which is both unaddressed and contradicts the traditional use of pooling layers. Additionally, the identity operation is described as a primitive, but the initialization routine seems to imply that the identity operation is available at every level.\nWe also encountered several reproducibility issues in implementing the described evolutionary algorithm. The probability distributions used for random sampling in mutation are not given. We set all of them as uniform, but this biases the mutation method towards increasing complexity. This causes the number of random mutations per architecture in initialization to become an important but unknown parameter. We checked the number of parameters produced by generating 300 random hierarchical and flat architectures, first with 50 mutations each, then with 100 mutations each. The networks were assembled into the “small” CIFAR-10 architecture to check parameter numbers. The hierarchical architectures had 46 potential edges to mutate while the flat architecture had 55. The results of this showed that the flat architecture produced networks with 1.033 ± .574 M parameters after 50 mutations and 1.784 ± .889 M parameters after 100. The hierarchical architecture produced networks with .279 ± .168 M parameters after 50 mutations and .478 ± .280 M parameters after 100. These results show that random mutation does create a diverse initial population, but the complexity of that population is proportional to the number of mutations.\nDespite all of the above mentioned issues, we were able to create a working implementation of the described system based solely off of the paper and so this submission must be given due credit as largely reproducible. Our small scale results do, however, indicate a few potential issues. We found a top validation fitness for random search on the hierarchical representation to be .73 with .42 M parameters and the top validation fitness for the flat representation to be .79 with 1.03 M parameters, both drawn from populations of 50. This is roughly in line with what’s shown in the figures at the top of section 4.2 of the submission, although the flat representation has far more parameters than any of the networks shown. The cause for this is likely due to above described omissions in the mutation and initiation routines. \nThe key obstacles to reproducing the results of this submission were the computational costs. The paper did clearly describe the costs of the experiments, but did not provide baseline results that could be replicated cheaply. The cheapest-to-compute reported results were the CIFAR-10 errors for randomly sampled architectures. Replicating these, however, is useless for evaluating the representation schemes in general or the search strategies. We did attempt this reproduction for about half the training time, producing inconclusive results of test accuracies of .79 for the hierarchical representation and .80 for the flat representation. Our recommendation, not just for these authors but for topology learning papers in general, is to augment the normal large scale benchmark-breaking experiments with mass small scale experiments. Ideally, an experiment could be run on a single GPU in one day. For this submission, this could be achieved by limiting training steps, evolution steps, or testing on an easier benchmark, like MNIST. The goal of these mass small scale experiments would be two fold: publishing results which are accessible for replication to a much larger population as well as conducting enough trials to demonstrate the statistical significance of the improvements shown by the paper’s novel methods. This would address a significant weak point of this paper, the indeterminate significance of the difference in performance between the flat and hierarchical representations.\n\nReferences:\n\nFrançois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2017"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1515642528072,"tcdate":1512062090439,"number":3,"cdate":1512062090439,"id":"HkM52nagG","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Review","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper on searching space of network design","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors present a novel evolution scheme applied to neural network architecture search. It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm. To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells. Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure. To speed up the search, they focus on finding cells instead of an entire network. In evaluation time, they insert these cells between layers of a network comparable in size to known networks. They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance.\n\nThe method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. It could lead to new insight on automating design of neural networks for given problems. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The paper presents a good work and is well articulated. However, it could benefit from additional details and a deeper analysis of the results.\n\nThe key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. The method is also appealing for its use of some kind of emergence between two levels of hierarchy. In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules. Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm. Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy? Are the cells found interpretable? The authors should try to give their opinion about the design obtained.\n\nThe implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description. During evaluation, what is a step? A batch or an epoch or other?\n\nThe method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. It raises questions on the usability of the method for small labs. At some point, we will have to use insights from this search to stop early, when no improvement is expected. Also, authors claim that their method consume less computation time than reinforcement learning. This should be supported by some quantitative results.\n\nThe paper would greatly benefit from a deeper comparison over other techniques. For instance, it could describe more the advantages over reinforcement learning. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. It could have taken more spaces in the paper.\n\nI am also concerned the computational efficiency of the results obtained with this method on current processors. Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand. Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand. Does the solution obtained with the optimization can be run as efficiently? A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper. This is a general comment over this kind of approach, but I think it should be addressed. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1515642528121,"tcdate":1511848728924,"number":2,"cdate":1511848728924,"id":"SkbQs_cgz","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Review","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Great results, needlessly overstated.","rating":"6: Marginally above acceptance threshold","review":"This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL. The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly.\n\nThe biggest two nitpicks:\n\n> In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs)\n\nThis is a gross overstatement. The architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution, separable convolution, and pooling layers. Only the topology of the cells themselves are designed. The work is still great, but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste. As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network. It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the \"[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search.\"\n\n> Table 1\n\nWhy is the second best method on CIFAR (“Hier. repr-n, random search (7000 samples)”) never tested on ImageNet? The omission is conspicuous. Just test it and report.\n\nSmaller nitpicks:\n\n> “New state of the art for evolutionary strategies on this task”\n\n“Evolutionary Strategies”, at least as used in Salimans 2017, has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do. It may be more clear to change this phrase to “evolutionary methods” or similar.\n\n> Our evolution algorithm is similar but more generic than the binary tournament selection (K = 2) used in a recent large-scale evolutionary method (Real et al., 2017).\n\nA K=5% tournament does not seem more generic than a binary K=2 tournament. They’re just different.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1515642528160,"tcdate":1511727484392,"number":1,"cdate":1511727484392,"id":"BJNK-sdxz","invitation":"ICLR.cc/2018/Conference/-/Paper893/Official_Review","forum":"BJQRKzbA-","replyto":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference/Paper893/AnonReviewer2"],"readers":["everyone"],"content":{"title":"In this paper, the authors propose a novel evolutionary algorithm for neural architecture search. ","rating":"6: Marginally above acceptance threshold","review":"The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy.\n\nManually designing novel neural architectures is a laborious, time-consuming process. Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field. \n\nOverall, the paper is well-written, clear in its exposition and technically sound. While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible. \n\nThe fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. \n\nNevertheless, while the idea behind the proposed approach is definitely interesting, I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications. In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small. Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3.\n\nAnother concern I have is regarding the definition of the mutation operators in Section 3.1. While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it). If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A.\n\nFinally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations. I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values.\n\nReferences:\n\n[1] Grosse, Roger, et al. \"Exploiting compositionality to explore a large space of model structures.\" UAI (2012).\n[2] Duvenaud, David, et al. \"Structure discovery in nonparametric regression through compositional kernel search.\" ICML (2013).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]}},{"tddate":null,"ddate":null,"tmdate":1510092386284,"tcdate":1509136843575,"number":893,"cdate":1510092362723,"id":"BJQRKzbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJQRKzbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hierarchical Representations for Efficient Architecture Search","abstract":"We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.","pdf":"/pdf/8d170a17a92217a270f4cdfbe1c84aba3c812530.pdf","TL;DR":"In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.","paperhash":"anonymous|hierarchical_representations_for_efficient_architecture_search","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Representations for Efficient Architecture Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJQRKzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper893/Authors"],"keywords":["deep learning","architecture search"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}