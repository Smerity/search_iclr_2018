{"notes":[{"tddate":null,"ddate":null,"tmdate":1514850399120,"tcdate":1514850399120,"number":4,"cdate":1514850399120,"id":"BkDvOSdQM","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Comment","forum":"r1vuQG-CW","replyto":"HybbMN0Zf","signatures":["ICLR.cc/2018/Conference/Paper809/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper809/Authors"],"content":{"title":"reply to public comment","comment":"Dear commenter, \n\nThank you for your interest in our paper. \n\nAlthough hexagonal grids have been used in signal processing for some time, our work is focused on the implementation of the group convolution for 6-fold rotational groups p6 and p6m. Thus, unlike other methods, our approach is able to exploit the symmetries of the hexagonal grid to improve statistical efficiency by parameter sharing. We have shown that our method convincingly beats a solid baseline on CIFAR, and outperforms the transfer learning baseline on AID. Our method is not related to adaptive, deformable, or separable convolution in either approach or intent.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1514850279037,"tcdate":1514850279037,"number":3,"cdate":1514850279037,"id":"Sy1xuHOXM","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Comment","forum":"r1vuQG-CW","replyto":"Syvd8Qcgf","signatures":["ICLR.cc/2018/Conference/Paper809/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper809/Authors"],"content":{"title":"reply to reviewer 3","comment":"Dear reviewer,\n\nWe thank you for your comments and suggestions.\n\nOn hexagonal literature: \nWe agree that it is important to recognize existing work on hexagonal signal processing, and have added references by Petersen, Hartman and Middleton in the updated paper. \n\nOn benefits to scaling:\nWe agree that CIFAR-10 is a relatively simple benchmark. In our experiments we do show that an identical network architecture where conventional conv layers are replaced by hexagonal g-conv layers, results in consistent improvements on two distinct datasets. Furthermore, we plan to release our codebase which can help further research to scale these methods to larger problems.\n\nOn the group convolution operator:\nWe think it is a very good suggestion to change group convolution operators from * to *_g, to clarify what type of convolution is used. We changed the relevant operators in the updated paper.\n\nOn exact translation equivariance in CNNs: \nWe agree with the reviewer that in order for equivariance to hold exactly, either:\nshifts should be cyclic, or\nOne should use “valid”-mode convolutions and consider the input image as a function defined on all of Z^2, where values outside of the original image are zero.\nIn practice, we use “same” convolution instead of “valid” convolution, because the latter would increase the size of the feature maps with each layer. Thus, a typical convolutional network is not exactly translation equivariant. We have added a footnote that addresses this detail.\n\nOn spellchecking:\nWe have run a spellchecker and fixed the spelling mistakes. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1514850013072,"tcdate":1514850013072,"number":2,"cdate":1514850013072,"id":"SJrJPrdQG","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Comment","forum":"r1vuQG-CW","replyto":"SysTGDdxf","signatures":["ICLR.cc/2018/Conference/Paper809/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper809/Authors"],"content":{"title":"reply to reviewer 2","comment":"Dear reviewer,\n\nThank you for your review.\n\nOn performance of G-HexaConvs:\nIn the experiments we show that performance consistently improves with increasing degrees of symmetry. We understand the concern of the reviewer that these differences are small for the CIFAR dataset. The results of the experiments were collected over 10 different runs with random parameter initializations. The experiments section of the paper has been updated to emphasize that the values are obtained by taking the average of 10 runs. To show statistical significance of 6-fold rotational symmetries over 4-fold rotational symmetries, we have done a significance test on our data. We test p4 and p4m versus p6 and p6m (our method) in a pairwise t-test, and find it passes with p=0.036. \n\nAlso, it should not be undervalued that our method outperforms a transfer learning approach on AID, that has been pretrained on ImageNet. Our method reduces classification error by 2% compared to networks that leverage only 4-fold symmetries. And our methods improve the error of conventional network by more than 11%.\n\nOn extensions to other groups than 2D rotations:\nThe reviewer is right to observe that in fronto-parallel scenes, this method can leverage global symmetries in the picture. Nonetheless, our experiments on CIFAR-10 show that although the margin of the benefits is smaller, our method can leverage local symmetries on a smaller scale and improve performance. These findings agree with earlier experiments by Cohen and Welling who used only 4-fold symmetries.\n\nOn masking limitations to the group of rotations:\nIt is not our intention to mask in any way that our method is limited to mirror and rotation transformations. Note that although the mathematical framework introduced by Cohen and Welling can be used for any group, in some cases, such the case of 6-fold rotational symmetry, the concrete implementation is far from trivial. Our paper is focused on the various data structures and indexing schemes that are required for an efficient implementation of hexagonal G-convolutions. If the reviewer is not entirely satisfied after the updates we made to the paper, perhaps the reviewer can help us by pointing to specific locations that could be improved in this respect.\n\nOn information loss conclusion by qualitative inspection: \nWe completely agree with the reviewer this is not a precise claim. None of the conclusions on our paper depend on this claim. Moreover, classification performance does not degrade when using hex-images. The paragraph is rephrased in the updated paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1514849747300,"tcdate":1514849747300,"number":1,"cdate":1514849747300,"id":"Byo0SHu7z","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Comment","forum":"r1vuQG-CW","replyto":"rJnr8zdgM","signatures":["ICLR.cc/2018/Conference/Paper809/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper809/Authors"],"content":{"title":"reply to reviewer 1","comment":"Dear reviewer,\n\nThank you for your support and comments.\n\nOn memory & computation complexity:\nIn our method, the memory and computational complexity scale as in the framework introduced by Cohen and Welling. Say n is the number of elements in a group (e.g. 6 rotations), and say we wish to keep the number of parameters fixed relative to a planar CNN. Then memory scales with sqrt(n) (e.g. ~2.5), and computational complexity scales with n. This is exactly the same cost as simply increasing the number of channels by ~2.5, which one would normally do only when the dataset was much larger.\n\nOn open source:\nTo facilitate the development of further research, in areas such as G-HexaConvs on other coordinate systems, we will release our source code on github. This also addresses the second point that the reviewer raises.\n\nOn Figure 1:\nTo improve the clarity of Fig. 1, we modified the borders and size. In addition, the caption also describes what the image f is. We hope that this addresses the reviewer’s concerns regarding the figure.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1513140728662,"tcdate":1513140728662,"number":1,"cdate":1513140728662,"id":"HybbMN0Zf","invitation":"ICLR.cc/2018/Conference/-/Paper809/Public_Comment","forum":"r1vuQG-CW","replyto":"r1vuQG-CW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comparison with other CNNs that use hex. shape kernels.","comment":"As the reviewers suggested, hex. shape kernels have been used for vision tasks for a long decade. \n\nIn the following paper, hex. shape kernels were used to train CNNs for image classification and detection, using Cifar 10/100 and Imagenet datasets:\n\nZ. Sun, M. Ozay, T. Okatani, Design of Kernels in Convolutional Neural Networks for Image Classification, ECCV 2016.\n\nThere are also works on adaptive convolution, which employ variations of adaptive shape convolution operations, for instance;\n\nS. Niklaus, L. Mai, F. Liu, Video Frame Interpolation via Adaptive Convolution, CVPR 2017.\n\nS. Niklaus, L. Mai, F. Liu, Video Frame Interpolation via Adaptive Separable Convolution, ICCV 2017.\n\nJ. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, Deformable Convolutional Networks, ICCV 2017.\n\n- What is the novelty of HexaConv compared to these previous works (i.e. Hex. kernels and adaptive/deformable convolution)? A detailed comparison is required in order to get the superiority of the proposed HexaConv.\n\n-  The performance of HexaConv is less than the perf. of these sota methods. Could you please provide a more detailed analysis, esp. using HexaConv on larger datasets of natural images, e.g. Imagenet?\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1515642514779,"tcdate":1511827054977,"number":3,"cdate":1511827054977,"id":"Syvd8Qcgf","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Review","forum":"r1vuQG-CW","replyto":"r1vuQG-CW","signatures":["ICLR.cc/2018/Conference/Paper809/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A good submission that shows how to practically implement G-convolutional layers for DNNs on hexagonal lattices and the benefits of doing so.","rating":"7: Good paper, accept","review":"The paper presents an approach to efficiently implement planar and group convolutions over hexagonal lattices to leverage better accuracy of these operations due to reduced anisotropy. They show that convolutional neural networks thus built lead to better performance - reduced inductive bias - for the same parameter budget.\n\nG-CNNs were introduced by Cohen and Welling in ICML, 2016. They proposed DNN layers that implemented equivariance to symmetry groups. They showed that group equivariant networks can lead to more effective weight sharing and hence more efficient networks as evinced by better performance on CIFAR10 & CIFAR10+ for the same parameter budget. This paper shows G-equivariance implemented on hexagonal lattices can lead to even more efficient networks. \n\nThe benefits of using hexagonal lattices over rectangular lattices is well known in the signal processing as well as in computer vision. For example, see   \n\nGolay M. Hexagonal parallel pattern transformation. IEEE Transactions on Computers 1969. 18(8): p. 733-740.\n\nStaunton R. The design of hexagonal sampling structures for image digitization and their use with local operators. Image and Vision Computing 1989. 7(3): p. 162-166. \n\nL. Middleton and J. Sivaswamy, Hexagonal Image Processing, Springer Verlag, London, 2005\n\nThe originality of the paper lies in the practical and efficient implementation of G-Conv layers. Group-equivariant DNNs could lead to more robust, efficient and (arguably) better performing neural networks.\n\nPros\n\n- A good paper that systematically pushes the state of the art towards the design of invariant, efficient and better performing  DNNs with G-equivariant representations.\n\n- It leverages upon the existing theory in a variety of areas - signal & image processing and machine learning, to design better DNNs.\n\n - Experimental evaluation suffices for a proof of concept validation of the presented ideas.   \n\n \nCons\n\n- The authors should relate the paper better to existing works in the signal processing and vision literature.\n\n- The results are on simple benchmarks like CIFAR-10. It is likely but not immediately apparent if the benefits scale to more complex problems.\n\n- Clarity could be improved in a few places\n\n: Since * is used for a standard convolution operator, it might be useful to use *_g as a G-convolution operator.\n\n: Strictly speaking, for translation equivariance, the shift should be cyclic etc.\n\n: Spelling mistakes - authors should run a spellchecker.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1515759455165,"tcdate":1511711426936,"number":2,"cdate":1511711426936,"id":"SysTGDdxf","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Review","forum":"r1vuQG-CW","replyto":"r1vuQG-CW","signatures":["ICLR.cc/2018/Conference/Paper809/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting general approach, not really convinced by the practical use","rating":"7: Good paper, accept","review":"\nThe authors took my comments nicely into account in their revision, and their answers are convincing. I increase my rating from 5 to 7. The authors could also integrate their discussion about their results on CIFAR in the paper, I think it would help readers understand better the advantage of the contribution.\n\n----\n\nThis paper is based on the theory of group equivariant CNNs (G-CNNs), proposed by Cohen and Welling ICML'16.\n\nRegular convolutions are translation-equivariant, meaning that if an image is translated, its convolution by any filter is also translated. They are however not rotation-invariant for example.  G-CNN introduces G-convolutions, which are equivariant to a given transformation group G.\n\nThis paper proposes an efficient implementation of G-convolutions for 6-fold rotations (rotations of multiple of 60 degrees), using a hexagonal lattice. The approach is evaluated on CIFAR-10 and AID, a dataset of aerial scene classification. The approach outperforms G-convolutions implemented on a squared lattice, which allows only 4-fold rotations on AID by a short margin. On CIFAR-10, the difference does not seem significative (according to Tables 1 and 2).\nI guess this can be explained by the fact that rotation equivariance makes sense for aerial images, where the scene is mostly fronto-parallel, but less for CIFAR (especially in the upper layers), which exhibits 3D objects.\n\nI like the general approach of explicitly putting desired equivariance in the convolutional networks. Using a hexagonal lattice is elegant, even if it is not new in computer vision (as written in the paper). However, as the transformation group is limited to rotations, this is interesting in practice mostly for fronto-parallel scenes, as the experiences seem to show. It is not clear how the method can be extended to other groups than 2D rotations.\n\nMoreover, I feel like the paper sometimes tries to mask the fact that the proposed method is limited to rotations. It is admittedly clearly stated in the abstract and introduction, but much less in the rest of the paper.\n\nThe second paragraph of Section 5.1 is difficult to keep in a paper. It says that \"From a qualitative inspection of these hexagonal interpolations we conclude that no information is lost during the sampling procedure.\"  \"No information is lost\" is a strong statement from a qualitative inspection, especially of a hexagonal image.  This statement should probably be removed. One way to evaluate the information lost could be to iterate interpolation between hexagonal and squared lattices to see if the image starts degrading at some point.\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1515642514861,"tcdate":1511691844220,"number":1,"cdate":1511691844220,"id":"rJnr8zdgM","invitation":"ICLR.cc/2018/Conference/-/Paper809/Official_Review","forum":"r1vuQG-CW","replyto":"r1vuQG-CW","signatures":["ICLR.cc/2018/Conference/Paper809/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper extends group equivariant convolutional networks to images with hexagonal pixelation.  While performance gains w.r.t. to the original squared lattices are not very large, the work can be inspiring for further research. ","rating":"7: Good paper, accept","review":"The paper proposes G-HexaConv, a framework extending planar and group convolutions for hexagonal lattices. Original Group-CNNs (G-CNNs) implemented on squared lattices were shown to be invariant to translations and rotations by multiples of 90 degrees. With the hexagonal lattices defined in this paper, this invariance can be extended to rotations by multiples of 60 degrees. This shows small improvements in the CIFAR-10 performances, but larger margins in an Aerial Image Dataset. \n\nDefining hexagonal pixel configurations in convolutional networks requires both resampling input images (under squared lattices) and reformulate image indexing. All these steps are very well explained in the paper, combining mathematical rigor and clarifications. \n\nAll this makes me believe the paper is worth being accepted at ICLR conference. \n\nSome issues that would require further discussion/clarification: \n- G-HexaConv critical points are memory and computation complexity. Authors claim to have an efficient implementation but the paper lacks a proper quantitative evaluation.  Memory complexity and computational time comparison between classic CNNs and G-HexaConv should be provided.\n- I encourage the authors to open the source  code for reproducibility and comparison with future transformational equivariant representations \n-Also, in Fig.1, I would recommend to clarify that image ‘f’ corresponds to a 2D view of a hexagonal image pixelation.  My first impression was a rectangular pixelation seen from a perspective view.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]}},{"tddate":null,"ddate":null,"tmdate":1514849537819,"tcdate":1509135215377,"number":809,"cdate":1509739086885,"id":"r1vuQG-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1vuQG-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"HexaConv","abstract":"The effectiveness of Convolutional Neural Networks stems in large part from their ability to exploit the translation invariance that is inherent in many learning problems. Recently, it was shown that CNNs can exploit other invariances, such as rotation invariance, by using group convolutions instead of planar convolutions. However, for reasons of performance and ease of implementation, it has been necessary to limit the group convolution to transformations that can be applied to the filters without interpolation. Thus, for images with square pixels, only integer translations, rotations by multiples of 90 degrees, and reflections are admissible.\n\nWhereas the square tiling provides a 4-fold rotational symmetry, a hexagonal tiling of the plane has a 6-fold rotational symmetry. In this paper we show how one can efficiently implement planar convolution and group convolution over hexagonal lattices, by re-using existing highly optimized convolution routines. We find that, due to the reduced anisotropy of hexagonal filters, planar HexaConv provides better accuracy than planar convolution with square filters, given a fixed parameter budget. Furthermore, we find that the increased degree of symmetry of the hexagonal grid increases the effectiveness of group convolutions, by allowing for more parameter sharing. We show that our method significantly outperforms conventional CNNs on the AID aerial scene classification dataset, even outperforming ImageNet pre-trained models.","pdf":"/pdf/4aba7fff681f0d6b9830a482bc730ff2a64bec51.pdf","TL;DR":"We introduce G-HexaConv, a group equivariant convolutional neural network on hexagonal lattices.","paperhash":"anonymous|hexaconv","_bibtex":"@article{\n  anonymous2018hexaconv,\n  title={HexaConv},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vuQG-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper809/Authors"],"keywords":["hexagonal","group","symmetry","representation learning","rotation","equivariance","invariance"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}