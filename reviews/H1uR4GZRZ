{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222781148,"tcdate":1511817058299,"number":3,"cdate":1511817058299,"id":"ry5D1Z5xf","invitation":"ICLR.cc/2018/Conference/-/Paper828/Official_Review","forum":"H1uR4GZRZ","replyto":"H1uR4GZRZ","signatures":["ICLR.cc/2018/Conference/Paper828/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting heuristic but little theoretical justification","rating":"6: Marginally above acceptance threshold","review":"The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations. Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking.\n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks. On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness. For example, how does this compare to random perturbation (say, zero-mean) of the weights? This adds stochasticity as well so why and why not this work? The authors do not give any insight in this regard.\n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner. The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic activation pruning for robust adversarial defense","abstract":"Following recent work, neural networks are widely-known to be vulnerable to\nadversarial examples. Carefully chosen perturbations to real images, while imperceptible\nto humans, induce misclassification, threatening the reliability of deep\nlearning in the wild. To guard against adversarial examples, we take inspiration\nfrom game theory and cast the problem as a minimax zero-sum game between\nthe adversary and the model. In general, in such settings, optimal policies are\nstochastic. We propose stochastic activation pruning (SAP), an algorithm that\nprunes a random subset of activations, scaling up the survivors to compensate. The\nalgorithm preferentially keeps activations with larger magnitudes. SAP can be\napplied to pre-trained neural networks, even adversarially trained models, without\nfine-tuning, providing robustness against adversarial examples. Experiments\ndemonstrate that in the adversarial setting, SAP confers robustness, increasing\naccuracy and preserving calibration.","pdf":"/pdf/10cd3f0a9e0c49a5f1ebe427373829ff76c0f124.pdf","paperhash":"anonymous|stochastic_activation_pruning_for_robust_adversarial_defense","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic activation pruning for robust adversarial defense},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1uR4GZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper828/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222782489,"tcdate":1511783856703,"number":2,"cdate":1511783856703,"id":"SJFnpOYxM","invitation":"ICLR.cc/2018/Conference/-/Paper828/Official_Review","forum":"H1uR4GZRZ","replyto":"H1uR4GZRZ","signatures":["ICLR.cc/2018/Conference/Paper828/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple and yet effective method against adversarial attack post traning.","rating":"7: Good paper, accept","review":"This paper propose a simple method for guarding trained models against adversarial attacks. The method is to prune the network’s activations at each layer and renormalize the outputs. It’s a simple method that can be applied post-training and seems to be effective.\n\nThe paper is well written and easily to follow. Method description is clear. The analyses are interesting and done well. I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods.\n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images. Would be good to know if the authors have any intuition why is that the case.\n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text.\n\nI would be interested to know more about the intuition behind the proposed method. It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method.\n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial.\n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014. Would be good to see the performance against other forms of adversarial attacks as well if they exist.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic activation pruning for robust adversarial defense","abstract":"Following recent work, neural networks are widely-known to be vulnerable to\nadversarial examples. Carefully chosen perturbations to real images, while imperceptible\nto humans, induce misclassification, threatening the reliability of deep\nlearning in the wild. To guard against adversarial examples, we take inspiration\nfrom game theory and cast the problem as a minimax zero-sum game between\nthe adversary and the model. In general, in such settings, optimal policies are\nstochastic. We propose stochastic activation pruning (SAP), an algorithm that\nprunes a random subset of activations, scaling up the survivors to compensate. The\nalgorithm preferentially keeps activations with larger magnitudes. SAP can be\napplied to pre-trained neural networks, even adversarially trained models, without\nfine-tuning, providing robustness against adversarial examples. Experiments\ndemonstrate that in the adversarial setting, SAP confers robustness, increasing\naccuracy and preserving calibration.","pdf":"/pdf/10cd3f0a9e0c49a5f1ebe427373829ff76c0f124.pdf","paperhash":"anonymous|stochastic_activation_pruning_for_robust_adversarial_defense","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic activation pruning for robust adversarial defense},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1uR4GZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper828/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222782539,"tcdate":1510585116729,"number":1,"cdate":1510585116729,"id":"ryrXQ4wyz","invitation":"ICLR.cc/2018/Conference/-/Paper828/Official_Review","forum":"H1uR4GZRZ","replyto":"H1uR4GZRZ","signatures":["ICLR.cc/2018/Conference/Paper828/AnonReviewer2"],"readers":["everyone"],"content":{"title":"overall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training. The presentation could be improved.","rating":"6: Marginally above acceptance threshold","review":"This paper investigates a new approach to prevent a given classifier from adversarial examples. The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks. Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training.\n\nThe proposed algorithm is clearly described. However there are issues in the presentation.\n\nIn section 2-3, the problem setting is not suitably introduced.\nIn particular one sentence that can be misleading:\n“Given a classifier, one common way to generate an adversarial example is to perturb the input in direction of the gradient…”\nYou should explain that given a classifier with stochastic output, the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient. The practical way in which the adversarial examples are generated is not known to the player. An adversary could choose any policy. The only thing the player knows is the best adversarial policy.\n\nIn section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient. Does it come from a high variance? If it is the case, you should explain that the optimal policy of the adversary is approximated by “fast gradient sign method”. \n\nIn comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples. SAP does not perform as well as adversarial training, but SAP could be used with a trained network. \n\nOverall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training. The presentation could be improved.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stochastic activation pruning for robust adversarial defense","abstract":"Following recent work, neural networks are widely-known to be vulnerable to\nadversarial examples. Carefully chosen perturbations to real images, while imperceptible\nto humans, induce misclassification, threatening the reliability of deep\nlearning in the wild. To guard against adversarial examples, we take inspiration\nfrom game theory and cast the problem as a minimax zero-sum game between\nthe adversary and the model. In general, in such settings, optimal policies are\nstochastic. We propose stochastic activation pruning (SAP), an algorithm that\nprunes a random subset of activations, scaling up the survivors to compensate. The\nalgorithm preferentially keeps activations with larger magnitudes. SAP can be\napplied to pre-trained neural networks, even adversarially trained models, without\nfine-tuning, providing robustness against adversarial examples. Experiments\ndemonstrate that in the adversarial setting, SAP confers robustness, increasing\naccuracy and preserving calibration.","pdf":"/pdf/10cd3f0a9e0c49a5f1ebe427373829ff76c0f124.pdf","paperhash":"anonymous|stochastic_activation_pruning_for_robust_adversarial_defense","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic activation pruning for robust adversarial defense},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1uR4GZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper828/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739079437,"tcdate":1509135568157,"number":828,"cdate":1509739076784,"id":"H1uR4GZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1uR4GZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stochastic activation pruning for robust adversarial defense","abstract":"Following recent work, neural networks are widely-known to be vulnerable to\nadversarial examples. Carefully chosen perturbations to real images, while imperceptible\nto humans, induce misclassification, threatening the reliability of deep\nlearning in the wild. To guard against adversarial examples, we take inspiration\nfrom game theory and cast the problem as a minimax zero-sum game between\nthe adversary and the model. In general, in such settings, optimal policies are\nstochastic. We propose stochastic activation pruning (SAP), an algorithm that\nprunes a random subset of activations, scaling up the survivors to compensate. The\nalgorithm preferentially keeps activations with larger magnitudes. SAP can be\napplied to pre-trained neural networks, even adversarially trained models, without\nfine-tuning, providing robustness against adversarial examples. Experiments\ndemonstrate that in the adversarial setting, SAP confers robustness, increasing\naccuracy and preserving calibration.","pdf":"/pdf/10cd3f0a9e0c49a5f1ebe427373829ff76c0f124.pdf","paperhash":"anonymous|stochastic_activation_pruning_for_robust_adversarial_defense","_bibtex":"@article{\n  anonymous2018stochastic,\n  title={Stochastic activation pruning for robust adversarial defense},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1uR4GZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper828/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}