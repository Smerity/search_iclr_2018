{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222710367,"tcdate":1511843378571,"number":3,"cdate":1511843378571,"id":"Bko4LP9eM","invitation":"ICLR.cc/2018/Conference/-/Paper659/Official_Review","forum":"HJXOfZ-AZ","replyto":"HJXOfZ-AZ","signatures":["ICLR.cc/2018/Conference/Paper659/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea, intriguing findings, many open questions","rating":"5: Marginally below acceptance threshold","review":"This paper studies the development of localist representations in the hidden layers\nof feed-forward neural networks.\n\nThe idea is interesting and the findings are intriguing.  Local codes\nincrease understandability and could be important for better\nunderstanding natural neural networks. Understanding how local codes\nform and the factors that increase their likelihood is critically\nimportant.  This is a good start in that direction, but still leaves\nopen many questions.  The issues raised in the Conclusions section are\nalso very interesting -- do the local codes increase with networks\nthat generalize better, or with overtrained networks?\n\nA  weakness in this paper (admitted by the authors in the\nConclusions section) is the dependence of the results on the form of input\nrepresentation.  If we consider the Jennifer Aniston cells, they do\nnot receive as input as well separated inputs as modeled in this\npaper.  In fact the input representation used in this study is already\na fairly localist representation as each 1 unit is fairly selectively\non for its own class and mostly off for the other classes.  It will be\nvery interesting to see the results of hidden layers in deep networks\noperating on natural images.\n\nPlease give your equation for selectivity.  On Page 2 it is stated \"We\nuse the word ‘selectivity’ as a quantitative measure of the difference\nbetween activations for the two categories, A and not-A, where A is\nthe class a neuron is selective for (and not-A being all other\nclasses).\"  However you state that neurons were counted as a local\ncode if the selectivity was above .05.  A difference between\nactivations for the two categories of .05 does not seem very\nselective, so I'm thinking you used something other than the\nmathematical difference.\n\nWhat is the selectivity of units in the input codewords?  With no\nperturbation, and S_x=.2, w_R=50, w_P=50, the units in the prototype\nblocks have a high selectivity responding with 1 for all patterns in\ntheir class and with 0 for 8/9 of the patterns in the other classes.\nCould this explain the much higher selectivity for this case in the\nhidden units?  I would like to see the selectivity of the input units\nfor each of the plots/curves.  This would be especially interesting\nfor Figure 5.\n\nIt is stated that LCs emerge with longer training and that ReLU\nneurons may produce more LCs because they train quicker and all\nexperiments were stopped at 45,000 epochs.  Why not investigate this\nby changing learning rates for one of ReLu or sigmoidal units to more\nclosely match their training speed?  It would be interesting to see if\nthe difference is simply due to learning rate, or something deeper\nabout the activation functions.\n\nYou found that very few local codes in the HLNs were found when a\n1-hot ouput encoding was used and suggest that this means that\nemergent local codes are highly unlikely to be found in the\npenultimate layer of deep networks.  If your inputs are a local code\n(e.g. for low w_R), you found local codes above the layer of local\ncodes but in this result not below the layer of local codes which\nmight also imply (as you say in the Conclusions) that more local\ncoding neurons may be found in the higher layers (though not the\npenultimate one as you argue).  Could you analyze how the selectivity\nof a hidden layer changes as a function of the selectivity in the\nlower and higher layers?\n\n\n\nMinor Note -- The Neural Network Design section looks like it still\nhas draft notes in it.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"When and where do feed-forward neural networks learn localist representations?","abstract":"Parallel distributed processing models of neural networks (NN) suggest that there can be no interpretable localist codes in a neural network, preventing researchers from looking for them and implying that they are biologically implausible. However, recent results from psychology, neuroscience and deep-learning neural networks have shown the occasional existence of local codes emerging from PDP models. In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network (used as a model for a single layer of a deep network), using generated input and output data with known qualities. We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data. Using a 1-HOT output code drastically decreases the number of local codes on the hidden layer, suggesting that localist encoding will be found at the deeper levels of a deep neural network. The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks. This data suggests that localist coding can emerge from PDP networks, and thus might be relevant to the functioning of deep networks and the brain, and that psychological models based on local codes should not be dismissed out of hand.","pdf":"/pdf/0a30e7b6502f8fcff17b5672f36270197cdc5893.pdf","TL;DR":"Local codes have been found in feed-forward neural networks","paperhash":"anonymous|when_and_where_do_feedforward_neural_networks_learn_localist_representations","_bibtex":"@article{\n  anonymous2018when,\n  title={When and where do feed-forward neural networks learn localist representations?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJXOfZ-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper659/Authors"],"keywords":["localist","pdp","neural network","representation","psychology","cognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222710407,"tcdate":1511769506771,"number":2,"cdate":1511769506771,"id":"ryuoSrKxM","invitation":"ICLR.cc/2018/Conference/-/Paper659/Official_Review","forum":"HJXOfZ-AZ","replyto":"HJXOfZ-AZ","signatures":["ICLR.cc/2018/Conference/Paper659/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper describes a method for determining to what degree individual neurons in a hidden layer of an MLP encode a localist code, which is studied for different input representations.","rating":"3: Clear rejection","review":"Quality and Clarity\nThe neural networks and neural codes are studied  in a concise way, most of the paper is clear. The section on data design, p3, could use some additional clarification wrt to how the data input is encoded (right now, it is hard to understand exactly what happens). \n\nOriginality\nI am not aware of other studies on this topic, the proposed approach seems original. \n\nSignificance\nThe biggest problem I have is with the significance: I don't see at all how finding somewhat localized responses in the hidden layer of an MLP with just one hidden layer has any bearing on deeper networks structured as CNNs: compared to MLPs, neurons in CNNs have much smaller receptive fields, and are known to be sensitive to selective and distinct  features.  \n\nOverall the results seem rather trivial without greater implications for modern deep neural networks: ie, of course adding dropout improves the degree of localist coding (sec 3.4). Similarly, for a larger network, you will find fewer localist codes (though this is hard to judge, as an exact definition of selectivity is missing). \n\nMinor issues: the \"selectivity\" p3 is not properly defined.  On p3, a figure is undefined. \nTypo: p2: \"could as be\". \nMany of the references are ugly : p3,  \"in kerasChollet (2015)\", this needs fixing. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"When and where do feed-forward neural networks learn localist representations?","abstract":"Parallel distributed processing models of neural networks (NN) suggest that there can be no interpretable localist codes in a neural network, preventing researchers from looking for them and implying that they are biologically implausible. However, recent results from psychology, neuroscience and deep-learning neural networks have shown the occasional existence of local codes emerging from PDP models. In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network (used as a model for a single layer of a deep network), using generated input and output data with known qualities. We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data. Using a 1-HOT output code drastically decreases the number of local codes on the hidden layer, suggesting that localist encoding will be found at the deeper levels of a deep neural network. The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks. This data suggests that localist coding can emerge from PDP networks, and thus might be relevant to the functioning of deep networks and the brain, and that psychological models based on local codes should not be dismissed out of hand.","pdf":"/pdf/0a30e7b6502f8fcff17b5672f36270197cdc5893.pdf","TL;DR":"Local codes have been found in feed-forward neural networks","paperhash":"anonymous|when_and_where_do_feedforward_neural_networks_learn_localist_representations","_bibtex":"@article{\n  anonymous2018when,\n  title={When and where do feed-forward neural networks learn localist representations?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJXOfZ-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper659/Authors"],"keywords":["localist","pdp","neural network","representation","psychology","cognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222710444,"tcdate":1511389356994,"number":1,"cdate":1511389356994,"id":"ryBhOOXlM","invitation":"ICLR.cc/2018/Conference/-/Paper659/Official_Review","forum":"HJXOfZ-AZ","replyto":"HJXOfZ-AZ","signatures":["ICLR.cc/2018/Conference/Paper659/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Hints of something interesting, but a bit over-simplistic and sloppy.","rating":"3: Clear rejection","review":"The authors ask when the hidden layer units of a multi-layer feed-forward neural network will display selectivity to object categories. They train 3-layer ANNs to categorize binary patterns, and find that typically at least some of the hidden layer units are category selective. The number of category selective (\"localist\") units varies depending on the size of the hidden layer, the structure of the outputs the network is trained to return (i.e., one-hot vs distributed), the neurons' activation functions, and the level of dropout-induced noise in the training procedure.\n\nOverall, I find the work to hint at an interesting phenomenon. However, the paper as presented uses an overly-simplistic task for the ANNs, and the work is sloppily presented. These factors detract from my enthusiasm. My specific criticisms are as follows:\n\n1) The binary pattern classification seems overly simplistic a task for this study. If you want to compare to the medial temporal lobe's Jennifer Aniston cells (i.e., the Quiroga result), then an object recognition task seems much more meaningful, as does a deeper network structure. Likewise, to inform the representations we see in deep object recognition networks, it is better to just study those networks, instead of simple shallow binary classification networks. Or, at least show that the findings apply to those richer settings, where the networks do \"real\" tasks.\n\n2) The paper is somewhat sloppy, and could use a thorough proofreading. For example, what are \"figures 3, ?? and 6\"? And which is Figure 3.3.1?\n\n3) What formula is used to quantify the selectivity? And do the results depend on the cut-off used to label units as \"selective\" or not (i.e., using a higher or lower cutoff than 0.05)? Given that the 0.05 number is somewhat arbitrary, this seems worth checking.\n\n4) I don't think that very many people would argue that the presence of distributed representations strictly excludes the possibility of some of the units having some category selectivity. Consequently, I find the abstract and introduction to be a bit off-putting, coming off almost as a rant against PDP. This is a minor stylistic thing, but I'd encourage the authors to tone it down a bit.\n\n5) The finding that more of the selective units arise in the hidden layer in the presence of higher levels of noise is interesting, and the authors provide some nice intuition for this phenomenon (i.e., getting redundant local representations makes the system robust to the dropout). This seems interesting in light of the Quiroga findings of Jennifer Aniston cells: the fact that the (small number of) units they happened to record from showed such selectivity suggests that many neurons in the brain would have this selectivity, so there must be a large number of category selective units. Does that finding, coupled with the result from Fig. 6, imply that those \"grandmother cell\" observations might reflect an adaptation to increase robustness to noise? \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"When and where do feed-forward neural networks learn localist representations?","abstract":"Parallel distributed processing models of neural networks (NN) suggest that there can be no interpretable localist codes in a neural network, preventing researchers from looking for them and implying that they are biologically implausible. However, recent results from psychology, neuroscience and deep-learning neural networks have shown the occasional existence of local codes emerging from PDP models. In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network (used as a model for a single layer of a deep network), using generated input and output data with known qualities. We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data. Using a 1-HOT output code drastically decreases the number of local codes on the hidden layer, suggesting that localist encoding will be found at the deeper levels of a deep neural network. The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks. This data suggests that localist coding can emerge from PDP networks, and thus might be relevant to the functioning of deep networks and the brain, and that psychological models based on local codes should not be dismissed out of hand.","pdf":"/pdf/0a30e7b6502f8fcff17b5672f36270197cdc5893.pdf","TL;DR":"Local codes have been found in feed-forward neural networks","paperhash":"anonymous|when_and_where_do_feedforward_neural_networks_learn_localist_representations","_bibtex":"@article{\n  anonymous2018when,\n  title={When and where do feed-forward neural networks learn localist representations?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJXOfZ-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper659/Authors"],"keywords":["localist","pdp","neural network","representation","psychology","cognition"]}},{"tddate":null,"ddate":null,"tmdate":1509739175891,"tcdate":1509130859007,"number":659,"cdate":1509739173225,"id":"HJXOfZ-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJXOfZ-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"When and where do feed-forward neural networks learn localist representations?","abstract":"Parallel distributed processing models of neural networks (NN) suggest that there can be no interpretable localist codes in a neural network, preventing researchers from looking for them and implying that they are biologically implausible. However, recent results from psychology, neuroscience and deep-learning neural networks have shown the occasional existence of local codes emerging from PDP models. In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network (used as a model for a single layer of a deep network), using generated input and output data with known qualities. We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data. Using a 1-HOT output code drastically decreases the number of local codes on the hidden layer, suggesting that localist encoding will be found at the deeper levels of a deep neural network. The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks. This data suggests that localist coding can emerge from PDP networks, and thus might be relevant to the functioning of deep networks and the brain, and that psychological models based on local codes should not be dismissed out of hand.","pdf":"/pdf/0a30e7b6502f8fcff17b5672f36270197cdc5893.pdf","TL;DR":"Local codes have been found in feed-forward neural networks","paperhash":"anonymous|when_and_where_do_feedforward_neural_networks_learn_localist_representations","_bibtex":"@article{\n  anonymous2018when,\n  title={When and where do feed-forward neural networks learn localist representations?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJXOfZ-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper659/Authors"],"keywords":["localist","pdp","neural network","representation","psychology","cognition"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}