{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642432934,"tcdate":1511847101379,"number":3,"cdate":1511847101379,"id":"HkrTEd9gf","invitation":"ICLR.cc/2018/Conference/-/Paper329/Official_Review","forum":"Hy8hkYeRb","replyto":"Hy8hkYeRb","signatures":["ICLR.cc/2018/Conference/Paper329/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good overall idea but needs further development","rating":"3: Clear rejection","review":"The paper attempts to extend the predictive coding model to a multilayer network.  The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images.\n\nThe overall idea and approach being pursued here is a good one, but the model needs further development.  It could also use better theoretical motivation - i.e., what sorts of representations do you expect to emerge in higher layers?  Can you demonstrate this with a toy example and then extend to real data?\n\nThat the model can reconstruct images per se is not particularly interesting.  What we would like to see is that it has somehow learned a more useful or meaningful representation of the data.  For example, what do the learned weights look like?  That would tell you something about what has been learned.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Deep Predictive Coding Network for Learning Latent Representations","abstract":"It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment. For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus. Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment. However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain. In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm. To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training. Despite the novel features of these objects the model is able to infer the latent representations for them. Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.","pdf":"/pdf/3f6f1173dec159cae89cc5b8b3670ce2df0e3697.pdf","TL;DR":"A predictive coding based learning algorithm for building deep neural network models of the brain","paperhash":"anonymous|a_deep_predictive_coding_network_for_learning_latent_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A Deep Predictive Coding Network for Learning Latent Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy8hkYeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper329/Authors"],"keywords":["Predictive coding","deep neural network","generative model","unsupervised learning","learning latent representations"]}},{"tddate":null,"ddate":null,"tmdate":1515642432971,"tcdate":1511807927267,"number":2,"cdate":1511807927267,"id":"ryJ6sRYlf","invitation":"ICLR.cc/2018/Conference/-/Paper329/Official_Review","forum":"Hy8hkYeRb","replyto":"Hy8hkYeRb","signatures":["ICLR.cc/2018/Conference/Paper329/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Major and minor concerns","rating":"3: Clear rejection","review":"The paper \"A Deep Predictive Coding Network for Learning Latent Representations\" considers learning of a generative neural network. The network learns unsupervised using a predictive coding setup. A subset of the CIFAR-10 image database (1000 images horses and ships) are used for training. Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown. It is then claimed that the generated images show that the network has learned good latent representations.\n\nI have some concerns about the paper, maybe most notably about the experimental result and the conclusions drawn from them. The numerical experiments are motivated as a way to \"understand the capacity of the network with regards to modeling the external environment\" (abstract). And it is concluded in the final three sentences of the paper that the presented network \"can infer effective latent representations for images of other objects\" (i.e., of objects that have not been used for training); and further, that \"in this regards, the network is better than most existing algorithms [...]\".\n\nI expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggested. Also some at least quantifiable (if not benchmarked) outcomes should have been presented given the rather strong claims/conclusions in abstract and discussion/conclusion sections. As a matter of fact, all images shown (including those in the appendix) are blurred versions of the original images, except of one single image: Fig. 4 last row, 2nd image (and that is not commented on). If these are the generated images, then some reconstruction is done by the network, fine, but also not unsurprising as the network was told to do so by the used objective function. What precisely do we learn here? I would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network. How can the reader conclude any functioning from these images? Using the right objective function, reconstructions can also be obtained using random (not learned) generative fields and relatively basic models. The fact that image reconstruction for shifted images or new images is evidence for a sophisticated latent representations is, to my mind, not at all shown here. What would be a good measure for an \"effective latent representation\" that substantiates the claims made? The reconstruction of unseen images is claimed central but as far as I could see, Figures 2, 3, and 4 are not even referred to in the text, nor is there any objective measure discussed. Studying the relation between predictive coding and deep learning makes sense, but I do not come to the same (strong) conclusions as the author(s) by considering the experimental results - and I do not see evidence for a sophisticated latent representation learned by the network. I am not saying that there is none, but I do not see how the presented experimental results show evidence for this.\n\nFurthermore, the authors stress that a main distinguishing feature of their approach (top of page 3) is that in their network information flows from latent space to observed space (e.g. in contrast to CNNs). That is a true statement but also one which is true for basically all generative models, e.g., of standard directed graphical models such as wake-sleep approaches (Hinton et al., 1995), deep SBNs and more recent generative models used in GANs (Goodfellow et al, 2014). Any of these references would have made a lot of sense.\n\nWith my evaluation I do not want to be discouraging about the general approach. But I can not at all give a good evaluation given the current experimental results (unless substantial new evidence which make me evaluate these results differently is provided in a discussion).\n\n\nMinor:\n\n- no legend for Fig. 1\n\n-notes -> noted\n\nhave focused\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Deep Predictive Coding Network for Learning Latent Representations","abstract":"It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment. For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus. Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment. However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain. In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm. To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training. Despite the novel features of these objects the model is able to infer the latent representations for them. Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.","pdf":"/pdf/3f6f1173dec159cae89cc5b8b3670ce2df0e3697.pdf","TL;DR":"A predictive coding based learning algorithm for building deep neural network models of the brain","paperhash":"anonymous|a_deep_predictive_coding_network_for_learning_latent_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A Deep Predictive Coding Network for Learning Latent Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy8hkYeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper329/Authors"],"keywords":["Predictive coding","deep neural network","generative model","unsupervised learning","learning latent representations"]}},{"tddate":null,"ddate":null,"tmdate":1515642433051,"tcdate":1511524853193,"number":1,"cdate":1511524853193,"id":"SJpl9FSlz","invitation":"ICLR.cc/2018/Conference/-/Paper329/Official_Review","forum":"Hy8hkYeRb","replyto":"Hy8hkYeRb","signatures":["ICLR.cc/2018/Conference/Paper329/AnonReviewer3"],"readers":["everyone"],"content":{"title":"deep predictive coding","rating":"4: Ok but not good enough - rejection","review":"Quality\n\nThe authors introduce a deep network for predictive coding. It is unclear how the approach improves on the original predictive coding formulation of Rao and Ballard, who also use a hierarchy of transformations. The results seem to indicate that all layers are basically performing the same. No insight is provided about the kinds of filters that are learned.\n\nClarity\n\nIn its present form it is hard to assess if there are benefits to the current formulation compared to already existing formulations. The paper should be checked for typos.\n\nOriginality\n\nThere exist alternative deep predictive coding models such as https://arxiv.org/abs/1605.08104. This work should be discussed and compared.\n\nSignificance \n\nIt is hard to see how the present paper improves on classical or alternative (deep) predictive coding results.\n\nPros\n\nRelevant attempt to develop new predictive coding architectures\n\nCons\n\nUnclear what is gained compared to existing work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Deep Predictive Coding Network for Learning Latent Representations","abstract":"It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment. For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus. Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment. However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain. In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm. To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training. Despite the novel features of these objects the model is able to infer the latent representations for them. Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.","pdf":"/pdf/3f6f1173dec159cae89cc5b8b3670ce2df0e3697.pdf","TL;DR":"A predictive coding based learning algorithm for building deep neural network models of the brain","paperhash":"anonymous|a_deep_predictive_coding_network_for_learning_latent_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A Deep Predictive Coding Network for Learning Latent Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy8hkYeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper329/Authors"],"keywords":["Predictive coding","deep neural network","generative model","unsupervised learning","learning latent representations"]}},{"tddate":null,"ddate":null,"tmdate":1509739360942,"tcdate":1509097390138,"number":329,"cdate":1509739358287,"id":"Hy8hkYeRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hy8hkYeRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Deep Predictive Coding Network for Learning Latent Representations","abstract":"It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment. For this purpose, it builds a model of the world around us and uses this model to infer the external stimulus. Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment. However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain. In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm. To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training. Despite the novel features of these objects the model is able to infer the latent representations for them. Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.","pdf":"/pdf/3f6f1173dec159cae89cc5b8b3670ce2df0e3697.pdf","TL;DR":"A predictive coding based learning algorithm for building deep neural network models of the brain","paperhash":"anonymous|a_deep_predictive_coding_network_for_learning_latent_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A Deep Predictive Coding Network for Learning Latent Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy8hkYeRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper329/Authors"],"keywords":["Predictive coding","deep neural network","generative model","unsupervised learning","learning latent representations"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}