{"notes":[{"tddate":null,"ddate":null,"tmdate":1512374329520,"tcdate":1512374329520,"number":4,"cdate":1512374329520,"id":"SyWSeKGZf","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Task not well defined","rating":"4: Ok but not good enough - rejection","review":"This paper tackles an interesting and timely problem. And it is mostly well-written. \n\nThe main weakness is that the solution the paper offers fall far short of its claim. The problem of generating explanations is formulated as generating low dimensional embeddings that satisfy certain properties such as reconstruction and sparsity. At the same time, the actually explanation of the embeddings is left to humans, i.e., what each dimension means needs to figured out by humans through a user interface where a user looks at images and neuron responses. Furthermore, the only thing that is directly human interpretable, the spatial attention map, is actually generated by applying an existing method to the embeddings. \n\nThus, I would have no problem of calling the proposed approach a dimensionality reduction method (like t-SNE), or a model compression method, but I think calling it an explanation is overclaiming its significance. \n\nI understand that it is very challenging to even define the problem of explanation, and any success in this deserves credit. But in my opinion this paper has not made significant progress in defining the task. \n\nThe evaluation part is also unsatisfying, in that the evaluation protocol is convoluted and heavily depends on the details of the proposed approach. Ideally, an evaluation protocol should be decoupled from a particular approach, such that very different approaches can be compared for the same task. \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/ef27b4f63b5cacd37cd41e17610045a359ea1ce4.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222588498,"tcdate":1512178250155,"number":3,"cdate":1512178250155,"id":"BJfLzKkZf","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction, but some evaluation is missing","rating":"4: Ok but not good enough - rejection","review":"Summary\n---\n\nThis paper proposes a new representation for visually explaining model predictions. It wants to create explanations which are expressed in terms of a small number of high level human interpretable concepts and are complete. To accomplish this it re-represents a network's hidden states using an explanation embedding which is constrained to be faithful to the model and represent independent concepts. Next the paper uses Excitation Backprop (saliency/heat map visualization) to produce a heat map for the concepts in the explanation space. These are verified to match some criteria for interpretability.\n\nExplanations are produced for image classification models. The explanation space is a learned embedding E(Z) attached to some fully connected hidden layer of a conv net. It learns a representation that has a few nice properties, each enforced by an objective:\n1. It should be able to mimic the classification scores of the model.\n2. It should be able to reconstruct features from the layer which feeds it input.\n3. Only some features need to be well reconstructed (reconstruction sparsity), so detailed/low-level information can be ignored.\nOnce this embedding is learned, each neuron in the embedding space is visualized using Excitation Backprop to highlight visual regions corresponding to each dimension of the learned space E(Z). These heat maps are displayed for each neuron across multiple images for humans to inspect.\n\nEvaluation compares the proposed Sparse Reconstruction Autoencoder (SRAE) to baselines that impose different priors on the explanation representation or perform feature selection on the layer which the SRAE re-represents. These including an MLP, a sparse autoencoder (the feature space is sparse instead of the reconstruction space Z), and Lasso. The following dimensions are evaluated:\n1. Fidelity: The SRAE representation can mimic model classifications accurately\n2. Orthogonality: Heat maps are computed for pairs of dimensions of E(Z). Correlation is computed between these heat maps (treated as 1d vectors). This correlation is is low for the SRAE, so different neurons of E(Z) capture somewhat independent concepts.\n3. Locality: The CUB dataset comes with part annotations. A representation which highlights only one or a small number of parts (beak, wing, legs) is said to be more local. Spatially local heat maps are good because they aid human understanding. SRAE representations tend to be local in this sense, though the original representation is equally local.\n4. Human evaluation: Multiple humans match feature visualizations to the available list of pre-annotated bird parts and they tend to label the same features with the same bird parts.\n\n\nStrengths\n---\n\nI think the premise of the paper provides a fruitful direction for work in interpretability. Most methods try to interpret neural net representations in a somewhat direct fashion, but it makes sense to sacrifice some faithfulness to obtain an explanation humans can understand (e.g., Hendricks et al. where justification is via learned language description).\n\nWeaknesses\n---\n\nAs I see it, this paper's only major weakness is that it doesn't evaluate the approach well enough. There are two kinds of problems with the evaluation. (1) First, it's not clear what makes a good explanation or how to measure that once some operational definition is in place. The metrics chosen in this paper have some clear flaws, which the paper acknowledges, but they are reasonable given this is still an open problem. (2) Second, tentatively accepting that the aforementioned metrics work well, some important experimental settings are missing.\n\n(2) Missing Experiments:\n\n* To show that the explanation representation E(Z) is useful it should be compared to representation Z it is trying to re-represent. This comparison is only provided in terms of the locality metric, where Z performs about the same as E(Z). F_reg, F_cls, O_1, and O_2 should also be provided for the Z column in table 2a to establish that E(Z) is a more interpretable representation than Z according to the chosen criteria.\n\n* Furthermore ablations of the proposed E(Z) embedding should be provided. I think the NN baseline is equivalent to SRAE without the reconstruction sparsity term. (Is that right?) In any case, a version of SRAE with/without both of the faithfulness loss and the reconstruction sparsity losses should be provided to establish what performance difference each of these contributes.\n\n(1) Choice and explanation of interpretability criteria and metrics (minor weakness):\n\n* Locality and orthogonality are properties of representations which are commonly assumed to make the representations more interpretable to humans. The evaluation in this paper relies heavily on this assumption and it does establish that SRAE representations are local and orthogonal. It would be nice to see how these properties effect human predictions about model behavior or how SRAE directly effects human predictions about model behavior. This would provide some measure of whether these properties actually do help interpretability or not. Can a human predict a model's output given SRAE visualizations, including\nfailure cases? (c.f. section 3 of \"Towards A Rigorous Science of Interpretable Machine Learning\" by Doshi-Velez and Kim, 2017)\n\n* At the end of section 2.1 the paper argues that we should train humans to understand the concepts a network learns to represent. Subsequent evaluation seems to contradict this viewpoint. It evaluates interpretability of SRAE features based on their similarity to pre-annotated parts (the locality metric). The human evaluation experiment also evaluates based on similarity to pre-annotated parts. Instead of training humans to understand features, it assumes that humans are familiar with the pre-annotated bird parts and asks them to match SRAE features to these parts. I think the point of the original argument, teaching humans to understand features, would be to allow a network to learn features that are better for the relevant task because they are not constrained to be explainable. Learned concepts might not align with concepts humans already think are important, but the actual evaluation only rewards features that align with known human-identifiable concepts.\n\nIn support of the actual evaluation procedure, the paper argues \"the majority of bird classifications are based on specific, discriminative parts of the bird.\" This seemingly further contradicts the previous motivation. I think the evaluation is still useful, but there should be a more careful discussion about which conclusions it warrants. It seems that features that perform worse on this metric could actually be more interpretable than features that perform better, but that's unlikely because it's likely that the most interpretable explanations will be in terms of human-identifiable concepts.\n\n* I'm not sure how table 2b makes a point about completeness. It cites (Gonzales-Garcia et al., 2016, section 6/figure 15) to explain the experiment, but in that paper a masked region is said to be more discriminative if classifier performance drops more compared to non-masked classification. According to table 2b, performance drops more on average for Excitation BP, so according to this metric Excitation BP is better at identifying discriminative regions than SRAE. Is that right?\n\nAs I understand it, heat maps are masked from an image one at a time and then accuracy is averaged over all these occluded+inpainted versions of the image.  How is this supposed to measure completeness?\n\n* The Orthogonality metric is computed against heat maps. Correlation should also be reported against the representations being visualized; i.e., across dimensions of Z and dimensions of E(Z).\n\n\nMinor Concerns/Comments\n---\n\n* The first paragraph of the related work makes it sound like work on captioning and VQA are proposed/used to make systems interpretable. I think this is accurate for (Hendricks et al. 2016), but the rest do not use these tasks to increase interpretability, but focus on vision and language models/tasks as target models to be understood or improved.\n\n* How exactly did the human experiment in Table 1b work? Orthogonality and locality are computed against attention maps. How are the maps used for human evaluation computed?\n\n* The paper cites visual saliency techniques other than Excitation Backprop. It would be interesting to see how SRAE performs using some of these other techniques (e.g. DeConvNet).\n\nPreliminary Evaluation\n---\n\nI think the paper proposes an interesting and somewhat novel idea, but the evaluation is not good enough. If the experiments described in the Missing Experiments section are provided and show that the explanation space E(Z) is better than the given latent space Z then I would be happy to accept the paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/ef27b4f63b5cacd37cd41e17610045a359ea1ce4.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222588537,"tcdate":1512177942230,"number":2,"cdate":1512177942230,"id":"B1oGZFkWG","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction, but some evaluation is missing","rating":"4: Ok but not good enough - rejection","review":"Summary\n---\n\nThis paper proposes a new representation for visually explaining model predictions. It wants to create explanations which are expressed in terms of a small number of high level human interpretable concepts and are complete. To accomplish this it re-represents a network's hidden states using an explanation embedding which is constrained to be faithful to the model and represent independent concepts. Next the paper uses Excitation Backprop (saliency/heat map visualization) to produce a heat map for the concepts in the explanation space. These are verified to match some criteria for interpretability.\n\nExplanations are produced for image classification models. The explanation space is a learned embedding E(Z) attached to some fully connected hidden layer of a conv net. It learns a representation that has a few nice properties, each enforced by an objective:\n1. It should be able to mimic the classification scores of the model.\n2. It should be able to reconstruct features from the layer which feeds it input.\n3. Only some features need to be well reconstructed (reconstruction sparsity), so detailed/low-level information can be ignored.\nOnce this embedding is learned, each neuron in the embedding space is visualized using Excitation Backprop to highlight visual regions corresponding to each dimension of the learned space E(Z). These heat maps are displayed for each neuron across multiple images for humans to inspect.\n\nEvaluation compares the proposed Sparse Reconstruction Autoencoder (SRAE) to baselines that impose different priors on the explanation representation or perform feature selection on the layer which the SRAE re-represents. These including an MLP, a sparse autoencoder (the feature space is sparse instead of the reconstruction space Z), and Lasso. The following dimensions are evaluated:\n1. Fidelity: The SRAE representation can mimic model classifications accurately\n2. Orthogonality: Heat maps are computed for pairs of dimensions of E(Z). Correlation is computed between these heat maps (treated as 1d vectors). This correlation is is low for the SRAE, so different neurons of E(Z) capture somewhat independent concepts.\n3. Locality: The CUB dataset comes with part annotations. A representation which highlights only one or a small number of parts (beak, wing, legs) is said to be more local. Spatially local heat maps are good because they aid human understanding. SRAE representations tend to be local in this sense, though the original representation is equally local.\n4. Human evaluation: Multiple humans match feature visualizations to the available list of pre-annotated bird parts and they tend to label the same features with the same bird parts.\n\n\nStrengths\n---\n\nI think the premise of the paper provides a fruitful direction for work in interpretability. Most methods try to interpret neural net representations in a somewhat direct fashion, but it makes sense to sacrifice some faithfulness to obtain an explanation humans can understand (e.g., Hendricks et al. where justification is via learned language description).\n\nWeaknesses\n---\n\nAs I see it, this paper's only major weakness is that it doesn't evaluate the approach well enough. There are two kinds of problems with the evaluation. (1) First, it's not clear what makes a good explanation or how to measure that once some operational definition is in place. The metrics chosen in this paper have some clear flaws, which the paper acknowledges, but they are reasonable given this is still an open problem. (2) Second, tentatively accepting that the aforementioned metrics work well, some important experimental settings are missing.\n\n(2) Missing Experiments:\n\n* To show that the explanation representation E(Z) is useful it should be compared to representation Z it is trying to re-represent. This comparison is only provided in terms of the locality metric, where Z performs about the same as E(Z). F_reg, F_cls, O_1, and O_2 should also be provided for the Z column in table 2a to establish that E(Z) is a more interpretable representation than Z according to the chosen criteria.\n\n* Furthermore ablations of the proposed E(Z) embedding should be provided. I think the NN baseline is equivalent to SRAE without the reconstruction sparsity term. (Is that right?) In any case, a version of SRAE with/without both of the faithfulness loss and the reconstruction sparsity losses should be provided to establish what performance difference each of these contributes.\n\n(1) Choice and explanation of interpretability criteria and metrics (minor weakness):\n\n* Locality and orthogonality are properties of representations which are commonly assumed to make the representations more interpretable to humans. The evaluation in this paper relies heavily on this assumption and it does establish that SRAE representations are local and orthogonal. It would be nice to see how these properties effect human predictions about model behavior or how SRAE directly effects human predictions about model behavior. This would provide some measure of whether these properties actually do help interpretability or not. Can a human predict a model's output given SRAE visualizations, including\nfailure cases? (c.f. section 3 of \"Towards A Rigorous Science of Interpretable Machine Learning\" by Doshi-Velez and Kim, 2017)\n\n* At the end of section 2.1 the paper argues that we should train humans to understand the concepts a network learns to represent. Subsequent evaluation seems to contradict this viewpoint. It evaluates interpretability of SRAE features based on their similarity to pre-annotated parts (the locality metric). The human evaluation experiment also evaluates based on similarity to pre-annotated parts. Instead of training humans to understand features, it assumes that humans are familiar with the pre-annotated bird parts and asks them to match SRAE features to these parts. I think the point of the original argument, teaching humans to understand features, would be to allow a network to learn features that are better for the relevant task because they are not constrained to be explainable. Learned concepts might not align with concepts humans already think are important, but the actual evaluation only rewards features that align with known human-identifiable concepts.\n\nIn support of the actual evaluation procedure, the paper argues \"the majority of bird classifications are based on specific, discriminative parts of the bird.\" This seemingly further contradicts the previous motivation. I think the evaluation is still useful, but there should be a more careful discussion about which conclusions it warrants. It seems that features that perform worse on this metric could actually be more interpretable than features that perform better, but that's unlikely because it's likely that the most interpretable explanations will be in terms of human-identifiable concepts.\n\n* I'm not sure how table 2b makes a point about completeness. It cites (Gonzales-Garcia et al., 2016, section 6/figure 15) to explain the experiment, but in that paper a masked region is said to be more discriminative if classifier performance drops more compared to non-masked classification. According to table 2b, performance drops more on average for Excitation BP, so according to this metric Excitation BP is better at identifying discriminative regions than SRAE. Is that right?\n\nAs I understand it, heat maps are masked from an image one at a time and then accuracy is averaged over all these occluded+inpainted versions of the image.  How is this supposed to measure completeness?\n\n* The Orthogonality metric is computed against heat maps. Correlation should also be reported against the representations being visualized; i.e., across dimensions of Z and dimensions of E(Z).\n\n\nMinor Concerns/Comments\n---\n\n* The first paragraph of the related work makes it sound like work on captioning and VQA are proposed/used to make systems interpretable. I think this is accurate for (Hendricks et al. 2016), but the rest do not use these tasks to increase interpretability, but focus on vision and language models/tasks as target models to be understood or improved.\n\n* How exactly did the human experiment in Table 1b work? Orthogonality and locality are computed against attention maps. How are the maps used for human evaluation computed?\n\n* The paper cites visual saliency techniques other than Excitation Backprop. It would be interesting to see how SRAE performs using some of these other techniques (e.g. DeConvNet).\n\nPreliminary Evaluation\n---\n\nI think the paper proposes an interesting and somewhat novel idea, but the evaluation is not good enough. If the experiments described in the Missing Experiments section are provided and show that the explanation space E(Z) is better than the given latent space Z then I would be happy to accept the paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/ef27b4f63b5cacd37cd41e17610045a359ea1ce4.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222588584,"tcdate":1511862656331,"number":1,"cdate":1511862656331,"id":"rJ_F-39ez","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer1"],"readers":["everyone"],"content":{"title":"EMBEDDING DEEP NETWORKS INTO VISUAL EXPLANATIONS","rating":"4: Ok but not good enough - rejection","review":"Summary: For the task of explainable machine learning, this paper proposes to generate visualizations that act as visual explanation of a classification decision. The proposed framework uses an explanation module attached to an intermediate layer of a pre-trained DNN to learn an interpretable and low-dimensional feature space. The explanation module aims to predict the final classification output (binary, one-vs-all) and to auto encode its input (intermediate DNNlayer) via Sparse Reconstruction Autoencoder (SRAE) with sparsity as additional target. Using a referenced method (ExcitationBP) the authors show that the features learned by the explanation module correspond to high level features in the original image (such as eye, beak of a bird) that the original DNN uses to make the classification. The experiments are conducted on 30 classes of CUB-200-2011 dataset which contains 200 classes in total.\n\nStrengths:\n\nExplainable artificial intelligence is a research topic is relevant for the community. Explainability of deep neural networks as motivated by the authors would improve trust between the machine and the user, hence it is an important direction.\n\nThe paper is well written. The evaluations are conducted on three introduced quantitative measures (faithfullness, locality, othorgonality),\n\nAs this topic has recently gained interest, not many papers exist in the literature. This paper covers the related work sufficiently.\n\nWeaknesses:\n\nThe main contribution relies on dimensionality reduction using an autoencoder with an additional prediction loss. A more direct approach could be to build a DNN where intermediate layers are inherently sparse, intuitively this may achieve the same result due to the high faithfulness of the proposed method. Have the authors tried this? \n\nThe sparsity term is unclear as the formula simply shows a nesting of two loss functions (i.e., squared autoencoder loss inside a log penalty function). How this loss enforces sparsity is not clearly explained. As this is one of the important aspects of the proposed model, clarity here is insufficient.\n\nIt is not clear if the proposed model actually generates justification visualizations of a classification decision or if it just outputs where the network is attending no matter whether it is relevant for the decision or not. I have not found any component in the model that restricts the non-class specific network activations to be visualized. Although visualizing all the intermediate network predictions is certainly an interesting output, only a small portion of these activations are important for the fine-grained class prediction.\n\nI have not detected any indication in the figures that the attention maps actually justify the classification decision. For instance in Figure 4, the Downy woodpecker has a distinct red spot on the head which is a visually salient region. However the same red spot does not appear in the female bird. In fact, the stripes on the ahead and the back are more explanatory of the classification decision. The CUB dataset contains examples of both of the situations. It would be interesting to see if the network activation focuses on the red spot even when it is not present. Similarly for Laysan Albatross the eyes are not actually distinguishing properties. Instead the shape of the beak, the color of the wings are more important to attend to to distinguish it from other types of albatrosses. Hence, I do not see any evidence in these results showing that the proposed model generates explanation visualizations. \n\nIn the experiments, the authors use a small subset of the CUB dataset (30 classes out of 200). It is not clear which 30 classes they choose and whether this subset preserves the fine-grained nature of the dataset (are there any classes which share the same supercategory?). Also, it is not clear if the authors include all the images in the evaluation including the ones whose class prediction is wrong or if they include only the images whose class is correctly predicted by the network. \n\nThe results on Table 2 indicate that if the network is presented with the entire image, it performs significantly higher than the case when the features are masked by the attention map that the model generates. Additionally, on 30 classes the presented accuracy seems to be lower than what the state of the art model would achieve (the state of the art is ~82% on 200 classes).  \n\nAs a general remark, the reviewer argues that although the topic presented in this paper is certainly interesting and relevant for the community, the proposed model is not convincing in terms of its capacity for generating “explanations”. The experiments are conducted on a small subset of the CUB dataset which is a design choice that is not sufficiently motivated in the paper. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/ef27b4f63b5cacd37cd41e17610045a359ea1ce4.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739432109,"tcdate":1509058797917,"number":202,"cdate":1509739429451,"id":"ry8lF1xAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/ef27b4f63b5cacd37cd41e17610045a359ea1ce4.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}