{"notes":[{"tddate":null,"ddate":null,"tmdate":1515204007702,"tcdate":1515204007702,"number":8,"cdate":1515204007702,"id":"rke3poaXG","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Comment","forum":"ry8lF1xAZ","replyto":"rJ_F-39ez","signatures":["ICLR.cc/2018/Conference/Paper202/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper202/Authors"],"content":{"title":"The updated results for 200 categories","comment":"Here are the updated results for 200 categories:\n\t\t               SRAE       NN       SAE\nF_reg Training      0.0812    0.0696    0.0972\n\t  Testing\t      0.1659    0.1304    0.1981\nF_cls  Training      99.99%   100.0%   99.99%\n\t  Testing        99.99%   100.0%   99.98%\nO_1\t  Positive\t      0.6554     0.9765   0.8794\nO_2\t  Positive\t      2.4312     4.9112   3.5057\nLocality Positive  1.9713     2.4360   2.1997"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514610292826,"tcdate":1514610292826,"number":7,"cdate":1514610292826,"id":"rk6uRq4QG","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Comment","forum":"ry8lF1xAZ","replyto":"S1rjOKFff","signatures":["ICLR.cc/2018/Conference/Paper202/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper202/Authors"],"content":{"title":"To Response 1","comment":"Thanks for the reviewer’s response.\n> The orthogonality from the 4096*4096 covariance of Z\nWe computed the orthogonality from the 4096*4096 covariance of Z, and showed the results as follows:\nO_1=1176.01 \nO_2=Inf (Because det(ZZ’) = 0)\nBoth O_1 and O_2 are not comparable with those from the 5*5 covariance of Z. Hence, we introduce another comparable metric O_3, which represents the average absolute value of all the elements in the covariance matrix C except the diagonal:\nO_3 = (\\sum_{i=1}^N \\sum_{j=1}^N |c_{i,j}| - N) / (N⋅(N-1))\nwhere c_{i,j} is the element in the covariance matrix C, N is the number of the neurons.\nFor Z, N=4096, O_3=0.2749\nFor our method SRAE, N=5, O_3=0.2921.\nThe orthogonality of the heatmaps of our method SRAE is a little worse than that of the input feature space Z. However, we still think it is meaningless to compare the orthogonality between the 4096 neurons of Z and the 5 neurons of x-layer. \nSince most of the 5 neurons in x-layer focus on one particular type of bird, while the 4096 neurons of Z focus on any feature from any of the 200 types of birds, or plain background. Since 5 is a small number, if there are 2 neurons focus on similar parts of the bird, the correlation of these 2 neurons will be high, and the average correlation over the 5 neurons will also be high. Therefore, the orthogonality of the heatmaps of our method SRAE being a little worse than the input feature space Z does not mean much.\n\nWe think the interpretability contains at least three aspects: (1) faithfulness; (2) conciseness; (3) orthogonality. Orthogonality is merely one of the aspects. We did not evaluate conciseness quantitatively because it seems obvious that human would prefer a concise explanation. We would like to suggest the reviewer to seriously consider the merit in the paper that we are one of the first to provide a concise and generalizable explanation of deep learning’s behavior of the type “This is class A because of B, C, D” to the human, without depending on a pre-defined vocabulary as in prior work. \n\n> Other comments\nWe have revised our paper to add the details of the inpainting experiments, the details of the human study in the Experiments, and the results to show that ExcitationBP doesn't succumb to the flaws described by Kindermans et. al. in Figure 10 of the Appendix.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513883804595,"tcdate":1513883804595,"number":6,"cdate":1513883804595,"id":"S1rjOKFff","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Comment","forum":"ry8lF1xAZ","replyto":"SyauCNPzM","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper202/AnonReviewer2"],"content":{"title":"Response 1","comment":"Thanks for the updates and the CAE results! I think these really help highlight the tradeoff taking place here.\n\n> We appreciate the reviewer’s suggestion on a human study to predict model behavior and evaluate how SRAE directly effects human predictions about model behavior. However, it’s difficult to find enough experts that can make good predictions on these fine-grained bird categories.\n\nI would actually think it better to use humans who aren't experts. If experts were used then they might be more likely to try to predict the correct answer instead of predicting the model's answer. In either case subjects would probably need some training with examples of explanations and model output so they can try to learn how the model works.\n\n> Z is the first fully-connected layer in the CNN (4096-dim). Directly generating a prediction from Z is what the original CNN did, hence 100% faithful.\n\nExactly. The tradeoff would be more clear if the table were filled in to reflect this (with 0, 0, 100%, 100% in the Z column).\n\n> We didn’t compute the orthogonality from the 4096*4096 covariance of Z, since it’s hard to compare that against that of a 5-dim space.\n\nWhy is this hard? Doesn't the LASSO column use an 8-dim (not 5-dim) space?\n\n> In our paper, we attempted to try to classify just using the regions...\n\nMakes sense, but please point out that difference explicitly in the paper.\n\n> In human study, the participants chose which parts...\n\nMakes sense, please include these details too.\n\n> Q: It would be interesting to see how SRAE performs using other visual saliency techniques (e.g. DeConv).\n> A: Excitation BP is a state-of-the-art visualization approach which beat several other methods (including DeConv) in (Zhang 2016b). Kindermans et. al (The (Un)reliability of saliency methods, arxiv, 2017) compare different visualization methods, which shows most of the existing saliency methods are sensitive to the transformation of the input except the PatternAttribution (PA) method. We have tried a similar experiment with ExcitationBP and found that it doesn’t suffer from this issue, hence it should be better than most of the methods compared in that paper also. We will do experiments to see how SRAE performs on PA when the code of PA becomes available online.\n\nI don't think the we really have evaluation criteria good enough to establish a clear state-of-the-art for saliency visualizations. It's good that E-BP doesn't succumb to the flaws described by Kindermans et. al. (please add those experiments in an appendix), but that alone isn't good enough to preclude other saliency techniques. Still, this is only a minor flaw since the core of the paper is still useful without those experiments.\n\n\n\n\n* I'd like to clarify my reasoning about this paper a bit.\n\nI'm trying to evaluate this paper in the framework from \"Towards A Rigorous Science of Interpretable Machine Learning\" (TRSIML) mentioned previously.  The key difficulty for me is establishing human understanding of explanations. This paper does that by starting with the assumption that orthogonal explanations will be better understood (works via section 3.3 of TRSIML). This is weak and I'm not sure it's true, but the idea is very common and still under investigation in the literature so it's reasonable to base a paper on it. Given that goal, I just want the paper to go one step farther by comparing SRAE to doing nothing (i.e., using Z) in the orthogonality dimension."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513734195613,"tcdate":1513734195613,"number":5,"cdate":1513734195613,"id":"rknVlSDzf","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Comment","forum":"ry8lF1xAZ","replyto":"rJ_F-39ez","signatures":["ICLR.cc/2018/Conference/Paper202/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper202/Authors"],"content":{"title":"Responds to: Embedding deep networks into visual explanations","comment":"Q: Why not build a DNN where intermediate layers are inherently sparse\nA: A direct approach of building a DNN where intermediate layers are inherently sparse may decrease the accuracy of the model, as well as being difficult to train. Our proposed method instead focuses on explaining existing high-accuracy DNN, we believe that’s a valuable alternative instead of rebuilding the entire CNN machinery.\n\nQ: The sparsity term is unclear as the formula simply shows a nesting of two loss functions. How this loss enforces sparsity is not clearly explained.\nA: Thanks for the comment. We’ve revised the paper to include this explanation. The log penalty log(1+q*x^2) is a robust loss function (Fig.2 (b)(c)) where x is the average reconstruction loss on each dimension over the training set. The loss function is robust since large x increases the loss function sublinearly (less than an L1 penalty |x| where the increase is linear). Some dimensions wouldn’t suffer a large loss even if they are not reconstructed. Hence this loss function achieves the goal that we would selectively reconstruct only some of the input dimensions, instead of all of them (since reconstruction errors on some dimensions can be large). The exact dimensions that are reconstructed are chosen automatically by the optimization itself.\n\n\nQ: Although visualizing intermediate network predictions is interesting, only a small portion of these activations are important for the fine-grained class prediction.\nA: For each visualization we have a quantitative value above it (Fig 4, 6, 7, 8), which equals the importance of the feature in the explanation space E(Z) to the final prediction. The final prediction of the deep network is approximated by a simple sum of these values. We will enlarge these quantitative values for each figure, and discuss them more in the experiments. Indeed, we didn’t find that many features are needed to explain the birds, for the CUB dataset, the number of useful neurons usually range between 3-5 for each type of bird, but these are usually different neurons for different types of birds (as in Fig.6 and 8). We believe maybe there are indeed only these few high-level features that discriminate each bird. We view the capability of our algorithm to not produce too many irrelevant neurons as an advantage of it.\n\nQ: The revealed properties were not distinguishing for Laysan Albatross and Downy Woodpecker, especially, eyes were not distinguishing property for Laysan Albatross.\nA: We are glad that the reviewer is more of an expert in the fine-grained classification of birds than we are. However, this comment would actually justify our superiority over the baseline. For Laysan Albatross, the first row (Excitation BP) in Fig 4(b) is the baseline approach that performing visualization directly on the predictions of DNN, which always focuses on eyes – wrong as deemed by the reviewer. The second row (Neuron 3) and the third row (Neuron 4) in Fig 4(b) are the 2 most important features of E(Z) in our proposed method SRAE. We can observe that Neuron 3 focuses on eyes, faithful to the original DNN (the first row). While Neuron 4 focuses on wings, which are the discriminative features as the reviewer mentioned, indicating that the explanation space of our proposed method SRAE provides a more comprehensive explanation compared with the baseline Excitation BP directly on predictions of DNN. For Downy woodpecker, in the paper revision we showed more neurons (original paper only contained the most important one). For the female bird, there are neurons that focuses on stripes instead of read spot. We have showed more examples both from male and female birds of downy woodpecker in our revised paper (see Figure 6 in Appendix). \n\nQ: It is not clear which 30 classes they choose and whether this subset preserves the fine-grained nature of the dataset. On 30 classes the presented accuracy seems to be lower than what the state of the art model would achieve (the state of the art is ~82% on 200 classes).\nA: We are running all the 200 classes now, and will reported the results by the discussion deadline.\n\nIn the previous experiments, although we randomly selected 30 classes, we reported the average accuracy on the original 200 categories’ classification task, not on the 30 categories’ classification task. The reviewer has misinterpreted the numbers there. Hence, the results are not lower than the state-of-the-art. For any class, all the 8000 negative examples randomly selected from the left 199 categories of birds were used in training. Hence discriminative power versus all the other categories were retained.\n\nIn Table 2 row 2, the accuracy of our model (0.8428) drops a little compared with that of presenting entire images (0.8798). However, our model beats the baseline which performs visualization directly on the predictions (0.6742) under the same threshold, indicating that the proposed model truly captures more comprehensive and discriminative features.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513734232013,"tcdate":1513733748675,"number":4,"cdate":1513733748675,"id":"SyauCNPzM","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Comment","forum":"ry8lF1xAZ","replyto":"BJfLzKkZf","signatures":["ICLR.cc/2018/Conference/Paper202/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper202/Authors"],"content":{"title":"Responds to: Interesting direction, but some evaluation is missing","comment":"Q: Why the choice of these interpretability metrics? Why not use human to predict model behavior?\nA: We agree with the reviewer that an objective measure of explanation is still an open problem. To the best of our knowledge, there is no standard quantitative metric proposed to evaluate the experimental results for the explanation of deep learning. The proposed metric is by no means perfect and merely reflects our current efforts in quantitatively measuring different explanations.\n\nWe appreciate the reviewer’s suggestion on a human study to predict model behavior and evaluate how SRAE directly effects human predictions about model behavior. However, it’s difficult to find enough experts that can make good predictions on these fine-grained bird categories. We do believe that different types of birds are more likely separated by their parts, other than some other visual categories which may require more holistic features, hence the proposed metrics that focus on parts.\n\nQ: Orthogonality and Faithfulness for Z\nA: Z is the first fully-connected layer in the CNN (4096-dim). Directly generating a prediction from Z is what the original CNN did, hence 100% faithful. We didn’t compute the orthogonality from the 4096*4096 covariance of Z, since it’s hard to compare that against that of a 5-dim space. However, our baseline method Lasso selects several most useful dimensions directly from Z and tries to mimic the network decision as a linear combination of these features. In the experiments from the original submission, we’ve already evaluated Lasso both quantitatively and with human study. From Table 2(a) we can observe that, it is almost impossible to only select few (8 to 232) most important features of Z to achieve faithfulness. From Table 1(a) and 1(b) we observe that the Locality, O_1, and O_2 of the most top important features of Z are all larger than SRAE both from quantitative evaluations and human study, indicating that SRAE is better than Lasso.\n\nQ: Furthermore ablations of the proposed E(Z)\nA: Our NN baseline corresponds to E(Z) with faithfulness loss but no reconstruction nor sparsity, and SAE is faithfulness+reconstruction without sparsity. Upon the reviewer’s request, we have done experiments for E(Z) only with reconstruction and without faithfulness loss (a classic autoencoder). The results are shown as follows (and updated in the paper):\nF_reg: Training 3.9202 Testing 3.8216\nF_cls: Training 62.38% Testing 67.21%\nLocality: 1.9989\nO_1: 0.5755\nO_2: 1.7585\nThe orthogonality of the classic autoencoder is better than SRAE, since its objective is only reconstruction. The locality of the classic autoencoder is slightly larger than SRAE. But importantly it’s impossible for the classic autoencoder to faithfully mimic the DNN predictions because of the lack of a faithfulness loss during training.\n\nQ: ExcitationBP should be better based on the inpainting experiment because its accuracy drops faster?\nA: In our paper, we attempted to try to classify just using the regions that are presented in the heatmaps, which means we kept the highlighted regions while masked the background. This is different from some prior work where the highlighted regions are removed. In our case, a kept region is more discriminative if classifier performance drops less compared to non-masked classification, which means our proposed method SRAE is better than Excitation BP. \n\nWe believe this is better because besides the most discriminative region, other regions may still contain varied amounts of discriminative information, hence the results of masking the most discriminative region may not be comparable across different categories. \n\nQ: How did the human experiment in Table 1b work?\nA: In human study, the participants chose which parts each heatmap represent for the given examples. For each neuron in E(Z), we summarize how many times it represents each part over the whole training set. Then we obtain a n*m matrix, where n is the number of neurons in E(Z), m is the number of parts, and the element is the times each neuron associates with each part. We use this matrix to compute the locality and orthogonality to validate whether the automatic ones have been computed properly.\n\nQ: It would be interesting to see how SRAE performs using other visual saliency techniques (e.g. DeConv).\nA: Excitation BP is a state-of-the-art visualization approach which beat several other methods (including DeConv) in (Zhang 2016b). Kindermans et. al (The (Un)reliability of saliency methods, arxiv, 2017) compare different visualization methods, which shows most of the existing saliency methods are sensitive to the transformation of the input except the PatternAttribution (PA) method. We have tried a similar experiment with ExcitationBP and found that it doesn’t suffer from this issue, hence it should be better than most of the methods compared in that paper also. We will do experiments to see how SRAE performs on PA when the code of PA becomes available online."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513728944629,"tcdate":1513728944629,"number":3,"cdate":1513728944629,"id":"ByYnoXPGz","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Comment","forum":"ry8lF1xAZ","replyto":"SyWSeKGZf","signatures":["ICLR.cc/2018/Conference/Paper202/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper202/Authors"],"content":{"title":"Responds to: Task not well defined","comment":"We thank the reviewer for the comments. However, there are 2 fundamental points that we differ from the reviewer:\n\n1) The reviewer seems to hold the belief that an explanation should be independent of humans, i.e., humans will understand it without putting any significant effort.\n\n2) The reviewer does not believe that dimensionality reduction methods can provide explanations to human.\n \nFor 1), the reviewer might be thinking about natural language-based explanations such as [Hendricks et al. 2016], etc. However, such explanations are based on models that are pre-trained on a human-annotated corpus. This works for the cases where we already have human experts, however, imagine a case where we are faced with massive amount of data, human experts do not exist, the first thing would be to train a deep network since it works better than most human heuristic (medical imaging could be an example). In such cases, a pre-defined language to train on would not exist and we would need to dig directly into the deep network to understand the data. Before our work, there has not been an approach that can decompose the decision of a deep network into a form of “this is A because of B, C, D” where B, C, D are high-level concepts. Such high-level concepts, absence of a pre-defined language, must be learned visually by human to advance human knowledge and improve his/her understanding to a problem. Hence, we believe the capability of our approach in generating high-level concepts (manifest in the neurons in the explanation space) has never been seen in prior work and marks a substantial contribution.\n \nFor 2), we agree that not any dimensionality reduction method can be called explanation, and we understand the sentiment of the reviewer that there had been too many dimensionality reduction methods in prior work. However, we argue that a good explanation should contain some model compression/dimensionality reduction so that it doesn’t provide too much information to overwhelm human. Our approach jointly optimizes over the faithfulness to the predictor and a robust reconstruction loss to the input (deep learning feature) layer. The result only partially reconstructs some dimensions of the input space that are directly relevant to the prediction, different from most dimensionality reduction methods. Such distillation of information made it a lot easier for human to understand. Furthermore, after human has understood each dimension of the explanation space, the final deep learning prediction can be explained as simple as “The neural network predicts it as a bird because of a specific type of eyes and a specific type of wings”. By presenting examples of these concepts for human to learn and name what they are, we no longer need to make the “known vocabulary” assumption and the “existent expert” assumption in the explanations in some previous work e.g. [Hendricks et al. 2016], nor do we need the human to understand thousands of neurons in some previous work e.g. [Bau et al. 2017]. Hence, we believe this is a significantly different type of explanation which could really advance human understanding of the neural network.\n\nQ: The heatmap approach used in the paper is not new\nA: We believe an important advantage of our approach is that we can use different heatmap visualization approaches as they are developed. Proposing a new heatmap visualization approach is orthogonal to the main goal of the paper in providing a new type of explanation.\n\nQ: the evaluation protocol is convoluted and heavily depends on the details of the proposed approach\nA: The evaluation protocol has no dependency on the proposed approach and would apply to all heat-map based visualization methods. The locality metric may sound complex, but it approximately represents the log of average number of parts each heatmap highlights on, hence easily understandable.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642408182,"tcdate":1512374329520,"number":4,"cdate":1512374329520,"id":"SyWSeKGZf","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Task not well defined","rating":"4: Ok but not good enough - rejection","review":"This paper tackles an interesting and timely problem. And it is mostly well-written. \n\nThe main weakness is that the solution the paper offers fall far short of its claim. The problem of generating explanations is formulated as generating low dimensional embeddings that satisfy certain properties such as reconstruction and sparsity. At the same time, the actually explanation of the embeddings is left to humans, i.e., what each dimension means needs to figured out by humans through a user interface where a user looks at images and neuron responses. Furthermore, the only thing that is directly human interpretable, the spatial attention map, is actually generated by applying an existing method to the embeddings. \n\nThus, I would have no problem of calling the proposed approach a dimensionality reduction method (like t-SNE), or a model compression method, but I think calling it an explanation is overclaiming its significance. \n\nI understand that it is very challenging to even define the problem of explanation, and any success in this deserves credit. But in my opinion this paper has not made significant progress in defining the task. \n\nThe evaluation part is also unsatisfying, in that the evaluation protocol is convoluted and heavily depends on the details of the proposed approach. Ideally, an evaluation protocol should be decoupled from a particular approach, such that very different approaches can be compared for the same task. \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642408222,"tcdate":1512178250155,"number":3,"cdate":1512178250155,"id":"BJfLzKkZf","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction, but some evaluation is missing","rating":"4: Ok but not good enough - rejection","review":"Summary\n---\n\nThis paper proposes a new representation for visually explaining model predictions. It wants to create explanations which are expressed in terms of a small number of high level human interpretable concepts and are complete. To accomplish this it re-represents a network's hidden states using an explanation embedding which is constrained to be faithful to the model and represent independent concepts. Next the paper uses Excitation Backprop (saliency/heat map visualization) to produce a heat map for the concepts in the explanation space. These are verified to match some criteria for interpretability.\n\nExplanations are produced for image classification models. The explanation space is a learned embedding E(Z) attached to some fully connected hidden layer of a conv net. It learns a representation that has a few nice properties, each enforced by an objective:\n1. It should be able to mimic the classification scores of the model.\n2. It should be able to reconstruct features from the layer which feeds it input.\n3. Only some features need to be well reconstructed (reconstruction sparsity), so detailed/low-level information can be ignored.\nOnce this embedding is learned, each neuron in the embedding space is visualized using Excitation Backprop to highlight visual regions corresponding to each dimension of the learned space E(Z). These heat maps are displayed for each neuron across multiple images for humans to inspect.\n\nEvaluation compares the proposed Sparse Reconstruction Autoencoder (SRAE) to baselines that impose different priors on the explanation representation or perform feature selection on the layer which the SRAE re-represents. These including an MLP, a sparse autoencoder (the feature space is sparse instead of the reconstruction space Z), and Lasso. The following dimensions are evaluated:\n1. Fidelity: The SRAE representation can mimic model classifications accurately\n2. Orthogonality: Heat maps are computed for pairs of dimensions of E(Z). Correlation is computed between these heat maps (treated as 1d vectors). This correlation is is low for the SRAE, so different neurons of E(Z) capture somewhat independent concepts.\n3. Locality: The CUB dataset comes with part annotations. A representation which highlights only one or a small number of parts (beak, wing, legs) is said to be more local. Spatially local heat maps are good because they aid human understanding. SRAE representations tend to be local in this sense, though the original representation is equally local.\n4. Human evaluation: Multiple humans match feature visualizations to the available list of pre-annotated bird parts and they tend to label the same features with the same bird parts.\n\n\nStrengths\n---\n\nI think the premise of the paper provides a fruitful direction for work in interpretability. Most methods try to interpret neural net representations in a somewhat direct fashion, but it makes sense to sacrifice some faithfulness to obtain an explanation humans can understand (e.g., Hendricks et al. where justification is via learned language description).\n\nWeaknesses\n---\n\nAs I see it, this paper's only major weakness is that it doesn't evaluate the approach well enough. There are two kinds of problems with the evaluation. (1) First, it's not clear what makes a good explanation or how to measure that once some operational definition is in place. The metrics chosen in this paper have some clear flaws, which the paper acknowledges, but they are reasonable given this is still an open problem. (2) Second, tentatively accepting that the aforementioned metrics work well, some important experimental settings are missing.\n\n(2) Missing Experiments:\n\n* To show that the explanation representation E(Z) is useful it should be compared to representation Z it is trying to re-represent. This comparison is only provided in terms of the locality metric, where Z performs about the same as E(Z). F_reg, F_cls, O_1, and O_2 should also be provided for the Z column in table 2a to establish that E(Z) is a more interpretable representation than Z according to the chosen criteria.\n\n* Furthermore ablations of the proposed E(Z) embedding should be provided. I think the NN baseline is equivalent to SRAE without the reconstruction sparsity term. (Is that right?) In any case, a version of SRAE with/without both of the faithfulness loss and the reconstruction sparsity losses should be provided to establish what performance difference each of these contributes.\n\n(1) Choice and explanation of interpretability criteria and metrics (minor weakness):\n\n* Locality and orthogonality are properties of representations which are commonly assumed to make the representations more interpretable to humans. The evaluation in this paper relies heavily on this assumption and it does establish that SRAE representations are local and orthogonal. It would be nice to see how these properties effect human predictions about model behavior or how SRAE directly effects human predictions about model behavior. This would provide some measure of whether these properties actually do help interpretability or not. Can a human predict a model's output given SRAE visualizations, including\nfailure cases? (c.f. section 3 of \"Towards A Rigorous Science of Interpretable Machine Learning\" by Doshi-Velez and Kim, 2017)\n\n* At the end of section 2.1 the paper argues that we should train humans to understand the concepts a network learns to represent. Subsequent evaluation seems to contradict this viewpoint. It evaluates interpretability of SRAE features based on their similarity to pre-annotated parts (the locality metric). The human evaluation experiment also evaluates based on similarity to pre-annotated parts. Instead of training humans to understand features, it assumes that humans are familiar with the pre-annotated bird parts and asks them to match SRAE features to these parts. I think the point of the original argument, teaching humans to understand features, would be to allow a network to learn features that are better for the relevant task because they are not constrained to be explainable. Learned concepts might not align with concepts humans already think are important, but the actual evaluation only rewards features that align with known human-identifiable concepts.\n\nIn support of the actual evaluation procedure, the paper argues \"the majority of bird classifications are based on specific, discriminative parts of the bird.\" This seemingly further contradicts the previous motivation. I think the evaluation is still useful, but there should be a more careful discussion about which conclusions it warrants. It seems that features that perform worse on this metric could actually be more interpretable than features that perform better, but that's unlikely because it's likely that the most interpretable explanations will be in terms of human-identifiable concepts.\n\n* I'm not sure how table 2b makes a point about completeness. It cites (Gonzales-Garcia et al., 2016, section 6/figure 15) to explain the experiment, but in that paper a masked region is said to be more discriminative if classifier performance drops more compared to non-masked classification. According to table 2b, performance drops more on average for Excitation BP, so according to this metric Excitation BP is better at identifying discriminative regions than SRAE. Is that right?\n\nAs I understand it, heat maps are masked from an image one at a time and then accuracy is averaged over all these occluded+inpainted versions of the image.  How is this supposed to measure completeness?\n\n* The Orthogonality metric is computed against heat maps. Correlation should also be reported against the representations being visualized; i.e., across dimensions of Z and dimensions of E(Z).\n\n\nMinor Concerns/Comments\n---\n\n* The first paragraph of the related work makes it sound like work on captioning and VQA are proposed/used to make systems interpretable. I think this is accurate for (Hendricks et al. 2016), but the rest do not use these tasks to increase interpretability, but focus on vision and language models/tasks as target models to be understood or improved.\n\n* How exactly did the human experiment in Table 1b work? Orthogonality and locality are computed against attention maps. How are the maps used for human evaluation computed?\n\n* The paper cites visual saliency techniques other than Excitation Backprop. It would be interesting to see how SRAE performs using some of these other techniques (e.g. DeConvNet).\n\nPreliminary Evaluation\n---\n\nI think the paper proposes an interesting and somewhat novel idea, but the evaluation is not good enough. If the experiments described in the Missing Experiments section are provided and show that the explanation space E(Z) is better than the given latent space Z then I would be happy to accept the paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642408268,"tcdate":1512177942230,"number":2,"cdate":1512177942230,"id":"B1oGZFkWG","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction, but some evaluation is missing","rating":"4: Ok but not good enough - rejection","review":"Summary\n---\n\nThis paper proposes a new representation for visually explaining model predictions. It wants to create explanations which are expressed in terms of a small number of high level human interpretable concepts and are complete. To accomplish this it re-represents a network's hidden states using an explanation embedding which is constrained to be faithful to the model and represent independent concepts. Next the paper uses Excitation Backprop (saliency/heat map visualization) to produce a heat map for the concepts in the explanation space. These are verified to match some criteria for interpretability.\n\nExplanations are produced for image classification models. The explanation space is a learned embedding E(Z) attached to some fully connected hidden layer of a conv net. It learns a representation that has a few nice properties, each enforced by an objective:\n1. It should be able to mimic the classification scores of the model.\n2. It should be able to reconstruct features from the layer which feeds it input.\n3. Only some features need to be well reconstructed (reconstruction sparsity), so detailed/low-level information can be ignored.\nOnce this embedding is learned, each neuron in the embedding space is visualized using Excitation Backprop to highlight visual regions corresponding to each dimension of the learned space E(Z). These heat maps are displayed for each neuron across multiple images for humans to inspect.\n\nEvaluation compares the proposed Sparse Reconstruction Autoencoder (SRAE) to baselines that impose different priors on the explanation representation or perform feature selection on the layer which the SRAE re-represents. These including an MLP, a sparse autoencoder (the feature space is sparse instead of the reconstruction space Z), and Lasso. The following dimensions are evaluated:\n1. Fidelity: The SRAE representation can mimic model classifications accurately\n2. Orthogonality: Heat maps are computed for pairs of dimensions of E(Z). Correlation is computed between these heat maps (treated as 1d vectors). This correlation is is low for the SRAE, so different neurons of E(Z) capture somewhat independent concepts.\n3. Locality: The CUB dataset comes with part annotations. A representation which highlights only one or a small number of parts (beak, wing, legs) is said to be more local. Spatially local heat maps are good because they aid human understanding. SRAE representations tend to be local in this sense, though the original representation is equally local.\n4. Human evaluation: Multiple humans match feature visualizations to the available list of pre-annotated bird parts and they tend to label the same features with the same bird parts.\n\n\nStrengths\n---\n\nI think the premise of the paper provides a fruitful direction for work in interpretability. Most methods try to interpret neural net representations in a somewhat direct fashion, but it makes sense to sacrifice some faithfulness to obtain an explanation humans can understand (e.g., Hendricks et al. where justification is via learned language description).\n\nWeaknesses\n---\n\nAs I see it, this paper's only major weakness is that it doesn't evaluate the approach well enough. There are two kinds of problems with the evaluation. (1) First, it's not clear what makes a good explanation or how to measure that once some operational definition is in place. The metrics chosen in this paper have some clear flaws, which the paper acknowledges, but they are reasonable given this is still an open problem. (2) Second, tentatively accepting that the aforementioned metrics work well, some important experimental settings are missing.\n\n(2) Missing Experiments:\n\n* To show that the explanation representation E(Z) is useful it should be compared to representation Z it is trying to re-represent. This comparison is only provided in terms of the locality metric, where Z performs about the same as E(Z). F_reg, F_cls, O_1, and O_2 should also be provided for the Z column in table 2a to establish that E(Z) is a more interpretable representation than Z according to the chosen criteria.\n\n* Furthermore ablations of the proposed E(Z) embedding should be provided. I think the NN baseline is equivalent to SRAE without the reconstruction sparsity term. (Is that right?) In any case, a version of SRAE with/without both of the faithfulness loss and the reconstruction sparsity losses should be provided to establish what performance difference each of these contributes.\n\n(1) Choice and explanation of interpretability criteria and metrics (minor weakness):\n\n* Locality and orthogonality are properties of representations which are commonly assumed to make the representations more interpretable to humans. The evaluation in this paper relies heavily on this assumption and it does establish that SRAE representations are local and orthogonal. It would be nice to see how these properties effect human predictions about model behavior or how SRAE directly effects human predictions about model behavior. This would provide some measure of whether these properties actually do help interpretability or not. Can a human predict a model's output given SRAE visualizations, including\nfailure cases? (c.f. section 3 of \"Towards A Rigorous Science of Interpretable Machine Learning\" by Doshi-Velez and Kim, 2017)\n\n* At the end of section 2.1 the paper argues that we should train humans to understand the concepts a network learns to represent. Subsequent evaluation seems to contradict this viewpoint. It evaluates interpretability of SRAE features based on their similarity to pre-annotated parts (the locality metric). The human evaluation experiment also evaluates based on similarity to pre-annotated parts. Instead of training humans to understand features, it assumes that humans are familiar with the pre-annotated bird parts and asks them to match SRAE features to these parts. I think the point of the original argument, teaching humans to understand features, would be to allow a network to learn features that are better for the relevant task because they are not constrained to be explainable. Learned concepts might not align with concepts humans already think are important, but the actual evaluation only rewards features that align with known human-identifiable concepts.\n\nIn support of the actual evaluation procedure, the paper argues \"the majority of bird classifications are based on specific, discriminative parts of the bird.\" This seemingly further contradicts the previous motivation. I think the evaluation is still useful, but there should be a more careful discussion about which conclusions it warrants. It seems that features that perform worse on this metric could actually be more interpretable than features that perform better, but that's unlikely because it's likely that the most interpretable explanations will be in terms of human-identifiable concepts.\n\n* I'm not sure how table 2b makes a point about completeness. It cites (Gonzales-Garcia et al., 2016, section 6/figure 15) to explain the experiment, but in that paper a masked region is said to be more discriminative if classifier performance drops more compared to non-masked classification. According to table 2b, performance drops more on average for Excitation BP, so according to this metric Excitation BP is better at identifying discriminative regions than SRAE. Is that right?\n\nAs I understand it, heat maps are masked from an image one at a time and then accuracy is averaged over all these occluded+inpainted versions of the image.  How is this supposed to measure completeness?\n\n* The Orthogonality metric is computed against heat maps. Correlation should also be reported against the representations being visualized; i.e., across dimensions of Z and dimensions of E(Z).\n\n\nMinor Concerns/Comments\n---\n\n* The first paragraph of the related work makes it sound like work on captioning and VQA are proposed/used to make systems interpretable. I think this is accurate for (Hendricks et al. 2016), but the rest do not use these tasks to increase interpretability, but focus on vision and language models/tasks as target models to be understood or improved.\n\n* How exactly did the human experiment in Table 1b work? Orthogonality and locality are computed against attention maps. How are the maps used for human evaluation computed?\n\n* The paper cites visual saliency techniques other than Excitation Backprop. It would be interesting to see how SRAE performs using some of these other techniques (e.g. DeConvNet).\n\nPreliminary Evaluation\n---\n\nI think the paper proposes an interesting and somewhat novel idea, but the evaluation is not good enough. If the experiments described in the Missing Experiments section are provided and show that the explanation space E(Z) is better than the given latent space Z then I would be happy to accept the paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642408315,"tcdate":1511862656331,"number":1,"cdate":1511862656331,"id":"rJ_F-39ez","invitation":"ICLR.cc/2018/Conference/-/Paper202/Official_Review","forum":"ry8lF1xAZ","replyto":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference/Paper202/AnonReviewer1"],"readers":["everyone"],"content":{"title":"EMBEDDING DEEP NETWORKS INTO VISUAL EXPLANATIONS","rating":"4: Ok but not good enough - rejection","review":"Summary: For the task of explainable machine learning, this paper proposes to generate visualizations that act as visual explanation of a classification decision. The proposed framework uses an explanation module attached to an intermediate layer of a pre-trained DNN to learn an interpretable and low-dimensional feature space. The explanation module aims to predict the final classification output (binary, one-vs-all) and to auto encode its input (intermediate DNNlayer) via Sparse Reconstruction Autoencoder (SRAE) with sparsity as additional target. Using a referenced method (ExcitationBP) the authors show that the features learned by the explanation module correspond to high level features in the original image (such as eye, beak of a bird) that the original DNN uses to make the classification. The experiments are conducted on 30 classes of CUB-200-2011 dataset which contains 200 classes in total.\n\nStrengths:\n\nExplainable artificial intelligence is a research topic is relevant for the community. Explainability of deep neural networks as motivated by the authors would improve trust between the machine and the user, hence it is an important direction.\n\nThe paper is well written. The evaluations are conducted on three introduced quantitative measures (faithfullness, locality, othorgonality),\n\nAs this topic has recently gained interest, not many papers exist in the literature. This paper covers the related work sufficiently.\n\nWeaknesses:\n\nThe main contribution relies on dimensionality reduction using an autoencoder with an additional prediction loss. A more direct approach could be to build a DNN where intermediate layers are inherently sparse, intuitively this may achieve the same result due to the high faithfulness of the proposed method. Have the authors tried this? \n\nThe sparsity term is unclear as the formula simply shows a nesting of two loss functions (i.e., squared autoencoder loss inside a log penalty function). How this loss enforces sparsity is not clearly explained. As this is one of the important aspects of the proposed model, clarity here is insufficient.\n\nIt is not clear if the proposed model actually generates justification visualizations of a classification decision or if it just outputs where the network is attending no matter whether it is relevant for the decision or not. I have not found any component in the model that restricts the non-class specific network activations to be visualized. Although visualizing all the intermediate network predictions is certainly an interesting output, only a small portion of these activations are important for the fine-grained class prediction.\n\nI have not detected any indication in the figures that the attention maps actually justify the classification decision. For instance in Figure 4, the Downy woodpecker has a distinct red spot on the head which is a visually salient region. However the same red spot does not appear in the female bird. In fact, the stripes on the ahead and the back are more explanatory of the classification decision. The CUB dataset contains examples of both of the situations. It would be interesting to see if the network activation focuses on the red spot even when it is not present. Similarly for Laysan Albatross the eyes are not actually distinguishing properties. Instead the shape of the beak, the color of the wings are more important to attend to to distinguish it from other types of albatrosses. Hence, I do not see any evidence in these results showing that the proposed model generates explanation visualizations. \n\nIn the experiments, the authors use a small subset of the CUB dataset (30 classes out of 200). It is not clear which 30 classes they choose and whether this subset preserves the fine-grained nature of the dataset (are there any classes which share the same supercategory?). Also, it is not clear if the authors include all the images in the evaluation including the ones whose class prediction is wrong or if they include only the images whose class is correctly predicted by the network. \n\nThe results on Table 2 indicate that if the network is presented with the entire image, it performs significantly higher than the case when the features are masked by the attention map that the model generates. Additionally, on 30 classes the presented accuracy seems to be lower than what the state of the art model would achieve (the state of the art is ~82% on 200 classes).  \n\nAs a general remark, the reviewer argues that although the topic presented in this paper is certainly interesting and relevant for the community, the proposed model is not convincing in terms of its capacity for generating “explanations”. The experiments are conducted on a small subset of the CUB dataset which is a design choice that is not sufficiently motivated in the paper. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514610476662,"tcdate":1509058797917,"number":202,"cdate":1509739429451,"id":"ry8lF1xAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry8lF1xAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Embedding Deep Networks into Visual Explanations","abstract":"In this paper, we propose a novel explanation module to explain the predictions made by a deep network. Explanation module works by embedding a high-dimensional deep network layer nonlinearly into a low-dimensional explanation space while retaining faithfulness, so that the original deep learning predictions can be constructed from the few concepts extracted by the explanation module. We then visualize such concepts for human to learn about the high-level concepts that deep learning is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A visualization system is then introduced for human understanding of features in the explanation space. The proposed method is applied to explain CNN models in image classification tasks, and several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement. Experiments show that the proposed approach could generate better explanations of the mechanisms CNN use for making predictions.","pdf":"/pdf/c4df6e110971ab55a851a73d5cbd26ff79796b40.pdf","paperhash":"anonymous|embedding_deep_networks_into_visual_explanations","_bibtex":"@article{\n  anonymous2018embedding,\n  title={Embedding Deep Networks into Visual Explanations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry8lF1xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper202/Authors"],"keywords":[]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}