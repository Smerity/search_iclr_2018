{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222795658,"tcdate":1511889594306,"number":3,"cdate":1511889594306,"id":"r1Ma5fixz","invitation":"ICLR.cc/2018/Conference/-/Paper845/Official_Review","forum":"B1DmUzWAW","replyto":"B1DmUzWAW","signatures":["ICLR.cc/2018/Conference/Paper845/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Novel architecture, good results","rating":"7: Good paper, accept","review":"This work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples (for few-shot classification) or action-reward pairs (for reinforcement learning) in order to take the appropriate action. The resulting model is general-purpose and experiments demonstrate efficacy on few-shot image classification and a range of reinforcement learning tasks.\n\nStrengths\n\n- The proposed model is a generic meta-learning useful for both classification and reinforcement learning.\n- A wide range of experiments are conducted to demonstrate performance of the proposed method.\n\nWeaknesses\n\n- Design choices made for the reinforcement learning setup (e.g. temporal convolutions) are not necessarily applicable to few-shot classification.\n- Discussion of results relative to baselines is somewhat lacking.\n\nThe proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining efficient.\n\nThe depth of the TC block is determined by the sequence length. In few-shot classification, the sequence length can be known a prior. How is the sequence length determined for reinforcement learning tasks? In addition, what is done at test-time if the sequence length differs from the sequence length at training time?\n\nThe causality assumption does not seem to apply to the few-shot classification case. Have the authors considered lifting this restriction for classification and if so does performance improve?\n\nThe Prototypical Networks results in Tables 1 and 2 do not appear to match the performance reported in Snell et al. (2017).\n\nThe paper is well-written overall. Some additional discussion of the results would be appreciated (for example, explaining why the proposed method achieves similar performance to the LSTM/OPSRL baselines).\n\nI am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient-based methods. I am wondering if the authors can clarify this point.\n\nOverall, the proposed approach is novel and achieves good results on a range of tasks.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Neural Attentive Meta-Learner","abstract":"Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.  In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks.  On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","pdf":"/pdf/9392954b49366dd851073e264f55ae663f3f0387.pdf","TL;DR":"a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks","paperhash":"anonymous|a_simple_neural_attentive_metalearner","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Neural Attentive Meta-Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1DmUzWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper845/Authors"],"keywords":["meta-learning","few-shot learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222795698,"tcdate":1511856191297,"number":2,"cdate":1511856191297,"id":"BJDSdqqxM","invitation":"ICLR.cc/2018/Conference/-/Paper845/Official_Review","forum":"B1DmUzWAW","replyto":"B1DmUzWAW","signatures":["ICLR.cc/2018/Conference/Paper845/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper involves intensive experiments for the proposed design paradigm; yet the emphasis of the contribution for its generalizability could be more clear on the generalization to reinforcement learning and lacks more theoretical/intuitional studies. ","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning. Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods. The experiment results seem solid and the proposed structure is with simple design and highly generalizable. The concern is that the contribution is quite incremental from the theoretical side though it involves large amount of experimental efforts, which could be impactful. Please see the major comment below.\n\nOne major comment:\n- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side. \nSpecifically, for two folds: \n(1) in general, some more work in investigating the task space would be nice. The paper assumes the tasks are “related” or “similar” and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution. But what exactly should the distribution be like to be learnable and how to quantify such “related” or “similar” relationship across tasks? \n(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods? The result from the paper seems to answer with “in all cases” but then that always brings the issue of “overfitting” or parameter tuning issue. I believe the paper would have been much stronger if either of the two above are further investigated.\n\nMore detailed comments:\n- On Page 1, “the optimal strategy for an arbitrary range of tasks” lacks definition of “range”; also, in the setting in this paper, these tasks should share “similarity” or follow the same “distribution” and thus such “arbitrariness” is actually constrained.\n\n- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined. It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the “task distribution” is too large to be meaningful. One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable.\n\n- For results in Table 1 and Table 2, how are the confidence intervals computed? Is it over multiple runs or within the same run? It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests. The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming “not statistically-significantly different” because no significance test is reported. \n\n- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Neural Attentive Meta-Learner","abstract":"Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.  In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks.  On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","pdf":"/pdf/9392954b49366dd851073e264f55ae663f3f0387.pdf","TL;DR":"a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks","paperhash":"anonymous|a_simple_neural_attentive_metalearner","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Neural Attentive Meta-Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1DmUzWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper845/Authors"],"keywords":["meta-learning","few-shot learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222795740,"tcdate":1511387319333,"number":1,"cdate":1511387319333,"id":"S1J6xOmgf","invitation":"ICLR.cc/2018/Conference/-/Paper845/Official_Review","forum":"B1DmUzWAW","replyto":"B1DmUzWAW","signatures":["ICLR.cc/2018/Conference/Paper845/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Unsure about novelty, would like to see analysis of model","rating":"6: Marginally above acceptance threshold","review":"The authors propose a model for sequence classification and sequential decision making. The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution. The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making.\n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control).\n\nThe experiment section definitely demonstrate the effort put into this work. However, my primary concern is that the model seems somewhat lacking in novelty. Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO. The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding. I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making. I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information).\n\nMy second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning. I think this information would be very useful to the community in terms of what to take away from this paper. In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains.\n\nOverall, I am inclined to accept this paper on the basis of its experimental results. However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain).\n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation\n- In the experiment section for Omniglot, when the authors say \"1200 classes for training and 432 for testing\", it sounds like the authors are performing zero-shot learning. How does this particular model generalize to classes not seen during training?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Neural Attentive Meta-Learner","abstract":"Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.  In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks.  On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","pdf":"/pdf/9392954b49366dd851073e264f55ae663f3f0387.pdf","TL;DR":"a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks","paperhash":"anonymous|a_simple_neural_attentive_metalearner","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Neural Attentive Meta-Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1DmUzWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper845/Authors"],"keywords":["meta-learning","few-shot learning"]}},{"tddate":null,"ddate":null,"tmdate":1511185034510,"tcdate":1511185034510,"number":1,"cdate":1511185034510,"id":"BJGc5UgxM","invitation":"ICLR.cc/2018/Conference/-/Paper845/Public_Comment","forum":"B1DmUzWAW","replyto":"B1DmUzWAW","signatures":["~Pranav_Shyam1"],"readers":["everyone"],"writers":["~Pranav_Shyam1"],"content":{"title":"Comparison with Attentive Recurrent Comparators","comment":"Hi,\n\nVery nice work! However, your results on Omniglot seem to be well within the error margins of results reported with Attentive Recurrent Comparators. I hope that you can consider citing the results in your future revisions. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Neural Attentive Meta-Learner","abstract":"Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.  In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks.  On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","pdf":"/pdf/9392954b49366dd851073e264f55ae663f3f0387.pdf","TL;DR":"a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks","paperhash":"anonymous|a_simple_neural_attentive_metalearner","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Neural Attentive Meta-Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1DmUzWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper845/Authors"],"keywords":["meta-learning","few-shot learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739069394,"tcdate":1509135903302,"number":845,"cdate":1509739066734,"id":"B1DmUzWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1DmUzWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Simple Neural Attentive Meta-Learner","abstract":"Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information.  In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks.  On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","pdf":"/pdf/9392954b49366dd851073e264f55ae663f3f0387.pdf","TL;DR":"a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks","paperhash":"anonymous|a_simple_neural_attentive_metalearner","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Neural Attentive Meta-Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1DmUzWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper845/Authors"],"keywords":["meta-learning","few-shot learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}