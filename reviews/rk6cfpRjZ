{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222587949,"tcdate":1511756012469,"number":3,"cdate":1511756012469,"id":"r14eWGtez","invitation":"ICLR.cc/2018/Conference/-/Paper2/Official_Review","forum":"rk6cfpRjZ","replyto":"rk6cfpRjZ","signatures":["ICLR.cc/2018/Conference/Paper2/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"The authors propose a technique to compress LSTMs in RNNs by using a group Lasso regularizer which results in structured sparsity, by eliminating individual hidden layer inputs at a particular layer. The authors present experiments on unidirectional and bidirectional LSTM models which demonstrate the effectiveness of this method. The proposed techniques are evaluated on two models: a fairly large LSTM with ~66.0M parameters, as well as a more compact LSTM with ~2.7M parameters, which can be sped up significantly through compression.\nOverall this is a clearly written paper that is easy to follow, with experiments that are well motivated. To the best of my knowledge most previous papers in the area of RNN compression focus on pruning or compression of the node outputs/connections, but do not focus as much on reducing the computation/parameters within an RNN cell. I only have a few minor comments/suggestions which are listed below:\n\n1. It is interesting that the model structure where the number of parameters is reduced to the number of ISSs chosen from the proposed procedure does not attain the same performance as when training with a larger number of nodes, with the group lasso regularizer. It would be interesting to conduct experiments for a range of \\lambda values: i.e., to allow for different degrees of compression, and then examine whether the model trained from scratch with the “optimal” structure achieves performance closer to the ISS-based strategy, for example, for smaller amounts of compression, this might be the case?\n\n2. In the experiment, the authors use a weaker dropout when training with ISS. Could the authors also report performance for the baseline model if trained with the same dropout (but without the group LASSO regularizer)?\n\n3. The colors in the figures: especially the blue vs. green contrast is really hard to see. It might be nicer to use lighter colors, which are more distinct.\n\n4. The authors mention that the thresholding operation to zero-out weights based on the hyperparameter \\tau is applied “after each iteration”. What is an iteration in this context? An epoch, a few mini-batch updates, per mini-batch? Could the authors please clarify.\n\n5. Clarification about the hyperparameter \\tau used for sparsification: Is \\tau determined purely based on the converged weight values in the model when trained without the group LASSO constraint? It would be interesting to plot a histogram of weight values in the baseline model, and perhaps also after the group LASSO regularized training.\n\n6. Is the same value of \\lambda used for all groups in the model? It would be interesting to consider the effect of using stronger sparsification in the earlier layers, for example.\n\n7. Section 4.2: Please explain what the exact match (EM) and F1 metrics used to measure performance of the BIDAF model are, in the text. \n\nMinor Typographical/Grammatical errors:\n- Sec 1: “... in LSTMs meanwhile maintains the dimension consistency.” → “... in LSTMs while maintaining the dimension consistency.”\n- Sec 1: “... is public available” → “is publically available”\n- Sec 2: Please rephrase: “After learning those structures, compact LSTM units remain original structural schematic but have the sizes reduced.”\n- Sec 4.1: “The exactly same training scheme of the baseline ...” → “The same training scheme as the baseline ...”","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Intrinsic Sparse Structures within Long Short-Term Memory","abstract":"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will decrease the sizes of all basic structures by one simultaneously and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs, without losing any perplexity of language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset.","pdf":"/pdf/a05bfda38df1bc8fbe4486df370cd3ca728a4139.pdf","paperhash":"anonymous|learning_intrinsic_sparse_structures_within_long_shortterm_memory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6cfpRjZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper2/Authors"],"keywords":["Sparsity","Structure","LSTMs","Recurrent Neural Networks","Model Compression","Structural Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222587991,"tcdate":1511691809942,"number":2,"cdate":1511691809942,"id":"B15mUMdef","invitation":"ICLR.cc/2018/Conference/-/Paper2/Official_Review","forum":"rk6cfpRjZ","replyto":"rk6cfpRjZ","signatures":["ICLR.cc/2018/Conference/Paper2/AnonReviewer1"],"readers":["everyone"],"content":{"title":"the text is verbose but method is simple","rating":"6: Marginally above acceptance threshold","review":"The paper spends lots of (repeated)  texts on motivating and explaining ISS. But the algorithm is simple, using group lasso to find components that are can retained to preserve the performance.  Thus the novelty is limited.\n\nThe experiments results are good.\n\nSec 3.1 should be made more concise. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Intrinsic Sparse Structures within Long Short-Term Memory","abstract":"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will decrease the sizes of all basic structures by one simultaneously and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs, without losing any perplexity of language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset.","pdf":"/pdf/a05bfda38df1bc8fbe4486df370cd3ca728a4139.pdf","paperhash":"anonymous|learning_intrinsic_sparse_structures_within_long_shortterm_memory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6cfpRjZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper2/Authors"],"keywords":["Sparsity","Structure","LSTMs","Recurrent Neural Networks","Model Compression","Structural Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512364626329,"tcdate":1511639539829,"number":1,"cdate":1511639539829,"id":"B13e9rDlM","invitation":"ICLR.cc/2018/Conference/-/Paper2/Official_Comment","forum":"rk6cfpRjZ","replyto":"rkmp_naTZ","signatures":["ICLR.cc/2018/Conference/Paper2/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper2/Authors"],"content":{"title":"More compact model (2.69M) than the mentioned one is tested and related paper will be cited","comment":"Hello Aaron,\n\nThanks for commenting on our work. The related paper will be cited.\n\nOur experiments are carefully designed to cover the spectrum from compact models to large models.\nIn language modeling, 2014 Zaremba paper is used to test how our approach works for a large model, which has 66.0M parameters. \nIn Machine Reading Comprehension, we used a state-of-the-art and very compact (2.69M) model to test how our approach works for small models.\n\nWe will first try to duplicate the mentioned work, and then try to add our approach on top of it. \nWe will make changes in a revised version to take care of your concerns.\n\nRegards"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Intrinsic Sparse Structures within Long Short-Term Memory","abstract":"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will decrease the sizes of all basic structures by one simultaneously and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs, without losing any perplexity of language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset.","pdf":"/pdf/a05bfda38df1bc8fbe4486df370cd3ca728a4139.pdf","paperhash":"anonymous|learning_intrinsic_sparse_structures_within_long_shortterm_memory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6cfpRjZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper2/Authors"],"keywords":["Sparsity","Structure","LSTMs","Recurrent Neural Networks","Model Compression","Structural Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222588032,"tcdate":1511285382871,"number":1,"cdate":1511285382871,"id":"SJ15MyGeG","invitation":"ICLR.cc/2018/Conference/-/Paper2/Official_Review","forum":"rk6cfpRjZ","replyto":"rk6cfpRjZ","signatures":["ICLR.cc/2018/Conference/Paper2/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice work on group lasso LSTM compression ","rating":"6: Marginally above acceptance threshold","review":"Quality: \nThe motivation and experimentation is sound, except that a better PTB baseline can be used (as mentioned in the comments by another reviewer). More minor comments below.\n\nOriginality:\nThis work is a natural follow up on previous work that used group lasso for CNNs, namely learning sparse RNNs with group-lasso. Not very original, but nevertheless important.\n\nClarity:\nThe fact that the method is using a group-lasso regularization is hidden in the intro section and only fully mentioned in section 3.2 I would mention that clearly in the abstract.\n\nSignificance:\nLeaning small models is important and previous sparse RNN work (Narang, 2017) did not do it in a structured way, which may lead to slower inference step time. So this is an investigation of interest for the community.\n\nMinor comments:\n- One main claim in the paper is that group lasso is better than removing individual weights, yet not experimental evidence is provided for that.\n- The authors found that their method beats \"direct design\". This is somewhat unintuitive, yet no explanation is provided. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Intrinsic Sparse Structures within Long Short-Term Memory","abstract":"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will decrease the sizes of all basic structures by one simultaneously and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs, without losing any perplexity of language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset.","pdf":"/pdf/a05bfda38df1bc8fbe4486df370cd3ca728a4139.pdf","paperhash":"anonymous|learning_intrinsic_sparse_structures_within_long_shortterm_memory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6cfpRjZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper2/Authors"],"keywords":["Sparsity","Structure","LSTMs","Recurrent Neural Networks","Model Compression","Structural Learning"]}},{"tddate":null,"ddate":null,"tmdate":1508915387006,"tcdate":1508915387006,"number":1,"cdate":1508915387006,"id":"rkmp_naTZ","invitation":"ICLR.cc/2018/Conference/-/Paper2/Public_Comment","forum":"rk6cfpRjZ","replyto":"rk6cfpRjZ","signatures":["~Aaron_Jaech1"],"readers":["everyone"],"writers":["~Aaron_Jaech1"],"content":{"title":"choice of baseline for language modeling experiments","comment":"If you are going to use the PTB dataset for your language modeling experiments, it would help if you use a newer baseline than the 2014 Zaremba paper. It would be better to cite \"On the State of the Art of Evaluation in Neural Language Models\" from July 2017. (https://arxiv.org/pdf/1707.05589.pdf) They report a perplexity of 59.6 using a single-layer LSTM with 10 million parameters."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Intrinsic Sparse Structures within Long Short-Term Memory","abstract":"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will decrease the sizes of all basic structures by one simultaneously and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs, without losing any perplexity of language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset.","pdf":"/pdf/a05bfda38df1bc8fbe4486df370cd3ca728a4139.pdf","paperhash":"anonymous|learning_intrinsic_sparse_structures_within_long_shortterm_memory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6cfpRjZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper2/Authors"],"keywords":["Sparsity","Structure","LSTMs","Recurrent Neural Networks","Model Compression","Structural Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739536650,"tcdate":1506886292915,"number":2,"cdate":1509739533988,"id":"rk6cfpRjZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk6cfpRjZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Intrinsic Sparse Structures within Long Short-Term Memory","abstract":"Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will decrease the sizes of all basic structures by one simultaneously and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Our method achieves 10.59x speedup in state-of-the-art LSTMs, without losing any perplexity of language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset.","pdf":"/pdf/a05bfda38df1bc8fbe4486df370cd3ca728a4139.pdf","paperhash":"anonymous|learning_intrinsic_sparse_structures_within_long_shortterm_memory","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6cfpRjZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper2/Authors"],"keywords":["Sparsity","Structure","LSTMs","Recurrent Neural Networks","Model Compression","Structural Learning"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}