{"notes":[{"tddate":null,"ddate":null,"tmdate":1512755241548,"tcdate":1512755241548,"number":5,"cdate":1512755241548,"id":"BkzEg8ubM","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Comment","forum":"SkJKHMW0Z","replyto":"B1kp5bvZM","signatures":["ICLR.cc/2018/Conference/Paper838/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper838/Authors"],"content":{"title":"More reasoning","comment":"> Thank you for clarifying your point of view. This is very much appreciated. However, the term \"relational reasoning\" has a long tradition within AI and ML. Consequently, one has to be very clear about how your reasoning links to \"relational reasoning\". Just referring to a recent paper is not enough given that reasoning is at the center of AI.  \n\nAnswer: Yes, we agree. We should clarify this and carefully define what we mean to avoid any confusion. We’ll update the paper to reflect this.\n\n> I would probably avoid the term \"relational\" in the title and then link to it in the main text. This could be a solution. Without, in my opinion, you raise too many expectations, which are (unfortunately) not met. \n\nAnswer: We can’t really update the title as it references the prior works we directly build upon; relation networks and references herein. We know it’s a loaded term, but believe that we can make it clear in the introduction how we define it, to avoid confusion. Also, we don’t think “relational reasoning” inherently implies relational logic in the first order logic sense. Relational reasoning, in our opinion, is an abstract concept, with mentions in multiple fields, e.g. psychology, cognitive neuroscience, logic, AI and ML, etc. Probabilistic first order logic, as typically used in SRL is one implementation of that concept, this is another one, each with their pros and cons. We’ll make sure to make this apparent in the introduction.\n\n> However, when downscaling, I am wondering how much your work differs from e.g. neural message-passing (even applied to a network induced from relational description). \n\nAnswer: Are you referring to https://arxiv.org/abs/1704.01212 and references herein, i.e. graph neural networks? If so, then yes, our method is very similar. We recognize this in the paper. The main difference is that we train it for many steps using a recurrent update rule, and show how it can be used for reasoning. The previous applications have been fairly simple and mostly in actual graph problems, e.g. proteins, molecules, social graphs, etc., and not really focused on the reasoning aspect.\n\n> Sure, you can realise end-to-end learning, which is nice, but wouldn't that mean that you also have to select other baselines? There are so many \"relational\" NNs popping up in the literature. \n\nAnswer: End-to-end learning is critical for the usefulness of our algorithm. It’s a central part of our claim to usefulness. When added to another neural network module and trained end-to-end the reasoning module imposes on the upstream networks (since it is downstream of the gradients) to recognize and represent objects in the inputs. Do you have any particular baselines in mind? We’d be open to see if we could compare to them.\n\n> About Sudoko, I am with you, and I just wanted to express my concern that you should mention related work and discuss the pros and cons of your approach w.r.t. it, also in terms of an empirical evaluation. \n\nOk. I’m glad. We agree. We should discuss the referenced previous work and mention pros and cons. As described in the last answer we think they’re quite different and both useful. SRL for instance is superior if you wish to encode prior knowledge or extract the learned logical clauses, but requires you to define a domain up front, etc.\n\nGiven that we’ve cleared up the disagreement on what kind of reasoning our algorithm offers, clarified the value proposition, and that we can amend the paper to reflect this do you still feel that this paper should not be published at ICLR? If so what is the paper missing that would make you excited about it? \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512671927200,"tcdate":1512671927200,"number":4,"cdate":1512671927200,"id":"B1kp5bvZM","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Comment","forum":"SkJKHMW0Z","replyto":"HyLeDbvZf","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper838/AnonReviewer2"],"content":{"title":"Relational reasonings","comment":"Thank you for clarifying your point of view. This is very much appreciated. However, the term \"relational reasoning\" has a long tradition within AI and ML. Consequently, one has to be very clear about how your reasoning links to \"relational reasoning\". Just referring to a recent paper is not enough given that reasoning is at the center of AI.  I would probably avoid the term \"relational\" in the title and then link to it in the main text. This could be a solution. Without, in my opinion, you raise too many expectations, which are (unfortunately) not met. However, when downscaling, I am wondering how much your work differs from e.g. neural message-passing (even applied to a network induced from relational description). Sure, you can realise end-to-end learning, which is nice, but wouldn't that mean that you also have to select other baselines? There are so many \"relational\" NNs popping up in the literature. About Sudoko, I am with you, and I just wanted to express my concern that you should mention related work and discuss the pros and cons of your approach w.r.t. it, also in terms of an empirical evaluation. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512670958334,"tcdate":1512670958334,"number":3,"cdate":1512670958334,"id":"HyLeDbvZf","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Comment","forum":"SkJKHMW0Z","replyto":"r17v3MDxG","signatures":["ICLR.cc/2018/Conference/Paper838/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper838/Authors"],"content":{"title":"Disagree what constitute reasoning","comment":"Thank you for the review. \n\n> Overall the paper is well written and structured. It also addresses an important research question: combining relational reasoning and neural networks is currently receiving a lot of attention, in particular when generally considering the question of bridging sub-symbolic and symbolic methods.\n\nAnswer: Thank you. \n\n> Unfortunately, it is current form, the paper has two major downsides. \n\nAnswer: We get the objections put forward below. Our terminology clearly has made you expect a paper on statistical relational learning. We believe that we are solving problems that requires what we associate with relational reasoning although that it does not involve explicit “relational logical” in the first order logic sense. So in short we think it is acceptable to view reasoning in a broader sense as also done by Santoro et al. Detailed answers below.\n\n> First of all, the sudoku example does not illustrate “complex relational reasoning” as claimed in the title. The problem is encoded at a positional level where messages encoded as MLPs and LSTMs implement the constraints for Sudoko. Indeed, this allows to realise end-to-end learning but does not illustrate complex reasoning. This is also reflected in the considered QA task, which is essentially coded as a positional problem. \n\nAnswer: Clearly, there are ample opportunity for misunderstandings given how ambiguous notions such as “complex”, “relational” and “reasoning” are. For your definitions of these concepts you are right that our network does not perform it. Obviously with our definitions, we think it does, otherwise we wouldn’t have made those experiments or claims. So our definitions are almost certainly different.\n\nOur use of the term “relational reasoning” follows Santoro et al. and is to be honest quite vague. By “relational reasoning” we mean to represent the world as, and perform inference over, a set of objects and their relations. We did not intend to claim that we are performing “relational logic” in the strict first-order logic sense, e.g. with variables and quantifiers. Is this the source of the disagreement? If so, we’re happy to amend our paper to make this more clear. If not would you be kind enough to clarify your definitions of those concepts such that it is immediately obvious that our network does not perform “relational reasoning” under those definitions? Also, specifically could you clarify what you mean by “positional encoding/problem” and how that nullifies “reasoning”?\n\n> Consequently, the claim of the conclusions, namely that “we have proposed a general relational reasoning model” is not validated, unfortunately. Such a module that can be connected to any existing neural network would be great. However, for that one should show capabilities of relational logic. Some standard (noisy) reasoning capabilities such as modus ponens.\n\nAnswer: Sudoku can be formulated as a logical problem using e.g. propositional logic or first-order logic, and solved using e.g. SAT a solver. It's clearly a logical problem that requires reasoning to solve (efficiently). Are you really arguing that solving Sudoku does not require reasoning? Also, w.r.t modus ponens, the first step of our RRN eliminates digits which demonstrates (fuzzy) modus ponens in which “x implies (not y), x, thus (not y)”. It’s not exact logic, and you can’t extract the logical clauses and inspect them, as in many SRL systems, but that does not mean it’s not reasoning or useful. Just like human reasoning is inexact and impossible to introspect, but still reasoning and very useful.\n\n> This also leads me to the second downside. Unfortunately, the paper falls short on discussion related work. First of all, there is the large field of statistical relational learning, see\n\nAnswer: Thank you for these references. These interesting papers are trying to solve different problems than those we consider. We will discuss these and clarify this in the updated paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512670069676,"tcdate":1512670069676,"number":2,"cdate":1512670069676,"id":"r10uXbvbf","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Comment","forum":"SkJKHMW0Z","replyto":"Syc551clG","signatures":["ICLR.cc/2018/Conference/Paper838/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper838/Authors"],"content":{"title":"Answer","comment":"Thank you for the review. And thank you for the kind words regarding motivation and clarity.\n\n>Overall I think that by itself the algorithm suggested in the paper is not enough to be presented in ICLR, and on the other hand the authors didn't show it has a big impact (could do so by adding more tasks - as they suggest in the discussion). \nThis is why I think the paper is marginally below the acceptance threshold but could be convinced otherwise.\n\nAnswer: The presented algorithm is a plug-n-play neural network module for solving problems requiring complex reasoning. We clearly show how it can solve a difficult reasoning problem, Sudoku, which comparable state-of-the-art methods cannot. We also show state-of-the-art results on a very different task, BaBi, showing its general applicability. We’ve released the code (but can’t link it yet, due to double blind). Simply put, with this algorithm the community can approach a swathe of difficult reasoning problems, which they couldn’t before. As such we think it merits publication. If you’re not convinced, what additional task would make you excited about this algorithm?\n\n>Can the authors give experimental evidences for their claim: \"As such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a non-greedy...\" - \n\nAnswer: To clarify, we are not claiming that it does this, just that it has the capacity since the output and the hidden state is separated by an arbitrarily complex function. Since the function is arbitrarily complex it can learn to conditionally ignore parts of the hidden state, similar to how a LSTM can learn to selectively update its memory. We don’t have any experimental evidence whether it actually does this. \n\n> The recurrent relational networks is basically a complex learned message passing algorithm. As the authors themselves state there are several works from recent years which also tackle this (one missing reference is Deeply Learning the Messages in Message Passing Inference of Lin et al from NIPS 2016). It would been interesting to compare results to these algorithms.\n\nAnswer: Yes, that’s a fair point. Comparing to those methods would be interesting. We know the Lin et al.’s paper but somehow forgot to cite it. It will be added in the update. One important difference is that those works retain parts of loopy belief propagation and learn others whereas ours is completely learned.\n\n> For the Sudoku the proposed architecture of the network seems a bit too complex, for example why do a 16 embedding is needed for representing a digit between 0-9? Some other choices (batch size of 252) seem very specific.\n\nAnswer: We’re reporting all the gory details out of a (misplaced?) sense of scientific rigor, not because they are important hyper parameters. The 16 dimensional embedding was simply the first thing we tried. It’s not the result of extensive hyper-parameter tuning. We could have used a one-hot encoding, but then the next matrix multiply would effectively be the embedding. We think it’s a bit cleaner to have the embedding separately. Also using an embedding the x vector more closely resembles the expected input to the RRN from a perceptual front-end, e.g. a dense vector. The 252 batch size was simply so that the batch size was divisible by 6, because we trained on 6 GPUs."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512669579008,"tcdate":1512669579008,"number":1,"cdate":1512669579008,"id":"HymqZbDbz","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Comment","forum":"SkJKHMW0Z","replyto":"rJFvPvqgz","signatures":["ICLR.cc/2018/Conference/Paper838/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper838/Authors"],"content":{"title":"Explanation","comment":"Thank you for the review.\n\n> The proposed method should be better explained. What’s the precise definition of interface? It’s claimed that other constraint propagation-based methods can solve Sudoku problems easily, but don’t respect the interface. \n\nAnswer: A function respecting the interface must accept a graph (set of nodes and set of edges) where the nodes are described with real valued vectors and most importantly output a solution which is *differentiable* w.r.t. the parameters of the function.\nTraditional Sudoku solvers, e.g. constraint propagation and search are not differentiable, since they use non-differentiable operations e.g. hard memory lookups, writes, if statements, etc.\n\nWe need the function to respect this interface so it can be used with other neural network modules, and trained end-to-end. For an example see “A simple neural network module for relational reasoning” in which a Relation Network is added to a Convolutional Neural Network and trained end-to-end to reason about objects in images. Similarly with our BaBi example we combine a LSTM that reads each sentence with a Recurrent Relational Network to reason about the sentences.\n\n>It is hard to appreciate without a precise definition of interface. The proposed recurrent relational networks are only defined informally. A definition of the model as well as related algorithms should be defined more formally. \n\nAnswer: The first draft of the paper actually had a very formal introduction of the interface and algorithm, but we re-worked it to the current informal, example-driven style since we thought it was easier to understand and follow. We’ll add the rigorous definition to the appendix. We’ll let you know once we’ve updated the paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642519044,"tcdate":1511843680817,"number":3,"cdate":1511843680817,"id":"rJFvPvqgz","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Review","forum":"SkJKHMW0Z","replyto":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The proposed method should be better explained","rating":"5: Marginally below acceptance threshold","review":"This paper describes a method called relational network to add relational reasoning capacity to deep neural networks. The previous approach can only perform a single step of relational reasoning, and was evaluated on problems that require at most three steps. The current method address the scalability issue and can solve tasks with orders of magnitude more steps of reasoning. The proposed methods are evaluated on two problems, Sudoku and Babi, and achieved state-of-the-art results. \n\nThe proposed method should be better explained. What’s the precise definition of interface? It’s claimed that other constraint propagation-based methods can solve Sudoku problems easily, but don’t respect the interface. It is hard to appreciate without a precise definition of interface. The proposed recurrent relational networks are only defined informally. A definition of the model as well as related algorithms should be defined more formally. \n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642519080,"tcdate":1511811730296,"number":2,"cdate":1511811730296,"id":"Syc551clG","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Review","forum":"SkJKHMW0Z","replyto":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, clearly presented new structured prediction algorithm. Paper marginally below acceptance threshold.","rating":"5: Marginally below acceptance threshold","review":"This paper introduces recurrent relational networks: a deep neural network for structured prediction (or relational reasoning). The authors use it to achieve state-of-the-art performance on Soduku puzzles and the BaBi task (a text based QA dataset designed as a set of to toy prerequisite tasks for reasoning).\n\nOverall I think that by itself the algorithm suggested in the paper is not enough to be presented in ICLR, and on the other hand the authors didn't show it has a big impact (could do so by adding more tasks - as they suggest in the discussion). This is why I think the paper is marginally below the acceptance threshold but could be convinced otherwise.\n\nC an the authors give experimental evidences for their claim: \"As such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a non-greedy...\" - \n\nPros\n- The idea of the paper is clearly presented, the algorithm is easy to follow.\n- The motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks.\n\nCons\n- The recurrent relational networks is basically a complex learned message passing algorithm. As the authors themselves state there are several works from recent years which also tackle this (one missing reference is Deeply Learning the Messages in Message Passing Inference of Lin et al from NIPS 2016). It would been interesting to compare results to these algorithms.\n- For the Sudoku the proposed architecture of the network seems a bit to complex, for example why do a 16 embedding is needed for representing a digit between 0-9? Some other choices (batch size of 252) seem very specific.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642519122,"tcdate":1511627867042,"number":1,"cdate":1511627867042,"id":"r17v3MDxG","invitation":"ICLR.cc/2018/Conference/-/Paper838/Official_Review","forum":"SkJKHMW0Z","replyto":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference/Paper838/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Very nice direction but no complex relational reasoning demonstrated and missing related work","rating":"3: Clear rejection","review":"The paper introduced recurrent relational network (RRNs), an enhanced version of the\nexisting relational network, that can be added to any neural networks to add\nrelational reasoning capacity. RRNs are illustrated on sudoku puzzles and textual QA.\n\nOverall the paper is well written and structured. It also addresses an important research question: combining relational reasoning and neural networks is currently receiving a lot of attention, in particular when generally considering the question of bridging sub-symbolic and symbolic methods. Unfortunately, it is current form, the paper has two major downsides. First of all,  the sudoku example does not illustrate “complex relational reasoning” as claimed in the title. The problem is encoded at a positional level where \nmessages encoded as MLPs and LSTMs implement the constraints for sudoko. Indeed, \nthis allows to realise end-to-end learning but does not illustrate complex reasoning. \nThis is also reflected in the considered QA task, which is essentially coded as a positional problem. Consequently, the claim of the conclusions, namely that “we have\nproposed a general relational reasoning model” is not validated, unfortunately. Such\na module that can be connected to any existing neural network would be great. However, \nfor that one should show capabilities of relational logic. Some standard (noisy) \nreasoning capabilities such as modus ponens. This also leads me to the second downside. \nUnfortunately, the paper falls short on discussion related work. First of all, \nthere is the large field of statistical relational learning, see \n\nLuc De Raedt, Kristian Kersting, Sriraam Natarajan, David Poole:\nStatistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool Publishers 2016\n\nfor a recent overview. As it has the very same goals, while not using a neural architecture for implementation, it is very much related and has to be discussed. That\none can also use a neural implementation can be seen in \n\nIvan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:\nLogic Tensor Networks for Semantic Image Interpretation. IJCAI 2017: 1596-1602\n\nMatko Bosnjak, Tim Rocktäschel, Jason Naradowsky, Sebastian Riedel:\nProgramming with a Differentiable Forth Interpreter. ICML 2017: 547-556\n\nLuciano Serafini, Artur S. d'Avila Garcez:\nLearning and Reasoning with Logic Tensor Networks. AI*IA 2016: 334-348\n\nGustav Sourek, Vojtech Aschenbrenner, Filip Zelezný, Ondrej Kuzelka:\nLifted Relational Neural Networks. CoCo@NIPS 2015\n\nTim Rocktäschel, Sebastian Riedel:\nEnd-to-end Differentiable Proving. CoRR abs/1705.11040 (2017)\n\nWilliam W. Cohen, Fan Yang, Kathryn Mazaitis:\nTensorLog: Deep Learning Meets Probabilistic DBs. CoRR abs/1707.05390 (2017)\n\nto list just some approaches. There are also (deep) probabilistic programming \napproaches such as Edward that should be mentioned as CPS like problems (Sudoku) can\ndefinitely be implement there. Moreover, there is a number of papers that discuss \nembeddings of relational data and rules such as \n\nWilliam Yang Wang, William W. Cohen:\nLearning First-Order Logic Embeddings via Matrix Factorization. IJCAI 2016: 2132-2138\n\nThomas Demeester, Tim Rocktäschel, Sebastian Riedel:\nLifted Rule Injection for Relation Embeddings. EMNLP 2016: 1389-1399\n\nand even neural-symbolic approaches with a long publication history. Unfortunately, \nnon of these approaches has been cited, giving the wrong impression that this is \nthe first paper that tackles the long lasting question of merging sub-symbolic and symbolic reasoning. BTW, there have been also other deep networks for optimisation, see e.g. \n\nBrandon Amos, J. Zico Kolter:\nOptNet: Differentiable Optimization as a Layer in Neural Networks. \nICML 2017: 136-145\n\nthat have also considered Sudoku. To summarise, I like very much the direction of the paper but it seems to be too early to be published. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515167224621,"tcdate":1509135735227,"number":838,"cdate":1509739070602,"id":"SkJKHMW0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkJKHMW0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Recurrent Relational Networks for complex relational reasoning","abstract":"Humans possess an ability to abstractly reason about objects and their interactions, an ability not shared with state-of-the-art deep learning models. Relational networks, introduced by Santoro et al. (2017), add the capacity for relational reasoning to deep neural networks, but are limited in the complexity of the reasoning tasks they can address. We introduce recurrent relational networks which increase the suite of solvable tasks to those that require an order of magnitude more steps of relational reasoning. We use recurrent relational networks to solve Sudoku puzzles and achieve state-of-the-art results by solving 96.6% of the hardest Sudoku puzzles, where relational networks fail to solve any. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with state-of-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can augment any neural network model with the capacity to do many-step relational reasoning.","pdf":"/pdf/b8777b69c8edec1613717f62a244fb93c75c8a4a.pdf","TL;DR":"We introduce Recurrent Relational Networks, a powerful and general neural network module for relational reasoning, and use it to solve 96.6% of the hardest Sudokus and 19/20 BaBi tasks.","paperhash":"anonymous|recurrent_relational_networks_for_complex_relational_reasoning","_bibtex":"@article{\n  anonymous2018recurrent,\n  title={Recurrent Relational Networks for complex relational reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkJKHMW0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper838/Authors"],"keywords":["relational reasoning","graph neural networks"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}