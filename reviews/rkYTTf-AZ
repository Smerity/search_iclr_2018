{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222540897,"tcdate":1512082879980,"number":3,"cdate":1512082879980,"id":"r1uaaZRxf","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Official_Review","forum":"rkYTTf-AZ","replyto":"rkYTTf-AZ","signatures":["ICLR.cc/2018/Conference/Paper1019/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Remarkable paper -- limited modeling details","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper introduces an architecture for training a MT model without any parallel material, and tests it on benchmark datasets (WMT and captions) for two language pairs. Although the resulting performance is only about half that of a more traditional model, the fact that this is possible at all is remarkable.\n\nThe method relies on fairly standard components which will be familiar to most readers: a denoising auto-encoder and an adversarial discriminator. Not much detail is given on the actual models used, for which the authors mainly refer to prior work. This is disappointing: the article would be more self-contained by providing even a high-level description of the models, such as provided (much too late) for the discriminator architecture.\n\nMisc comments:\n\n\"domain\" seems to be used interchangeably with \"language\". This is unfortunate as \"domain\" has another, specific meaning in NLP in general and SMT in partiular. Is this intentional (if so what is the intention?) or is this just a carry-over from other work in cross-domain learning?\n\nSection 2.3: How do you sample permutations for the noise model, with the constraint on reordering range, in the general case of sentences of arbitrary lengths?\n\nSection 2.5: \"the previously introduced loss [...] mitigates this concern\" -- How? Is there a reference backing this?\n\nFigure 3: In the caption, what is meant by \"(t) = 1\"? Are these epochs only for the first iteration (from M(1) to M(2))?\n\nSection 4.1: Care is taken to avoid sampling corresponding src and tgt sentences. However, was the parallel corpus checked for duplicates or near duplicates? If not, \"aligned\" segments may still be present. (Although it is clear that this information is not used in the algorithm)\n\nThis yields a natural question: Although the two monolingual sets extracted from the parallel data are not aligned, they are still very close. It would be interesting to check how the method behaves on really comparable corpora where its advantage would be much clearer.\n\nSection 4.2 and Table 1: Is the supervised learning approach trained on the full parallel corpus? On a parallel corpus of similar size?\n\nSection 4.3: What are the quoted accuracies (84.48% and 77.29%) measured on?\n\nSection 4.5: Experimental results show a regular inprovement from iteration 1 to 2, and 2 to 3. Why not keep improving performance? Is the issue training time?\n\nReferences: (He, 2016a/b) are duplicates\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1512053227262,"tcdate":1512053227262,"number":2,"cdate":1512053227262,"id":"BJQe95agz","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Official_Comment","forum":"rkYTTf-AZ","replyto":"rJPiWDkJf","signatures":["ICLR.cc/2018/Conference/Paper1019/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1019/Authors"],"content":{"title":"related work","comment":"Thank you for your note. This paper indeed builds upon previous work, and we did our best to give credit to what we thought were the most relevant papers; in fact, we have 2 pages of citations already.\nAs per your suggestion, we will add some of the references you mention. However, please keep in mind that our paper focuses on machine translation, while the references you pointed us at are more pertinent to the work on learning a bilingual dictionary."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1512222540947,"tcdate":1511868375780,"number":2,"cdate":1511868375780,"id":"HJlJ_aqgf","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Official_Review","forum":"rkYTTf-AZ","replyto":"rkYTTf-AZ","signatures":["ICLR.cc/2018/Conference/Paper1019/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Need some clarifications","rating":"6: Marginally above acceptance threshold","review":"The authors present an approach for unsupervised MT which uses a weighted loss function containing 3 components: (i) self reconstruction (ii) cross reconstruction and (iii) adversarial loss. The results are interesting (but perhaps less interesting than what is hinted in the abstract). \n\n1) In the abstract the authors mention that they achieve a BLEU score of 32.8 but omit the fact that this is only on Multi30K dataset and not on the more standard WMT datasets. At first glance, most people from the field would assume that this is on the WMT dataset. I request the authors to explicitly mention this in the abstract itself (there is clearly space and I don't see why this should be omitted)\n\n2) In section 2.3, the authors talk about the Noise Model which is inspired by the standard Denoising Autoencoder setup.  While I understand the robustness argument in the case of AEs I am not convinced that the same applies to languages. Such random permutations can often completely alter the meaning of the sentence. The ablation test seems to suggest that this process helps. I read another paper which suggests that this noise does not help (which intuitively makes sense). I would like the authors to comment on this (of course, I am not asking you to compare with  the other paper but I am just saying that I have read contradicting observations - one which seems intuitive and the other does not).\n\n3) How were the 3 lambdas in Equation 3 selected ? What ranges did you consider. The three loss terms seem to have very different ranges. How did you account for that?\n\n4) Clarification: In section 2.5 what exactly do you mean by \"as long as the two monolingual corpora exhibit strong structure in feature space.\" How do you quantify this ?\n\n5) In section 4.1, can you please mention the exact number of sentences that you sampled from WMT'14. You mention that selected sentences from 15M random pairs but how many did you select ? The caption of one of the figure mentions that there were 10M sentences. Just want to confirm this.\n\n6) The improvements are much better on the Multi30k dataset. I guess this is because this dataset has smaller sentences with smaller vocabulary. Can you provide a table comparing the average number of sentences and vocabulary size of the two datasets (Multi30k and WMT).\n\n7) The ablation results are provided only for the Multi30k dataset. Can you provide similar results for the WMT dataset. Perhaps this would help in answering my query in point (2) above.\n\n8) Can you also check the performance of a PBSMT system trained on 100K parallel sentences? Although NMT outperforms PBSMT when the data size is large, PBSMT might still be better suited for low resource settings.\n\n9) There are some missing citations (already pointed by others in the forum) . Please add those.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1512222542017,"tcdate":1511803758669,"number":1,"cdate":1511803758669,"id":"B1POjpKef","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Official_Review","forum":"rkYTTf-AZ","replyto":"rkYTTf-AZ","signatures":["ICLR.cc/2018/Conference/Paper1019/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A thorough exploration of techniques for unsupervised translation, a very strong start for this problem","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper describes an approach to train a neural machine translation system without parallel data. Starting from a word-to-word translation lexicon, which was also learned with unsupervised methods, this approach combines a denoising auto-encoder objective with a back-translation objective, both in two translation directions, with an adversarial objective that attempts to fool a discriminator that detects the source language of an encoded sentence. These five objectives together are sufficient to achieve impressive English <-> German and Engish <-> French results in Multi30k, a bilingual image caption scenario with short simple sentences, and to achieve a strong start for a standard WMT scenario.\n\nThis is very nice work, and I have very little to criticize. The approach is both technically interesting, and thorough in that it explores and combines a host of ideas that could work in this space (initial bilingual embeddings, back translation, auto-encoding, and adversarial techniques). And it is genuinely impressive to see all these pieces come together into something that translates substantially better than a word-to-word baseline. But the aspect I like most about this paper is the experimental analysis. Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.\n\nIn terms of specific criticisms:\n\nIn Equations (2), consider replacing C(y) with C(M(x)), or use compose notation, in order to make x-hat's relationship to x clear and self-contained within the equation.\n\nI am glad you take the time to give your model selection criterion it's own section in 3.2, as it does seem to be an important part of this puzzle. However, it would be nice to provide actual correlation statistics rather than an anecdotal illustration of correlation.\n\nIn the first paragraph of Section 4.5, I disagree with the sentence, \"Similar observations can be made for the other language pairs we considered.\" In fact, I would go so far as to say that the English to French scenario described in that paragraph is a notable outlier, in that it is the other language pair where you beat the oracle re-ordering baseline in both Multi30k and WMT.\n\nWhen citing Shen et al., 2017, consider also mentioning the following:\n\nControllable Invariance through Adversarial Feature Learning; Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig; NIPS 2017; https://arxiv.org/abs/1705.11122","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1510157457943,"tcdate":1510157457943,"number":1,"cdate":1510157457943,"id":"Hy59njgkG","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Official_Comment","forum":"rkYTTf-AZ","replyto":"SkrSo6EC-","signatures":["ICLR.cc/2018/Conference/Paper1019/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1019/Authors"],"content":{"title":"more references in related work section","comment":"Thank you for pointing out these references. We will definitely revise the paper accordingly. In particular, our model is reminiscent of CycleGAN, as well as other methods successfully applied in vision (such as the Coupled Generative Adversarial Networks of Liu et al.). The major conceptual difference is that we cannot easily chain as they do in these other works, because we deal with a discrete sequence of symbols as opposed to continuous vectors. Therefore, we resort to using the model at the previous iteration to produce translations in the other language, but we do not back-prop through this. The iterative nature of our approach together with the weight sharing between our encoder/decoders are the most important differences."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1510072735219,"tcdate":1510072735219,"number":2,"cdate":1510072735219,"id":"rJPiWDkJf","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Public_Comment","forum":"rkYTTf-AZ","replyto":"rkYTTf-AZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Great work/related work","comment":"This looks like great work, and I think merits a clear accept.\n\nThat said, I am also concerned about the lack of discussion of prior work. Almost all of the contributions which enabled the work are relegated to the companion article (\"Word translation without parallel data\"). While I understand the authors want to focus on their own contribution, the introduction/related work to a major achievement like this should convey how the community as a whole reached this point, especially since much of the progress was made outside of the big labs.\n\nFrom following the citations, it seems like some key steps towards unsupervised translation were:\n\n1. Monolingual high quality word vectors (https://arxiv.org/abs/1301.3781)\n2. The linear transform for word translation from small dictionaries (https://arxiv.org/abs/1309.4168)\n3. The orthogonal transform/SVD to improve resilience to low quality dictionaries (https://arxiv.org/abs/1702.03859, http://www.anthology.aclweb.org/D/D16/D16-1250.pdf)\n4. The use of a GAN, regularized towards orthogonal transform, to obtain unsupervised bilingual word vectors (http://www.aclweb.org/anthology/P17-1179)\n5. The iterative SVD procedure to enhance the GAN solution to supervised accuracy (http://www.aclweb.org/anthology/P17-1042)\n6. The realization that aligned mean word vector provides a surprisingly good bilingual sentence space (https://arxiv.org/abs/1702.03859)\n7. Finally the (significant) contribution of this work is to iterate this initial unsupervised shared sentence space towards a word order dependent translation model. A similar paper was submitted to ICLR simultaneously, \"Unsupervised Neural Machine Translation\".\n\nMy apologies to other important prior work I have missed!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1509379648587,"tcdate":1509378877187,"number":1,"cdate":1509378877187,"id":"SkrSo6EC-","invitation":"ICLR.cc/2018/Conference/-/Paper1019/Public_Comment","forum":"rkYTTf-AZ","replyto":"rkYTTf-AZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"More references in related work section","comment":"The related work section can be improved by providing more references to earlier research on learning unsupervised alignment in different domains.\n\nFor example:\nhttps://arxiv.org/abs/1611.02200 - Unsupervised cross domain image generation\nhttps://arxiv.org/abs/1612.05424 - Unsupervised Pixelâ€“Level Domain Adaptation with Generative Adversarial Networks\nhttps://arxiv.org/abs/1606.03657 - InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\nhttps://arxiv.org/abs/1703.10593 - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]}},{"tddate":null,"ddate":null,"tmdate":1510092382031,"tcdate":1509137878574,"number":1019,"cdate":1510092360551,"id":"rkYTTf-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkYTTf-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unsupervised Machine Translation Using Monolingual Corpora Only","abstract":"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores up to 32.8, without using even a single parallel sentence at training time.","pdf":"/pdf/1d802bb8648d72779a723e7b582de5cb6b5331df.pdf","TL;DR":"We propose a new unsupervised machine translation model that can learn without using parallel corpora; experimental results show impressive performance on multiple corpora and pairs of languages.","paperhash":"anonymous|unsupervised_machine_translation_using_monolingual_corpora_only","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Machine Translation Using Monolingual Corpora Only},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkYTTf-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1019/Authors"],"keywords":["unsupervised","machine translation","adversarial"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}