{"notes":[{"tddate":null,"ddate":null,"tmdate":1516400570442,"tcdate":1516400570442,"number":30,"cdate":1516400570442,"id":"Skza1ggrG","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"ryq2cm9xG","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Your issues have been addressed...","comment":"We revised our paper considerably over a month ago. We have since had a long back and forth conversation with AnonReviewer3 discussing the privacy approximation, which seems to have addressed their misgivings. \n\nWe would much appreciate it if you could update your reviews and/or score. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516400439394,"tcdate":1516400439394,"number":29,"cdate":1516400439394,"id":"Bk1HygxSM","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"r1dNqr9xf","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"The issues you've raised have been addressed.","comment":"Dear AnonReviewer1,\n\nWe have addressed all your concerns.  We've also had a lengthy conversation with AnonReviewer3 around the privacy approximation. That reviewer appears to be now convinced of the reasonableness of our approximation. \n\nWe would very much appreciate if you could update your reviews/scores.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516400234090,"tcdate":1516400234090,"number":28,"cdate":1516400234090,"id":"Hyfu0ylHM","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"H1c9BkxHz","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Further response","comment":"Our PAC-Bayes bound relies on a private prior, which has  some privacy level, epsilon. The prior is determined a vector of weights and so the bound depends on the privacy of that weight vector alone. \n\nSGLD produces the weight vector w_N by way of simulating a Markov chain w_1,w_2,...,w_N.  The privacy of w_N is  determined *entirely* by its distribution. It is irrelevant how the vector is produced. Its distribution is all that matters. This is basic differential privacy.\n\nThere is also no approximation up until this point. We then note that this distribution is known to converge weakly to the Gibbs distribution=exponential release.  And so we approximate the privacy by that of the exponential release.\n\nSo yes, we get to apply the bound for \"algorithm (a)\". \n\nOur \"We want to address...\" was simply saying that scientific progress sometimes relies on approximations. We think it's a reasonable approximation when one is far away from pathological distributions. \n\n\nRegarding clarity, we truly believe we can address these issues in another minor revision, but since the ACs will have to render their decision very soon, you would likely have to take it on faith that we could execute on this based on what we've explained above.  The process of discussing it with you has provided a road map.  \n\nWe'd like to point out a couple issues:\n\n1. Some of the confusion stems from hold overs from Version 1 of the paper. Future readers will, fortunately, not have to suffer through Version 1.  \n\n2. We were under the misconception that the privacy analysis of a Markov chain + post processing the final element was straightforward, but we now see that it is worth explaining in greater detail, even if the argument is a standard one in the privacy literature.\n\n\nFinally, to put it in simple terms, if you don't change your score, our paper surely gets rejected. The other reviewers appear not to have engaged at all with our responses, for whatever reason. They had similar confusion about the privacy approximation, which we have now addressed and can bake into the paper by adding further clarity. We appreciate you spending the time doing this back and forth. We've never experienced anything like this in reviewing.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516394484456,"tcdate":1516394473840,"number":26,"cdate":1516394473840,"id":"SkflOAJrG","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"Hy4CAakSz","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Privacy analysis of a Markov chain when result depends on last element","comment":"I believe that our post above (\"Addressing the ONE sample issue.\") exactly hits on the reason why we've not been understanding each other.\n\nSo, yes, absolutely.  If you get N samples and your mixing time is k, then the privacy of the ENTIRE trajectory is certainly no better than N/k releases.\n\nHowever, if you get N samples w_1,....,w_N, and then your algorithm returns g(w_N) as its output, where g(.) doesn't use data, then the privacy of our algorithm is *no worse than* the privacy of w_N.  You don't pay for w_1,....,w_(N-1) because the privacy of g(w_N) depends only on its distribution and its distribution is independent of w_1,...,w_N-1 *conditioned* on w_N.\n\nHope this clears things up."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516394758126,"tcdate":1516393577010,"number":24,"cdate":1516393577010,"id":"HkWuERkHz","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"H1yaFakSf","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Addressing the ONE sample issue.","comment":"We think we understand the confusion about \"one\" sample.  We'll address that first, and then address the inner loop issue.\n\nUPDATE: Our short response (\"Privacy analysis of a Markov chain when result depends on last element\") below to your follow up may be a good place to start as it quickly lays out the technical issue with privacy analysis.\n\nFocusing on the outer loop and ignoring any potential bias in the inner loop: SGLD produces a Markov chain w_1, w_2, ... where w_j is the vector of parameters after j iterations of SGLD.  \n\nEarlier papers on SGLD show that, for large N, the distribution of w_N is close to the Gibbs distribution=exponential release.  The privacy of w_N depends *only* on its distribution and so this is the basis of our approximation. In other words, if my algorithm uses data to  compute w_1,...,w_N, and then only uses w_N for subsequent calculations, then the privacy of my algorithm is *no worse than* the privacy of w_N. Differential privacy is powerful.\n\nAt time step N, we use *only* w_N and no other w_j in our bound calculation.  Therefore, the privacy we must pay for is the privacy of w_N alone. We don't have to pay for w_1,w_2,....,w_N-1.  We say we use one sample because we use only the value of w_N to compute our bound at time step N.\n\nExisting analyses of SGLD analyze the privacy of the entire trajectory (w_1,w_2,...,w_N). This obviously leaks a lot of information! Indeed, say the chain mixes every k steps, then the privacy of the whole trajectory it is at least as bad as N/k samples from the exponential mechanism. But existing analyses do not handle the fact that SGLD is asymptotically ergodic. They instead do a very coarse analysis of each step. Because they do this step by step analysis they MIGHT AS WELL release the whole trajectory because there analysis is one for the whole trajectory.\n\nWe mentioned the experimental justification in the comment below (\"Why our approximation is reasonable.\")  Just to reiterate, look at our random label experiments.  (Figure 1, bottom right.) The true error here is 0.5 and so generalization error is determined by gap between training error and 0.5.  \n\nIf running SGLD for many many steps caused leakage that allowed us to overfit, we would see the empirical generalization error (empirical risk - test error) increasing. However, we see in our experiments that the generalization error is not increasing over time. It's steady. This plot covers 16,000 passes through the data! (We actually ran this experiment for 100,000 passes through the data with no difference.)\n\nFinally, regarding the inner loop. Note that there's no inherent issue with the inner loop gradients leaking information. To see this, recall that exact gradients have no (!) privacy, but SGLD uses exact gradients. SGLD adds noise to the exact gradients and over time the magnitude of this noise dominates and FK dynamics take over and you converge to the exponential release.  Our approximate gradients (inner SGLD loop) are not unbiased... that's the issue! (They have way more privacy than exact gradients.) It is possible that the bias in the gradients leads us to converge to a different Gibbs distribution, with different privacy. But, as we've argued, for the settings of Gaussian variance that we have studied, this inner sampling step is over a tiny (!) region in weight space and so we believe this sampling step is pretty straightforward. E.g., we saw absolutely no change when we doubled the number of inner iterations. So we don't actually think that there's much bias.\n\n\nWe want to address one more issue about the motivation behind this approximation.  In an ideal world, we would know what the privacy of Entropy-SGLD was, and we would plug that into our new differentially private PAC-Bayes bound.  Current privacy analyses of SGLD are borderline useless because they don't deal with mixing.  So we've made some strong approximations to see what type of bounds we might have gotten optimistically.  And the paper makes it clear that things are optimistic.  But we learn quite a bit from this exercise.  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516388754724,"tcdate":1516388754724,"number":19,"cdate":1516388754724,"id":"rJic-pJBf","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"SyGJQhkHM","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Misunderstanding is due to issues with version 1 versus version 2.","comment":"OK. So this is our third response but now that we've reread your comment, we're certain that the issue you are highlighting is not a problem, and that the confusion stems from clarity issues in version 1 of the paper.\n\nBasically, in version 1, it sounded like we were analyzing a \"Perfect SGD\" algorithm, but actually we meant to communicate that we were analyzing a Perfect SGLD algorithm.\n\nEntropy-SGD is\ninner: SGLD\nouter: SGD\n\nPerfectSGD would be\ninner: exact gradient\nouter: SGD\n\nWe absolutely agree that PerfectSGD would have terrible privacy!\n\nWe propose\n\nEntropy-SGLD \ninner: SGLD\nouter: SGLD\n\nand analyze it ignoring any privacy issues with the inner loop, which we think is reasonable, and assuming the outer loop is run for a long time.\n\nPerfect-SGLD\ninner: essentially exact gradient\nouter: SGLD\n\nVersion 2 of the paper makes this crystal clear now. We apologize for Version 1 being so unclear. \n\nThe inner loop of Entropy-SGLD is sampling from the empirical risk surface TIMES an extremely-low-variance Gaussian.  This sampling problem is VERY easy and gets even easier as you get to regions with low empirical risk.  So ignoring the inner loop approximation is not really a concern for us.\n\nAgain, we are comforted by the fact that we NEVER see the empirical generalization error get worse over very long runs.  We would expect it to if we were leaking information."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516388211450,"tcdate":1516388211450,"number":18,"cdate":1516388211450,"id":"H1id1T1Hf","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"SyGJQhkHM","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Why our approximation is reasonable.","comment":"We're rushing this out, because the ACs have to make up their minds today.  We'll post this comment and then go back and re-read your last two comments. Sorry for the rush, but your comment only appeared to us on Jan 19.\n\nWe want to start by distinguishing Entropy-SGD, Entropy-SGLD in terms of their inner/outer loops.  We believe these confusions are due to the first version of the paper where these issues were muddled. It should now be crystal clear in version 2.\n\nIn summary:\n\nEntropy-SGD (original Chaudhari et al.)\ninner loop: SGLD\nouter loop: SGD\n\nEntropy-SGLD\ninner loop: SGLD\nouter loop: SGLD\n\nOur privacy analysis is of the OUTER loop of Entropy-SGLD:\n\nWhen we talk about getting one sample, we mean running the OUTER loop of Entropy-SGLD many many times, and then only using the last sample.  Your comment focuses on the INNER loop. The inner loop wouldn't be necessary if we could calculate the gradient of the local entropy exactly. However, this inner loop works very well because the gaussian prior is so sharp/focused. Even if we had a perfect inner SGD, we would still need SGLD on the outer loop to get generalization bounds. It is the outer loop that's important. Yes, there's a little bit of bias on the gradient calculation, but we state we'll ignore this.  In practice, changing the number of inner loop steps has no affect unless you choose  a really small number. Subsequent work by Chaudhari et al agrees with this.\n\nSo the question is: is it reasonable to analyze many iterations of the outer-loop of Entropy-SGLD as having the same privacy of its limiting stationary distribution?\n\nFirst of all, experiments bear this out (or at least don't contradict this).  Look at any of the random-label experiments. The generalization error does not get worse despite running  getting worse and worse as information about the labels slips through and allows the network to overfit. But this doesn't happen.\n\nNow, to be clear, to produce our pretty figures, we are looking at the parameter at every stage. But this doesn't effect the bound we calculate at every point in time. (It would present problems if you wanted to use the figures to do early stopping, but I suspect one could do another analysis on the optional stopping time to do much better than a per-iteration analysis.)\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1516386125450,"tcdate":1516384985879,"number":15,"cdate":1516384985879,"id":"SyGJQhkHM","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"rJQ0AokSG","signatures":["ICLR.cc/2018/Conference/Paper32/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/AnonReviewer3"],"content":{"title":"Final comments","comment":"I am not saying that the ideas in this paper are not good. I very much like these ideas. But it seems that the authors guarantee that every gradient step on the local entropy objective is privacy preserving and hence (by the results in this paper) imply valid generalization bounds (using PAC Bayes theory) on the network obtained after one such step. \n\nWhat is not clear to me is what happens if I run many gradient steps on the local entropy objective - intuitively privacy of multiple releases from the exponential mechanisms would decay with iterations-  this is not clear to me at all at this point. I think the next submission after clearly clarifying these issues would certainly be a nice one.\n\nBecause this aspect is unclear, I am uncertain about raising the scores.\n"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1515770141501,"tcdate":1515770141501,"number":10,"cdate":1515770141501,"id":"rkS7ZILEz","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"S13L525Xf","signatures":["ICLR.cc/2018/Conference/Paper32/Area_Chair"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Area_Chair"],"content":{"title":"Discussion","comment":"Yes I have encouraged them to discuss and see if their impression of your paper has improved after your response."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1513833234147,"tcdate":1513833234147,"number":4,"cdate":1513833234147,"id":"Hk9z76OfG","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Summary of the major changes we made addressing reviewer feedback","comment":"This comment summarizes the major changes we made to the document while addressing the reviewers' comments. We have also crafted responses to each individual reviewer.\n\nWe took all of the reviewers’ comments seriously and made extensive edits to the article. Some of the major changes include:\n\n1. stating our main results as theorems and writing up the analysis in the form of a proof. This should make our contributions clearer to readers. These results include: i) the connection between Entropy-SGD optimization and PAC-Bayes prior optimization, ii) our differentially private PAC-Bayes bound, iii) our privacy analysis for the data-dependent prior.\n\n2. giving a single unified description of the Entropy-SGD and Entropy-SGLD algorithms, so the difference is obvious.\n\n3. rewriting our differential privacy analysis, to make it easier for the reader to understand our assumptions/approximations.\n\n3. adding experiments comparing SGLD and Entropy-SGD at different levels of thermal noise, which highlights the role of thermal noise in generalization and the difference between empirical risk minimization and local entropy maximization.\n\n4. discussing the relationship between our differentially private PAC-Bayes priors and data-distribution-dependent priors. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1513833160961,"tcdate":1513833160961,"number":3,"cdate":1513833160961,"id":"BkbCG6dzM","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"ryq2cm9xG","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Response","comment":"Thank you for your feedback. \n\nYou raise two issues regarding novelty and clarity/presentation. We will address these in turns.\n\nRegarding novelty. We have recently presented this work to experts at a PAC-Bayes workshop. They expressed great interest in our results using differential privacy, and we have fielded a number of requests for preprints. Our private data-dependent priors can be viewed as a new type of data-distribution-dependent prior. The classical technique for dealing with data-distribution-dependent priors is due to Catoni and Lever et al., but these techniques have only been applied to Gibbs distributions, whereas our approaches offers much more flexibility. We now explain this connection more carefully in the related work section. We believe that our approach opens up the avenue to more advanced uses of stable, data-dependent priors. \n\nBeyond connecting PAC-Bayes theory and privacy, our work makes a number of other contributions: \n- We reveal the importance of the thermal noise to the generalization performance of Entropy-SGD, and tie this parameter to stability/privacy.  We also make a detailed study of the role of thermal noise in overfitting on MNIST, not only for Entropy-SGD, but also for SGLD and Entropy-SGLD.\n- We identify the deep connection between Entropy-SGD and PAC-Bayes bounds, which guides us to new ways to improve the generalization performance of Entropy-SGD. Our modifications lead to new learning algorithms that do not overfit, yet still have very good risk.\n- We obtain risk/generalization bounds for neural networks that, up to our privacy approximation, are much tighter than any bounds previously published for MNIST.\n\nRegarding clarity/presentation. We have rewritten several sections in the paper using your feedback as a guideline. Our connection between Entropy-SGD and PAC-Bayes priors is now stated as a theorem and our argument is now structured as a proof. Our derivations concerning privacy are now also organized into a theorem in Section 5. Indeed, Section 5 has been reworked from the ground up to have much clearer logical structure. We have reproduced all figures with larger fonts and careful attention to readability. The organization of Figure 1 now makes it immediately clear which figures are on true or random labels, and which algorithms are being compared.\f"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1513833097625,"tcdate":1513833097625,"number":2,"cdate":1513833097625,"id":"SJMcMTOMz","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"r1dNqr9xf","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Response to your questions","comment":"Thank you for your questions. We'll address both in turns, paraphrasing each as we understood it. We close with two remarks about uniform stability.\n\n1. In our paper we repeat the statement by Chaudhari et al. that their analysis has some violated assumptions about curvature. How does our result sidestep this issue with the curvature.\n\nOur PAC-Bayes bound is tight provided that the KL(Q||P) term is small. In our case, P is a Gaussian whose mean is differentially private. Q is then the corresponding Gibbs posterior. Whether the empirical risk surface near the mean of P is exactly flat or nearly flat does not matter. In both cases, Q and P will be nearly identical and KL(Q||P) will be very small. This is what we find empirically.\n\n2. What are the \"unrealistic assumptions\" you refer to?\n\nThere is one approximation made in our paper: our \"privacy approximation\".  We now discuss this approximation in the Section 1, Introduction; Section 5, Data-dependent PAC-Bayes priors via differential privacy; Section 6, Numerical results on MNIST, and Section 7, Discussion. \n\nOur approximation is as follows (Section 1 and especially 5 give these details): The gold standard way to minimize a bounded function f is the exponential mechanism, namely generating a sample from the distribution with density exp(- c*f) where c > 0 is a constant. The bound on f and c determine the privacy. However, if f is high-dimensional and nonconvex, then exact sampling can be intractable. SGLD is a way to get an approximate sample, and it is know that the longer you run SGLD, the better the approximation. We approximate the exponential mechanism (i.e., an exact sample), with an approximate sample from SGLD, and calculate the privacy as if we got an exact sample. Differential privacy is a worst case framework and so we might not notice this approximation on \"nice\" data. An adversary might be able to exploit our approximation if they could carefully craft the data distribution. In the text, we point out that our bounds may be optimistic as a result, but they behave in a way that the theory predicts, and so we can still learn something from studying them.\n\nFinally, we'll make two remarks about the uniform stability of SGD and Entropy-SGD.\n\nFirst, the stability analysis in Chaudhari et al.'s Entropy-SGD paper does not account for the thermal noise required to get reasonable empirical results. Once you add in the amount of thermal noise they were advocating in their experiments, their results flips: Entropy-SGD is less stable. Our results actually point to using less thermal noise in order to get good generalization at the cost of excess empirical risk.\n\nSecond, in the now well-known \"Rethinking generalization\" paper by Zhang, et al 2017, the authors, which include Hardt and Recht themselves, say that the uniform stability result cannot explain the difference between the performance on random and true labels, because stability does not care about the labels. The uniform stability bounds degrade to vacuous bouds after several passes through the data. The same issues are relevant to the stability analyses of Entropy-SGD.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1513833050713,"tcdate":1513833050713,"number":1,"cdate":1513833050713,"id":"rJXPfp_GM","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Comment","forum":"ry9tUX_6-","replyto":"Hy0bdarZG","signatures":["ICLR.cc/2018/Conference/Paper32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper32/Authors"],"content":{"title":"Response to your comments","comment":"Thank you for the comments and pinpointing several typos. \n\nWe have made an extensive rewrite to address the weaknesses you identified. We will respond to each of them, but in a different order.\n\n(e) and (c) Regarding clarity/presentation and typos.\n\nWe have rewritten and rearranged much of the paper to improve the logical structure. We have also addressed all the typos. \n\n- Entropy-SGD and Entropy-SGLD are now presented in the main body of the paper as a single combined algorithm, with the one difference highlighted.\n- Our analysis of the idealized exponential mechanism (what you refer to as gibbs sampling) is now presented as Theorem 5.5, and its relationship to Entropy-SGLD is clearly laid out in the same section. We also discuss our privacy approximation here in depth.\n- Our result relating Entropy-SGD and PAC-Bayes bound optimization are now presented as Theorem 4.1.  \n- Our argument establishing the differentially-private PAC-Bayes bound is now structured as a proof.\n\n\n(d) and (a). Regarding strong gibb sampling (i.e., the exponential mechanism and our \"privacy approximation\" regarding SGLD). \n\nWe have updated this part of the paper considerably, and the logical structure is much improved. The material is now entirely in the main body. We highlight some aspects of the argument here:\n\n- Note that we only use a SINGLE sample produced by SGLD (namely the last one). This last sample is what is used as the prior mean to produce the resulting Gibbs posterior classifier. When we plot the learning curves, the bounds are the bounds that would hold if we stopped SGLD at that iteration. \n- The fact we only use one sample is the reason why we think it is reasonable to approximate the privacy of SGLD by that of its limiting invariant distribution (i.e., the exponential mechanism). Since we are far from the worst case with MNIST, we expect not to see much difference. There is likely a worst-case distribution where our bounds would end up being badly violated.\n- Typical analyses of SGLD don't try to deal with the fact that it begins to mix. So they make a step by step analysis, where information is leaked at every stage. Because they do this, there is no reason not to release the whole trajectory. However, in an analysis that took advantage of mixing (very hard!), they would NOT release the whole trajectory (or at least, they certainly wouldn't release the early parts).\n- In our experiments where we run SGLD for 1000's of epochs (!) on random noise, we see zero overfitting when we set the thermal noise to the settings suggested by theory.\n\n\n(b) Regarding the goal of the experiments.\n\nWe have significantly revised the section describing our numerical experiments. We feel that the motivation for our experiments in much clearer now. Here are some particular points we wanted to highlight:\n\nPAC-Bayes bounds are data dependent and so it is an empirical question whether they are useful or not, and how they compare to previously established bounds. On top of this, we are using private data-dependent priors and a differentially private PAC-Bayes theorem and so it is an empirical question whether a sufficiently private optimization finds a decent prior. (Generalization bounds require a very high degree of privacy!) One way to think about the quantity tau/m (which determines the privacy along with our loss bound) is that, when tau/m < 1, it specifies what fraction of your data you \"throw away\" while doing your sampling in order to not learn \"too much\" about your data itself, rather than the distribution underlying it. We have to \"throw away\" quite a bit of data while privately optimizing our PAC-bayes prior. And so it is an empirical question whether we can find anything useful still. Indeed, we do. We can also study our privacy approximation regarding SGLD empirically. If the privacy/stability of SGLD degraded over time, we might have seen overfitting occur on very long runs. In fact, we don't see this, even after 1000's of epochs! The private versions of the algorithms we tested reach some level of performance and stay there. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642431489,"tcdate":1512589317958,"number":3,"cdate":1512589317958,"id":"Hy0bdarZG","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Review","forum":"ry9tUX_6-","replyto":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference/Paper32/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Reasonably good idea (but with lots of strong assumptions) connecting generalization of entropy SGD and PAC-Bayes risk bound. ","rating":"6: Marginally above acceptance threshold","review":"Brief summary:\n    Assume any neural net model with weights w. Assume a prior P on the weights. PAC-Bayes risk bound show that for ALL other distributions Q on the weights, the the sample risk (w.r.t to the samples in the data set) and expected risk (w.r.t distribution generating samples) of the random classifier chosen according to Q, averaged over Q, are close by a fudge factor that is KL divergence of P and Q scaled by m^{-1} + some constant.\n\nNow, the authors first show that optimizing the objective of the Entropy SGD algorithm is equivalent to optimizing the empiricial risk term + fudge term over all data dependent priors P and the best Q for that prior. However, PAC-Bayes bound  holds only when P is NOT dependent on the data. So the authors invoke results from differential privacy to show that as long as the prior choosing mechanism in the optimization algorithm is differentially private with respect to data, differentially private priors can be substituted for valid PAC-Bayes bounds rectifying the issue. They show that when entrop SGD is implemented with pure gibbs sampling steps (as in Algorithm 3), the bounds hold.\n\nWeakness that remains is that the gibbs sampling step in Entropy SGD (as in algo 3 in the appendix) is actually approximated by samples from SGLD that converges to this gibbs distribution when run for infinite hops. The authors leave this hole unsolved. But under the very strong sampling assumption, the bound holds. The authors do some experiments with MNIST to demonstrate that their bounds are not trivial. \n\nStrengths:\n  Simple connections between PAC-Bayes bound and entropy SGD objective is the first novelty. Invoking results from differential privacy for fixing the issue of validity of PAC-Bayes bound is the second novelty. Although technically the paper is not very deep, leveraging existing results (with strong assumptions) to show generalization properties of entropy-SGD is good.\n\nWeakness:\n  a) Obvious issue : that analysis assumes the strong gibbs sampling step.\n  b) Experimental results are ok. I see that the bounds computed are non-vacuous. - but can the authors clarify what exactly they seek to justify ? \n c) Typos: \n   Page 4 footnote \"the local entropy should not be <with>..\" - with is missing.\n   Eq 14 typo - r(h) instead of e(h) \n   Definition A.2 in appendix - must have S and S' in the inequality -both seem S.\n\nd) Most important clarification: The way Thm 5.1, 5.2 and the exact gibbs sampling step connect with each other to produce Thm 6.1 is in Thm B.1. How do multiple calls on the same data sample do not degrade the loss ? Explanation is needed. Because the whole process of optimization in TRAIN with may steps is the final 'data dependent prior choosing mechanism' that has to be shown to be differentially private. Can the authors argue why the number of iterations of this does not matter at all ?? If I get run this long enough, and if I get several w's in the process (like step 8 repeated many times in algorithm 3) I should have more leakage about the data sample S intuitively right ?\n\ne) The paper is unclear in many places. Intro could be better written to highlight the connection at the expression level of PAC-Bayes bound and entropy SGD objective and the subsequent fix using differentially private prior choosing mechanism to make the connection provably correct. Why are all the algorithms in the appendix on which the theorems are claimed in the paper ??\n\nFinal decision: I waver between 6 and 7 actually. However I am willing to upgrade to 7 if the authors can provide sound arguments to my above concerns.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642431624,"tcdate":1511836208162,"number":2,"cdate":1511836208162,"id":"r1dNqr9xf","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Review","forum":"ry9tUX_6-","replyto":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference/Paper32/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Weak Accept","rating":"6: Marginally above acceptance threshold","review":"1) I would like to ask for the clarification regarding the generalization guarantees. The original Entropy-SGD paper shows improved generalization over SGD using uniform stability, however the analysis of the authors rely on an unrealistic assumption regarding the eigenvalues of the Hessian (they are assumed to be away from zero, which is not true at least at local minima of interest). What is the enabling technique in this submission that avoids taking this assumption? (to clarify: the analysis is all-together different in both papers, however this aspect of the analysis is not fully clear to me).\n2) It is unclear to me what are the unrealistic assumptions made in the paper. Please, list them all in one place in the paper and discuss in details.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642431662,"tcdate":1511828145939,"number":1,"cdate":1511828145939,"id":"ryq2cm9xG","invitation":"ICLR.cc/2018/Conference/-/Paper32/Official_Review","forum":"ry9tUX_6-","replyto":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference/Paper32/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper connects Entropy-SGD with PAC-Bayes learning. It shows that maximizing the local entropy during the execution of Entropy-SGD essentially minimize a PAC-Bayes bound on the risk of the Gibbs posterior. Despite this connection, Entropy-SGD could lead to dependence between prior and data and thus violate the requirement of PAC-Bayes theorem. The paper then proposes to use a differentially private prior to get a valid PAC-Bayes bound with SGLD. Experiments on MNIST shows such algorithm does generalize better.\n\nLinking Entropy-SGD to PAC-Bayes learning and making use of differential privacy to improve generalization is quite interesting. However, I'm not sure if the ideas and techniques used to solve the problem are novel enough.\nIt would be better if the presentation of the paper is improved. The result in Section 4 can be presented in a theorem, and any related analysis can be put into the proof. Section 5 about previous work on differentially private posterior sampling and stability could follow other preliminaries in Section 2. The figures are a bit hard to read. Adding sub-captions and re-scaling y-axis might help.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1513832951436,"tcdate":1508550274397,"number":32,"cdate":1509739518349,"id":"ry9tUX_6-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry9tUX_6-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy","abstract":"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we show that an ε-differentially private prior yields a valid PAC-Bayes bound, a straightforward consequence of results connecting generalization with differential privacy. Using stochastic gradient Langevin dynamics (SGLD) to approximate the well-known exponential release mechanism, we observe that generalization error on MNIST (measured on held out data) falls within the (empirically nonvacuous) bounds computed under the assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.","pdf":"/pdf/973c38ba45d8720ead2c7d267b2027499a0a40da.pdf","TL;DR":"We show that Entropy-SGD optimizes the prior of a PAC-Bayes bound, violating the requirement that the prior be independent of data; we use differential privacy to resolve this and improve generalization.","paperhash":"anonymous|entropysgd_optimizes_the_prior_of_a_pacbayes_bound_datadependent_pacbayes_priors_via_differential_privacy","_bibtex":"@article{\n  anonymous2018entropy-sg(l)d,\n  title={Entropy-SG(L)D Optimizes the Prior of a (Valid) PAC-Bayes Bound},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry9tUX_6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper32/Authors"],"keywords":["generalization error","neural networks","statistical learning theory","PAC-Bayes theory"]},"nonreaders":[],"replyCount":16,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}