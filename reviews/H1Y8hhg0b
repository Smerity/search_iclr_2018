{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222644883,"tcdate":1511910177550,"number":3,"cdate":1511910177550,"id":"S1tmowoeG","invitation":"ICLR.cc/2018/Conference/-/Paper402/Official_Review","forum":"H1Y8hhg0b","replyto":"H1Y8hhg0b","signatures":["ICLR.cc/2018/Conference/Paper402/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Important problem with convincing results but lack some important comparisons","rating":"7: Good paper, accept","review":"The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network. The basic problem is empirical risk minimization with a incremental penalty for each non zero weight. To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables. The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights. The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training. \n\nThe results presented in the paper are convincing. They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time. They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models. The hard concrete distribution is a small but nice contribution on its own.\n\nMy only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832). Although the focus is apparently different, these methods are clearly closely related. A discussion of this relationship seems really important.\n\nSpecific comments/questions:\n- The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this. For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation. This would give a stronger sense of the kind of wins that are possible in this framework.\n- Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations. Extra evaluations of different relaxations would be appreciated. At the very least a comparison to concrete would be nice.\n- In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Neural Networks through L_0 Regularization","abstract":"In order to learn the model structure, and for reasons of improved computational efficiency and generalization, we are often interested in learning the parameters of neural networks while strongly encouraging (blocks of) weights to take the value of exactly zero. The most general way to enforce this is by adopting an $L_0$ norm as a penalty on the weights. However, since the $L_0$ norm is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters for certain well-chosen probability distributions over the gates. To this end, we employ a novel distribution over gates, which we name the \\emph{hard concrete}; it is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.","pdf":"/pdf/1f6a6ec066b3e65fb45056f4988449b9b4e9ae0b.pdf","TL;DR":"We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.","paperhash":"anonymous|learning_sparse_neural_networks_through_l_0_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Neural Networks through L_0 Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Y8hhg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper402/Authors"],"keywords":["Sparsity","compression","hard and soft attention."]}},{"tddate":null,"ddate":null,"tmdate":1512222644931,"tcdate":1511804560705,"number":2,"cdate":1511804560705,"id":"BJF5RpKgG","invitation":"ICLR.cc/2018/Conference/-/Paper402/Official_Review","forum":"H1Y8hhg0b","replyto":"H1Y8hhg0b","signatures":["ICLR.cc/2018/Conference/Paper402/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A sensible and interesting approach to parameter estimation with L0 sparsity which could potentially be applied to many different models. However, in this specific application to standard feedforward network the main advantage was a theoretical speed-up and a robust empirical demonstration over other methods was missing. ","rating":"6: Marginally above acceptance threshold","review":"Learning sparse neural networks through L0 regularisation\n\nSummary: \n\nThe authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty. The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks. Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal. The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations. \n\nPros:\n\nThe paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. \n\nOptimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters. \n\nThe work is put in context and related to some previous relaxation approaches to sparsity. \n\nThe method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. \n\nCons:\n\nThe method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods. Therefore the main advance is in terms of learning speed to obtain this similar performance. However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout. It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered, e.g. does the proposed sparse learning method converge at the same rate as the others? I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. \n\nIt was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time. It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning. \n\nMinor point: it wasn’t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix). I don’t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2), it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. In the context here this didn’t seem a particularly relevant addition to the paper. \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Neural Networks through L_0 Regularization","abstract":"In order to learn the model structure, and for reasons of improved computational efficiency and generalization, we are often interested in learning the parameters of neural networks while strongly encouraging (blocks of) weights to take the value of exactly zero. The most general way to enforce this is by adopting an $L_0$ norm as a penalty on the weights. However, since the $L_0$ norm is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters for certain well-chosen probability distributions over the gates. To this end, we employ a novel distribution over gates, which we name the \\emph{hard concrete}; it is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.","pdf":"/pdf/1f6a6ec066b3e65fb45056f4988449b9b4e9ae0b.pdf","TL;DR":"We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.","paperhash":"anonymous|learning_sparse_neural_networks_through_l_0_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Neural Networks through L_0 Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Y8hhg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper402/Authors"],"keywords":["Sparsity","compression","hard and soft attention."]}},{"tddate":null,"ddate":null,"tmdate":1512222644980,"tcdate":1511630558351,"number":1,"cdate":1511630558351,"id":"rJUkD7vgf","invitation":"ICLR.cc/2018/Conference/-/Paper402/Official_Review","forum":"H1Y8hhg0b","replyto":"H1Y8hhg0b","signatures":["ICLR.cc/2018/Conference/Paper402/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper constructs a continuous surrogate for ell_0 norm via simple modifications of the binary concrete relaxation with additional stretching and hard-sigmoid transformation steps. The method is easily implemented and empirically seems effective.","rating":"6: Marginally above acceptance threshold","review":"This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization. The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations. Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components. The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods. \n\nPros: \n\n- The paper is clearly written, self-contained and a pleasure to read. \n- Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization\n\nCons: \n\n- It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices \n- It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc.   \n- If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Sparse Neural Networks through L_0 Regularization","abstract":"In order to learn the model structure, and for reasons of improved computational efficiency and generalization, we are often interested in learning the parameters of neural networks while strongly encouraging (blocks of) weights to take the value of exactly zero. The most general way to enforce this is by adopting an $L_0$ norm as a penalty on the weights. However, since the $L_0$ norm is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters for certain well-chosen probability distributions over the gates. To this end, we employ a novel distribution over gates, which we name the \\emph{hard concrete}; it is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.","pdf":"/pdf/1f6a6ec066b3e65fb45056f4988449b9b4e9ae0b.pdf","TL;DR":"We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.","paperhash":"anonymous|learning_sparse_neural_networks_through_l_0_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Neural Networks through L_0 Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Y8hhg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper402/Authors"],"keywords":["Sparsity","compression","hard and soft attention."]}},{"tddate":null,"ddate":null,"tmdate":1509739322362,"tcdate":1509112912955,"number":402,"cdate":1509739319687,"id":"H1Y8hhg0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1Y8hhg0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Sparse Neural Networks through L_0 Regularization","abstract":"In order to learn the model structure, and for reasons of improved computational efficiency and generalization, we are often interested in learning the parameters of neural networks while strongly encouraging (blocks of) weights to take the value of exactly zero. The most general way to enforce this is by adopting an $L_0$ norm as a penalty on the weights. However, since the $L_0$ norm is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters for certain well-chosen probability distributions over the gates. To this end, we employ a novel distribution over gates, which we name the \\emph{hard concrete}; it is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.","pdf":"/pdf/1f6a6ec066b3e65fb45056f4988449b9b4e9ae0b.pdf","TL;DR":"We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating.","paperhash":"anonymous|learning_sparse_neural_networks_through_l_0_regularization","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Sparse Neural Networks through L_0 Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Y8hhg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper402/Authors"],"keywords":["Sparsity","compression","hard and soft attention."]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}