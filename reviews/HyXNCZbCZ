{"notes":[{"ddate":null,"tddate":1511845798791,"tmdate":1512222738696,"tcdate":1511845753114,"number":3,"cdate":1511845753114,"id":"SJbY1_5xG","invitation":"ICLR.cc/2018/Conference/-/Paper741/Official_Review","forum":"HyXNCZbCZ","replyto":"HyXNCZbCZ","signatures":["ICLR.cc/2018/Conference/Paper741/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"7: Good paper, accept","review":"The paper incorporated hierarchical representation of complex, reichly-structured data to extend the Adversarially Learned Inference (Dumoulin et al. 2016) to achieve hierarchical generative model. The hierarchical ALI (HALI) learns a hierarchy of latent variables with a simple Markovian structure in both the generator and inference. The work fits into the general trend of hybrid approaches to generative modeling that combine aspects of VAEs and GANs. \n\nThe authors showed that within a purely adversarial training paradigm, and by exploiting the model’s hierarchical structure, one can modulate the perceptual fidelity of the reconstructions. We provide theoretical arguments for why HALI’s adversarial game should be sufficient to minimize the reconstruction cost and show empirical evidence supporting this perspective.\n\nThe performance of HALI were evaluated on four datasets, CIFAR10, SVHN, ImageNet 128x128 and CelebA. The usefulness of the learned hierarchical representations were demonstrated on a semi-supervised task on MNIST and an attribution prediction task on the CelebA dataset. The authors also noted that the introduction of a hierarchy of latent variables can add to the difficulties in the training. \n\nSummary:\n——\nIn summary, the paper discusses a very interesting topic and presents an elegant approach for modeling complex, richly-structured data using hierarchical representation. The numerical experiments are thorough and HALI is shown to generate better results than ALI. Overall, the paper is well written. However, it would provide significantly more value to a reader if the authors could provide more details and clarify a few points. See comments below for details and other points.\n\nComments:\n——\n1.\tCould the authors comment on the training time for HALI? How does the training time scale with the levels of the hierarchical structure?\n\n2.\tHow is the number of hierarchical levels $L$ determined? Can it be learned from the data? Are the results sensitive to the choice of $L$?\n\n3.\tIt seems that in the experimental results, $L$ is at most 2. Is it because of the data or because of the lack of efficient training procedures for the hierarchical structure?\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Adversarially Learned Inference","abstract":"We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error.  The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset.  There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task. ","pdf":"/pdf/a616d7d1673709e963b2bbfbb2feeec196a9552b.pdf","TL;DR":"Adversarially trained hierarchical generative model with robust and semantically learned latent representation.","paperhash":"anonymous|hierarchical_adversarially_learned_inference","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Adversarially Learned Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXNCZbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper741/Authors"],"keywords":["generative","hierarchical","unsupervised","semisupervised","latent","ALI","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1512222738740,"tcdate":1511817337583,"number":2,"cdate":1511817337583,"id":"SJbtlZ5gf","invitation":"ICLR.cc/2018/Conference/-/Paper741/Official_Review","forum":"HyXNCZbCZ","replyto":"HyXNCZbCZ","signatures":["ICLR.cc/2018/Conference/Paper741/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors propose a hierarchical GAN variant of ALI, but offer little novelty or insights.","rating":"3: Clear rejection","review":"The authors propose a hierarchical GAN setup, called HALI, where they can learn multiple sets of latent variables.\nThey utilize this in a deep generative model for image generation and manage to generate good-looking images, faithful reconstructions and good inpainting results.\n\nAt the heart of the technique lies the stacking of GANS and the authors claim to be proposing a novel model here.\nFirst, Emily Denton et. al proposed a stacked version of GANs in \"Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks\", which goes uncited here and should be discussed as it was the first work stacking GANs, even if it did so with layer-wise pretraining.\nFurthermore, the differences to another very similar work to that of the authors (StackGan by Huan et al) are unclear and not well motivated.\nAnd third, the authors fail to cite 'Adversarial Message Passing' by Karaletsos 2016, which has first introduced joint training of generative models with structure by hierarchical GANs and generalizes the theory to a particular form of inference for structured models with GANs in the loop. \nThis cannot be called concurrent work as it has been around for a year and has been seen and discussed at length in the community, but the authors fail to acknowledge that their basic idea of a joint generative model and inference procedure is subsumed there. In addition, the authors also do not offer any novel technical insights compared to that paper and actually fall short in positioning their paper in the broader context of approximate inference for generative models.\n\nGiven these failings, this paper has very little novelty and does not perform accurate attribution of credit to the community.\nAlso, the authors propose particular one-off models and do not generalize this technique to an inference principle that could be reusable.\n\nAs to its merits, the authors manage to get a particularly simple instance of a 'deep gan' working for image generation and show the empirical benefits in terms of image generation tasks. \nIn addition, they test their method on a semi-supervised task and show good performance, but with a lack of details.\n\nIn conclusion, this paper needs to flesh out its contributions on the empirical side and position its exact contributions accordingly and improve the attribution.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Adversarially Learned Inference","abstract":"We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error.  The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset.  There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task. ","pdf":"/pdf/a616d7d1673709e963b2bbfbb2feeec196a9552b.pdf","TL;DR":"Adversarially trained hierarchical generative model with robust and semantically learned latent representation.","paperhash":"anonymous|hierarchical_adversarially_learned_inference","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Adversarially Learned Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXNCZbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper741/Authors"],"keywords":["generative","hierarchical","unsupervised","semisupervised","latent","ALI","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1512222739838,"tcdate":1511753973594,"number":1,"cdate":1511753973594,"id":"SkRgK-YxM","invitation":"ICLR.cc/2018/Conference/-/Paper741/Official_Review","forum":"HyXNCZbCZ","replyto":"HyXNCZbCZ","signatures":["ICLR.cc/2018/Conference/Paper741/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting idea, but important details are missing","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to perform Adversarially Learned Inference (ALI) in a layer-wise manner. The idea is interesting, and the authors did a good job to describe high-level idea, and demonstrate one advantage of hierarchy: providing different levels reconstructions. However, the advantage of better reconstruction could be better demonstrated.  Some major concerns should be clarified before publishing:\n\n(1) How did the authors implement p(x|z) and q(z|x), or p(z_l | z_{l+1}) and q(z_{l+1} | z_l )? Please provide the details, as this is key to the reconstruction issues of ALI.\n\n(2) Could the authors provide the pseudocode procedure of the proposed algorithm? In the current form of the writing, it is not clear what the HALI procedure is, whether (1) one discriminator is used to distinguish the concatenation of (x, z_1, ..., z_L), or (2) L discriminators are used to distinguish the concatenation of (z_l, z_{l+1}) at each layer, respectively?\n\nThe above two points are important. If not correctly constructed, it might reveal potential flaws of the proposed technique.\n\nSince one of the major claims for HALI is to provide better reconstruction with higher fidelity than ALI. Could the authors provide quantitative results on MNIST and CIFAR to demonstrate this? The reconstruction issues have first been highlighted and theoretically analyzed in ALICE [*], and some remedy has been proposed to alleviate the issue.  Quantitative comparison on MNIST and CIFAR are also conducted. Could the authors report numbers to compare with them (ALI and ALICE)? \n\nThe 3rd paragraph in Introduction should be adjusted to correctly clarify details of algorithms, and reflect up-to-date literature. \"One interesting feature highlighted in the original ALI work (Dumoulin et al., 2016) is that ... never explicitly trained to perform reconstruction, this can nevertheless be easily done...\". Note that ALI can only perform reconstruction when the deterministic mapping is used, while ALI itself adopted the stochastic mapping. Further, the deterministic mapping is the major difference of BiGAN from ALI. Therefore, more rigorous way to phrase is that \"the original ALI work with deterministic mappings\", or \"BiGAN\" never explicitly trained to perform reconstruction, this can nevertheless be easily done... This tiny difference between deterministic/stochastic mappings makes major difference for the quality of reconstruction, as theoretically analyzed and experimentally compared in ALICE. In ALICE, the authors confirmed further source of poor reconstructions of ALI in practice. It would be better to reflect the non-identifiability issues raised by ALICE in Introduction, rather than hiding it in Future Work as \"Although recent work designed to improve the stability of training in ALI does show some promise (Chunyuan Li, 2017), more work is needed on this front.\"\n\nAlso, please fix the typo in reference as:\n[*] Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao and Lawrence Carin. ALICE: Towards understanding adversarial learning for joint distribution matching. In Advances in Neural Information Processing Systems (NIPS), 2017.\n\n\n ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Adversarially Learned Inference","abstract":"We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error.  The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset.  There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task. ","pdf":"/pdf/a616d7d1673709e963b2bbfbb2feeec196a9552b.pdf","TL;DR":"Adversarially trained hierarchical generative model with robust and semantically learned latent representation.","paperhash":"anonymous|hierarchical_adversarially_learned_inference","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Adversarially Learned Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXNCZbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper741/Authors"],"keywords":["generative","hierarchical","unsupervised","semisupervised","latent","ALI","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1509739129551,"tcdate":1509133867508,"number":741,"cdate":1509739126893,"id":"HyXNCZbCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyXNCZbCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hierarchical Adversarially Learned Inference","abstract":"We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error.  The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset.  There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task. ","pdf":"/pdf/a616d7d1673709e963b2bbfbb2feeec196a9552b.pdf","TL;DR":"Adversarially trained hierarchical generative model with robust and semantically learned latent representation.","paperhash":"anonymous|hierarchical_adversarially_learned_inference","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Adversarially Learned Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXNCZbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper741/Authors"],"keywords":["generative","hierarchical","unsupervised","semisupervised","latent","ALI","GAN"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}