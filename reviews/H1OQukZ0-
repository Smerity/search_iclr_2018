{"notes":[{"tddate":null,"ddate":null,"tmdate":1512269976029,"tcdate":1512258411724,"number":2,"cdate":1512258411724,"id":"HkNOs3g-z","invitation":"ICLR.cc/2018/Conference/-/Paper487/Official_Review","forum":"H1OQukZ0-","replyto":"H1OQukZ0-","signatures":["ICLR.cc/2018/Conference/Paper487/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea, weak execution","rating":"5: Marginally below acceptance threshold","review":"\n# Summary of paper\nThe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method. \n\n# Summary of review\nI find the contribution to be incremental, and the validation weak. Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution. None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject. \n\n# Detailed comments\n\n1. The distinction between parameters and hyperparameters (section 3) should be revised. First, the definition of parameters should not include the word parameters. Second, it is not clear what \"parameters of the regularization\" means. Typically, the regularization depends on both hyperparameters and parameters. The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are \"begin learnt\", just from different datasets.\n\n2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct. This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015).\n\n3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to. theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used).\n\n4. The hypothesis are not correctly specified. Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated.\n\n5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this. While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm.\n\n# Minor\n\nmissing . after \"hypergradient exactly\".\n\n\"we could optimization the hyperparam-\" (typo)\n\nReferences:\n Justin  Domke.    Generic  methods  for  optimization-based modeling.  In\nInternational Conference on Artificial Intelligence and Statistics, 2012.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Online Hyper-Parameter Optimization","abstract":"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters. While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks. Finally, we show its effectiveness on two distinct tasks.","pdf":"/pdf/21496881b7a90436f2111d49832f38fbf0ecfcc8.pdf","TL;DR":"An algorithm for optimizing regularization hyper-parameters during training","paperhash":"anonymous|online_hyperparameter_optimization","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Hyper-Parameter Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1OQukZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper487/Authors"],"keywords":["hyper-parameters","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222669395,"tcdate":1511797851376,"number":1,"cdate":1511797851376,"id":"S1Qv42tlz","invitation":"ICLR.cc/2018/Conference/-/Paper487/Official_Review","forum":"H1OQukZ0-","replyto":"H1OQukZ0-","signatures":["ICLR.cc/2018/Conference/Paper487/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Online Hyper-Parameter Optimization","rating":"4: Ok but not good enough - rejection","review":"Summary of the paper\n---------------------------\nThe paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning.  The covered framework is limited to regularization parameters. These hyper-parameters, noted $\\lambda$, are updated along the training of model parameters $\\theta$ by relying on the generalization performance (validation error). The paper proposes a dynamical system including the dynamical update of $\\theta$ and the update of the gradient $y$, derivative of $\\theta$ w.r.t. to the hyper-parameters. The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $\\lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system. Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach.\n\nComments\n-------------\n- The materials of the paper sometimes may be quite not easy to follow. Nevertheless the paper is quite well written.\n- The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016). As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors.\n- One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017). The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed.\n- The goal of the paper is to address automatically the learning of regularization parameters. Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $\\eta$) which choices are not clearly discussed. It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious. This fact weakens the scope of the online hyper-parameter optimization approach.\n- It may be helpful to indicate the standard deviations of the experimental results.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Online Hyper-Parameter Optimization","abstract":"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters. While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks. Finally, we show its effectiveness on two distinct tasks.","pdf":"/pdf/21496881b7a90436f2111d49832f38fbf0ecfcc8.pdf","TL;DR":"An algorithm for optimizing regularization hyper-parameters during training","paperhash":"anonymous|online_hyperparameter_optimization","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Hyper-Parameter Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1OQukZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper487/Authors"],"keywords":["hyper-parameters","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739275582,"tcdate":1509124128529,"number":487,"cdate":1509739272917,"id":"H1OQukZ0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1OQukZ0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Online Hyper-Parameter Optimization","abstract":"We propose an efficient online hyperparameter optimization method which uses a joint dynamical system to evaluate the gradient with respect to the hyperparameters. While similar methods are usually limited to hyperparameters with a smooth impact on the model, we show how to apply it to the probability of dropout in neural networks. Finally, we show its effectiveness on two distinct tasks.","pdf":"/pdf/21496881b7a90436f2111d49832f38fbf0ecfcc8.pdf","TL;DR":"An algorithm for optimizing regularization hyper-parameters during training","paperhash":"anonymous|online_hyperparameter_optimization","_bibtex":"@article{\n  anonymous2018online,\n  title={Online Hyper-Parameter Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1OQukZ0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper487/Authors"],"keywords":["hyper-parameters","optimization"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}