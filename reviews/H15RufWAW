{"notes":[{"tddate":null,"ddate":null,"tmdate":1512409625909,"tcdate":1512409625909,"number":5,"cdate":1512409625909,"id":"rkG7qbQZz","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Comment","forum":"H15RufWAW","replyto":"SkRp15ZWM","signatures":["ICLR.cc/2018/Conference/Paper876/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper876/Authors"],"content":{"title":"Re: Issue about duplicate model name","comment":"As stated in our reply to an earlier comment (see below), we are aware of this; while we believe that both works are very distinct, we are considering alternative names to avoid confusion."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512312774834,"tcdate":1512312774834,"number":5,"cdate":1512312774834,"id":"SkRp15ZWM","invitation":"ICLR.cc/2018/Conference/-/Paper876/Public_Comment","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Issue about duplicate model name","comment":"Neat Work,\nBut I found that there was another paper named \"GraphGAN\" on ArXiv: https://arxiv.org/abs/1711.08267, which has been accepted by AAAI 2018.\nIt might be confusing for readers to distinguish these two models."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512245409390,"tcdate":1512245409390,"number":3,"cdate":1512245409390,"id":"rkKo_YeWG","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Review","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["ICLR.cc/2018/Conference/Paper876/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Claims and evaluation need some work","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a WGAN formulation for generating graphs based on random walks. The proposed generator model combines node embeddings, with an LSTM architecture for modeling the sequence of nodes visited in a random walk; the discriminator distinguishes real from fake walks.\n\nThe model is learned from a single large input graph (for three real-world networks) and evaluated against one baseline generative graph model: degree-corrected stochastic block models. \n\nThe primary claims of the paper are as follows:\ni) The proposed approach is a generative model of graphs, specifically producing \"sibling\" graphs\nii) The learned latent representation provides an interpretation of generated graph properties\niii) The model generalizes well in terms of link and node classification\n\nThe proposed method is novel and the incorporated ideas are quite interesting (e.g., discriminating real from fake random walks, generating random walks from node embeddings and LSTMs). However, from a graph generation perspective, the problem formulation and evaluation do not sufficiently demonstrate the utility of proposed method. \n\nFirst, wrt claim (i) the problem of generating \"sibling\" graphs is ill-posed. Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution. The notion of a \"sibling\" graph used in this paper is not clearly defined, but it seems to only be useful if the sibling graphs are likely under the distribution. Unfortunately, the likelihood of the sampled graphs is not explicitly evaluated. On the other hand, since many of the edges are shared the \"siblings\" may be nearly isomorphic to the input graph, which is not useful from a graph modeling perspective. \n\nFor claim (i), the comparison to related work is far from sufficient to demonstrate its utility as a graph generation model. There are many graph models that are superior to DC-SBM, including KPGMs, BETR, ERGMs, hierarchical random graph models and latent space models. Moreover, a very simple baseline to assess the LSTM component of the model, would be to produce a graph by sampling links repeatedly from the latent space of node embeddings. \n\nNext, the evaluation wrt to claim (ii) is novel and may help developers understand the model characteristics. However, since the properties are measured based on a set of random walks it is still difficult to interpret the impact on the generated graphs (since an arbitrary node in the final graph will have some structure determined from each of the regions). Do the various regions generate different parts of the final graph structure (i.e., focusing on only a subset of the nodes)?   \n\nLastly, the authors evaluate the learned model on link and node prediction tasks and state that the model's so-so performance supports the claim that the model can generalize. This is the weakest claim of the paper. The learned node embeddings appear to do significantly worse than node2vec, and the full model is worse than DC-SBM. Given that the proposed model is transductive (when there is significant edge overlap) it should do far better than DC-SBM which is inductive. \n\nOverall, while the paper includes a wide range of experimental evaluation, they are aimed too broadly (and the results are too weak) to support any specific claim of the work. If the goal is to generate transductively (with many similar edges), then it would be better to compare more extensively to alternative node embedding and matrix factorization approaches, and assess the utility of the various modeling choices (e.g., LSTM, in/out embedding). If the goal is to generate inductively, over the full distribution of graphs, then it would be better to (i) assess whether the sampled graphs are isomorphic, and (ii) compare more extensively to alternative graph models (many of which have been published since 2010).  \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1511861706290,"tcdate":1511861706290,"number":4,"cdate":1511861706290,"id":"B1M0aoceM","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Comment","forum":"H15RufWAW","replyto":"rJa0pv5ef","signatures":["ICLR.cc/2018/Conference/Paper876/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper876/Authors"],"content":{"title":"Re: one more question","comment":"We use the Straight-Through Gumbel-Softmax estimator that is described in [1]. In a nutshell, this allows us to approximate sampling from a categorical distribution in a differentiable way.\n\n[1] Jang, Eric, Shixiang Gu, and Ben Poole. \"Categorical reparameterization with Gumbel-softmax.\" ICLR 2017"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"ddate":null,"tddate":1511924334540,"tmdate":1511924340194,"tcdate":1511845332624,"number":4,"cdate":1511845332624,"id":"rJa0pv5ef","invitation":"ICLR.cc/2018/Conference/-/Paper876/Public_Comment","forum":"H15RufWAW","replyto":"B1xzITYxf","signatures":["~Junliang_Guo1"],"readers":["everyone"],"writers":["~Junliang_Guo1"],"content":{"title":"one more question","comment":"Thanks for your clear reply! And one more question:\n\nIn Section 3.1, the next sample is generated as v_{t} = onehot(argmax v_{t}^{*}). How is this step differentiable? As argmax is a hard assignment, the gradients cannot be passed to v_{t}^{*} during backward as you claimed. Maybe I misunderstand somewhere?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1511802376372,"tcdate":1511802376372,"number":3,"cdate":1511802376372,"id":"B1xzITYxf","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Comment","forum":"H15RufWAW","replyto":"SknlQLFef","signatures":["ICLR.cc/2018/Conference/Paper876/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper876/Authors"],"content":{"title":"Re: The constructed matrix S while training with EO early stop strategy ","comment":"Thank you for your comment and interest in our work!\n\n1) For the EO criterion, S is constructed the same as in Val. In both cases S is constructed based on the last 1k iterations (not 1k random walks), which typically amounts to around 750k random walks (depending on the specific setting). Thus, we expect the approximation to be reasonably good. Note, that we update the list of the RWs generated in the last 1k iterations incrementally (in a sliding window / queue fashion). This means that at every iteration we only need to subtract the 750 oldest entries from S, and add the 750 newest, which is highly efficient.\n\n2) We are not sure whether we correctly understood your definition of a generative model. We base our notion of a generative model for graphs on [1]. In this context, such a model is used to generate entire graphs that exhibit desired properties (e.g., matching the properties of a given input graph). Some examples include the Configuration Model [2] and Barabási–Albert Model [3]. While the purpose of GraphGAN is to generate entire graphs, we can also use it for node-level tasks such as link prediction, as shown in the experimental section.\n\nAs for the concrete application scenarios, we again refer to [1]. One case where our model is readily applicable is simulation studies (using the language of [1]). Imagine that we are developing a new algorithm for some graph-related problem, e.g. community detection. Often, we don't have access to much labeled data that all comes from the same distribution. However, GraphGAN still lets us estimate how our new algorithm will behave in the wild. For this, we can create sibling graphs using GraphGAN and evaluate performance of the new algorithm on them.\n\nThere are surely many other tasks that GraphGAN could be applied to, that we leave for follow-up work, such as anomaly detection, graph compression, data anonymization, etc.\n\nWe hope this answer clarifies the uncertainties you had about our work. Please do not hesitate to post follow-up questions.\n\nReferences:\n[1] Deepayan Chakrabarti and Christos Faloutsos. Graph mining:  Laws, generators, and algorithms.\nACM computing surveys (CSUR), 38(1):2, 2006.\n[2] http://homepage.divms.uiowa.edu/~sriram/196/spring12/lectureNotes/Lecture11.pdf\n[3] Albert-Laszlo Barabasi and Reka Albert.  Emergence of scaling in random networks. Science, 286 (5439):509–512, 1999.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1511772915733,"tcdate":1511772915733,"number":3,"cdate":1511772915733,"id":"SknlQLFef","invitation":"ICLR.cc/2018/Conference/-/Paper876/Public_Comment","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["~Junliang_Guo1"],"readers":["everyone"],"writers":["~Junliang_Guo1"],"content":{"title":"The constructed matrix S while training with EO early stop strategy","comment":"It's a very interesting work!  There are two parts that I'm confused after reading the paper:\n\n1. In Section 3.2, while training with EO-Criterion early stopping strategy, you construct a score matrix S at every validation step. How is S constructed in EO? In Val-Criterion, S is constructed through 1k recently generated random walks. While in EO, is S still constructed the same as in Val, or generated through 500k generated random walks as you described in Section 3.3? It's time-consuming if you construct S from a large corpus of random walks at every validation iteration, and if you use the small corpus as used in Val, how to guarantee the approximation error of edge overlap ratio is bounded, i.e., won't be too large to damage the performance?\n\n2. This work generates sibling graphs of the original graph. In what applications can we utilize this method? Normal graph generative models generate a relevant node given a prior node, but this paper generates a new graph given a prior graph, thus seems cannot be directly used in node-level graph applications. Any references will be better :)\n\nLooking forward to your reply! Thanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222801549,"tcdate":1511772196343,"number":2,"cdate":1511772196343,"id":"SJhXxLYgz","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Review","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["ICLR.cc/2018/Conference/Paper876/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A pioneering work with great potential","rating":"7: Good paper, accept","review":"The authors proposed a generative model of random walks on graphs. Using GAN, the architecture allows for model-agnostic learning, controllable fitting, ensemble graph generation. It also produces meaningful node embeddings with semi-interpretable latent spaces. The overall framework could be relevant to multiple areas in graph analytics, including graph comparison, graph sampling, graph embedding and relational feature selection. The draft is well written with convincing experiments. I support the acceptances of this paper.\n\nI do have a few questions that might help further improve the draft. More baseline besides DC-SBM could better illustrate the power of GAN in learning longer random walk trajectories. DC-SBM, while a generative model, inherently can only capture first order random walks with target degree biases, and generally over-fits into degree sequences. Are there existing generative models based on walk paths?\n\nThe choice of early stopping is a very interesting problem especially for the EO-creitenrion. In Fig3 (b), it seems assortativity is over-fitted beyond 40k iterations. It might be helpful to discuss more about the over-fitting of different graph properties.\n\nThe node classification experiment could use a bit more refinement. The curves in Fig. 5(a) are not well explained. What is the \"combined\"? The claim of competitive performance needs better justification according to the presentation of the F1 scores.\n\nThe Latent variable interpolation experiment could also use more explanations. How is the 2d subspace chosen? What is the intuition behind the random walks and graphs of Fig 6? Can you provide visualizations of the communities of the interpolated graphs in Fig 7? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"ddate":null,"tddate":1511747315285,"tmdate":1512222801591,"tcdate":1511747301798,"number":1,"cdate":1511747301798,"id":"BkCkJetef","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Review","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["ICLR.cc/2018/Conference/Paper876/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Positive about manuscript but it needs improvement","rating":"6: Marginally above acceptance threshold","review":"I am overall positive about the work but I would like to see some questions addressed. \n\nQuality: The paper is good but does not address some important issues. The paper proposes a GAN model to generate graphs with non-trivial properties. This is possibly one of the best papers on graph generation using GANs currently in the literature. However, there are a number of statistical issues that should be addressed. I fear the paper is not ready yet, but I am not opposed to publication as long as there are warnings in the paper about the shortcomings.\n\nOriginality: This is an original approach. Random walks sometimes are overused in the graph literature, but they seem justified in this work. But it also requires extra work to ensure they are generating meaningful graphs.\n\nSignificance: The problem is important. Learn to generate graphs is a key task in drug discovery, relational learning, and knowledge discovery.\n\nEvaluation: The link prediction task is too easy, as links are missing at random. It would be more useful to predict links that are removed with an unknown bias. The graph (wedge, claw, etc) characteristics are good (but simple) metrics; however, it is unclear how a random graph with the same size and degree distribution (configuration model) would generate for the same metrics (it is not shown for comparison). \n\nIssues that I wish were addressed in the paper: \na)\tHow is the method learning a generator from a single graph? What are the conditions under which the method is likely to perform well? It seems to rely on some mixing RW conditions to model the distinct graph communities. What are these mixing conditions? These are important questions that should have at least an empirical exploration.\nb)\tWhat is the spatial independence assumption needed for such a generator? \nc)\tWould this approach be able to generate a lattice? Would it be able to generate an expander graph? What about a graph with poorly connect communities? Is there any difficulties with power law graphs? \nd)\tHow is the RW statistically addressing the generation of high-order (subgraph) features?\ne)\tCan this approach be used with multiple i.i.d. graphs? \nf)\tIsn’t learning the random walk sample path a much harder / higher-dimensional task than it is necessary? Again, the short walk may be capturing the communities but the high-dimensional random walk sample path seems like a high price to pay to learn community structure.\ng)\tClearly, with a large T (number of RW steps), the RW is not modeling just a single community. Is there a way to choose T? How larger values of T to better model inter-community links? Would different communities have different choices of T? \nh)\tAnd a related question, how well can the method generate the inter-community links?\ni)\tThe RW model is actually similar to an HMM. Would learning a mixture of HMMs (one per community) have similar performance?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1511537375425,"tcdate":1511537375425,"number":2,"cdate":1511537375425,"id":"B1wkj2BgM","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Comment","forum":"H15RufWAW","replyto":"SyDRyMBlf","signatures":["ICLR.cc/2018/Conference/Paper876/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper876/Authors"],"content":{"title":"Re: relation with a recent paper","comment":"While both models coincidentally have the same acronym and use the GAN framework, they are very distinct in their nature and have different goals.\n\nThe model proposed in the paper you referenced is an explicit (prescribed) probabilistic model whose goal is to learn node embeddings. The explicitly specified probability distribution G(v | v_c) can be computed directly given the embedding \\theta_G (Equation 5). Such model could also be learned by other means (e.g., by directly minimizing cross entropy + negative sampling for non-edges), and the use of GANs in this setting is rather unconventional.\n\nIn contrast, our approach defines an implicit generative model for random walks in the graph. Its main goal is to generate new graphs that have similar properties to original (but are not exact replicas). As is the case for implicit models, samples can be drawn from it, but direct computation of the probabilities is not possible. In such a scenario, GAN training is one of the few available options. Our implicit model is not restricted to pairwise interactions and can capture higher-order properties of the graph.\n\nNote, that link prediction is the optimization objective in the work you mentioned. Thus, it is not surprising that the obtained node embeddings achieve high scores in the related tasks. Meanwhile, our model is not trained for link prediction, and the embeddings are just a byproduct of the learning process.\n\nIn addition to pointing out these fundamental differences, we would also like to highlight that the above-mentioned work was just made public on arXiv two days ago (Nov 22nd); which is why it could not be included in the Related Work section of our paper at the time of submission almost a month ago.\n\nTL;DR: While at first glance the approaches appear to be related (both are called GraphGAN), after carefully reading the papers, it becomes clear that the two models are fundamentally different and have orthogonal goals. The other work: explicit model + pairwise interactions for learning node embeddings. Our work: implicit model + higher-order interactions, with the goal of generating new graphs.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1511493614546,"tcdate":1511493582874,"number":2,"cdate":1511493582874,"id":"SyDRyMBlf","invitation":"ICLR.cc/2018/Conference/-/Paper876/Public_Comment","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"relation with a recent paper","comment":"An AAAI18 paper released recently also propose a graph GAN framework https://arxiv.org/abs/1711.08267, what's the difference between this paper and their paper? It seems that their results is more dominant in link prediction than this paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1510223359102,"tcdate":1510223234106,"number":1,"cdate":1510223234106,"id":"SycFao-kf","invitation":"ICLR.cc/2018/Conference/-/Paper876/Official_Comment","forum":"H15RufWAW","replyto":"Bk-RVrRAW","signatures":["ICLR.cc/2018/Conference/Paper876/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper876/Authors"],"content":{"title":"Re: About latent space interpolation experiment","comment":"Thank you very much for your interest in our paper and your comment. \n\nIt seems that the source of confusion is that on the one hand, we show that graphs generated using random sampling from the latent space are similar to the input graph (Table 2, Fig. 3a), while on the other hand, in the latent space interpolation (Fig. 6 and 7), the generated graphs have very different properties compared to the input graph.\n \nLet's recap how the latent space interpolation is performed for clarity. Remember that a single noise vector does not produce a complete graph, but rather one random walk. We therefore sample a large number of random walks from the latent space and use the method described in Sec. 3.3 to assemble a graph from these random walks.\n\nIf we now restrict the sampling to specific subregions of the latent space, intuitively, we obtain random walks that have some specific properties, which in turn makes the graphs assembled from them have specific properties. However, if we sample from the entire latent space, we are in a way \"averaging\" over all of these properties and the sampled random walks (and the resulting graph) have similar properties as the original, e.g. as you have noticed in Table 2 and Figure 3a.\n\nWe hope that this answer helps you to better understand our experiment. Please do not hesitate to comment again if you have any other or follow-up questions.\n\ntl;dr: Specific regions of the latent space encode specific properties, the \"average\" over all regions has properties similar to the original. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1509999816762,"tcdate":1509999816762,"number":1,"cdate":1509999816762,"id":"Bk-RVrRAW","invitation":"ICLR.cc/2018/Conference/-/Paper876/Public_Comment","forum":"H15RufWAW","replyto":"H15RufWAW","signatures":["~Jiaxuan_You1"],"readers":["everyone"],"writers":["~Jiaxuan_You1"],"content":{"title":"About latent space interpolation experiment","comment":"It's a great paper to read. I have a question on the latent space interpolation experiment of the paper. \nIf I got it right, you do random sampling in hidden space to produce the results in Table 2, and the statistics seem to be pretty stable. However, when you do latent space interpolation, the statistics seem to vary a lot. Why does this happen?\nYou mention that \"certain regions of z correspond to generated graphs with very different degree distributions\", however it Figure 3(a), by random generating a graph, the degree distribution matches the ground truth well. I'm confused about why the latent space has such kind of property.\nMaybe I made some mistakes when trying to understand the experiment. Looking forward to your reply! Thanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]}},{"tddate":null,"ddate":null,"tmdate":1509739053368,"tcdate":1509136593609,"number":876,"cdate":1509739050710,"id":"H15RufWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H15RufWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GraphGAN: Generating Graphs via Random Walks","abstract":"We propose GraphGAN -- the first implicit generative model for graphs that enables to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over a single input graph. Our model is based on a stochastic neural network that generates discrete output samples, and is trained using the Wasserstein GAN objective. GraphGAN enables us to generate sibling graphs, which have similar properties yet are not exact replicas of the original graph. Moreover, GraphGAN learns a semantic mapping from the latent input space to the generated graph's properties. We discover, that sampling from certain regions of the latent space leads to varying properties of the output graphs, with smooth transitions between them. Strong generalization properties of GraphGAN are highlighted by its competitive performance in link prediction and node classification, even though not specifically trained for these tasks.","pdf":"/pdf/97f95191af801049263661e8d0c4c1dd560c5578.pdf","TL;DR":"Using GANs to generate graphs via random walks.","paperhash":"anonymous|graphgan_generating_graphs_via_random_walks","_bibtex":"@article{\n  anonymous2018graphgan:,\n  title={GraphGAN: Generating Graphs via Random Walks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H15RufWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper876/Authors"],"keywords":["GAN","graphs","random walks","implicit generative models"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}