{"notes":[{"tddate":null,"ddate":null,"tmdate":1512336448116,"tcdate":1512336448116,"number":3,"cdate":1512336448116,"id":"SJOrnJf-M","invitation":"ICLR.cc/2018/Conference/-/Paper97/Official_Review","forum":"r111KtCp-","replyto":"r111KtCp-","signatures":["ICLR.cc/2018/Conference/Paper97/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting toy task but without real new insights or contribution to Autoencoders","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a simple task (learning the manifold of all the images of disks) to study some properties of Autoencoders. They show that Autoencoders don't generalize to disks of radius not in the training set and propose several regularization to improve generalisation.\n\nThe task proposed in the paper is interesting but the study made is somewhat limited:\n\n- They only studied one choice of Autoencoder architecture, and the results shown depends heavily on the choice of the activation, in particular sigmoid should not suffer from the same problem. \n\n- It would be interesting to study the generalization in terms of the size of the gap.\n\n- The regularization proposed is quite simple and already known, and other regularization have been proposed (e.g. dropout, ...). A more detailed comparison with all previous regularization scheme would be much needed. \n\n- The choice of regularization at the end seems quite arbitrary, it works better on this example but it's not clear at all why, and if this choice would work for other tasks.\n\nAlso Denoising Autoencoders (Pascal et al.) should probably be mentioned in the previous work section, as they propose a solution to the regularization of Autoencoder.\n\nOverall nothing really new was discovered or proposed, the lack of generalization of those kind of architecture is a well known problem and the regularization proposed was already known. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Taking Apart Autoencoders: How do They Encode Geometric Shapes ?","abstract":"We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures. \n","pdf":"/pdf/50055473ac3c9bcda1a6f908fd156029d461f0ce.pdf","TL;DR":"We study the functioning of autoencoders in a simple setting and advise new strategies for their regularisation in order to obtain bettre generalisation with latent interpolation in mind for image sythesis. ","paperhash":"anonymous|taking_apart_autoencoders_how_do_they_encode_geometric_shapes_","_bibtex":"@article{\n  anonymous2018taking,\n  title={Taking Apart Autoencoders: How do They Encode Geometric Shapes ?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r111KtCp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper97/Authors"],"keywords":["autoencoders","CNN","image synthesis","latent space"]}},{"tddate":null,"ddate":null,"tmdate":1512222828903,"tcdate":1511945131312,"number":2,"cdate":1511945131312,"id":"SJQ2Xg2eM","invitation":"ICLR.cc/2018/Conference/-/Paper97/Official_Review","forum":"r111KtCp-","replyto":"r111KtCp-","signatures":["ICLR.cc/2018/Conference/Paper97/AnonReviewer1"],"readers":["everyone"],"content":{"title":"a bit trivial and lacking in justification/insight as to the regularisation method","rating":"4: Ok but not good enough - rejection","review":"The paper considers a toy problem: the space of images of discs of variable radius - a one dimensional manifold.\n\nAn autoencoder based on convolutional layers with ReLU is experimented with, with a 1D embedding.\n\nIt is shown that \n1) if the bias is not included, the resulting function is homogeneous (meaning f(ax)=af(x)), and so it fails because the 1D representation should be the radius, and the relationship from radius to image is more complex than a homogeneous function.\n- if we include the bias and L2 regularise only the encoder weights, it works better in terms of interpolation for a limited data sample.\n\nThe thing is that 1) is trivial (the composition of homogeneous functions is homogeneous... so their proof is overly messy btw). Then, they continue by further analysing (see proposition 2) the solution for this case. Such analysis does not seem to shed much light on anything relevant, given that we know the autoencoder fails in this case due to the trivial proposition 1.\n\nAnother point: since the homogeneous function problem will not arise for other non-linearities (such as the sigmoid), the focus on the bias as the culprit seems arbitrary.\n\nThen, the story about interpolation and regularisation is kind of orthogonal, and then is solved by an arbitrary regularisation scheme. The lesson learned from this case is basically the second last paragraph of section 3.2. In other words, it just works.\n\nSince it's a toy problem anyway, the insights seem somewhat trivial.\n\nOn the plus side, such a toy problem seems like it might lead somewhere interesting. I'd like to see a similar setup but with a suite of toy problems. e.g. vary the aspect ratio of an oval (rather than a disc), vary the position, intensity, etc etc.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Taking Apart Autoencoders: How do They Encode Geometric Shapes ?","abstract":"We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures. \n","pdf":"/pdf/50055473ac3c9bcda1a6f908fd156029d461f0ce.pdf","TL;DR":"We study the functioning of autoencoders in a simple setting and advise new strategies for their regularisation in order to obtain bettre generalisation with latent interpolation in mind for image sythesis. ","paperhash":"anonymous|taking_apart_autoencoders_how_do_they_encode_geometric_shapes_","_bibtex":"@article{\n  anonymous2018taking,\n  title={Taking Apart Autoencoders: How do They Encode Geometric Shapes ?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r111KtCp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper97/Authors"],"keywords":["autoencoders","CNN","image synthesis","latent space"]}},{"tddate":null,"ddate":null,"tmdate":1512222828947,"tcdate":1511728307264,"number":1,"cdate":1511728307264,"id":"H1s3VoOlf","invitation":"ICLR.cc/2018/Conference/-/Paper97/Official_Review","forum":"r111KtCp-","replyto":"r111KtCp-","signatures":["ICLR.cc/2018/Conference/Paper97/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This is an interesting study to understand the innerworkings of an autoencoder, however, the study is not quite convncing, yet.","rating":"4: Ok but not good enough - rejection","review":"1. The idea is interesting, but the study is not comprehensive yet\n2. need to visualize the input data space, with the training data, test data, the 'gaps' in training data [see a recent related paper - Stoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017).]. \n3. What's the effect of training data size? \n4. How do the intermediate feature maps look like? \n5. Is there an effect of number of layers? Maybe the network architecture is too deep for the simple data characteristics and size of training set. \n6. Other shapes are said to be part of future work, but I am not convinced that serious conclusions can be drawn from this study only? \n7. What about the possible effects of Batch normalization and dropout?  \n8. size of 'd' is critical for autoencoders, only one example in appendix does not do justice, also it seems other color channels show up in the results (fig 10), wasn't it binary input?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Taking Apart Autoencoders: How do They Encode Geometric Shapes ?","abstract":"We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures. \n","pdf":"/pdf/50055473ac3c9bcda1a6f908fd156029d461f0ce.pdf","TL;DR":"We study the functioning of autoencoders in a simple setting and advise new strategies for their regularisation in order to obtain bettre generalisation with latent interpolation in mind for image sythesis. ","paperhash":"anonymous|taking_apart_autoencoders_how_do_they_encode_geometric_shapes_","_bibtex":"@article{\n  anonymous2018taking,\n  title={Taking Apart Autoencoders: How do They Encode Geometric Shapes ?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r111KtCp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper97/Authors"],"keywords":["autoencoders","CNN","image synthesis","latent space"]}},{"tddate":null,"ddate":null,"tmdate":1509739486179,"tcdate":1508968662690,"number":97,"cdate":1509739483509,"id":"r111KtCp-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r111KtCp-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Taking Apart Autoencoders: How do They Encode Geometric Shapes ?","abstract":"We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures. \n","pdf":"/pdf/50055473ac3c9bcda1a6f908fd156029d461f0ce.pdf","TL;DR":"We study the functioning of autoencoders in a simple setting and advise new strategies for their regularisation in order to obtain bettre generalisation with latent interpolation in mind for image sythesis. ","paperhash":"anonymous|taking_apart_autoencoders_how_do_they_encode_geometric_shapes_","_bibtex":"@article{\n  anonymous2018taking,\n  title={Taking Apart Autoencoders: How do They Encode Geometric Shapes ?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r111KtCp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper97/Authors"],"keywords":["autoencoders","CNN","image synthesis","latent space"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}