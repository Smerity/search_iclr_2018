{"notes":[{"tddate":null,"ddate":null,"tmdate":1514955763559,"tcdate":1514955763559,"number":6,"cdate":1514955763559,"id":"H12e4JcQz","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Comment","forum":"rywDjg-RW","replyto":"B1rMMpYMz","signatures":["ICLR.cc/2018/Conference/Paper624/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper624/Authors"],"content":{"title":"revision with ML-based ranker","comment":"Following reviewers' feedback, we have updated the draft (Appendix C) with experiments that employ an ML-based ranking function as against the state-of-the-art ranker of PROSE that involves hand engineering. We observe that NGDS achieves ~2X speed-ups on average while still achieving highly comparable generalization accuracy as compared to PROSE with the ML-based ranker. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1513898508800,"tcdate":1513898508800,"number":4,"cdate":1513898508800,"id":"B1rMMpYMz","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Comment","forum":"rywDjg-RW","replyto":"rywDjg-RW","signatures":["ICLR.cc/2018/Conference/Paper624/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper624/Authors"],"content":{"title":"New revision","comment":"We have uploaded a new paper revision, as per the reviewers' feedback. Here's a summary of the changes:\n\n- Restructured the introduction, making NGDS details clearer and moving them earlier.\n- Added analysis of some erroneous scenarios in the Evaluation.\n- Expanded related work overview with more symbolic methods such as ILP and LOPSTR.\n- Added details of the training process, including all the hyperparameters.\n- Moved Appendix A into the main text.\n- Replaced the table in Appendix B (earlier C): we found that we selected a wrong model to generate the table in the previous submission. The summary results in Tables 1-2 and their analysis were on the correct best model (so no change was needed in the Evaluation), but the spreadsheet for detailed results in the appendix was not. We apologize for this confusion. The distribution of the speed-ups did not change substantially, although the correct spread is now from 12x to 0.2x.\n\nWe will upload one more revision later this month, in which we'll include experiments we're currently performing with an ML-learned ranking function (as opposed to the state-of-the-art PROSE ranking function, used in the current submission)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1513347099129,"tcdate":1513185170026,"number":3,"cdate":1513185170026,"id":"Bkq9JykMG","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Comment","forum":"rywDjg-RW","replyto":"SyFsGdSlM","signatures":["ICLR.cc/2018/Conference/Paper624/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper624/Authors"],"content":{"title":"We combine deep learning & symbolic methods in a new milestone for the program synthesis application, as opposed to a pure deep learning contribution","comment":"Thank you for the related work suggestions -- we will update this discussion in the next draft. We address your concerns below: \n\n> Q: Limited innovation in terms of deep learning:\n\nRather than being a pure contribution to deep learning, this work applies deep learning to the important field of program synthesis, where statistical approaches are still underexplored. Our main contribution is a hybrid approach to program synthesis that utilizes the best of both neural and symbolic synthesis techniques. Combining insights from both worlds in this way achieves a new milestone in program synthesis performance: from a single example it generates programs that generalize better than prior state-of-the-art (including neural RobustFill, symbolic PROSE, and hybrid DeepCoder), the generated program is provably correct, and the generation is 50% faster on average\n\nDeepCoder (Balog et al., ICLR 2017) first explored a hybrid approach last year by first predicting the likelihood of various operators and then using it to guide an external symbolic synthesis engine. Since deep networks are data-hungry, Balog et al. obtain training data by randomly sampling programs from the DSL and generating satisfying random strings as input-output examples. As noted in Section 1 and as evidenced by its inferior performance against our method, the generated programs tend to be unnatural leading to poor generalization. In contrast, NGDS closely integrates neural models at each step of the synthesis and so, it is possible to obtain large amounts of training data while utilizing a relatively small number of real-world examples. \n\n> Q: Learning the ranking function instead of taking it as a given: \n\nWhile related, this problem is orthogonal to our work: a ranking function evaluates whether a given full program generalizes well, whereas we aim to predict the generalization of the best program produced from a given partial search state.\n\nImportantly, the proposed technique, NGDS is independent of the ranking function and can be trivially integrated with any high-quality ranking function. For instance, the manually written ranking function of FlashFill in PROSE that we use is a result of 7 years of engineering and heavy fine-tuning for industrial applications. An even better-quality learned ranking function would only improve the accuracy of predictions, which are already on par with baseline PROSE (68.49% vs 67.12%).\n\nIn fact, a lot of recent prior work focuses on learning a ranking function for program induction, see (Singh & Gulwani, CAV 2015) and (Ellis & Gulwani, IJCAI 2017). For comparison, we are currently performing a set of experiments with an ML-learned ranking function; we'll update with the new results once it's done.\n\n> Q: What does \"without attention\" mean?\n\nAll the models we explore encode input and output examples using (possibly multi-layered, bi-directional) LSTMs with or without an attention mechanism (Bahdanau et al., ICLR 2015). As mentioned in Section 8, the most accurate predictions arise when we attend to the input string while encoding the output string similar to the attention-based models proposed by Devlin et al., 2017. We will make this clearer in the next version of the paper. \n\nSuch an attention mechanism allows the network to learn complex features like \"whether the output is a substring of the input\". Unfortunately, such accuracy comes at a cost of increasing the network evaluation time to quadratic instead of linear. As a result, prediction time at every node of the search tree dominates the search time, and NGDS is slower than PROSE even when its predictions are accurate. Therefore, we only use LSTM models without any attention mechanism in our evaluations. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1513184981491,"tcdate":1513184981491,"number":2,"cdate":1513184981491,"id":"HJTR0CRbG","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Comment","forum":"rywDjg-RW","replyto":"SkPNib9ez","signatures":["ICLR.cc/2018/Conference/Paper624/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper624/Authors"],"content":{"title":"Training details + Clarification on the generalization accuracy","comment":"> Q: Please clarify how the system is trained.\n\n1) We use the industrially collected set of 375 string transformation tasks. Each task is a single input-output examples and 2-10 unseen inputs for evaluating generalization. Further, we split the 375 tasks into 65% train, 15% validation, and 20% test ones.\n2) We run PROSE on each of those tasks and collect the (symbol, production, spec input, spec output -> best program score after learning) information on all nodes of the search tree. As mentioned in the introduction, such traces provide a rich description of the synthesis problem thanks to the Markovian nature of deductive search in PROSE and enabling the creation of large datasets required for learning deep models. As a result, we obtain a dataset of ~450,000 search outcomes from mere 375 tasks.\n3) We further split all the search outcomes by the used symbol or its depth in the grammar. In our final evaluation, we present the results for the models trained on the decisions on the `transform` (depth=1), `pp`, `pos` symbols. We have also trained other symbol models as well as a single common model for all symbols/depths, but they didn’t perform as well.\n4) We employ Adam (Kingma and Ba, 2014) to optimize the objective. We use a batch size of 32 and a learning rate of 0.01 and use early stopping to pick the final model. The model architecture and the corresponding loss function (squared error) are discussed in Section 3.1. We will add the specific training details in the next revision of the paper. \n5) As discussed in Section 3.3, the learned models are integrated in the corresponding PROSE controller when the current search tree node matches the model's conditions (i.e. it is on the same respective symbol or depth).\n\n> Q: Is the approach useful for synthesis when there is a loss in program accuracy?\n\nIn fact, NGDS achieves higher average test accuracy than baseline PROSE (68.49% vs. 67.12%), although with slightly lower validation accuracy (63.83% vs. 70.21%) which effectively corresponds to 4 tasks.\n\nHowever, this is not the most important factor: PBE is bound to often fail in synthesizing the _intended_ program from a single input-output example. Even a machine-learned ranking function picks the wrong program 20% of the time (Ellis & Gulwani, IJCAI 2017).\n\nThus, the main goal of this work is speeding up the synthesis process on difficult scenarios without sacrificing the generalization accuracy too much. As a result, we achieve on average 50% faster synthesis time, with 10x speed-ups for many difficult tasks that require multiple seconds while still retaining competitive accuracy. Appendix C shows the breakdown of time and accuracy: out of 120 validation/test tasks, there are:\n- 76 tasks where both systems are correct,\n- 7 tasks where PROSE learns a correct program and NGDS learns a wrong one,\n- 4 tasks where PROSE learns a wrong program and NGDS learns a correct one,\n- 33 tasks where both systems are wrong."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1513184772806,"tcdate":1513184772806,"number":1,"cdate":1513184772806,"id":"rJT-R0RZM","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Comment","forum":"rywDjg-RW","replyto":"S1qCIfJWz","signatures":["ICLR.cc/2018/Conference/Paper624/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper624/Authors"],"content":{"title":"Error analysis","comment":"Thank you for the constructive feedback! We’ll add more details and clarify the introduction in the next revision.\n\nQ: Which factors lead to NGDS being slower than PROSE on some tasks?\nOur method is slower than PROSE when the predictions do not satisfy the requirements of the controller i.e. all the predicted scores are within the threshold or they violate the actual scores in branch and bound exploration. This leads to NGDS evaluating the LSTM for branches that were previously pruned. This can be especially harmful when branches that got pruned out at the very beginning of the search need to be reconsidered -- as it could lead to evaluating the network many times. While evaluating the network leads to minor additions in run-time, there are many such additions, and since PROSE performance is already << 1s for such cases, this results in considerable relative slowdown.\n\nWhy do the predictions violate the controller's requirements? This happens when the neural network is either indecisive (its predicted scores for all branches are too close) or wrong (its predicted scores have exactly the opposite order of the actual program scores). \nWe will update the draft with this discussion and present some examples below\n\nSome examples:\nA) \"41.711483001709,-91.4123382568359,41.6076278686523,-91.6373901367188\"  ==>  \"41.711483001709\"\n\tThe intended program is a simple substring extraction. However, at depth 1, the predicted score of Concat is much higher than the predicted score of Atom, and thus we end up exploring only the Concat branch. The found Concat program is incorrect because it uses absolute position indexes and does not generalize to other similar extraction tasks with different floating-point values in the input strings.\nWe found this scenario relatively common when the output string contains punctuation - the model considers it a strong signal for Concat.\nB) \"type size =  36: Bartok.Analysis.CallGraphNode type size =  32: Bartok.Analysis.CallGraphNode CallGraphNode\"  ==> \"36->32\"\n\tWe correctly explore only the Concat branch, but the slowdown happens at the level of the `pos` symbol. There are many different logics to extract the “36” and “32” substrings. NGDS explores RelativePosition branch first, but the score of the resulting program is less then the prediction for RegexPositionRelative. Thus, the B&B controller explores both branches anyway and we end up with a relative slowdown caused by the network inference time."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642480730,"tcdate":1512150737563,"number":3,"cdate":1512150737563,"id":"S1qCIfJWz","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Review","forum":"rywDjg-RW","replyto":"rywDjg-RW","signatures":["ICLR.cc/2018/Conference/Paper624/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Strong paper; accept","rating":"8: Top 50% of accepted papers, clear accept","review":"This is a strong paper. It focuses on an important problem (speeding up program synthesis), it’s generally very well-written, and it features thorough evaluation. The results are impressive: the proposed system synthesizes programs from a single example that generalize better than prior state-of-the-art, and it does so ~50% faster on average.\n\nIn Appendix C, for over half of the tasks, NGDS is slower than PROSE (by up to a factor of 20, in the worst case). What types of tasks are these? In the results, you highlight a couple of specific cases where NGDS is significantly *faster* than PROSE—I would like to see some analysis of the cases were it is slower, as well. I do recognize that in all of these cases, PROSE is already quite fast (less than 1 second, often much less) so these large relative slowdowns likely don’t lead to a noticeable absolute difference in speed. Still, it would be nice to know what is going on here.\n\nOverall, this is a strong paper, and I would advocate for accepting it.\n\n\nA few more specific comments:\n\n\nPage 2, “Neural-Guided Deductive Search” paragraph: use of the word “imbibes” - while technically accurate, this use doesn’t reflect the most common usage of the word (“to drink”). I found it very jarring.\n\nThe paper is very well-written overall, but I found the introduction to be unsatisfyingly vague—it was hard for me to evaluate your “key observations” when I couldn’t quite yet tell what the system you’re proposing actually does. The paragraph about “key observation III” finally reveals some of these details—I would suggest moving this much earlier in the introduction.\n\nPage 4, “Appendix A shows the resulting search DAG” - As this is a figure accompanying a specific illustrative example, it belongs in this section, rather than forcing the reader to hunt for it in the Appendix.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515802555432,"tcdate":1511820079417,"number":2,"cdate":1511820079417,"id":"SkPNib9ez","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Review","forum":"rywDjg-RW","replyto":"rywDjg-RW","signatures":["ICLR.cc/2018/Conference/Paper624/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Incremental paper but well-written","rating":"6: Marginally above acceptance threshold","review":"This paper extends and speeds up PROSE, a programming by example system, by posing the selection of the next production rule in the grammar as a supervised learning problem.\n\nThis paper requires a large amount of background knowledge as it depends on understanding program synthesis as it is done in the programming languages community. Moreover the work mentions a neurally-guided search, but little time is spent on that portion of their contribution. I am not even clear how their system is trained.\n\nThe experimental results do show the programs can be faster but only if the user is willing to suffer a loss in accuracy. It is difficult to conclude overall if the technique helps in synthesis.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642481830,"tcdate":1511518880762,"number":1,"cdate":1511518880762,"id":"SyFsGdSlM","invitation":"ICLR.cc/2018/Conference/-/Paper624/Official_Review","forum":"rywDjg-RW","replyto":"rywDjg-RW","signatures":["ICLR.cc/2018/Conference/Paper624/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Although the search method chosen was reasonable, the only real innovation here is to use the LSTM to learn a search heuristic.","rating":"6: Marginally above acceptance threshold","review":"The paper presents a branch-and-bound approach to learn good programs\n(consistent with data, expected to generalise well), where an LSTM is\nused to predict which branches in the search tree should lead to good\nprograms (at the leaves of the search tree). The LSTM learns from\ninputs of program spec + candidate branch (given by a grammar\nproduction rule) and ouputs of quality scores for programms. The issue\nof how greedy to be in this search is addressed.\n\nIn the authors' set up we simply assume we are given a 'ranking\nfunction' h as an input (which we treat as black-box). In practice\nthis will simply be a guess (perhaps a good educated one) on which\nprograms will perform correctly on future data. As the authors\nindicate, a more ambitious paper would consider learning h, rather\nthan assuming it as a given.\n\nThe paper has a number of positive features. It is clearly written\n(without typo or grammatical problems). The empirical evaluation\nagainst PROSE is properly done and shows the presented method working\nas hoped. This was a competent approach to an interesting (real)\nproblem. However, the 'deep learning' aspect of the paper is not\nprominent: an LSTM is used as a plug-in and that is about it. Also,\nalthough the search method chosen was reasonable, the only real\ninnovation here is to use the LSTM to learn a search heuristic.\n\n\nThe authors do not explain what \"without attention\" means.\n\n\nI think the authors should mention the existence of (logic) program\nsynthesis using inductive logic programming. There are also (closely\nrelated) methods developed by the LOPSTR (logic-based program\nsynthesis and transformation) community. Many of the ideas here are\nreminiscent of methods existing in those communities (e.g. top-down search\nwith heuristics). The use of a grammar to define the space of programs\nis similar to the \"DLAB\" formalism developed by researchers at KU\nLeuven.\n\nADDED AFTER REVISIONS/DISCUSSIONS\n\nThe revised paper has a number of improvements which had led me to give it slightly higher rating.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515186860746,"tcdate":1509129055467,"number":624,"cdate":1509739192577,"id":"rywDjg-RW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rywDjg-RW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples","abstract":"Synthesizing user-intended programs from a small number of input-output exam-\nples is a challenging problem with several important applications like spreadsheet\nmanipulation, data wrangling and code refactoring. Existing synthesis systems\neither completely rely on deductive logic techniques that are extensively hand-\nengineered or on purely statistical models that need massive amounts of data, and in\ngeneral fail to provide real-time synthesis on challenging benchmarks. In this work,\nwe propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique\nthat combines the best of both symbolic logic techniques and statistical models.\nThus, it produces programs that satisfy the provided specifications by construction\nand generalize well on unseen examples, similar to data-driven systems. Our\ntechnique effectively utilizes the deductive search framework to reduce the learning\nproblem of the neural component to a simple supervised learning setup. Further,\nthis allows us to both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness\nof our method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12× speed-up compared to state-of-the-art systems.","pdf":"/pdf/b8d49936f75c9724bf83484c866d42e4a07607d0.pdf","TL;DR":"We integrate symbolic (deductive) and statistical (neural-based) methods to enable real-time program synthesis with almost perfect generalization from 1 input-output example.","paperhash":"anonymous|neuralguided_deductive_search_for_realtime_program_synthesis_from_examples","_bibtex":"@article{\n  anonymous2018neural-guided,\n  title={Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rywDjg-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper624/Authors"],"keywords":["Program synthesis","deductive search","deep learning","program induction","recurrent neural networks"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}