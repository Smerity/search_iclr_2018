{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222618107,"tcdate":1511799251217,"number":3,"cdate":1511799251217,"id":"H1iRKhYxf","invitation":"ICLR.cc/2018/Conference/-/Paper308/Official_Review","forum":"B1uvH_gC-","replyto":"B1uvH_gC-","signatures":["ICLR.cc/2018/Conference/Paper308/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Incremental improvement of an old manifold learning algorithm","rating":"3: Clear rejection","review":"The key contribution of the paper is a new method for nonlinear dimensionality reduction. \n\nThe proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling. While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry. The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets). \n\nThe paper revisits mostly familiar ideas. The importance of preserving both local and global information in manifold learning is well known, so unclear what the main conceptual novelty is. This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution.\n\nMoreover, in this reviewer's experience, the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold, and such an estimation is simply too difficult for any reasonable dataset encountered in practice. However, the authors do not address this, and instead simply use the Isomap approach for approximating geodesics by graph distances, which opens up a completely different set of challenges (how to construct the graph, how to deal with \"holes\" in the manifold, how to avoid short circuiting in the all-pairs shortest path computations etc etc). \n\nFinally, the experimental results are somewhat uninspiring. It seems that the proposed method does roughly as well as Landmark Isomap (with slightly better generalization properties) but is slower by a factor of 1000x. \n\nThe horizon articulation data, as well as the pose articulation data, are both far too synthetic to draw any practical conclusions. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parametric Manifold Learning Via Sparse Multidimensional Scaling","abstract":"We propose a metric-learning framework for computing distance-preserving maps that generate low-dimensional embeddings for a certain class of manifolds. We employ Siamese networks to solve the problem of least squares multidimensional scaling for generating mappings that preserve geodesic distances on the manifold. In contrast to previous parametric manifold learning methods we show a substantial reduction in training effort enabled by the computation of geodesic distances in a farthest point sampling strategy. Additionally, the use of a network to model the distance-preserving map reduces the complexity of the multidimensional scaling problem and leads to an improved non-local generalization of the manifold compared to analogous non-parametric counterparts. We demonstrate our claims on point-cloud data and on image manifolds and show a numerical analysis of our technique to facilitate a greater understanding of the representational power of neural networks in modeling manifold data.","pdf":"/pdf/4dd39b562bc61c6903dcd291f92b06ce5ee666f7.pdf","TL;DR":"Parametric Manifold Learning with Neural Networks in a Geometric Framework ","paperhash":"anonymous|parametric_manifold_learning_via_sparse_multidimensional_scaling","_bibtex":"@article{\n  anonymous2018parametric,\n  title={Parametric Manifold Learning Via Sparse Multidimensional Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1uvH_gC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper308/Authors"],"keywords":["Manifold Learning","Non-linear Dimensionality Reduction","Neural Networks","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222618144,"tcdate":1511796612734,"number":2,"cdate":1511796612734,"id":"rkTKyhFxG","invitation":"ICLR.cc/2018/Conference/-/Paper308/Official_Review","forum":"B1uvH_gC-","replyto":"B1uvH_gC-","signatures":["ICLR.cc/2018/Conference/Paper308/AnonReviewer1"],"readers":["everyone"],"content":{"title":"many small issues and not much novelty","rating":"4: Ok but not good enough - rejection","review":"The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension. They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure. The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework.\n\nThe paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional).\n\nThe abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc.\n\nIn the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities. This is in general not true. Even if the number of parameters is small, learning them might require complex computations on the whole data set. On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N).\n\nIn section 2.1, the authors claim \"Spectral techniques are non-parametric in nature\"; this is wrong again. E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words.\n\nIn section 2.2, it says \"observation that the double centering...\". Can you provide a citation for this?\n\nIn section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity. It is not quite clear from the text what the resulting complexity would be. With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2). Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is.\n\nFigure 3, contrary to text, does not provide a visualisation to the sampling mechanism.\n\nIn the experiments section, can you provide a citation for ADAM and explain how the parameters were selected? Also, it is not meaningful to measure the quality of a visualisation via the MDS fit. There are more useful approaches to this task, such as the quality framework [*].\n\nIn figure 4a, x-axis should be \"number of landmarks\".\n\nIt is not clear why the equation 6 holds. Citation?\nIt is also not clear how exactly the equation 7 is evaluated. It says \"By varying the number of layers and the number of nodes...\", but the nodes and layer are not a part of the equation.\n\nThe notation for equation 8 is not explained.\n\nFigure 6a shows visualisations by different techniques and is evaluated \"by looking at it\". Again, use [*].\n\n[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parametric Manifold Learning Via Sparse Multidimensional Scaling","abstract":"We propose a metric-learning framework for computing distance-preserving maps that generate low-dimensional embeddings for a certain class of manifolds. We employ Siamese networks to solve the problem of least squares multidimensional scaling for generating mappings that preserve geodesic distances on the manifold. In contrast to previous parametric manifold learning methods we show a substantial reduction in training effort enabled by the computation of geodesic distances in a farthest point sampling strategy. Additionally, the use of a network to model the distance-preserving map reduces the complexity of the multidimensional scaling problem and leads to an improved non-local generalization of the manifold compared to analogous non-parametric counterparts. We demonstrate our claims on point-cloud data and on image manifolds and show a numerical analysis of our technique to facilitate a greater understanding of the representational power of neural networks in modeling manifold data.","pdf":"/pdf/4dd39b562bc61c6903dcd291f92b06ce5ee666f7.pdf","TL;DR":"Parametric Manifold Learning with Neural Networks in a Geometric Framework ","paperhash":"anonymous|parametric_manifold_learning_via_sparse_multidimensional_scaling","_bibtex":"@article{\n  anonymous2018parametric,\n  title={Parametric Manifold Learning Via Sparse Multidimensional Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1uvH_gC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper308/Authors"],"keywords":["Manifold Learning","Non-linear Dimensionality Reduction","Neural Networks","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222618183,"tcdate":1511464927949,"number":1,"cdate":1511464927949,"id":"Bku1giNxf","invitation":"ICLR.cc/2018/Conference/-/Paper308/Official_Review","forum":"B1uvH_gC-","replyto":"B1uvH_gC-","signatures":["ICLR.cc/2018/Conference/Paper308/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper proposes a parametric manifold learning method based on deep learning and Siamese networks. Key references are missing (SAMMANN, auto-encoders); Experiments are limited.","rating":"5: Marginally below acceptance threshold","review":"The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks. The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension.\n\nThe paper has several major shortcomings:\n* Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000. Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example. In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE. Please add the Swiss roll example.\n* Distance preservation appears more and more like a dated DR paradigm. Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation. Computation accuracy of the geodesic distances in high-dimensional spaces can be poor. This could be discussed and some experiments on very HD data should be reported.\n* Some key historical references are overlooked, like the SAMMANN. There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago. Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity). Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too.\n* As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach. The motivation in the end of page 3 seems to be computational only.\n* Section 3 should be further detailed (step 2 in particular).\n* The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress. The running times are not in favor of the proposed method. The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold.\n* The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...'). What is the take-home message of the paper?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parametric Manifold Learning Via Sparse Multidimensional Scaling","abstract":"We propose a metric-learning framework for computing distance-preserving maps that generate low-dimensional embeddings for a certain class of manifolds. We employ Siamese networks to solve the problem of least squares multidimensional scaling for generating mappings that preserve geodesic distances on the manifold. In contrast to previous parametric manifold learning methods we show a substantial reduction in training effort enabled by the computation of geodesic distances in a farthest point sampling strategy. Additionally, the use of a network to model the distance-preserving map reduces the complexity of the multidimensional scaling problem and leads to an improved non-local generalization of the manifold compared to analogous non-parametric counterparts. We demonstrate our claims on point-cloud data and on image manifolds and show a numerical analysis of our technique to facilitate a greater understanding of the representational power of neural networks in modeling manifold data.","pdf":"/pdf/4dd39b562bc61c6903dcd291f92b06ce5ee666f7.pdf","TL;DR":"Parametric Manifold Learning with Neural Networks in a Geometric Framework ","paperhash":"anonymous|parametric_manifold_learning_via_sparse_multidimensional_scaling","_bibtex":"@article{\n  anonymous2018parametric,\n  title={Parametric Manifold Learning Via Sparse Multidimensional Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1uvH_gC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper308/Authors"],"keywords":["Manifold Learning","Non-linear Dimensionality Reduction","Neural Networks","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739373156,"tcdate":1509094751620,"number":308,"cdate":1509739370497,"id":"B1uvH_gC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1uvH_gC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Parametric Manifold Learning Via Sparse Multidimensional Scaling","abstract":"We propose a metric-learning framework for computing distance-preserving maps that generate low-dimensional embeddings for a certain class of manifolds. We employ Siamese networks to solve the problem of least squares multidimensional scaling for generating mappings that preserve geodesic distances on the manifold. In contrast to previous parametric manifold learning methods we show a substantial reduction in training effort enabled by the computation of geodesic distances in a farthest point sampling strategy. Additionally, the use of a network to model the distance-preserving map reduces the complexity of the multidimensional scaling problem and leads to an improved non-local generalization of the manifold compared to analogous non-parametric counterparts. We demonstrate our claims on point-cloud data and on image manifolds and show a numerical analysis of our technique to facilitate a greater understanding of the representational power of neural networks in modeling manifold data.","pdf":"/pdf/4dd39b562bc61c6903dcd291f92b06ce5ee666f7.pdf","TL;DR":"Parametric Manifold Learning with Neural Networks in a Geometric Framework ","paperhash":"anonymous|parametric_manifold_learning_via_sparse_multidimensional_scaling","_bibtex":"@article{\n  anonymous2018parametric,\n  title={Parametric Manifold Learning Via Sparse Multidimensional Scaling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1uvH_gC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper308/Authors"],"keywords":["Manifold Learning","Non-linear Dimensionality Reduction","Neural Networks","Unsupervised Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}