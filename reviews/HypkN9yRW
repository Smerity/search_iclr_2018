{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222576337,"tcdate":1512143187818,"number":2,"cdate":1512143187818,"id":"Sk3LFlJZf","invitation":"ICLR.cc/2018/Conference/-/Paper146/Official_Review","forum":"HypkN9yRW","replyto":"HypkN9yRW","signatures":["ICLR.cc/2018/Conference/Paper146/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Presentation of the approach is not clear","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThe paper presents a generic dynamic architecture for CLEVR VQA and Reverse Polish notation problems. Experiments on CLEVR show that the proposed model DDRprog outperforms existing models, but it requires explicit program supervision. The proposed architecture for RPN, called DDRstack outperforms an LSTM baseline.\n\nStrengths:\n— For CLEVR VQA task, the proposed model outperforms the state-of-the-art with significantly less number of parameters.\n— For RPN task, the proposed model outperforms baseline LSTM model by a large margin.\n\nWeaknesses:\n— The paper doesn’t describe the model clearly. After reading the paper, it’s not clear to me what the components of the model are, what each of them take as input and produce as output, what these modules do and how they are combined. I would recommend to restructure the paper to clearly mention each of the components, describe them individually and then explain how they are combined for both cases - DDRprog and DDRstack. \n— Is the “fork” module the main contribution of the paper? If so, at least this should be described in detail. So, if no fork module is required for a question, the model architecture is effectively same as IEP?\n— Machine accuracy is already par with human accuracy on CLEVR and very close to 100%. Why is this problem still important? \n— Given that the performance of state-on-art on CLEVR dataset is already very high ( <5% error) and the performance numbers of the proposed model are not very far from the previous models, it is very important to report the variance in accuracies along with the mean accuracies to determine if the performance of the proposed model is statistically significantly better than the previous models or not.\n— In Figure 4, why are the LSTM32/128 curves different for Length 10 and Length 30 till subproblem index 10? They are both trained on the same training data, only test data is of different length and ideally both models should achieve similar accuracy for the first 10 subproblems (same trend as DDRstack).\n— Why is DDRstack not compared to StackRNN?\n— Can the authors provide training time comparison of their model and other/baseline models? Because that is more important than the number of epochs required in training.\n— There are only 3 curves (instead of 4) in Figure 3.\n— In a number of places, the authors are referring to left and right program branches. What are they? These names have not being defined formally in the paper.\n\nOverall:\nI think the research work in the paper is interesting and significant, but given the current presentation and level of detail in the paper, I don’t think it will be helpful for the research community. By proper restructuring of paper and adding more details, the paper can be converted to a solid submission.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer","abstract":"We present a generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode complex assumptions about the problem data structure. We adapt and apply our model to CLEVR Visual Question Answering, giving rise to the DDRprog architecture; compared to previous approaches, our model achieves higher accuracy in half as many epochs with five times fewer learnable parameters. Our model directly models underlying question logic using a recurrent controller that jointly predicts and executes functional neural modules; it explicitly forks subprocesses to handle logical branching. While FiLM and other competitive models are static architectures with less supervision, we argue that inclusion of program labels enables learning of higher level logical operations--our architecture achieves particularly high performance on questions requiring counting and integer comparison. We further demonstrate the generality of our approach though DDRstack -- an application of our method to reverse Polish notation expression evaluation in which the inclusion of a stack assumption allows our approach to generalize to long expressions, significantly outperforming an LSTM with ten times as many learnable parameters.","pdf":"/pdf/03d440f35a568c0fe9c80189afa2bf676326f281.pdf","TL;DR":"A generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode hard data structure assumptions. Applied to CLEVR VQA and expression evaluation.","paperhash":"anonymous|ddrprog_a_clevr_differentiable_dynamic_reasoning_programmer","_bibtex":"@article{\n  anonymous2018ddrprog:,\n  title={DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypkN9yRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper146/Authors"],"keywords":["CLEVR","VQA","Visual Question Answering","Neural Programmer"]}},{"tddate":null,"ddate":null,"tmdate":1512222576383,"tcdate":1511833864671,"number":1,"cdate":1511833864671,"id":"BJZGbH9lz","invitation":"ICLR.cc/2018/Conference/-/Paper146/Official_Review","forum":"HypkN9yRW","replyto":"HypkN9yRW","signatures":["ICLR.cc/2018/Conference/Paper146/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good performance but requires explicit program supervision to slightly beat the state of the art","rating":"5: Marginally below acceptance threshold","review":"\nSummary: This paper leverages an explicit program format and proposes a stack based RNN to solve question answering. The paper shows state-of-the art performance on the CLEVR dataset.\n\nClarity:\n- The description of the model is vague: I have to looking into appendix on what are the Cell and Controller function.\n- The authors should also improve the intro and related work section. Currently there is a sudden jump between deep learning and the problem of interest. Need to expand the related work section to go over more literature on structured RNN.\n\nPros:\n- The model is fairly easy to understand and it achieves state-of-the-art performance on CLEVR.\n- The model fuses text and image features in a single model.\n\nCons:\n- This paper doesn’t mention recursive NN (Socher et al., 2011) and Tree RNN (Tai et al., 2015). I think they have fairly similar structure, at least conceptually, the stack RNN can be thought as a tree parser. And since the push/pop operations are static (based on the inputs), it’s no more different than encoding the question structure in the tree edges.\n- The IEP (Cells) module (Johnson et al., 2017) seems to do all the heavy-lifting in my opinion. That’s why the proposed method only uses 9M parameters. The task isn’t very challenging to learn because all the stack operations are already given. Table 1 should note clearly which methods use problem specific parsing information to train and which use raw text. Based on my understanding of FiLM at least, they use raw words instead of groundtruth parse trees. So it’s not very surprising that the proposed method can outperform FiLM (by a little bit).\n- I don’t fully agree with the title - the stack operations are not differentiable. So whatever network that outputs the stack operation cannot be jointly learned with gradients. This is based on the if-else statements I see in Algorithm 1.\n\nConclusion:\n- Since the novelty is limited and it requires explicit program supervision, and the performance is only on par with the state-of-the-art (FiLM), I am not convinced that this paper brings enough contribution to be accepted. Weak reject.\n\nReferences:\n- Socher, R., Lin, C., Ng, A.Y., Manning, C.D. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. The 28th International Conference on Machine Learning (ICML 2011).\n- Tai, K.S., Socher, R., Manning C.D. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. The 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015).","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer","abstract":"We present a generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode complex assumptions about the problem data structure. We adapt and apply our model to CLEVR Visual Question Answering, giving rise to the DDRprog architecture; compared to previous approaches, our model achieves higher accuracy in half as many epochs with five times fewer learnable parameters. Our model directly models underlying question logic using a recurrent controller that jointly predicts and executes functional neural modules; it explicitly forks subprocesses to handle logical branching. While FiLM and other competitive models are static architectures with less supervision, we argue that inclusion of program labels enables learning of higher level logical operations--our architecture achieves particularly high performance on questions requiring counting and integer comparison. We further demonstrate the generality of our approach though DDRstack -- an application of our method to reverse Polish notation expression evaluation in which the inclusion of a stack assumption allows our approach to generalize to long expressions, significantly outperforming an LSTM with ten times as many learnable parameters.","pdf":"/pdf/03d440f35a568c0fe9c80189afa2bf676326f281.pdf","TL;DR":"A generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode hard data structure assumptions. Applied to CLEVR VQA and expression evaluation.","paperhash":"anonymous|ddrprog_a_clevr_differentiable_dynamic_reasoning_programmer","_bibtex":"@article{\n  anonymous2018ddrprog:,\n  title={DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypkN9yRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper146/Authors"],"keywords":["CLEVR","VQA","Visual Question Answering","Neural Programmer"]}},{"tddate":null,"ddate":null,"tmdate":1509739460835,"tcdate":1509037029491,"number":146,"cdate":1509739458174,"id":"HypkN9yRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HypkN9yRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer","abstract":"We present a generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode complex assumptions about the problem data structure. We adapt and apply our model to CLEVR Visual Question Answering, giving rise to the DDRprog architecture; compared to previous approaches, our model achieves higher accuracy in half as many epochs with five times fewer learnable parameters. Our model directly models underlying question logic using a recurrent controller that jointly predicts and executes functional neural modules; it explicitly forks subprocesses to handle logical branching. While FiLM and other competitive models are static architectures with less supervision, we argue that inclusion of program labels enables learning of higher level logical operations--our architecture achieves particularly high performance on questions requiring counting and integer comparison. We further demonstrate the generality of our approach though DDRstack -- an application of our method to reverse Polish notation expression evaluation in which the inclusion of a stack assumption allows our approach to generalize to long expressions, significantly outperforming an LSTM with ten times as many learnable parameters.","pdf":"/pdf/03d440f35a568c0fe9c80189afa2bf676326f281.pdf","TL;DR":"A generic dynamic architecture that employs a problem specific differentiable forking mechanism to encode hard data structure assumptions. Applied to CLEVR VQA and expression evaluation.","paperhash":"anonymous|ddrprog_a_clevr_differentiable_dynamic_reasoning_programmer","_bibtex":"@article{\n  anonymous2018ddrprog:,\n  title={DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HypkN9yRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper146/Authors"],"keywords":["CLEVR","VQA","Visual Question Answering","Neural Programmer"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}