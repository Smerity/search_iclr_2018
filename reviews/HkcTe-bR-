{"notes":[{"tddate":null,"ddate":null,"tmdate":1516287747762,"tcdate":1516287747762,"number":5,"cdate":1516287747762,"id":"ryhZvNRNM","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Comment","forum":"HkcTe-bR-","replyto":"HJlpDGpEG","signatures":["ICLR.cc/2018/Conference/Paper646/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper646/Authors"],"content":{"title":"\"Relevant\" paper was published one month after ours.","comment":"The first version our paper was uploaded to openreview on 27th Oct 2017.\n\nThe paper by Popova et al. was put on arXiv on 29th Nov 2017, that is about a month later than our paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516214200175,"tcdate":1516214200175,"number":1,"cdate":1516214200175,"id":"HJlpDGpEG","invitation":"ICLR.cc/2018/Conference/-/Paper646/Public_Comment","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relevant paper not cited","comment":"There is a recently published paper on molecular design with deep reinforcement learning which is not addressed in your work:\nPopova, Mariya, Olexandr Isayev, and Alexander Tropsha. \"Deep reinforcement learning for de-novo drug design.\" arXiv preprint arXiv:1711.10907 (2017).\nThis paper discusses alternative to GANs method of novel compounds generation with recurrent neural network and reinforce algorithm, which is relevant to your work. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515178353596,"tcdate":1515178353596,"number":4,"cdate":1515178353596,"id":"r19_YHamz","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Comment","forum":"HkcTe-bR-","replyto":"rkUfmabyM","signatures":["ICLR.cc/2018/Conference/Paper646/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper646/Authors"],"content":{"title":"We have adapted the manuscript to make the contributions and scope clearer","comment":"\nWe thank the reviewer for their critical feedback. We have adapted the manuscript to make the contributions and scope clearer.\n\nAlgorithms that allow the generation of small molecules (small graphs) that satisfy given desirable properties are rapidly evolving for medicine, materials and agriculture.  This is currently an area of intense investigation, theoretically as well as practically, as highlighted by several ICLR submissions on molecule/graph generation this year, which all employ different and inconsistent benchmarks and training sets. Currently, it is not possible to compare these results.\n\nWith this paper, we unify the problem space of small molecule generation and thoroughly investigate various approaches (including algorithms which have yet to be explored in this domain, e.g. PPO), and include evaluations of our new benchmarks on pre-existing work.\n\nOur results surpass state-of-the-art results previously reported on a few of the sub-domains, establishing new baselines, and come to the perhaps surprising result that the relatively simple hill-climbing MLE method achieves results on par with the some of the most advanced recently-developed RL algorithms such as PPO.\n\n\nSpecific Comments:\n\n* Regarding lack of focus: we have reorganized the paper to improve the clarity of the work.\n  \n* Regarding purpose: We hope the above responses address your concern of this being a survey work. We believe this work introduces new benchmarks, evaluates pre-existing algorithms, demonstrates novel pairings of algorithm and domain, and establishes a new state-of-the art as well as a few surprising additional insights (hill-climbing MLE supremacy, temperature ineffectiveness).\n\n* Regarding preprocessing steps: the steps taken in this work are standard and in line with the field of computational chemistry [1,2]. This includes the removal of Sodium, Calcium and Potassium, and other counterions.\n\n* Regarding clarity of RL vs. train/test set: for algorithms that rely on pretraining to help navigate the extremely large space of small molecule generation (~10^60), it is important that the algorithms have not been exposed to a correct solution in the training set, hence the train-test split.  As an additional benefit, a train/test split permits the benchmarks to be used with rule-based GOFAI systems, supervised algorithms as well as RL.\n\n* Regarding molecular property prediction: indeed, this is an important sub-field of computational chemistry and is explored under the family of QSAR models [3, 4].  However, that is out-of-scope of this paper, as it attempts to address a different concern.\n\n* Regarding data: the table is a bit overwhelming already, so we chose not to exhaustively show all results for all models and instead focus on representative key models.  Due to time and computational constraints, we had not run more than three initializations, but can do so for the revision.\n\n\nWe hope this addresses your concerns.\n\n[1] Glaab, Enrico. \"Building a virtual ligand screening pipeline using free software: a survey.\" Briefings in bioinformatics 17.2 (2015): 352-366.\n[2] Lionta, Evanthia, et al. \"Structure-based virtual screening for drug discovery: principles, applications and recent advances.\" Current topics in medicinal chemistry 14.16 (2014): 1923-1938.\n[3] Tropsha, Alexander. \"Best practices for QSAR model development, validation, and exploitation.\" Molecular informatics 29.6‐7 (2010): 476-488.\n[4] Tropsha, Alexander, and Alexander Golbraikh. \"Predictive QSAR modeling workflow, model applicability domains, and virtual screening.\" Current pharmaceutical design 13.34 (2007): 3494-3504.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515177053671,"tcdate":1515177053671,"number":3,"cdate":1515177053671,"id":"HyIP4S6QM","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Comment","forum":"HkcTe-bR-","replyto":"rkejdYtxz","signatures":["ICLR.cc/2018/Conference/Paper646/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper646/Authors"],"content":{"title":"Reply","comment":"\nWe are grateful for your comments.  We hope your concern about novelty is addressed with our main comment; indeed, the pairing here is in the algorithm to this particular application area.  \nWe further hope that a foundational framework proposed will allow the emergence of future, novel algorithms.  Your minor comments have been addressed in the manuscript.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515176879086,"tcdate":1515176879086,"number":2,"cdate":1515176879086,"id":"H1P3mSpQf","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Comment","forum":"HkcTe-bR-","replyto":"S1ZlQfqeM","signatures":["ICLR.cc/2018/Conference/Paper646/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper646/Authors"],"content":{"title":"Reply","comment":"We thank the referee for their comments and perspective on our work. \n\nWe hope this reviewer’s comments have been addressed in our overall reply and in the responses to the other reviewers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515176918154,"tcdate":1515176801677,"number":1,"cdate":1515176801677,"id":"Hkqv7raQz","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Comment","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper646/Authors"],"content":{"title":"Global Reply","comment":"\nWe thank the reviewers for their effort and advice towards improving our submission.  We are pleased that the reviewers have identified our important contributions of dataset curation and preprocessing steps, proposed benchmarks, and baseline results using recently-developed algorithms.  While we introduce no new reinforcement learning algorithms in this work, our primary aim was to substantially lower the barrier towards automated molecular design to allow computer scientists with no prior background in chemistry to develop novel algorithms to improve molecular design.  Indeed, here the novelty lies in the pairing of task and algorithm, and this work is foundational to clearly lay out steps and provide code to apply reinforcement learning algorithms to molecule design.  \n\nFinally, we are able to demonstrate results in this manuscript that establish a new state-of-the-art in single and multi objective physicochemical property optimization and chemical space exploration tasks.  Subsequent work can then build on this set of standardized molecular design benchmarks to introduce new methods. The benchmark framework is general enough to be used with any possible small molecule generation method, whether rule-based or learned, and is not limited to sequence-based generation relying on SMILES. We have therefore amended the manuscript to reflect the importance of our introduced benchmarks.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642484944,"tcdate":1511822057453,"number":3,"cdate":1511822057453,"id":"S1ZlQfqeM","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Review","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/AnonReviewer3"],"readers":["everyone"],"content":{"title":"empirical evaluation of recurrent models and RL for molecule design","rating":"6: Marginally above acceptance threshold","review":"Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules. The experiments seem extensive, using many recently proposed RL methods, and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception.  \n\nOriginality and significance: \n\nThe conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically. \n\nThe paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniques.\n\nClarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642484980,"tcdate":1511786648334,"number":2,"cdate":1511786648334,"id":"rkejdYtxz","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Review","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is a solid paper about model evaluation in the chemical domain. ","rating":"7: Good paper, accept","review":"Summary:\nThis work is about model evaluation for molecule generation and design. 19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design.\n\non the positive side:\nThe paper is well written, quality and clarity of the work are good. The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation. It is investigated how several RL strategies perform on a large, standardized data set. Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed.  An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym. \n\n\non the negative side:\nThere is no new novel contribution on the methods side.  \n\n\n\nminor comments:\n\nSection 2.1. \nsee Fig.2 —> see Fig.1\npage 4just before equation 8: the the","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642485016,"tcdate":1510228750269,"number":1,"cdate":1510228750269,"id":"rkUfmabyM","invitation":"ICLR.cc/2018/Conference/-/Paper646/Official_Review","forum":"HkcTe-bR-","replyto":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference/Paper646/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a set of benchmarks for molecular design, and compares different deep models against them. The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks. The paper does not seem to include any method development.\n\nThe paper suffers from a lack of focus. Several existing models are discussed to some length, while the benchmarks are introduced quite shortly. The dataset is not very clearly defined: it seems that there are 1.2 million training instance, does this apply for all benchmarks? The paper's title also does not seem to fit: this feels like a survey paper, which is not reflected in the title. Biologically lots of important atoms are excluded from the dataset, for instance natrium, calcium and kalium. I don't see any reason to exlude these. What does \"biological activities on 11538 targets\" mean? \n\nThe paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used. Are the test molecules somehow generated in a directed or undirected fashion? Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion? There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties. Currently it's unclear which type of problems are solved here.\n\nTable 1 lists 5 models, while fig 3 contains 7, why the discrepancy? In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83). Variances should be added, and preferably more than 3 initialisations used.\n\nOverall this is an interesting paper, but does not have any methodological contribution, and there is also few insightful results about the compared methods, nor is there meaningful analysis of the problem domain of molecules either.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515176536136,"tcdate":1509130433985,"number":646,"cdate":1509739180434,"id":"HkcTe-bR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkcTe-bR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design","abstract":"The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works.  This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design.  The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry.  Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.","pdf":"/pdf/5d588e627b43b3439f7fef2564614907bc5766ca.pdf","TL;DR":"We investigate a variety of RL algorithms for molecular generation and define new benchmarks (to be released as an OpenAI Gym), finding PPO and a hill-climbing MLE algorithm work best.","paperhash":"anonymous|exploring_deep_recurrent_models_with_reinforcement_learning_for_molecule_design","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkcTe-bR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper646/Authors"],"keywords":["reinforcement learning","molecule design","de novo design","ppo","sample-efficient reinforcement learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}