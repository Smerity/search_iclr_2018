{"notes":[{"tddate":null,"ddate":null,"tmdate":1512314762407,"tcdate":1512314762407,"number":1,"cdate":1512314762407,"id":"SkGcPcZ-z","invitation":"ICLR.cc/2018/Conference/-/Paper517/Public_Comment","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Statement about the KL divergence term in VAEs","comment":"You state in the paper that the variational auto-encoder objective is composed of reconstruction cost plus a KL divergence term that captures how distinct the image by the encoder of each training example is from the prior p(z), and then go on to say that this KL term is not guaranteeing that the overall encoded distribution matches the prior p(z).\n\nHowever, as shown in the paper \"ELBO surgery: yet another way to carve up the variational evidence lower bound\" by Hoffman and Johnson, the KL term in the VAE objective can be decomposed into exactly this KL(q(z)||p(z)) between the average encoder distribution and the prior plus a mutual information term, and that the former is a heavy contributor towards the overall KL term. This means that VAE does indeed try to match the overall encoder distribution of q to the prior, but also includes a regularizing term that aims to minimize the mutual information between the hidden code z and the index of the observation x that encourages the VAE to have the encoder produce the same codes z for different observations.\n\nIn conclusion, it would be more accurate to state that in comparison to VAEs you simply exclude the mutual information regularisation term from the objective as formulated in the ELBO surgery paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/f2ab8cdf4e28419d476e6820f2d914ae8d4b21c0.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512256147793,"tcdate":1512256147793,"number":3,"cdate":1512256147793,"id":"SJncf2gWz","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Review","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference/Paper517/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A well-written paper that generalizes Wasserstein distance to VAEs ","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric. By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework. In my opinion, this is one of the crucial contributions of this paper. While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture. The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons. In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations.\n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al. which also attempts at unifying VAEs and GANs. While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/f2ab8cdf4e28419d476e6820f2d914ae8d4b21c0.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222678625,"tcdate":1511970931556,"number":2,"cdate":1511970931556,"id":"Hk2dO8ngz","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Review","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference/Paper517/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Excellent tutorial papers with novel contributions and convincing results","rating":"8: Top 50% of accepted papers, clear accept","review":"This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y.\n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution.\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max\n- The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival\n\nThe experiments are very convincing, both numerically and visually.\n\nSource of confusion: in algorithm 1 and 2, \\tilde{z} is \"sampled\" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/f2ab8cdf4e28419d476e6820f2d914ae8d4b21c0.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222678670,"tcdate":1511716362715,"number":1,"cdate":1511716362715,"id":"SJQzLO_gM","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Review","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference/Paper517/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This is a well-written paper which provides a useful generalisation of some existing methods for inferring generative models.","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper satisfies the following necessary conditions for\nacceptance. The writing is clear and I was able to understand the\npresented method (and its motivation) despite not being too familiar\nwith the relevant literature. Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful. I found no technical\nerrors. The problem addressed is one worth solving - building a\ngenerative model of observed data. There is some empirical testing\nwhich show the presented method in a good light.\n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE. I suppose one could argue that the\nclose connection to existing methods means that this paper is not\ninnovative enough. I think that would be unfair - most new methods\nhave close relations with existing ones - it is just that sometimes\nthe authors do not flag this up as they should.\n\nWAE is a bit oversold. The authors state that WAE generates \"samples\nof better quality\" (than VAE) without any condition being put on when\nit does this. There is no proof that it is always better, and I can't\nsee how there could be. Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions. Surely one could\ndevise situations where VAE outperforms WAE. I think this issue should\nhave been examined in more depth.\n\nI found no typo or grammatical errors which is unusual - good careful\njob!\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/f2ab8cdf4e28419d476e6820f2d914ae8d4b21c0.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739259525,"tcdate":1509125149607,"number":517,"cdate":1509739256869,"id":"HkL7n1-0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/f2ab8cdf4e28419d476e6820f2d914ae8d4b21c0.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}