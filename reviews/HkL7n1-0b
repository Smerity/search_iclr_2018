{"notes":[{"tddate":null,"ddate":null,"tmdate":1516644640373,"tcdate":1516644640373,"number":4,"cdate":1516644640373,"id":"Sy_QFsmHG","invitation":"ICLR.cc/2018/Conference/-/Paper517/Public_Comment","forum":"HkL7n1-0b","replyto":"HyBIaDXBM","signatures":["~Min_Lin1"],"readers":["everyone"],"writers":["~Min_Lin1"],"content":{"title":"Clarification","comment":"Let me clarify the markov chain point.\n\nIn the case Q(Z|X) is stochastic, the encode/decode chain X->Z->X' is stochastic. Namely, P(X'|X) is not a deterministic function, it is a distribution. A markov chain can be constructed if we sample X from P_X and use P(X'|X) as the transition probability.\n\nBy optimizing the Wasserstein distance between P(X') and P_X, we hope to get the parameter such that P(X') == P_X. The reconstruction term in this paper requires that X' == X, which is stronger than P(X') == P_X."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1516629324913,"tcdate":1516629324913,"number":5,"cdate":1516629324913,"id":"HyBIaDXBM","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Comment","forum":"HkL7n1-0b","replyto":"rJSDX-xSG","signatures":["ICLR.cc/2018/Conference/Paper517/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper517/Authors"],"content":{"title":"Regarding random / deterministic encoders in WAE","comment":"Thank you for the question.\n\nUnfortunately, we did not quite get the point of your Markov chain example. But we would like to make it clear that the paper does not assume anything specific about the encoder Q(Z|X). As long as the aggregated posterior Qz matches the prior Pz, the encoder can be either deterministic or random. The same holds true for the WAE algorithm. We will try to emphasize it better in the updated version of the paper.\n\nThe decoder is indeed a different story: for Theorem 1 we need it to be deterministic, but a very similar result holds also for the random decoder (Supplementary B)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1516405677420,"tcdate":1516405597208,"number":3,"cdate":1516405597208,"id":"rJSDX-xSG","invitation":"ICLR.cc/2018/Conference/-/Paper517/Public_Comment","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["~Min_Lin1"],"readers":["everyone"],"writers":["~Min_Lin1"],"content":{"title":"Possibility of a Markov Chain instead of Reconstruction.","comment":"Thanks for the great work. It's nice to see there is theoretical support for the (auto-encoder + constraint on Z) objective.\n\nIt seems to me the expectation over X could not be moved out in theorem 1, as this breaks the independence of Z and X.\nConsider the case Q(Z|X) is not deterministic, we can have a markov chain X_{t+1} ~ \\int_Z [ P_G(X'|Z)Q(Z|X_t) ], which has a stationary distribution same as P_X.  The algorithm in this paper gives a special case where Q(Z|X) is deterministic and X_{t+1} = X_{t}.\n\nIn supplementary B, the case where the decoder is random is discussed. It would be nice to also discuss the cases where Q(Z|X) is random vs deterministic.\n\nDo correct me if I'm wrong, thanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1513679235667,"tcdate":1513678621985,"number":4,"cdate":1513678621985,"id":"BkU7vv8ff","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Comment","forum":"HkL7n1-0b","replyto":"SkxfpL8GG","signatures":["ICLR.cc/2018/Conference/Paper517/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper517/Authors"],"content":{"title":"Assumptions clarified","comment":"Dear Mathieu,\n\nthank you for the suggestion. We will update the paper accordingly."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1513676040473,"tcdate":1513676040473,"number":2,"cdate":1513676040473,"id":"SkxfpL8GG","invitation":"ICLR.cc/2018/Conference/-/Paper517/Public_Comment","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["~Mathieu_Blondel1"],"readers":["everyone"],"writers":["~Mathieu_Blondel1"],"content":{"title":"Assumptions of Theorem 1 should be clarified","comment":"Congratulations on this nice paper. \n\nThe ability to remove one of the two marginal constraints in Theorem 1 relies on the assumption that P_G(Y|Z=z) is a Dirac. I know that you stated in the intro that you focus on deterministic maps but it would be nice to repeat the assumptions made in Theorem 1."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1513373069341,"tcdate":1513373069341,"number":3,"cdate":1513373069341,"id":"BkrqpnbGG","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Comment","forum":"HkL7n1-0b","replyto":"SJQzLO_gM","signatures":["ICLR.cc/2018/Conference/Paper517/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper517/Authors"],"content":{"title":"Answer to AnonReviewer2","comment":"We are pleased that the reviewer found the paper well written. \n\nWe tried to be modest in our claims, in particular we never implied that WAEs produce better samples for *all data distributions*. As noticed by the reviewer this would be indeed impossible to prove, especially because the question of how to evaluate and compare sample qualities of unsupervised generative models is still open. We will double-check that there are no bold and unsupported statements in the final version of the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1513373114249,"tcdate":1513372980414,"number":2,"cdate":1513372980414,"id":"BJ2VpnZff","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Comment","forum":"HkL7n1-0b","replyto":"Hk2dO8ngz","signatures":["ICLR.cc/2018/Conference/Paper517/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper517/Authors"],"content":{"title":"Answer to AnonReviewer1","comment":"We thank the reviewer for the positive feedback and the kind words regarding the overview part of the paper.\n\nWe will make sure to make notations clearer and include all the details of architectures used in experiments in the updated version of the paper. Of course we will also open source the code."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1513372936746,"tcdate":1513372936746,"number":1,"cdate":1513372936746,"id":"H1bGp3bfz","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Comment","forum":"HkL7n1-0b","replyto":"SJncf2gWz","signatures":["ICLR.cc/2018/Conference/Paper517/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper517/Authors"],"content":{"title":"Answer to AnonReviewer3","comment":"We thank the reviewer for the positive feedback. \n\nComparing properties of WAE-MMD and WAE-GAN is indeed an intriguing direction and we intend to look into the details in our future research. In this paper we only report initial empirical observations, which can be concluded by saying that WAE-MMD enjoys a stable training but does not match Pz and Qz perfectly, while the training of WAE-GAN is not so stable but leads to much better matches once succeeded. \n\nIn this paper we decided that comparing to VAE was sufficient for our purposes: both VAE and AVB follow the same objective of maximizing the marginal log likelihood in contrast to the minimization of the optimal transport studied in our work. However, we do agree that in future it would be interesting to compute the FID scores of the AVB samples. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512314762407,"tcdate":1512314762407,"number":1,"cdate":1512314762407,"id":"SkGcPcZ-z","invitation":"ICLR.cc/2018/Conference/-/Paper517/Public_Comment","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Statement about the KL divergence term in VAEs","comment":"You state in the paper that the variational auto-encoder objective is composed of reconstruction cost plus a KL divergence term that captures how distinct the image by the encoder of each training example is from the prior p(z), and then go on to say that this KL term is not guaranteeing that the overall encoded distribution matches the prior p(z).\n\nHowever, as shown in the paper \"ELBO surgery: yet another way to carve up the variational evidence lower bound\" by Hoffman and Johnson, the KL term in the VAE objective can be decomposed into exactly this KL(q(z)||p(z)) between the average encoder distribution and the prior plus a mutual information term, and that the former is a heavy contributor towards the overall KL term. This means that VAE does indeed try to match the overall encoder distribution of q to the prior, but also includes a regularizing term that aims to minimize the mutual information between the hidden code z and the index of the observation x that encourages the VAE to have the encoder produce the same codes z for different observations.\n\nIn conclusion, it would be more accurate to state that in comparison to VAEs you simply exclude the mutual information regularisation term from the objective as formulated in the ELBO surgery paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642459783,"tcdate":1512256147793,"number":3,"cdate":1512256147793,"id":"SJncf2gWz","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Review","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference/Paper517/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A well-written paper that generalizes Wasserstein distance to VAEs ","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric. By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework. In my opinion, this is one of the crucial contributions of this paper. While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture. The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons. In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations.\n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al. which also attempts at unifying VAEs and GANs. While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642459820,"tcdate":1511970931556,"number":2,"cdate":1511970931556,"id":"Hk2dO8ngz","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Review","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference/Paper517/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Excellent tutorial papers with novel contributions and convincing results","rating":"8: Top 50% of accepted papers, clear accept","review":"This very well written paper covers the span between W-GAN and VAE. For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations. The appendix are very useful, and tutorial paper material (especially A). \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work. There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous. One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y.\n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1. I could not verify its novelty, but this seems to be a great contribution.\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max\n- The use of MMD in the context of GANs has also been tried. The authors claim that their use in the latent space makes it more practival\n\nThe experiments are very convincing, both numerically and visually.\n\nSource of confusion: in algorithm 1 and 2, \\tilde{z} is \"sampled\" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642459859,"tcdate":1511716362715,"number":1,"cdate":1511716362715,"id":"SJQzLO_gM","invitation":"ICLR.cc/2018/Conference/-/Paper517/Official_Review","forum":"HkL7n1-0b","replyto":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference/Paper517/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This is a well-written paper which provides a useful generalisation of some existing methods for inferring generative models.","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper satisfies the following necessary conditions for\nacceptance. The writing is clear and I was able to understand the\npresented method (and its motivation) despite not being too familiar\nwith the relevant literature. Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful. I found no technical\nerrors. The problem addressed is one worth solving - building a\ngenerative model of observed data. There is some empirical testing\nwhich show the presented method in a good light.\n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE. I suppose one could argue that the\nclose connection to existing methods means that this paper is not\ninnovative enough. I think that would be unfair - most new methods\nhave close relations with existing ones - it is just that sometimes\nthe authors do not flag this up as they should.\n\nWAE is a bit oversold. The authors state that WAE generates \"samples\nof better quality\" (than VAE) without any condition being put on when\nit does this. There is no proof that it is always better, and I can't\nsee how there could be. Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions. Surely one could\ndevise situations where VAE outperforms WAE. I think this issue should\nhave been examined in more depth.\n\nI found no typo or grammatical errors which is unusual - good careful\njob!\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1513679686964,"tcdate":1509125149607,"number":517,"cdate":1509739256869,"id":"HkL7n1-0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkL7n1-0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Wasserstein Auto-Encoders","abstract":"We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE).\nThis regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality.","pdf":"/pdf/dfb7680ba058c276d33cc67ae946aacea30dae87.pdf","TL;DR":"We propose a new auto-encoder based on the Wasserstein distance, which improves on the sampling properties of VAE.","paperhash":"anonymous|wasserstein_autoencoders","_bibtex":"@article{\n  anonymous2018wasserstein,\n  title={Wasserstein Auto-Encoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkL7n1-0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper517/Authors"],"keywords":["auto-encoder","generative models","GAN","VAE","unsupervised learning"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}