{"notes":[{"tddate":null,"ddate":null,"tmdate":1515072672635,"tcdate":1515072672635,"number":3,"cdate":1515072672635,"id":"B1Fj2sjmf","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Comment","forum":"SJw03ceRW","replyto":"r1R0L9Def","signatures":["ICLR.cc/2018/Conference/Paper359/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper359/Authors"],"content":{"title":"Regarding the Drop in accuracy on base classes","comment":"Thank you for emphasizing the topic of performance drop when adding new classes.\nThe drop in the base classes is common in all of the competing methods, e.g. the  Soft-Dis/ICarl Adaptation where the base classes weights are adapted.\nIt is expected that as the number of categories increase, and classes that are similar to one another are introduced, there will be a drop in classification accuracy, even for a fully trained network. Please notice that the proposed method outperforms the competing methods in most cases.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1515072616952,"tcdate":1515072616952,"number":2,"cdate":1515072616952,"id":"BJWd2ismz","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Comment","forum":"SJw03ceRW","replyto":"r1W-h8Flz","signatures":["ICLR.cc/2018/Conference/Paper359/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper359/Authors"],"content":{"title":"Experiments focus","comment":"We thank the reviewer. We would like to draw the reviewer’s attention to the following points:\n\n“Influence of forgetting on base classes:“\nThe average accuracy of the base classes is presented in APPENDIX B BASE & NOVEL CLASSES TEST ERROR and in Figure 6,7,8,  In which we demonstrate the accuracy of the base classes as a function of the number of novel samples. We present the base class and novel class accuracy in each of the designed scenarios: generic classes from imagenet, Domain specific with similar novel classes,  Domain specific with similar class in base.\n\n“Influence of the number of components used in the GMM:”\nIn section 4.5 RESULTS: DEEP-FEATURES GMM EVALUATION \nwe’ve explored the effect of different number of GMM components on the ability to restore and re-create the accuracy on the base classes. We’ve explored the effect of GMM components on 5 imagenet based dataset and 3 based UT-Zappos50K dataset.\n\n“The influence of the low-shot:”\nIn Sections 4.3 and 4.4 the effect of the number of novel samples on the performance is presented. We’ve experimented with a range of number of novel samples.\n\n“The dropout rate:”\nIn Section 4.2.3 GRADIENT DROPOUT, we describe the experiment to evaluate the effect of Gradient Dropout in Low-Shot Expansion. We compare the results obtained with and without the use of gradient dropout, those are presented in Figures 2,3,4. Due to the already dense experimental section we did not add experiments done on various dropout ratios, though we agree that this is an interesting question.\n \n\n“The second major drawback is that the experimental setting seems very unrealistic: 5 base classes and 2 novel classes:” \nPlease see answer above (to first reviewer) regarding the design of benchmark.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1515072485221,"tcdate":1515072485221,"number":1,"cdate":1515072485221,"id":"SyTJhosXM","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Comment","forum":"SJw03ceRW","replyto":"BJK7re9ez","signatures":["ICLR.cc/2018/Conference/Paper359/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper359/Authors"],"content":{"title":"Design considerations of the proposed benchmark","comment":"We would like to thank the reviewers for their commentary. We ask to draw the reviewer's attention to the following:\n\nIn this work, we focus on a robotic unit in real-life scenarios. It is often desired to be able to adapt to a single or two new classes available with only very few samples. We choose to establish the proposed benchmark for the task of Low-Shot Network expansion in a manner that will reflect this task. That is, we wanted the number of novel classes to be relatively small so the effect of Network adaptation to small quantities of new data can be studied. \n\nWe defined the class (base + novel) average accuracy to be our performance metric.  \nWe did not want the base classes accuracy to have overwhelmly more weight than the novel classes in the average accuracy metric, hence we defined the number of base classes to be in a range similar to the number of novel classes. We’ve composed a dataset with a common cardinality (similar to CIFAR10 is O(10), MNIST is O(10) and SVHN is O(10)). While O(10) is considerably smaller than imagenet 1000 classes classification task. It is still a common classification task, which is also common in robotic unit applications. \n\nIn order to avoid bias in the constructed dataset, we’ve performed numerous random experiment, and reported their average result: \n \nThe test on imagenet partitions was done by randomly selecting 50 classes, and then randomly partitioning to 5 groups of 10 , which are then further randomly partitioned to 5 base and 5 novel classes. The result of every experiment done with a given number of novel samples is averaged on 25 = 5X5 trails. That is Figure 2 is the result of 125 tests = 25 averaging X 5 #novel samples.\n\nFigure 5a Is the result of 25 (base,novel group avg) X 8 #novel samples = 200 trails.\n\nIn this unbiased test case, we addressed 3 typical scenarios: generic classes from imagenet, Domain specific with similar novel classes,  Domain specific with similar class in base. We’ve further explored the effect and gain of Gen-LSNE compared to other methods in each of these scenarios. We concentrated our effort to analyze and explore the qualities of the proposed method in scenarios that are common in robotic unit real-life scenarios, and to the best of our knowledge the designed benchmark realistically reflect those.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1515642438452,"tcdate":1511814433024,"number":3,"cdate":1511814433024,"id":"BJK7re9ez","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Review","forum":"SJw03ceRW","replyto":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference/Paper359/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper proposes a network/classifier expansion method to learn to classify with additional novel  classes in the future, without re-training with all the original data. It fine tunes the new parameters added with the new data (from novel classes), and with sampled examples from  simple generative models of the old classes. Overall the paper has a simple idea which is validated on limited settings (~10 classes only). ","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a method for adapting a pre-trained network, trained on a fixed number of\nclasses, to incorporate novel classes for doing classification, especially when the novel classes\nonly have a few training examples available. They propose to do a `hard' distillation, i.e. they\nintroduce new nodes and parameters to the network to add the new classes, but only fine-tune the new\nnetworks without modifying the original parameters. This ensures that, in the new expanded and\nfine-tuned network, the class confusions will only be between the old and new classes and not\nbetween the old classes, thus avoiding catastrophic forgetting. In addition they use GMMs trained on\nthe old classes during the fine-tuning process, thus avoiding saving all the original training data.\nThey show experiments on public benchmarks with three different scenarios, i.e.  base and novel\nclasses from different domains, base and novel classes from the same domain and novel classes have\nsimilarities among themselves, and base and novel classes from the same domain and each novel class\nhas similarities with at least one of the base class.                        \n                                                                             \n- The paper is generally well written and it is clear what is being done     \n- The idea is simple and novel; to the best of my knowledge it has not been tested before\n- The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation\n  (iCARL; where all weights are fine-tuned). The proposed method performs better in low-shot\n  settings and comparably when large number of training examples of the novel classes are available\n- My main criticism will be the limited dataset size on which the method is validated. The ILSVRC12\n  subset contains 5 base and 5 novel classes and the UT-Zappos50K subset also has 10 classes. The\n  idea is simple and novel, which is good, but the validation is limited and far from any realistic\n  use. Having only O(10) classes is not convincing, especially when the datasets used do have large\n  number of classes. I agree that this will not allow or will takes some involved manual effort to\n  curate subsets for the settings proposed, but it is necessary for being convincing.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1515642438489,"tcdate":1511775225175,"number":2,"cdate":1511775225175,"id":"r1W-h8Flz","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Review","forum":"SJw03ceRW","replyto":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference/Paper359/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting, yet (by far) not sufficiently explored","rating":"4: Ok but not good enough - rejection","review":"The goal of this paper is to study generalisation to novel classes. This paper stipulates some interesting ideas, using an idea of expansion layers (using a form of hard distillation, where the weights of known classes are fixed), a GMM to model the already learned classes (to reduce storage), and a form of gradient dropout (updating just a subset of the weights using a dropout mask). All of these assume a fixed representation, trained on the base classifier, then only the final classification layer is adjusted for the novel examples. \n\nThe major drawback is that none of these ideas are fully explored. Given fixed representation, for example the influence of forgetting on base classes, the number of components used in the GMM, the influence of the low-shot, the dropout rate, etc etc.  The second major drawback is that the experimental setting seems very unrealistic: 5 base classes and 2 novel classes. \n\nTo conclude: the ideas in this paper are very interesting, but difficult to gather insights given the focus of the experiments.\n\nMinor remarks\n- Sect 4.1 \"The randomly ... 5 novel classes\" is not a correct sentence.\n- The extended version of NCM (4.2.1), here uses as prototype-kNN (Hastie 2001) has also been explored in the paper of NCM, using k-means per class to extract prototypes.\n- Given fixed representations, plenty of work has focused on few-shot (linear) learning, this work should be compared to these. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1515642438524,"tcdate":1511659222261,"number":1,"cdate":1511659222261,"id":"r1R0L9Def","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Review","forum":"SJw03ceRW","replyto":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference/Paper359/AnonReviewer3"],"readers":["everyone"],"content":{"title":"a paper proposing hard-distillation for few-shot learning ","rating":"6: Marginally above acceptance threshold","review":"On few-shot learning problem, this paper presents a simple yet powerful distillation method where the base network is augmented with additional weights to classify the novel classes, while keeping the weights of the base network unchanged. Thus the so-called hard distillation is proposed. This paper is well-written and well organized. The good points are as follows,\n\n1. The paper proposes a well-performance method for the important low-shot learning problem based on the transform learning.\n2. The Gen-LSNE maintains a small memory footprint using a generative model for base examples and requires a few more parameters to avoid overfitting and take less time to train.\n3. This paper builds up a benchmark for low-shot network expansion.\n\nThere are some problems,\n1. There still is drop in accuracy on the base classes after adding new classes, and the accuracy may still drop as adding more classes due to the fixed parameters corresponding to the base classes. This is slightly undesired.\n2. Grammatical mistake: page 3, line 5(“a additional layers”)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1509739344982,"tcdate":1509104846800,"number":359,"cdate":1509739342325,"id":"SJw03ceRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}