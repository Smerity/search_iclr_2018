{"notes":[{"tddate":null,"ddate":null,"tmdate":1515263097145,"tcdate":1515263097145,"number":2,"cdate":1515263097145,"id":"H1-Y4cCmz","invitation":"ICLR.cc/2018/Conference/-/Paper37/Official_Comment","forum":"rkhxwltab","replyto":"SyRcMDPeG","signatures":["ICLR.cc/2018/Conference/Paper37/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper37/Authors"],"content":{"title":"Clarifications","comment":"Thank you for the review. \n\nI would like to address the weaknesses that have been pointed out and try to give an explanation for them.\n\n1.) The choice of a baseline was a bit unclear, since all the recorded models present on the MNIST leaderboard only perform classification and do not have the reconstruction module through the same network. Besides, I perceive that comparing just the forward performance, as I have mentioned in the paper, is a bit unfair in this case.\n\n2.) The new revision of the paper has now included the details about the test set results. \n\n3.) I have presented a by eye comparison since quantitatively measuring the likeness among the reconstructed images and the original images is mathematically challenging, whilst also being susceptible to the pixel level difference in the noise smoothing caused by the reconstruction network. It has been mentioned in the paper, that a metric that takes into consideration all these factors while evaluating the backward performance faithfully is needed.\n\n4.) The mention of interpretable encodings (representations) is made since through this free normalization loss function, all the information about the digit's positions, orientations, thickness and curvedness is summerized along a positive real number range. \n\nThank you."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AANN: Absolute Artificial Neural Network","abstract":"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","pdf":"/pdf/9839a8adb077ae40368c6ff4c15a3a4dc67c95ca.pdf","TL;DR":"Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.","paperhash":"anonymous|aann_absolute_artificial_neural_network","_bibtex":"@article{\n  anonymous2018aann:,\n  title={AANN: Absolute Artificial Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhxwltab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper37/Authors"],"keywords":["Neural Network architecture","Learned representation space","absolute valued function","bidirectional neuron"]}},{"tddate":null,"ddate":null,"tmdate":1515261879049,"tcdate":1515261879049,"number":1,"cdate":1515261879049,"id":"rJy6y907M","invitation":"ICLR.cc/2018/Conference/-/Paper37/Official_Comment","forum":"rkhxwltab","replyto":"S1itI_FxM","signatures":["ICLR.cc/2018/Conference/Paper37/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper37/Authors"],"content":{"title":"Clarifications","comment":"To state explicitly what the intended features are:\n\n1.) Use of abs function (which has been duly noted)\n\n2.) Use of free normalization in objective function definition. It has been seen that usually the loss (objective) function is defined not only such that the prediction is close to the actual label but also so that the probabilities of other labels are minimised. My proposal is to let the network only focus on getting the correct label right while the latter is taken care of automatically if reconstruction is to be done through the same network.\n\n3.) The hypothesis that the process of reconstruction should be symbiotic with the process of classification / prediction and not adversarial. In the paper (Sabour et. al. 2017) the CapsNet uses a separate fully connected reconstruction module and uses the reconstruction loss as a regularizer similar to the technique described in this paper. By simply summing the reconstruction loss with the objective function, the process of learning becomes more symbiotic.\n\n4.) From the visualization of the reconstructed digits from the encodings, it can be seen that the forward classification function just doesn't learn discrete mappings of input - output pairs, but learns a smooth function that encodes different positions, orientations, thickness and curvedness of the input digits along a simple positive real number range.\n\n5.) All these code implementations and visualization videos are there, but I couldn't mention them in the paper due to the anonymity clause.\n\n6.) The intention of using the term 'bidirectional artificial neuron' was to give a simpler perspective at an Autoencoder. It is not different from an Autoencoder, it is merely a simpler explanatory view of it which I put forth through the article.\n\nThank you for the reviews. All the comments are very helpful and strengthen my further work. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AANN: Absolute Artificial Neural Network","abstract":"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","pdf":"/pdf/9839a8adb077ae40368c6ff4c15a3a4dc67c95ca.pdf","TL;DR":"Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.","paperhash":"anonymous|aann_absolute_artificial_neural_network","_bibtex":"@article{\n  anonymous2018aann:,\n  title={AANN: Absolute Artificial Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhxwltab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper37/Authors"],"keywords":["Neural Network architecture","Learned representation space","absolute valued function","bidirectional neuron"]}},{"tddate":null,"ddate":null,"tmdate":1515642440071,"tcdate":1511843144612,"number":3,"cdate":1511843144612,"id":"B1J8Hw9xz","invitation":"ICLR.cc/2018/Conference/-/Paper37/Official_Review","forum":"rkhxwltab","replyto":"rkhxwltab","signatures":["ICLR.cc/2018/Conference/Paper37/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting ideas but not fully explored","rating":"6: Marginally above acceptance threshold","review":"This paper introduces a reversible network with absolute value used as\nthe activation function.  The network is run in the forward direction\nto classify and in the reverse direction to generate.\n\nThe key points of the network are the use of the absolute value\nactivation function and the use of (free) normalization to match\ntarget output. This allows the network to perfectly map inputs to any\npoint on a vector that goes through the one-hot encoding, allowing for\ndeterministic generation from different vectors (of different lengths)\nwith the same normalized output.\n\nI think there are a lot of novel and interesting ideas in this paper\nthough they have not been fully explored.  The use of the absolute\nvalue transfer function is new to me, though I was able to find a couple of old\nreferences to its use.   In a paper by Gad et al. (2000), it is stated \n\" For example, the algorithm presented in Lin and\nUnbehauen (1995) < I think they mean Lin and Unbehauen 1990)> \n is used to train networks with a single hidden layer\nemploying the absolute value as the activation function of the hidden\nneuron. This algorithm was further generalized to multilayer networks\nwith cascaded structures in Batruni (1991).\"   Exploring the properties \nof the abs activation function seems worth exploring.\n\nMore details on the training are needed for full clarity in the paper.\n(Though it is recognized that some of these could be determined from\nlinks when made active, they should be included in the paper).  How\ndid you select the training parameters given at the bottom of page 5?\nHow many layers and units/layer did you use? And how were these\nselected?  (The use of the links for providing code and visualizations (when active)\n is a nice feature of this paper).\n\nAlso, did you compare to using the leaky ReLU activation function --\nThat would be interesting as it also doesn't have any areas of zero\nslope?  Did you compare the generated digits to those obtained using GANs?\n\nI am also curious, how does accuracy on digit classification differ\nwhen trained only to optimize the forward error?\n\nThe MNIST site referenced lists 60,000 training data and test data of\n10,000.  How/why did you select 42,000 and then split it to 39900 in\nthe train set and 2100 in the dev set?\n\nAlso, the goal for the paper is presented as creating highly\ninterpretable representations of the input data.  My interpretation of\ninterpretable is that the hidden units are \"interpretable\" and that it\nis clear how the combined hidden unit representations allow for\naccurate classification.  Towards that end, it would be nice to see\nsome of the interpretations of the hidden unit representations.  In\nthe abstract it states \" ...These representations are generated by\npenalizing the learning of the network in such a way that those\nlearned representations correspond to the respective labels present in\nthe labelled dataset used for supervised training\".  Does this\nstatement refer only to the encoding of the representation vector or\nalso the hidden layers?  If the former, isn't that true for all\nsupervised algorithms.  If the latter, you should show this.\n\nBatruni, R. (1991). A multilayer neural network with piecewise-linear\nstructure and backpropagation learning. IEEE Transactions on Neural\nNetworks, 2, 395–403.\n\nLin, J.-N., & Unbehauen, R. (1995). Canonical piecewise-linear neural\nnetworks. IEEE Transactions on Neural Networks, 6, 43–50.\n\nLin, J.-N, & Unbehauen, R. (1990). Adaptive Nonlinear Digital Filter with Canonical Piecewise-Linear Structure,\nIEEE Transactions on Circuits and Systems, 37(3) 347-353.\n\nGad, E.F et al (2000). A new algorithm for learning in piecewise-linear neural networks.\nNeural Networks 13,  485-505.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AANN: Absolute Artificial Neural Network","abstract":"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","pdf":"/pdf/9839a8adb077ae40368c6ff4c15a3a4dc67c95ca.pdf","TL;DR":"Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.","paperhash":"anonymous|aann_absolute_artificial_neural_network","_bibtex":"@article{\n  anonymous2018aann:,\n  title={AANN: Absolute Artificial Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhxwltab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper37/Authors"],"keywords":["Neural Network architecture","Learned representation space","absolute valued function","bidirectional neuron"]}},{"tddate":null,"ddate":null,"tmdate":1515642440110,"tcdate":1511782019462,"number":2,"cdate":1511782019462,"id":"S1itI_FxM","invitation":"ICLR.cc/2018/Conference/-/Paper37/Official_Review","forum":"rkhxwltab","replyto":"rkhxwltab","signatures":["ICLR.cc/2018/Conference/Paper37/AnonReviewer3"],"readers":["everyone"],"content":{"title":"incremental idea, insufficient experimental evaluation","rating":"3: Clear rejection","review":"The paper proposes using the absolute value activation function in (what seems to be) an autoencoder architecture with an additional supervised learning term in the objective function that encourages the bottleneck layer representation to be discriminative. A few examples of reconstructed images and classification performance are reported for the MNIST dataset.\n\nThe contribution of the paper is not clear. The idea of combining autoencoders with supervised learning has been explored before, see e.g., \"Learning Deep Architectures for AI\" by Bengio, 2009, and many other papers. Alternative activation functions have also been studied in many papers, see https://arxiv.org/pdf/1710.05941.pdf for a recent example. Even without novel algorithmic contributions, the paper would have been interesting if there was an extensive evaluation across several challenging datasets of different ways of combining autoencoders with supervised learning and different activation functions that gives better insight into what works and why.\n\nIt would be helpful not to introduce new terminology like \"bidirectional artificial neuron\" unless there is a significant difference from existing concepts. It is not clear from the paper how a network of bidirectional neurons is different from an autoencoder.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AANN: Absolute Artificial Neural Network","abstract":"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","pdf":"/pdf/9839a8adb077ae40368c6ff4c15a3a4dc67c95ca.pdf","TL;DR":"Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.","paperhash":"anonymous|aann_absolute_artificial_neural_network","_bibtex":"@article{\n  anonymous2018aann:,\n  title={AANN: Absolute Artificial Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhxwltab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper37/Authors"],"keywords":["Neural Network architecture","Learned representation space","absolute valued function","bidirectional neuron"]}},{"tddate":null,"ddate":null,"tmdate":1515642440145,"tcdate":1511645846270,"number":1,"cdate":1511645846270,"id":"SyRcMDPeG","invitation":"ICLR.cc/2018/Conference/-/Paper37/Official_Review","forum":"rkhxwltab","replyto":"rkhxwltab","signatures":["ICLR.cc/2018/Conference/Paper37/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Preliminary report on a classification/reconstruction network with absolute value activation function","rating":"2: Strong rejection","review":"SUMMARY \n\nThe model is an ANN whose units have the absolute value function abs as their activation function (in place of ReLU, sigmoid, etc.). The network has bi-directional connections (with equal weights) between consecutive layers, but it operates only in one direction at a time. In the forward direction, it is a feed-forward net from image to classification (say); in the reverse direction, it is a feed-forward net from classification to image. In both directions it operates in supervised fashion, trained with backpropagation (subject to the constraint that the weight matrix is symmetric). In the forward direction, the activation vector y over the classification layer is L2-normalized so the activation of a class c is the cosine of the angle between y and the 1-hot vector for c.\nAlthough there is a reverse pass through the net, the training loss function is not adversarial; the loss is just the classification error in the forward pass plus the reconstruction error in the backward pass.\nThe generalization accuracy in classification on 42k-image MNIST is 97.4%.\n\nSTRENGTHS\n\n* Comparisons are made of the reconstruction performance with the proposed abs activation function and with ReLU on one or both passes, and a linear activation function.\n* The model has the virtue of simplicity, as the authors point out.\n\nWEAKNESSES\n\n* The discussion of evaluation of the model is weak. \n  - No baselines are given. (A kaggle leaderboard shows the 50th-ranked model at 97.8% and the top 8 models at 100%.)\n  - The paper talks of a training set and a \"dev\" set, but no test set, and generalization performance is given for the dev set rather than a test set.\n  - No quantitative evaluation of the reconstruction (backward pass) performance is given, just by-eye comparison of the reconstruction error through figures.\n  - Some explanation is needed of why the ReLU cases were fatally plagued with NaN errors.\n* Claims of interpretability advantage seem unwarranted since the claimed interpretability applies to any classification ANN, as far as I can see.\n* The work seems to be at too preliminary a stage to warrant acceptance at ICLR.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AANN: Absolute Artificial Neural Network","abstract":"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","pdf":"/pdf/9839a8adb077ae40368c6ff4c15a3a4dc67c95ca.pdf","TL;DR":"Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.","paperhash":"anonymous|aann_absolute_artificial_neural_network","_bibtex":"@article{\n  anonymous2018aann:,\n  title={AANN: Absolute Artificial Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhxwltab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper37/Authors"],"keywords":["Neural Network architecture","Learned representation space","absolute valued function","bidirectional neuron"]}},{"tddate":null,"ddate":null,"tmdate":1515260291350,"tcdate":1508603636433,"number":37,"cdate":1509739516123,"id":"rkhxwltab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkhxwltab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"AANN: Absolute Artificial Neural Network","abstract":"This research paper describes a simplistic architecture named as AANN: Absolute Artificial Neural Network, which can be used to create highly interpretable representations of the input data. These representations are generated by penalizing the learning of the network in such a way that those learned representations correspond to the respective labels present in the labelled dataset used for supervised training; thereby, simultaneously giving the network the ability to classify the input data. The network can be used in the reverse direction to generate data that closely resembles the input by feeding in representation vectors as required. This research paper also explores the use of mathematical abs (absolute valued) functions as activation functions which constitutes the core part of this neural network architecture. Finally the results obtained on the MNIST dataset by using this technique are presented and discussed in brief.","pdf":"/pdf/9839a8adb077ae40368c6ff4c15a3a4dc67c95ca.pdf","TL;DR":"Tied weights auto-encoder with abs function as activation function, learns to do classification in the forward direction and regression in the backward direction due to specially defined cost function.","paperhash":"anonymous|aann_absolute_artificial_neural_network","_bibtex":"@article{\n  anonymous2018aann:,\n  title={AANN: Absolute Artificial Neural Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhxwltab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper37/Authors"],"keywords":["Neural Network architecture","Learned representation space","absolute valued function","bidirectional neuron"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}