{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222690574,"tcdate":1511893479273,"number":3,"cdate":1511893479273,"id":"H1Je5milG","invitation":"ICLR.cc/2018/Conference/-/Paper557/Official_Review","forum":"H1rRWl-Cb","replyto":"H1rRWl-Cb","signatures":["ICLR.cc/2018/Conference/Paper557/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good analysis. Contributions are not so clear though.","rating":"5: Marginally below acceptance threshold","review":"\n- I think that VAEs are rather forced to be interpreted from an information theoretic point of view for the sake of it, rather than for the sake of a clear and unequivocal contribution from the perspective of VAEs and latent-variable models themselves. How is that useful for a VAE? \n\n- \"The left vertical line corresponds to the zero rate setting. ...\": All these limits are again from an information theoretic point of view and no formulation nor demonstration is provided on how this can actually be as useful. As mentioned earlier in the paper, there are well-known problems with taking this information theory perspective, e.g. difficulties in estimating MI values, etc.\n\n- Breaking (some of) the long sentences and paragraphs in page 3 with an unequivocal mathematical formulation would smooth the flow a bit.\n\n- \"(2) an upper bound that measures the rate, or how costly it is to transmit information about the latent variable.\": I am not entirely sure about this one and why it is massively important to be compromised against the obviously big first term.\n\n- Toy Model experiment: I do not see any indication of how this is not just a lucky catch and that VAEs consistently suffer from a problem leading to such effect.\n\n- Section 5: \"can shed light on many different models and objectives that have been proposed in the literature... \": Again the contribution aspect is not so clear through the word \"shed light\".\n\n\nMinor\n- Although apparently VAEs represent the main and most influential latent-variable model example, I think switching too much between citing them as VAEs and then as latent-variable models in general was a bit confusing. I propose mentioning in the beginning (as happened) that VAEs are the seminal example of latent-variable models and then going on from this point onwards with VAEs without too much alternation between latent-variable models and VAEs.\n\n- page 8: \"as show\"","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An information-theoretic analysis of deep latent-variable models","abstract":"We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.","pdf":"/pdf/a4faf871c00982428fdd631074c9a965a8f9ed52.pdf","TL;DR":"We provide an information theoretic and experimental analysis of state-of-the-art variational autoencoders.","paperhash":"anonymous|an_informationtheoretic_analysis_of_deep_latentvariable_models","_bibtex":"@article{\n  anonymous2018an,\n  title={An information-theoretic analysis of deep latent-variable models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1rRWl-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper557/Authors"],"keywords":["information theory","generative models","latent variable models","variational autoencoders"]}},{"tddate":null,"ddate":null,"tmdate":1512222690617,"tcdate":1511814163080,"number":2,"cdate":1511814163080,"id":"HJoM4eclG","invitation":"ICLR.cc/2018/Conference/-/Paper557/Official_Review","forum":"H1rRWl-Cb","replyto":"H1rRWl-Cb","signatures":["ICLR.cc/2018/Conference/Paper557/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review: Valuable formulation of VAE training tradeoffs","rating":"7: Good paper, accept","review":"\nSummary\n\nThis paper proposes assessing VAEs via two quantities: rate R (E[ KLD[q(z|x) || p(z)] ]) and distortion D (E[ log p(x|z) ]), which can be used to bound the mutual information (MI) I(x,z) from above and below respectively (i.e. H[x] - D <= I(x,z) <= R).  This fact then implies the inequality H[x] <= R + D, where H[x] is the entropy of the true data distribution, and allows for the construction of a phase diagram (Figure 1) with R and D on the x and y axis respectively.  Models can be plotted on the diagram to show the degree to which they favor reconstruction (D) or sampling diversity (R).  The paper then reports several experiments, the first being a simulation to show that a VAE trained with vanilla ELBO cannot recover the true rate in even a 1D example.  For the second experiment, 12 models are trained by varying the encoder/decoder strength (CNN vs autoregressive) and prior (fact. Gauss vs autoregressive vs VampPrior).  Plots of the D vs R and ELBO vs R are shown for the models, revealing that the same ELBO value can be decomposed into drastically different R and D values.  The point is further made through qualitative results in Figure 4.   \n\n\nEvaluation\n\nPros:  While no one facet of the paper is particularly novel (as similar observations and discussion has been made by [1-4]), the paper, as far as I’m aware, is the first to formally decompose the ELBO into the R vs D tradeoff, which is natural.  As someone who works with VAEs, I didn’t find the conclusions surprising, but I imagine the paper would be valuable to someone learning about VAEs.  Moreover, it’s nice to have a clear reference for the unutilized-latent-space-behavior mentioned in various other VAE papers.  The most impressive aspect of the paper is the number of models trained for the empirical investigation.  Placing such varied models (CNN vs autoregressive vs VampPrior etc) onto the same plot from comparison (Figure 3) is a valuable contribution.       \n\nCons:  As mentioned above, I didn’t find the paper conceptually novel, but this isn’t a significant detraction as its value (at least for VAE researchers) is primarily in the experiments (Figure 3).  I do think the paper---as the ‘Discussion and Further Work’ section is only two sentences long---could be improved by providing a better summary of the findings and recommendations moving forward.  Should generative modeling papers be reporting final R and D values in addition to marginal likelihood?  How should an author demonstrate that their method isn’t doing auto-decoding?  The conclusion claims that “[The rate-distortion tradeoff] provides new methods for training VAE-type models which can hopefully advance the state of the art in unsupervised representation learning.”  Is this referring to the constrained optimization problem given in Equation #4?  It seems to me that the optimal R-vs-D tradeoff is application dependant; is this not always true?      \n\nMiscellaneous / minor comments:  Figure 3 would be easier to read if the dots better reflected their corresponding tuple (although I realize representing all tuple combinations in terms of color, shape, etc is hard).  I had to keep referring to the legend, losing my place in the scatter plot.  I found sections 1 and 2 rather verbose; I think some text could be cut to make room for more final discussion / recommendations.  For example, I think the first two whole paragraphs could be cut or at least condensed and moved to the related works section, as they are just summarizing research history/trends.  The paper’s purpose clearly starts at the 3rd paragraph (“We are interested in understanding…”).  The references need cleaned-up.  There are several conference publications that are cited via ArXiv instead of the conference (IWAE should be ICLR, Bowman et al. should be CoNLL, Lossy VAE should be ICLR, Stick-Breaking VAE should be ICLR, ADAM should be ICLR, Inv Autoregressive flow should be NIPS, Normalizing Flows should be ICML, etc.), and two different versions of the VAE paper are cited (ArXiv and ICLR).  \n\n\nConclusions\n\nI found this paper to present valuable analysis of the ELBO objective and how it relates to representation learning in VAEs.  I recommend the paper be accepted, although it could be substantially improved by including more discussion at the end.  \n\n\n\n1.  S. Zhao, J. Song, and S. Ermon.  “InfoVAE: Information Maximizing Variational Autoencoders.”  ArXiv 2017.\n\n2.  X. Chen, D. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Shulman, I. Sutskever, and P. Abbeel.  “Variational Lossy Autoencoder.”  ICLR 2017.\n\n3.  I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. “Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.”  ICLR 2017\n\n4.  S. Bowman, L. Vilnis, O. Vinyas,  A. Dai, R. Jozefowicz, and S. Bengio.  “Generating Sentences from a Continuous Space.”  CoNLL 2016.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An information-theoretic analysis of deep latent-variable models","abstract":"We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.","pdf":"/pdf/a4faf871c00982428fdd631074c9a965a8f9ed52.pdf","TL;DR":"We provide an information theoretic and experimental analysis of state-of-the-art variational autoencoders.","paperhash":"anonymous|an_informationtheoretic_analysis_of_deep_latentvariable_models","_bibtex":"@article{\n  anonymous2018an,\n  title={An information-theoretic analysis of deep latent-variable models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1rRWl-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper557/Authors"],"keywords":["information theory","generative models","latent variable models","variational autoencoders"]}},{"tddate":null,"ddate":null,"tmdate":1512222690664,"tcdate":1511724959553,"number":1,"cdate":1511724959553,"id":"SkPsD5dxz","invitation":"ICLR.cc/2018/Conference/-/Paper557/Official_Review","forum":"H1rRWl-Cb","replyto":"H1rRWl-Cb","signatures":["ICLR.cc/2018/Conference/Paper557/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Potentially interesting but insights not clear","rating":"5: Marginally below acceptance threshold","review":"Summary:\n\nThis paper optimizes the beta-VAE objective and analyzes the resulting models in terms of the two components of the VAE loss: the reconstruction error (which the authors refer to as distortion, “D”) and the KL divergence term (which the authors refer to as rate, “R”). Various VAEs using either PixelCNN++ or a simpler model for the encoder, decoder, or marginal distribution of a VAE are trained on MNIST (with some additional results on OMNIGLOT) and analyzed in terms of samples, reconstructions, and their rate-distortion trade-off.\n\nReview:\n\nI find it difficult to point my finger to novel conceptual or theoretical insights in this paper. The idea of maximizing information for unsupervised learning of representations has of course been explored a lot (e.g., Bell & Sejnowski, 1995). Deeper connections between variational inference and rate-distortion have been made before (e.g., Balle et al., 2017; Theis et al., 2017), while this paper merely seems to rename the reconstruction and KL terms of the ELBO. Variational lower and upper bounds on mutual information have been used before as well (e.g., Barber & Agakov, 2003; Alemi et al., 2017), although they are introduced like new results in this paper. The derived “sandwich equation” only seems to be used to show that H - D - R <= 0, which also follows directly from Gibbs’ inequality (since the left-hand side is a negative KL divergence). The main contributions therefore seem to be the proposed analysis of models in the R-D plane, and the empirical contribution of analyzing beta-VAEs.\n\nBased on the R-D plots, the authors identify a potential problem of generative models, namely that none of the trained models appear to get close to the “auto-encoding limit” where the distortion is close zero. Wouldn’t this gap easily be closed by a model with identity encoder, identity decoder, and PixelCNN++ for the marginal distribution? Given that autoregressive models generally perform better than VAEs in terms of log-likelihood, the model’s performance would probably be closer to the true entropy than the ELBO plotted in Figure 3a). What about increasing the capacity of the used in this paper? This makes me wonder what exactly the R-D plot can teach us about building better generative models.\n\nThe toy example in Figure 2 is interesting. What does it tell us about how to build our generative models? Should we be using powerful decoders but a lower beta?\n\nThe authors write: “we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable”. Yet in Figure 3b) it appears that changing the rate of a model can influence the generative performance (ELBO) quite a bit?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An information-theoretic analysis of deep latent-variable models","abstract":"We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.","pdf":"/pdf/a4faf871c00982428fdd631074c9a965a8f9ed52.pdf","TL;DR":"We provide an information theoretic and experimental analysis of state-of-the-art variational autoencoders.","paperhash":"anonymous|an_informationtheoretic_analysis_of_deep_latentvariable_models","_bibtex":"@article{\n  anonymous2018an,\n  title={An information-theoretic analysis of deep latent-variable models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1rRWl-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper557/Authors"],"keywords":["information theory","generative models","latent variable models","variational autoencoders"]}},{"ddate":null,"tddate":1510843659934,"tmdate":1510852297908,"tcdate":1510843557830,"number":1,"cdate":1510843557830,"id":"S10iNmsJM","invitation":"ICLR.cc/2018/Conference/-/Paper557/Public_Comment","forum":"H1rRWl-Cb","replyto":"H1rRWl-Cb","signatures":["~Ethan_Fetaya4"],"readers":["everyone"],"writers":["~Ethan_Fetaya4"],"content":{"title":"Great work, overly complex proof of B.3 and B.4 ","comment":"Really enjoyed your paper, gave very useful insights. \n\nOne small thing, the proofs for the optimal encoder/decoder are much more complicated then they need be. The only inequality is from the KL divergence \"positive semidefinite quality\", so the bound is tight exactly when the KL divergence is zero i.e. when the probabilities (a.s.) match and that is all you need."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An information-theoretic analysis of deep latent-variable models","abstract":"We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.","pdf":"/pdf/a4faf871c00982428fdd631074c9a965a8f9ed52.pdf","TL;DR":"We provide an information theoretic and experimental analysis of state-of-the-art variational autoencoders.","paperhash":"anonymous|an_informationtheoretic_analysis_of_deep_latentvariable_models","_bibtex":"@article{\n  anonymous2018an,\n  title={An information-theoretic analysis of deep latent-variable models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1rRWl-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper557/Authors"],"keywords":["information theory","generative models","latent variable models","variational autoencoders"]}},{"tddate":null,"ddate":null,"tmdate":1509739237383,"tcdate":1509126604859,"number":557,"cdate":1509739234727,"id":"H1rRWl-Cb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1rRWl-Cb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"An information-theoretic analysis of deep latent-variable models","abstract":"We present an information-theoretic framework for understanding trade-offs in unsupervised learning of deep latent-variables models using variational inference. This framework emphasizes the need to consider latent-variable models along two dimensions: the ability to reconstruct inputs (distortion) and the communication cost (rate). We derive the optimal frontier of generative models in the two-dimensional rate-distortion plane, and show how the standard evidence lower bound objective is insufficient to select between points along this frontier. However, by performing targeted optimization to learn generative models with different rates, we are able to learn many models that can achieve similar generative performance but make vastly different trade-offs in terms of the usage of the latent variable. Through experiments on MNIST and Omniglot with a variety of architectures, we show how our framework sheds light on many recent proposed extensions to the variational autoencoder family.","pdf":"/pdf/a4faf871c00982428fdd631074c9a965a8f9ed52.pdf","TL;DR":"We provide an information theoretic and experimental analysis of state-of-the-art variational autoencoders.","paperhash":"anonymous|an_informationtheoretic_analysis_of_deep_latentvariable_models","_bibtex":"@article{\n  anonymous2018an,\n  title={An information-theoretic analysis of deep latent-variable models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1rRWl-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper557/Authors"],"keywords":["information theory","generative models","latent variable models","variational autoencoders"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}