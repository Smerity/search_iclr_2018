{"notes":[{"tddate":null,"ddate":null,"tmdate":1512420511268,"tcdate":1512420511268,"number":3,"cdate":1512420511268,"id":"r1Dj4EXbf","invitation":"ICLR.cc/2018/Conference/-/Paper364/Official_Review","forum":"SkHkeixAW","replyto":"SkHkeixAW","signatures":["ICLR.cc/2018/Conference/Paper364/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interetsing review but not appropriate for ICLR proceedings","rating":"4: Ok but not good enough - rejection","review":"The main aim of ICLR conference, at least as it is written on its website, is to provide new results on theories, methods and algorithms, supporting further breakthroughs in AI and DL.\n\nIn this respect the authors of the paper claim that their “systematic approach enables the discovery of new, improved regularization methods by combining the best properties of the existing ones.”\n\nHowever, the authors did not provide any discoveries concerning new approaches to regularisation supporting this claim. Thus, the main contribution of the paper is that the authors made a review and performed classification of available regularisation methods. So, the paper is in fact a survey paper, which is more appropriate for full-scale journals. The work, developed by the authors, is really big. However, I am not sure it will bring a lot of benefits for readers except those who need review for some reports, introductions in PhD thesis, etc.\n\nAlthough the authors mentioned some approaches to combine different regularisations, they did not performed any experiments supporting their ideas.\n\nThus, I think that\n- the paper is well written in general,\n- it can be improved (by taking into account several important comments from the Reviewer 2) and served as a review paper in some appropriate journal,\n- the paper is not suited for ICLR proceedings due to reasons, mentioned above.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularization for Deep Learning: A Taxonomy","abstract":"Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.","pdf":"/pdf/5438d7312501470559e1b4765fac7d4f147165e0.pdf","TL;DR":"Systematic categorization of regularization methods for deep learning, revealing their similarities.","paperhash":"anonymous|regularization_for_deep_learning_a_taxonomy","_bibtex":"@article{\n  anonymous2018regularization,\n  title={Regularization for Deep Learning: A Taxonomy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHkeixAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper364/Authors"],"keywords":["neural networks","deep learning","regularization","data augmentation","network architecture","loss function","dropout","residual learning","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222626176,"tcdate":1511838546578,"number":2,"cdate":1511838546578,"id":"HysLX85lf","invitation":"ICLR.cc/2018/Conference/-/Paper364/Official_Review","forum":"SkHkeixAW","replyto":"SkHkeixAW","signatures":["ICLR.cc/2018/Conference/Paper364/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper attempts to build a taxonomy for regularization techniques employed in deep learning. The authors categorize existing works related to regularization into five big categories, including data, model architecture, regularization term, error term and optimization. Subgroups are identified based on certain attributes. \n\nThe paper is written as a survey paper on literatures related to regularization in deep learning. The five top-level categories are quite obvious.  The authors organize works belonging to the first three categories into three big tables, and summarizing the key point of each one using one-liners to provide an overview for readers. While it is a worthy effort, I am not sure it offers much value to readers. Also there is a mix of trainability and regularization. Some of the works were proposed to address trainability issues instead of regularization, for example, densenet, and some of the initialization techniques. \n\nThe authors try to group items in each category into sub-groups according to certain attributes, however,  little explanation on how and why these attributes are identified was provided. For example, in table 1, what kind of information does the transformation space or phase provide in terms of helping readers choosing a particular data transformation / augmentation technique. At the end of section 3, the authors claim that dropout, BN are close to each other. Please elaborate on this point. \n\nThe authors offer some recommendation on how to choose or combine different regularization techniques at the end of the paper. However, it is not clear from reading the paper where these insights came from. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularization for Deep Learning: A Taxonomy","abstract":"Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.","pdf":"/pdf/5438d7312501470559e1b4765fac7d4f147165e0.pdf","TL;DR":"Systematic categorization of regularization methods for deep learning, revealing their similarities.","paperhash":"anonymous|regularization_for_deep_learning_a_taxonomy","_bibtex":"@article{\n  anonymous2018regularization,\n  title={Regularization for Deep Learning: A Taxonomy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHkeixAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper364/Authors"],"keywords":["neural networks","deep learning","regularization","data augmentation","network architecture","loss function","dropout","residual learning","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222626211,"tcdate":1511557982764,"number":1,"cdate":1511557982764,"id":"rywwiW8xz","invitation":"ICLR.cc/2018/Conference/-/Paper364/Official_Review","forum":"SkHkeixAW","replyto":"SkHkeixAW","signatures":["ICLR.cc/2018/Conference/Paper364/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of 'Regularization for DL: a taxonomy'","rating":"5: Marginally below acceptance threshold","review":"This paper is unusual in that it is more of a review than contributing novel knowledge. It considers a taxonomy of all the ways that machine learning (mostly deep learning) methods can achieve a form of regularization. \n\nUnfortunately, it starts with a definition of regularization ('making the model generalize better') which I believe misses the point which was made in Goodfellow et al 2016 ('intend to improve test error but not necessarily training error'), i.e., that we would like to separate as much as possible the regularization effects from the optimization effect. Indeed, under the definition proposed here, any improvement in the optimizer could be considered like a regularizer, so long as we are not in the overfitting regime. That does not sound right to me.\n\nThere are several places where the authors make TOO STRONG STATEMENTS, taking for truth what are simply beliefs with no strong supporting evidence (at least published). This is not good for a review and when making recommendations.\n\nThe other weakness I estimate in this paper is that I did not get a sense that the taxonomy really helped us (me at least) to get insight into the different mentions being cited. Besides the obvious proposal to combine ideas to write new papers (but we did not need that paper to figure that out) I did not find much meat in the 'future directions' section.\n\nHowever, I that except in a few places the understand of the field displayed by the authors is pretty good and, with correction, could serve as a useful reference for students of deep learning. The recommendations were reasonable although lacking empirical support (or pointers to the literature), so I would take them somewhat carefully, more as the current 'group think' than ground truth.\n\nFinally, here a few minor points which could be fixed.\n\nEq. 1: in typical DL, minimization is approximate, not exact, so the proposed formalism does not reflect reality.\n\nEq. 4: in many cases, the noise is not added (e.g. dropout), so that should be clarified there.\n\npage 3, first bullet of 'Effect on the data representation': not clear, may want to give translations as an example of such transformations,.\n\npage 8, activation functions: the ReLU is actually older than the cited papers, it was used by computational neuroscientists a long time ago. Jarrett 2009 did not use the ReLU but an absolute-value rectifier and it was Glorot 2011 who showed that the ReLU really kicked ass for deeper networks. Nair 2010 used the ReLU in a very different context (RBMs), not really feedforward multi-layer networks where it shines now.\nIn that same section (and probably elsewhere) there are TOO STRONG STATEMENTS, e.g., the \"facts\" mentioned are not facts but merely folk belief, as far as I know, and I would like to see well-done supporting evidence before treating those as facts. Note that approximating the sigmoid precisely would require many ReLUs!\n\npage 8: it is not clear how multi-task learning fits under the 'architecture' formalism provided at the beginning of section 4.\n\nsection 7 (page 10): there is earlier work on the connection between early stopping and L2 regularization, at least dating back to Ronan Collobert's PhD thesis (with neural nets), probably earlier for linear systems.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularization for Deep Learning: A Taxonomy","abstract":"Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.","pdf":"/pdf/5438d7312501470559e1b4765fac7d4f147165e0.pdf","TL;DR":"Systematic categorization of regularization methods for deep learning, revealing their similarities.","paperhash":"anonymous|regularization_for_deep_learning_a_taxonomy","_bibtex":"@article{\n  anonymous2018regularization,\n  title={Regularization for Deep Learning: A Taxonomy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHkeixAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper364/Authors"],"keywords":["neural networks","deep learning","regularization","data augmentation","network architecture","loss function","dropout","residual learning","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739342257,"tcdate":1509105629414,"number":364,"cdate":1509739339599,"id":"SkHkeixAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkHkeixAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Regularization for Deep Learning: A Taxonomy","abstract":"Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.","pdf":"/pdf/5438d7312501470559e1b4765fac7d4f147165e0.pdf","TL;DR":"Systematic categorization of regularization methods for deep learning, revealing their similarities.","paperhash":"anonymous|regularization_for_deep_learning_a_taxonomy","_bibtex":"@article{\n  anonymous2018regularization,\n  title={Regularization for Deep Learning: A Taxonomy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHkeixAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper364/Authors"],"keywords":["neural networks","deep learning","regularization","data augmentation","network architecture","loss function","dropout","residual learning","optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}