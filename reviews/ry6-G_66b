{"notes":[{"tddate":null,"ddate":null,"tmdate":1514295385011,"tcdate":1514295385011,"number":4,"cdate":1514295385011,"id":"S1Zvx0Jmf","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Comment","forum":"ry6-G_66b","replyto":"SJsY_I3MG","signatures":["ICLR.cc/2018/Conference/Paper77/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper77/AnonReviewer3"],"content":{"title":"Thank you for the correction","comment":"Having seen the other reviews and rebuttals, I maintain my rating at 8 (top 50%, clear accept)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514068099438,"tcdate":1514068099438,"number":3,"cdate":1514068099438,"id":"SJsY_I3MG","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Comment","forum":"ry6-G_66b","replyto":"S1a6mx5xM","signatures":["ICLR.cc/2018/Conference/Paper77/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper77/Authors"],"content":{"title":"Author response to AnonReviewer3","comment":"We thank the reviewer for their valuable comments and feedback.\n\n> Minor note: I am surprised that the cognitive map reference (Gupta et al, 2017) was dropped, as it seemed relevant.\nWe agree that this reference is relevant, we have added the reference to the revision.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514068000925,"tcdate":1514068000925,"number":2,"cdate":1514068000925,"id":"r1tmOU3MM","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Comment","forum":"ry6-G_66b","replyto":"rJ74wm5xM","signatures":["ICLR.cc/2018/Conference/Paper77/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper77/Authors"],"content":{"title":"Author response to AnonReviewer2","comment":"We thank the reviewer for their valuable comments and feedback.\n\n> What are the practical benefits to learning the measurement and policy model?\nThe example at the end of the paper (see Figure 4) highlights the importance of deciding actions for fast and accurate localization. We agree that the benefits can be better motivated in the introduction and we are looking into restructuring the paper to have a motivating example in the introduction.\n\n> it is not clear from the evaluation whether the resulting distribution that is maintained is consistent:\nLooking at the output of the model manually, it seems that the estimates are consistent. It is very difficult to quantify the consistency of the resulting distribution because there is no straightforward way to calculate the ground-truth distribution/posterior in 3D environments.\n\n> while the computational requirements at test time are significantly lower than the baselines, the time required for training is likely very large:\nWe designed the Active Markov Localization (Slow) baseline keeping this in mind. The proposed model was trained for 24hrs for all experiments. AML (Slow) represents the Generalized AML algorithm using the values of hyperparameters which maximize the performance while keeping the runtime for 1000 episodes below 24hrs in each environment. This means the runtime of AML (Slow) is comparable to the training time of the proposed model. However, we agree that this point should be stated explicitly and we have made relevant changes in the paper.\n\n> The nature of the observation space is not clear.\nThe observation space in 2D environments is just the depth of the one column in front of the agent and in 3D environments, it is the 108x60 RGB image showing the first-person view of the agent.\n\n> It is not clear why the space over which the belief is maintained flips as the robot turns and shifts as it moves.\nThis happens due to the transition function as each channel represents a quantized orientation (North/East/West/South). The details of the transition function are provided in the appendix. For example, if the agent turns left, the probability of it facing north at any x-y coordinate becomes the probability of it facing west at the same-coordinate. This is why the belief flips when turning left. Similarly, the belief flips in the opposite direction when turning right and shifts when moving forward.\n\n> The 3D evaluation states that a 360 deg view is available. What happens when the agent can only see in one (forward) direction?\nThis seems to be a misunderstanding. The agent only sees in one forward direction, it needs to take actions to turn around to get the view in other directions. This misunderstanding might be due to the likelihood and belief presented in 4 directions. Note that each of these 4 channels represents the likelihood/belief of the agent’s orientation being that direction, not the likelihood/belief of the view in that direction.\n\n> AML includes a cost term in the objective. Did the author(s) experiment with setting this cost to zero?\nIn our environment, all actions have the same cost. This is equivalent to setting the cost to zero (i.e. it does not affect the optimal policy in the environment), but we found it helps the optimization of our model.\n\n> What would happen if the test environment was larger than those encountered in training?\nWe will need to discretize the test environment such that its belief is at most the size of the training environment, i.e. 70x70. The discretization of the training environments can be changed according to the desired level of accuracy in the test environment. For example, if we discretize 35m x 35m environment to a grid of 35x35, each cell would be a length of 1m. Due to this discretization, the model can make errors up to 0.5m even if it predicts the correct cell. The discretization can be increased to 70x70 to reduce errors to 0.25m.\n\n> The comment that the PoseNet and VidLoc methods \"lack a straightforward method to utilize past map data to do localization in a new environment\" is unclear.\nThe network weights in these models memorize the environment. The model has no way to ingest information about the map as input, thus the model trained in one map cannot be transferred to another map. These models need to be retrained on any new map.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514067637462,"tcdate":1514067637462,"number":1,"cdate":1514067637462,"id":"HJphLU2MG","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Comment","forum":"ry6-G_66b","replyto":"BJovaI9gf","signatures":["ICLR.cc/2018/Conference/Paper77/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper77/Authors"],"content":{"title":"Author response to AnonReviewer1","comment":"We thank the reviewer for their valuable comments and feedback.\n\nConcerns regarding the objective function:\nThis is a very interesting point and we thank the reviewer for this observation. We agree that one of the tasks of the localizer is to accurately approximate the true posterior under the prior and observations. But another task is to learn to take actions which lead it to arrive at a belief state with high probability mass on the true location. We provide rewards only for the correct prediction of the location and not for the correct prediction of the posterior because of two primary reasons:\n- Defining an appropriate reward function for the true posterior would require some way of estimating the true posterior, which is very difficult especially in 3D environments.\n- We want the model to be penalized if it fails to take actions in order to reach a state where it can predict its correct location, even if its estimation of the posterior under the prior and the observations is accurate.\n\nThe second point can potentially be mitigated by having an auxiliary loss on the belief, which back-propagates only through the perceptual model. This will only reward the perceptual model for predicting the true posterior, and the policy loss would still penalize the whole model for taking unfavorable actions. However, this will still require defining a reward or a loss function for the true posterior, which is difficult as there is generally no straightforward way of computing the ground-truth posterior in 3D environments with unknown models.\n\n> although the online speed of your learned method is much better than for active Markov localization, the offline training cost is dramatically higher:\nWe designed the Active Markov Localization (Slow) baseline keeping this in mind. The proposed model was trained for 24hrs for all experiments. AML (Slow) represents the Generalized AML algorithm using the values of hyperparameters which maximize the performance while keeping the runtime for 1000 episodes below 24hrs in each environment. This means that the runtime of AML (Slow) is comparable to the training time of the proposed model. However, we agree that this point should be stated explicitly and we have made relevant changes in the paper.\n\n> How is exploration done during the RL phase?.\nExploration is done implicitly using the stochastic policy in Asynchronous Advantage Actor-Critic method. As in the original A3C paper, to encourage exploration, we used an entropy loss scale of 0.01. \n\n> Please explain in more detail what the memory images are doing.\nMemory images are a part of the map information given to the agent in 3D Environments. They are used to calculate the likelihood given the current observation of the agent as follows: The perceptual model is used to get the feature representation of all the memory images and the current agent observation. The likelihood of each state in the set of memory images is calculated by taking the cosine similarity of the feature representation of the agent’s observation with the feature representation of the memory image. \n\n> It is not obvious to me that it is sensible to take the cosine similarity between the feature representation of the observation and the feature representation of the state to get the entry in the likelihood map:\nThe basic assumption here is that images containing the same “landmark” (unique texture or object) would have similar representations (e.g. high inner product value). Taking cosine similarity is similar to the standard attention operation commonly used in Deep Learning, which is exponentiated inner product. The cosine similarity, in contrast, is scaled to remain within a range of values, which can help training stability and prevent the likelihood model from becoming too sharp.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642505572,"tcdate":1511841123307,"number":3,"cdate":1511841123307,"id":"BJovaI9gf","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Review","forum":"ry6-G_66b","replyto":"ry6-G_66b","signatures":["ICLR.cc/2018/Conference/Paper77/AnonReviewer1"],"readers":["everyone"],"content":{"title":"End-to-end training of two parts of an active localization system","rating":"7: Good paper, accept","review":"This is an interesting paper that builds a parameterized network to select actions for a robot in a simulated environment, with the objective of quickly reaching an internal belief state that is predictive of the true state.  This is an interesting idea and it works much better than I would have expected.  \n\nIn more careful examination it is clear that the authors have done a good job of designing a network that is partly pre-specified and partly free, in a way that makes the learning effective.  In particular\n- the transition model is known and fixed (in the way it is used in the belief update process)\n- the belief state representation is known and fixed (in the way it is used to decide whether the agent should be rewarded)\n- the reward function is known and fixed (as above)\n- the mechanics of belief update\nBut we learn\n- the observation model\n- the control policy\n\nI'm not sure that global localization is still an open problem with known models.  Or, at least, it's not one of our worst.\n\nEarly work by Cassandra, Kurien, et al used POMDP models and solvers for active localization with known transition and observation models.   It was computationally slow but effective.\n\nSimilarly, although the online speed of your learned method is much better than for active Markov localization, the offline training cost is dramatically higher;  it's important to remember to be clear on this point.\n\nIt is not obvious to me that it is sensible to take the cosine similarity between the feature representation of the observation and the feature representation of the state to get the entry in the likelihood map.   It would be good to make it clear this is the right measure.\n\nHow is exploration done during the RL phase?  These domains are still not huge.\n\nPlease explain in more detail what the memory images are doing.\n\nIn general, the experiments seem to be well designed and well carried out, with several interesting extensions.\n\nI have one more major concern:  it is not the job of a localizer to arrive at a belief state with high probability mass on the true state---it is the job of a localizer to have an accurate approximation of the true posterior under the prior and observations.   There are situations (in which, for example, the robot has gotten an unusual string of observations) in which it is correct for the robot to have more probability mass on a \"wrong\" state.  Or, it seems that this model may earn rewards for learning to make its beliefs overconfident.  It would be very interesting to see if you could find an objective that would actually cause the model to learn to compute the appropriate posterior.\n\nIn the end, I have trouble making a recommendation:\nCon:  I'm not convinced that an end-to-end approach to this problem is the best one\nPro: It's actually a nice idea that seems to have worked out well\nCon: I remain concerned that the objective is not the right one\n\nMy rating would really be something like 6.5 if that were possible.\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1516030403664,"tcdate":1511827243398,"number":2,"cdate":1511827243398,"id":"rJ74wm5xM","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Review","forum":"ry6-G_66b","replyto":"ry6-G_66b","signatures":["ICLR.cc/2018/Conference/Paper77/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Provides Reasonable Evaluation but would Benefit from Clearer Motivation","rating":"6: Marginally above acceptance threshold","review":"The paper describes a neural network-based approach to active localization based upon RGB images. The framework employs Bayesian filtering to maintain an estimate of the agent's pose using a convolutional network model for the measurement (perception) function. A convolutional network models the policy that governs the action of the agent. The architecture is trained in an end-to-end manner via reinforcement learning. The architecture is evaluated in 2D and 3D simulated environments of varying complexity and compared favorably to traditional (structured) approaches to passive and active localization.\n\nAs the paper correctly points out, there is large body of work on map-based localization, but relatively little attention has been paid to decision theoretic formulations to localization, whereby the agent's actions are chosen in order to improve localization accuracy. More recent work instead focuses on the higher level objective of navigation, whereby any effort act in an effort to improve localization are secondary to the navigation objective. The idea of incorporating learned representations with a structured Bayesian filtering approach is interesting, but it's utility could be better motivated. What are the practical benefits to learning the measurement and policy model beyond (i) the temptation to apply neural networks to this problem and (ii) the ability to learn these in an end-to-end fashion? That's not to say that there aren't benefits, but rather that they aren't clearly demonstrated here. Further, the paper seems to assume (as noted below) that there is no measurement uncertainty and, with the exception of the 3D evaluations, no process noise.\n\nThe evaluation demonstrates that the proposed method yields estimates that are more accurate according to the proposed metric than the baseline methods, with a significant reduction in computational cost. However, the environments considered are rather small by today's standards and the baseline methods almost 20 years old. Further, the evaluation makes a number of simplifying assumptions, the largest being that the measurements are not subject to noise (the only noise that is present is in the motion for the 3D experiments). This assumption is clearly not valid in practice. Further, it is not clear from the evaluation whether the resulting distribution that is maintained is consistent (e.g., are the estimates over-/under-confident?). This has important implications if the system were to actually be used on a physical system. Further, while the computational requirements at test time are significantly lower than the baselines, the time required for training is likely very large. While this is less of an issue in simulation, it is important for physical deployments. Ideally, the paper would demonstrate performance when transferring a policy trained in simulation to a physical environment (e.g., using diversification, which has proven effective at simulation-to-real transfer).\n\nComments/Questions:\n\n* The nature of the observation space is not clear.\n\n* Recent related work has focused on learning neural policies for navigation, and any localization-specific actions are secondary to the objective of reaching the goal. It would be interesting to discuss how one would balance the advantages of choosing actions that improve localization with those in the context of a higher-level task (or at least including a cost on actions as with the baseline method of Fox et al.).\n\n* The evaluation that assigns different textures to each wall is unrealistic.\n\n* It is not clear why the space over which the belief is maintained flips as the robot turns and shifts as it moves.\n\n* The 3D evaluation states that a 360 deg view is available. What happens when the agent can only see in one (forward) direction?\n\n* AML includes a cost term in the objective. Did the author(s) experiment with setting this cost to zero?\n\n* The 3D environments rely upon a particular belief size (70 x 70) being suitable for all environments. What would happen if the test environment was larger than those encountered in training?\n\n* The comment that the PoseNet and VidLoc methods \"lack a strainghtforward method to utilize past map data to do localization in a new environment\" is unclear.\n\n* The environments that are considered are quite small compared to the domains currently considered for\n\n* Minor: It might be better to move Section 3 into Section 4 after introducing notation (to avoid redundancy).\n* The paper should be proofread for grammatical errors (e.g., \"bayesian\" --> \"Bayesian\", \"gaussian\" --> \"Gaussian\")\n\n\nUPDATES FOLLOWING AUTHORS' RESPONSE\n\n(Apologies if this is a duplicate. I added a comment in light of the authors' response, but don't see it and so I am updating my review for completeness).\n\nI appreciate the authors's response to the initial reviews and thank them for addressing several of my comments.\n\nRE: Consistency\nMy concerns regarding consistency remain. For principled ways of evaluating the consistency of an estimator, see Bar-Shalom \"Estimation with Applications to Tracking and Navigation\".\n\nRE: Measurement/Process Noise\nThe fact that the method assumes perfect measurements and, with the exception of the 3D experiments, no process noise is concerning as neither assumptions are valid for physical systems. Indeed, it is this noise in particular that makes localization (and its variants) challenging.\n\nRE: Motivation\nThe response didn't address my comments about the lack motivation for the proposed method. Is it largely the temptation of applying an end-to-end neural method to a new problem? The paper should be updated to make the advantages over traditional approaches to active localization.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642505644,"tcdate":1511814084899,"number":1,"cdate":1511814084899,"id":"S1a6mx5xM","invitation":"ICLR.cc/2018/Conference/-/Paper77/Official_Review","forum":"ry6-G_66b","replyto":"ry6-G_66b","signatures":["ICLR.cc/2018/Conference/Paper77/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Convincing paper about an active learning neural version of Bayesian filter localisation","rating":"8: Top 50% of accepted papers, clear accept","review":"I have evaluated this paper for NIPS 2017 and gave it an \"accept\" rating at the time, but the paper was ultimately not accepted. This resubmission has been massively improved and definitely deserves to be published at ICLR.\n\nThis paper formulates the problem localisation on a known map using a belief network as an RL problem. The goal of the agent is to minimise the number of steps to localise itself (the agent needs to move around to accumulate evidence about its position), which corresponds to reducing the entropy of the joint distribution over a discretized grid over theta (4 orientations), x and y. The model is evaluated on a grid world, on textured 3D mazes with simplified motion (Doom environment) and on a photorealistic environment using the Unreal engine. Optimisation is done through A3C RL. Transfer from the crude simulated Doom environment to the photorealistic Unreal environment is achieved.\n\nThe belief network consists of an observation model, a motion prediction model that allows for translations along x or y and 90deg rotation, and an observation correction model that either perceives the depth in front of the agent (a bold and ambiguous choice) and matches it to the 2D map, or perceives the image in front of the agent. The map is part of the observation.\n\nThe algorithm outperforms Bayes filters for localisation in 2D and 3D and the idea of applying RL to minimise the entropy of position estimation is brilliant. Minor note: I am surprised that the cognitive map reference (Gupta et al, 2017) was dropped, as it seemed relevant.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513905731235,"tcdate":1508897285271,"number":77,"cdate":1509739496802,"id":"ry6-G_66b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry6-G_66b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Active Neural Localization","abstract":"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine. ","pdf":"/pdf/2e2c71503b01f471a2734f8feb8c788966876ba7.pdf","TL;DR":"\"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.","paperhash":"anonymous|active_neural_localization","_bibtex":"@article{\n  anonymous2018active,\n  title={Active Neural Localization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry6-G_66b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper77/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}