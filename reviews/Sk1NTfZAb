{"notes":[{"tddate":null,"ddate":null,"tmdate":1513977609709,"tcdate":1513977609709,"number":5,"cdate":1513977609709,"id":"SkMMwxszG","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Comment","forum":"Sk1NTfZAb","replyto":"SkS7qR3-M","signatures":["ICLR.cc/2018/Conference/Paper997/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper997/Authors"],"content":{"title":"Update","comment":"We address the problem of sharing samples for a common class, In the revised version of the paper. We have added a new section (Section 5.5)  where we discuss and empirically verify that participants may have training examples of overlapping classes without sharing their private keys. (Taken partially from our answer to AnonReviwer2.)\n\nThank you very much for pointing out the ambiguity in the formulation. It has been corrected now.\n\nSince \\phi_{\\theta}(.) is a deterministic mapping that outputs a vector, we just compute the L2 norm of the output vector, simply as a function of the output vector. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513977510172,"tcdate":1513977510172,"number":4,"cdate":1513977510172,"id":"ry0o8eofG","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Comment","forum":"Sk1NTfZAb","replyto":"r1qWlNtlM","signatures":["ICLR.cc/2018/Conference/Paper997/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper997/Authors"],"content":{"title":"We need more explanation for your concerns","comment":"In our approach, we protect participants by hiding class scores from any other participant in CLF. For this purpose, we let participants to create private keys for its local training classes. Please note that private keys are completely randomly distributed, and participants do not share any information about their keys throughout training. (The revised paper, we believe, explains the procedure much more clearly.)\n\nTherefore, we do not see how a GAN attack without a guidance score or feedback signal can be executed to reconstruct the private class keys. \n\nWe will be more than happy to discuss if you can elaborate this objection.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513977305548,"tcdate":1513977305548,"number":3,"cdate":1513977305548,"id":"BJMyLesGG","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Comment","forum":"Sk1NTfZAb","replyto":"SJyQ2wqlf","signatures":["ICLR.cc/2018/Conference/Paper997/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper997/Authors"],"content":{"title":"Update","comment":"We address the problem of sharing samples for a common class, in the revised version of the paper. We have added a new section (Section 5.5)  where we discuss and empirically verify that participants may have training examples of overlapping classes without sharing their private keys. \n\nWe have also added new attacking results for MNIST showing that there can be multiple attackers in CLF (indeed every participant can be an attacker) in Figure-6. For such cases, the GAN attacks still fail without damaging the learning process. The reconstructions show that generators trained by attackers can capture likelihood of data given the guessed key. However these likelihoods are far from data distributions of the handwritten digits. Which is the expected outcome of our methodology and reflects our success.\n\nFurthermore we speak of how we benefit from the fixed layer in Section 5.4. By using a fixed layer, we are able to control complexity of local models, which is crucial in preventing participants to overfit their local datasets in one epoch of local training."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513977071898,"tcdate":1513977071898,"number":2,"cdate":1513977071898,"id":"SyOgBeizf","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Comment","forum":"Sk1NTfZAb","replyto":"Sk1NTfZAb","signatures":["ICLR.cc/2018/Conference/Paper997/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper997/Authors"],"content":{"title":"Clarification on DP","comment":"We thank for the interesting comments and suggestions in this thread. We have just published the comprehensively revised paper where we have removed all of the controversial arguments regarding differential privacy (DP), as suggested by the reviewers.\n \nOur paper, however, is not (directly) about DP: we show that our proposed approach allows privacy-preserving collaborative training without introducing DP or other techniques that corrupt model parameters / parameter updates with noise injection. More importantly, our CLF formulation is resilient against active GAN attacks (Hitaj et.al. 2017).\n \nIn more detail, there are two main reasons why we think our approach is of significance:\n \n(1) DP typically requires making a difficult trade-off decision between model accuracy and privacy. In particular, the privacy budget per parameter plots in Shokri et al. (2015) show that in order to reach an acceptable (90%) level of test-set accuracy on MNIST, one may need to use very high \"epsilon\" values (ie. very low noise), which may significantly reduce the effectiveness of DP in terms of privacy preservation.  Our approach does not necessarily involve such a trade-off between privacy and accuracy (except that using excessively high-dimensional class keys may lead to issues during training).\n \n(2) Our approach prevents CLF against GAN attacks (Hitaj et al. 2017), which can be difficult to avoid using DP, without (significantly) sacrificing the classification accuracy. \n \nTherefore, in summary, what we propose is not built upon DP, instead, it can be seen as a new and alternative approach for privacy preserving collaborative training that builds upon participant-specific keys, as opposed to hiding information through mixing models updates/parameters with noise."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513362091569,"tcdate":1513362091569,"number":7,"cdate":1513362091569,"id":"SJ43M9ZMz","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"S1ZKptezz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"\"differential privacy fails to prevent the attack\"","comment":"The above statement in this paper is false or, at best, misleading. The fact that it is attributed to someone else, doesn't change that."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513295224925,"tcdate":1513295224925,"number":6,"cdate":1513295224925,"id":"S1ZKptezz","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"B197fWcbf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"RE: Misleading","comment":"But that's the point, OTHERS have used RSA with 16-bit keys and Hitaj et al. CCS'17 show this is ill-considered. It's an attack paper, no new scheme is proposed. It is reported that properly set DP will thwart these attacks (but at the cost of utility, see the conclusions)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513101368384,"tcdate":1513101368384,"number":5,"cdate":1513101368384,"id":"S1xr_5abG","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"Hyku30n-G","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relevance","comment":"This submission makes a false statement. It is mathematically impossible to reconstruct training examples while satisfying differential privacy. That statement needs to be corrected. And it is relevant to the motivation for this work.\n\nI did not mean to start a debate about the Hitaj et al. paper. My comment is only about the false statement in this submission, which is justified by citing the Hitaj et al. paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513053287184,"tcdate":1513053287184,"number":1,"cdate":1513053287184,"id":"Hyku30n-G","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Comment","forum":"Sk1NTfZAb","replyto":"B1xjZwmbz","signatures":["ICLR.cc/2018/Conference/Paper997/AnonReviewer4"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper997/AnonReviewer4"],"content":{"title":"Really?","comment":"This reviewer does not have a problem with the paper under study, but believes that Hitaj et al. paper is wrong. \n\nMy take is that this review should be removed, because it is only concern with the validity of a already publish work and they should talk to CCS'17 committee about it. \n\nAlso, the code for Hitaj et al. 2017 is available if the reviewer thinks the parameters are incorrectly set, they should work with the code to show that the authors maliciously played with the parameters and publish a paper or a blog showing why it does not work. The blog link above does not do that. I think this is the best way to show that Hitaj et al. is not valid. But trashing other conferences with grievances is an old technique that some people use all too frequently and it is becoming really tiring. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1515642538898,"tcdate":1513052701418,"number":3,"cdate":1513052701418,"id":"SkS7qR3-M","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Review","forum":"Sk1NTfZAb","replyto":"Sk1NTfZAb","signatures":["ICLR.cc/2018/Conference/Paper997/AnonReviewer4"],"readers":["everyone"],"content":{"title":"The paper is unclear and needs more work","rating":"3: Clear rejection","review":"In this paper, the authors proposed a counter measure to protect collaborative training of DNN against the GAN attack in (Hitaj et al. 2017). The motivation of the paper is clear and so is the literature review. But for me the algorithm is not clearly defined and it is difficult to evaluate how the proposed procedure works. I am not saying that this is not the solution. I am just saying that the paper is not clear enough to say that it is (or it is not). From, my perspective this will make the paper a clear reject. \n\nI think the authors should explain a few things more clearly in order to make the paper foolproof. The first one seems to me the most clear problem with the approach proposed in the paper:\n\n1 $\\psi(c)$ defines the mapping from each class to a high dimensional vector that allows protection against the GAN attack. $\\psi(c)$ is suppose to be private for each class (or user if each class belong only to one user). This is the key aspect in the paper. But if more than one user have the same class they will need to share this key. Furthermore, at test time, these keys need to be known by everyone, because the output of the neural network needs to be correlated against all keys to see which is the true label. Of course the keys can only be released after the training is completed. But the adversary can also claim to have examples from the class it is trying to attack and hence the legitimate user that generated the key will have to give the attacker the key from the training phase. For example, let assume the legitimate user only has ones from MNIST and declares that it only has one class. The attacker says it has two classes the same one that the legitimate user and some other label. In this case the legitimate user needs to share $\\psi(c)$ with the attacker. Of course this sounds “fishy” and might be a way of finding who the attacker is, but there might be many cases in which it makes sense that two or more users shares the same labels and in a big system might be complicated to decide who has access to which key.\n\n2 I do not understand the definition of $\\phi(x)$. Is this embedding fixed for each user? Is this embedding the DNN? In Eq. 4 I would assume that $\\phi(x)$ is the DNN and that it should be $\\phi_\\theta(x)$, because otherwise the equation does not make sense. But this is not clearly explained in the paper and Eq 4 makes no sense at all. In a way the solution to the maximization in Eq 4 is Theta=\\infty. Also the term $\\phi(x)$ is not mentioned in the paper after page 5. My take is that the authors want to maximize the inner product, but then the regularizer should go the other way around. \n\n3 In the paper in page 5 we can read: “Here, we emphasize the first reason why it is important to use l2-normalized class keys and embedding outputs: in this manner, the resulting classification score is by definition restricted to the range [-1; +1],” If I understand correctly the authors are dividing the inner product by ||$\\psi(c)|| ||$\\phi(x)||. I can see that we can easily divide by ||$\\psi(c)||, but I cannot see how we can do dive by ||$\\phi(x)||, if this term depends on \\theta. If this term does not depend on \\theta, then Eq 4 does not make sense.\n\nTo summarize, I have the impression that there are many elements in the paper that does not makes sense in the way that they are explained and that the authors need to tell the paper in a way that can be easily understood and replicated. I recommend the authors to run the paper by someone in their circle that could help them rewrite the paper in a way that is more accessible. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512866740131,"tcdate":1512866740131,"number":4,"cdate":1512866740131,"id":"SJ227bqbM","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"B1xjZwmbz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"related work","comment":"Differential privacy is tangential to the work in this submission and the flaws of the Hitaj et al. paper should not be held against it. \n\nI am commenting because the quote about the related work needs to be clarified. Both Shokri & Smatikov and Hitaj et al. use differential privacy with extremely large parameters, which render it meaningless."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512866338514,"tcdate":1512866338514,"number":3,"cdate":1512866338514,"id":"B197fWcbf","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"S1B5euOZz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Misleading","comment":"The Hitaj et al. CCS'17 paper is misleading; only upon close scrutiny does one realize that, when they refer to differential privacy, they mean with crazy parameters.\n\nIt's analogous to claiming that RSA cryptography is broken and then only on page 3 clarifying that what you really mean is that RSA with 16-bit keys is susceptible to a brute force factoring attack. \n\nIn particular, the above quote from this submission does not clarify this issue. It says \"differential privacy fails to prevent the attack\" without providing details. This is on its face false, as the default interpretation is \"differential privacy with reasonable parameters.\" \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512763532685,"tcdate":1512763532685,"number":2,"cdate":1512763532685,"id":"S1B5euOZz","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"B1xjZwmbz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"RE: Differential Privacy","comment":"The CCS’17 (Hitaj et al.)  paper mentions several times they don't \"break\" DP or use DP in any way, but they show that DP is inadequate when epsilon is large (as used and implemented by others) or at the record level. See throughout the paper (https://acmccs.github.io/papers/p603-hitajA.pdf) and the conclusions in particular. \n\nSo the blog misses several crucial points and this paper (\"Key Protected Classification… “) also provides clear evidence of the privacy risks of CLFs."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512432023641,"tcdate":1512432023641,"number":1,"cdate":1512432023641,"id":"B1xjZwmbz","invitation":"ICLR.cc/2018/Conference/-/Paper997/Public_Comment","forum":"Sk1NTfZAb","replyto":"Sk1NTfZAb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Differential Privacy","comment":"This paper states (page 2, second paragraph):\n\nHowever, it has recently been shown that [collaborative learning frameworks (CLFs)] can be vulnerable to not only passive attacks, but also much more powerful active attacks, i.e., training-time attacks, for which the CLF with differential privacy fails to prevent the attack and there is no known prevention technique in general (Hitaj et al., 2017). More specifically, a training participant can construct a generative adversarial network (GAN) (Goodfellow et al., 2014) such that its GAN model learns to reconstruct training examples of one of the other participants over the training iterations. \n\nThis is given as the motivation for this work, but this statement is very flawed. Hitaj et al. do not \"break\" differential privacy. The problem is that they use differential privacy with extremely large parameter values, which yields a meaningless privacy guarantee.\n\nFrank McSherry has posted a detailed critique of the Hitaj et al. paper here:\n\nhttps://github.com/frankmcsherry/blog/blob/master/posts/2017-10-27.md"},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1515642538935,"tcdate":1511844886817,"number":2,"cdate":1511844886817,"id":"SJyQ2wqlf","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Review","forum":"Sk1NTfZAb","replyto":"Sk1NTfZAb","signatures":["ICLR.cc/2018/Conference/Paper997/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea to mitigate the GAN attack","rating":"5: Marginally below acceptance threshold","review":"Collaborative learning has been proposed as a way to learn over federated data while preserving privacy. However collaborative learning has been shown to be suscepti\nble to active attacks in which one of the participants uses a GAN to reveal information about another participant.\n\nThis paper proposes a collaborative learning framework (CLF) that mitigates the GAN attack. The framework involves using the neural net to learn a mapping of the inp\nut to a high-dimensional vector and computing the inner product of this vector to a random class-specific key (the final class prediction is the argmax of this inner product). The class-specific key can be chosen randomly by each participant. By choosing sufficiently long random keys, the probability of an attacker guessing the key can be reduced. Experiments on two datasets show that this scheme successfully avoids the GAN attack.\n \n1. Some of the details of key sharing are not clear and would appear to be important for the scheme to work. For example, if participants have instances associated with the same class, then they would need to share the key. This would require a central key distribution scheme which would then allow the attacker to also get access to the key.\n\n2. I would have  liked to see how the method works with an increasing fraction of adversarial participants (I could only see experiments with one adversary). Similarly, I would have liked to see experiments with and without the fixed dense layer to see its contribution to effective learning. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1515642538979,"tcdate":1511763970510,"number":1,"cdate":1511763970510,"id":"r1qWlNtlM","invitation":"ICLR.cc/2018/Conference/-/Paper997/Official_Review","forum":"Sk1NTfZAb","replyto":"Sk1NTfZAb","signatures":["ICLR.cc/2018/Conference/Paper997/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The weak assumption on the adversary undermines the usefulness of the protection scheme","rating":"4: Ok but not good enough - rejection","review":"This paper is a follow-up work to the CCS'2017 paper on the GAN-based attack on collaborative learning system where multiple users contribute their private and sensitive data to joint learning tasks. In order to avoid the potential risk of adversary's mimic based on information flow among distributed users, the authors propose to embed the class label into a multi-dimensional space, such that the joint learning is conducted over the embedding space without knowing the accurate representation of the classes. Under the assumption that the adversary can only generate fake and random class representations, they show their scheme is capable of hiding information from individual samples, especially over image data.\n\nThe paper is clearly written and easy to understand. The experiments show interesting results, which are particularly impressive with the face data. However, the reviewer feels the assumption on the adversary is generally too weak, such that slightly smarter adversary could circumvent the protection scheme and remain effective on sample recovery.\n\nBasically, instead of randomly guessing the representations of the classes from other innocent users, the adversary could apply GAN to learn the representation based on the feedback from these users. This can be easily done by including the representations in the embedding space in the parameters in GAN for learning.\n\nThis paper could be an interesting work, if the authors address such enhanced attacks from the adversary and present protection results over their existing experimental settings.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1513976942558,"tcdate":1509137721933,"number":997,"cdate":1510092360786,"id":"Sk1NTfZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk1NTfZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Key Protected Classification for GAN Attack Resilient Collaborative Learning","abstract":"Large-scale publicly available datasets play a fundamental role in training deep learning models. However, large-scale\ndatasets are difficult to collect in problems that involve processing of sensitive information.\nCollaborative learning techniques provide a privacy-preserving solution in such cases, by enabling\ntraining over a number of private datasets that are not shared by their owners.\nExisting collaborative learning\ntechniques, combined with differential privacy, are shown to be resilient against a passive\nadversary which tries to infer the training data only from the model parameters. However, recently, it has\nbeen shown that the existing collaborative learning techniques are vulnerable to an active adversary that runs a GAN\nattack during the learning phase. In this work, we propose a novel key-based collaborative learning technique that is\nresilient against such GAN attacks. For this purpose, we present a collaborative learning formulation in which class scores \nare protected by class-specific keys, and therefore, prevents a GAN attack. We also show that\nvery high dimensional class-specific keys can be utilized to improve robustness against attacks, without increasing the model complexity. \nOur experimental results on two popular datasets, MNIST and AT&T Olivetti Faces, demonstrate the effectiveness of the proposed technique\nagainst the GAN attack. To the best of our knowledge, the proposed approach is the first collaborative learning\nformulation that effectively tackles an active adversary, and, unlike model corruption or differential privacy formulations,\nour approach does not inherently feature a trade-off between model accuracy and data privacy.","pdf":"/pdf/c9d430b79680585a9af140c8a72433e556b56ddc.pdf","paperhash":"anonymous|key_protected_classification_for_gan_attack_resilient_collaborative_learning","_bibtex":"@article{\n  anonymous2018key,\n  title={Key Protected Classification for GAN Attack Resilient Collaborative Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk1NTfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper997/Authors"],"keywords":["privacy preserving deep learning","collaborative learning","adversarial attack"]},"nonreaders":[],"replyCount":15,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}