{"notes":[{"tddate":null,"ddate":null,"tmdate":1515189001993,"tcdate":1515189001993,"number":5,"cdate":1515189001993,"id":"SkzfXdpmf","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Comment","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper462/Authors"],"content":{"title":"New revision","comment":"Thanks to the feedback of the reviewers, we have updated our submission.  The key differences are these:\n- All performance numbers are now gathered on a V100 GPU\n- We added more information about Lamport timestamps in the text to clarify their behavior and benefit\n- We add Deep Speech 2 to the case study in the appendix (up to a 3x speedup for baseline accuracy)\n- We mention the speedups from the case study in the main text to make concrete layer speedups on real tasks clear to readers\n- We make the \"side effect\" of our algorithm more clear: it is now worthwhile to prune recurrent layers, whereas persistent kernels before would regularly outperform sparse GEMMs for a target accuracy.  We see this point as a key contribution."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1514918704508,"tcdate":1514918561799,"number":4,"cdate":1514918561799,"id":"SJ5iMUt7f","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Comment","forum":"HkxF5RgC-","replyto":"S1gFF5fmM","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper462/AnonReviewer1"],"content":{"title":"response","comment":"When I read the paper (and not the Appendix), I was left wondering how much this benefits real applications as opposed to synthetic workloads. Figure 3 is in the right direction. But can you connect the dots for the reader and describe some applications which especially benefit from large layers of the specific sizes you have mentioned?\n\nI agree that the optimizations are non-trivial but if they can be made interesting to the larger ICLR community, it will be great! \n\nI have upgraded my score. I still find the paper little bit weak on novelty but I am confident that you will fix the other issues/clarifications raised in my review, in the final revision."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1514478415174,"tcdate":1514478415174,"number":3,"cdate":1514478415174,"id":"BkwIocfQz","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Comment","forum":"HkxF5RgC-","replyto":"rkoKvifef","signatures":["ICLR.cc/2018/Conference/Paper462/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper462/Authors"],"content":{"title":"Accuracy-sparsity tradeoffs in past work","comment":"Thank you for your comments and observations.  Let us first address the critical importance of network accuracy after pruning.  We completely agree that large speed improvements are a moot point if the accuracy does not hold up.  However, an exhaustive study of the sparsity/accuracy tradeoff is out of the scope of this paper.  Instead, we refer to several other published results that show good accuracy results for recurrent networks around the 10% density point [Han et al. 2016(a,b), Narang et al. 2017, See et al. 2016, Anonymous 2018].  So, we centered our experiments around this density and swept from 1% to 30% to cover a wider range.  After submission, densities down to 3% have been recently used to achieve state of the art results on some workloads (https://blog.openai.com/block-sparse-gpu-kernels/), and we show good speedups for higher densities, especially when the layer size is too large for a dense persistent kernel.  Finally, we provided more accuracy vs. sparsity vs. speed results in the appendix to show why our technique is important.  We'll gladly move this analysis into the main paper if extending beyond 8 pages is preferable to merely including references to other works with accuracy results.\n\nWe feel that this work is relevant to the ICLR audience.  As you noted, sparsity is not regularly accelerated by deep learning libraries.  More importantly, as we show in our appendix, some recurrent layers are actually better off staying dense and using a persistent approach if possible (without our sparse persistent technique).  Simply increasing accuracy for the same number of effective parameters is not sufficient to claim success; the network's speed may not increase over a dense network!  Thus, one of the fundamental benefits of sparsity is tempered in some cases.  Our main contribution shifts this balance back in favor of pruning for recurrent layers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1514477943969,"tcdate":1514477943969,"number":2,"cdate":1514477943969,"id":"S1gFF5fmM","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Comment","forum":"HkxF5RgC-","replyto":"BJ6cxWFlM","signatures":["ICLR.cc/2018/Conference/Paper462/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper462/Authors"],"content":{"title":"Benefit over dense persistent RNNs, request for clarification","comment":"Thank you comments and suggestions.  It is fair to wonder about the performance on real workloads; we decided to show the performance of our technique over a wide range of synthetic workloads so that practitioners can look to see where their application lives in the space and judge the relative performance accordingly.  Our appendix shows the performance of recurrent layers of one particular application.\n\nWith respect to the speedup over the dense persistent LSTMs in the OpenNMT network, 0.3-0.5s (looking at layers of the same size) is not the proper comparison.  Instead, we think that the comparison should be between networks of the same accuracy.  In this case, the improvement is up to 0.7ms (from 1.26ms for a BLEU score of 24.62 to 0.55ms for a BLEU score of 24.60).  Also, this is a per-layer improvement; a full network will be composed of several such layers leading to a larger absolute improvement for the network as a whole.  More important than absolute speedup for a single iteration, however, is the potential speedup for training networks.  This absolute 0.7ms reduces the run time to 44% of the previous time, roughly halving the time needed to train the network to a given accuracy.    We'll make this clear in the final text.  Finally, it's worth noting that without our contributions, the benefit of sparsity would be negative: existing sparse methods are worse than persistent kernels for a given accuracy or speed target on the workloads we studied.\n\nWe have a question about your suggestion to claim support for a larger number of methods.  We do claim this: Figure 3 shows that we can support larger layers in a persistent approach than existing methods.  Please let us know if we've misunderstood; we welcome opportunities to strengthen this paper!\n\nWe will certainly move the NMT evaluation into the main paper if the reviewers think it warrants the extra space.  We agree that it naturally belongs there.\n\nWe're also willing to emphasize the non-trivial aspects of the optimizations we used, as opposed to the brief mention in past work you point out.  It is exactly these optimizations which take the bulk of the main paper; was there something in particular you suggest adding?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1514476988038,"tcdate":1514476988038,"number":1,"cdate":1514476988038,"id":"HJ4pB5M7f","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Comment","forum":"HkxF5RgC-","replyto":"H1PcMAKeG","signatures":["ICLR.cc/2018/Conference/Paper462/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper462/Authors"],"content":{"title":"Clarifying relevance","comment":"Thank you for your time and comments.  With respect to showing the relevance to ICLR, we think the results of our work are very important.  Let us try to clarify this relevance by presenting the results of the appendix _without_ the context of the main paper: \"For the recurrent layers of the network we studied, there's no need to prune the weights.  A dense persistent implementation of the network is faster for the same accuracy as a pruned network, or more accurate at a given speed target.\"  \n\nThere has been significant interest in model pruning, mostly for the purposes of increasing performance.  However, realizing increased performance often requires some type of structured pruning, such as pruning filters or channels from convolutional networks, or leaving dense blocks in recurrent networks.  (As Narang et al. noted in their 2017 work at ICLR, cuSPARSE achieves limited speedup for unstructured sparsity, especially for large batch sizes.)  However, imposing structure on the sparsity reduces the degrees of freedom; unstructured sparsity can represent a proper superset of the patterns that any structured sparse layer can represent.  Therefore, it is preferable from a model's point of view to use sparsity without any structure (if sparsity is to be used at all, and second-order regularization effects of imposed structure notwithstanding).  So, we are motivated to find the efficient method, presented in the main section, to accelerate recurrent layers with unstructured sparsity.\n\nHowever, presenting an efficient method is only half the story; to start filling in the pieces, we included our appendix (as an appendix, in order to stay within the suggested page limit).  We show how both accuracy and speed change with sparsity.  In particular, without our method, unstructured sparsity (preferred by the model) is inferior to a dense network.  Dense persistent kernels are faster and more accurate than their pruned cuSPARSE counterparts for the model we studied.  We will make these points more clear in the next version of the text -- as well as spending some more space on describing Lamport Timestamps!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1515642452333,"tcdate":1511805582581,"number":3,"cdate":1511805582581,"id":"H1PcMAKeG","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Review","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Novel and impactful contributions, but unclear relevance and expected audience","rating":"6: Marginally above acceptance threshold","review":"The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels. The authors further propose model-pruning optimizations which are aware of the persistent implementation.\n\nIt's not clear if the paper is relevant to the ICLR audience due to its emphasize on low-level optimization which has little insight in learning representations. The exposition in the paper is also not well-suited for people without a systems background, although I'll admit I'm mostly using myself as a proxy for the average machine learning researcher here. For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation.\n\nModulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs. The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1515642452368,"tcdate":1511751829119,"number":2,"cdate":1511751829119,"id":"BJ6cxWFlM","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Review","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Sparse Persistent RNNs Review: Limited novelty over persistent RNNs","rating":"6: Marginally above acceptance threshold","review":"This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip. The paper describes the use additional mechanisms for synchronization and memory loading. \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).  With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads. Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise. \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip. One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those.\n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs. However, Narang et. al., 2017 already explore this idea, although briefly. Furthermore, the gains from the sparsity appear rather limited over real applications. I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads). Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial. However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1515642452404,"tcdate":1511335811237,"number":1,"cdate":1511335811237,"id":"rkoKvifef","invitation":"ICLR.cc/2018/Conference/-/Paper462/Official_Review","forum":"HkxF5RgC-","replyto":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference/Paper462/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper devises sparse GPU kernels for RNNs","rating":"6: Marginally above acceptance threshold","review":"The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones). Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference. Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper). It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal.\n\nPros:\n- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs\n- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well\n\nCons:\n- This work may be better reviewed at a more GPU-centric conference\n- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]}},{"tddate":null,"ddate":null,"tmdate":1515188510079,"tcdate":1509120632404,"number":462,"cdate":1509739287988,"id":"HkxF5RgC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkxF5RgC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip","abstract":"Recurrent Neural Networks (RNNs) are powerful tools for solving sequence-based problems, but their efficacy and execution time are dependent on the size of the network. Following recent work in simplifying these networks with model pruning\nand a novel mapping of work onto GPUs, we design an efficient implementation for sparse RNNs. We investigate several optimizations and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight layout. With these optimizations, we achieve speedups of over 6x over the next best algorithm for a hidden layer of size 2304, batch size of 4, and a\ndensity of 30%. Further, our technique allows for models of over 5x the size to fit on a GPU for a speedup of 2x , enabling larger networks to help advance the state-of-the-art. We perform case studies on NMT and speech recognition tasks in the appendix, accelerating their recurrent layers by up to 3x.","pdf":"/pdf/f59d7941d866b80bc6c3e162d89fe249889c51d4.pdf","TL;DR":"Combining network pruning and persistent kernels into a practical, fast, and accurate network implementation.","paperhash":"anonymous|sparse_persistent_rnns_squeezing_large_recurrent_networks_onchip","_bibtex":"@article{\n  anonymous2018sparse,\n  title={Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkxF5RgC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper462/Authors"],"keywords":["Sparsity","Pruning","Compression","RNN","LSTM","Persistent","RF-Resident","GPU"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}