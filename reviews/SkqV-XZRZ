{"notes":[{"tddate":null,"ddate":null,"tmdate":1516128957021,"tcdate":1516128957021,"number":9,"cdate":1516128957021,"id":"HyrTqpoVf","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"SkL8dLWEf","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Clarification of the proposed objective","comment":"We apologize for the confusion. If we consider the Jensen's inequality derived from the term \\log p(b,h) as a starting point, then your argument about using alpha and beta equal to 1 would be absolutely correct. However, we do not have the term \\log p(b,h) in the original objective (equation 4). To arrive at our objective, consider that we have designed the architecture as shown in figure 1. Then we derive equation 10 from equation 5 as follows:\n\n1. The term  p(x_{t+1} | x_{1:t}, z_t, \\tilde{b}_t) in equation 5 is instantiated with p(x_{t+1} | h_t) in equation 10.\n2. The KLD term in equation 10 is an instantiation of the KLD term in equation 5 based on our architecture.\n3. Finally, we add 3 regularization terms: p(x_{t+1} | b_t) + alpha p(b_t | z_t) + beta p(h_t | z_t)\n\nNotice that using only the first two steps as our objective leads to a b_t that is a deterministic but random function of x_t and b_{t-1} depending completely on the initialization of the weights of the backward LSTM. The 3rd step adds two regularizations related to b_t and a regularization on h_t. In other words, you are right in pointing out that equation 10 is not exactly the lower bound stated in equation 5, but rather it is one with added regularization terms which help the model generalize better. Specifically, the regularization term p(x_{t+1}|b_t) helps b_t learn information about future, the second regularization term alpha p(b_t | z_t) makes \\tild{b_t} learn to be close to b_t that can help predict x_{t+1}, and the last regularization term beta p(h_t | z_t) regularizes h_t by imposing a reconstruction loss. \n\nThank you for pointing it out that the name “stochastic gradient” can be misleading since it has been used previously. We have changed it to “skipping gradient” in the latest version of our paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515444302481,"tcdate":1515444302481,"number":7,"cdate":1515444302481,"id":"SkL8dLWEf","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"SkeZ-Btff","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer2"],"content":{"title":"Updated Review","comment":"\nAfter reading the author's response, R3's review, and the revised paper, I am more concerned about the clarity in this work than I was initially.\n\n\"Regarding your comment .... that’s true...... We treat alpha as a hyperparameter, so its value should be chosen based on validation set. The positive value of alpha governs how much weightage is given to the reconstruction of b_t vs the rest of the terms in the cost.\"\n\nOne my concerns that still remains from my initial review is that there is no probabilistic reason provided for using alpha as a hyperparameter.\n\nLet me try and illustrate why I think alpha (and beta, but for the moment, we'll ignore the latent variable \\tilde{h_t-1} and focus just on \\tilde{b}_t) should have the value 1.\nConsider a simplification of the model that looks at a single time slice of the model -- i.e. just the variables z_t, \\tilde{b}_t and h_t in Figure 1(b) (we'll refer to them as z, b, and h respectively).\n\nFor a single time-slice variant of this model, the joint distribution is p(z)p(b|z)p(h|b,z).\nAs mentioned earlier and acknowledged, at training time, b and h are observed while z is latent. So if we want to maximize the likelihood of the observed data during training, we have:\n\\log p(b,h) = \\log \\int_z p(b,h,z) = \\log \\int_z  p(b|z) p(h|b,z) p(z) * q(z|h)/q(z|h) \\geq (Jensens) E_{q(z|h)}[\\log p(b|z)+ \\log p(h|b,z)] - KL[q(z|h)||p(z)]\n\nContrast this with Equation (10) in the paper. Note that using alpha <1 implies that we multiply a negative number (namely log p(b|z)) with a fraction which always *increases* the number artificially. This means the resulting objective is no longer a valid lower-bound on the marginal likelihood of the observed data.\nWhile I can potentially see a case for annealing alpha to 1, I'm a little concerned by the numbers reported when alpha is set to 0.0001. \nWhat is the number reported at test time -- dose it use alpha? Please do correct me if I've missed something and there is a probabilistic reason for why alpha can be <1; as far as I can tell, it has only appeared in Equation (10) and not before in Equation (5).\n\nOverall, while I think the paper outlines an interesting idea,\nin its current form (in the revised version) I still find it difficult to follow and not appropriately motivated\nor set in context of recent work (see also comments by R3). Finally, a minor point -- At the bottom of Page 4 there is a paragraph about a heuristic used at training time.\nPlease expand on this further if you found it useful by explicitly stating how it changes the lower-bound at training time. \nPlease also use a different name than \"Stochastic Backpropagation\" which has been used before in https://arxiv.org/abs/1401.4082."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513865527374,"tcdate":1513865463908,"number":6,"cdate":1513865463908,"id":"SkeZ-Btff","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"H1BQNZqgf","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Comments and explanations","comment":"Thank you for your positive comments. Indeed, making the forward LSTM ‘aware’ of the backward LSTM’s state is a crucial factor in improving the expressivity of our model. \n\nWe apologize for the lack of clarity in the submitted version; we have made the text clearer in our latest version. To clarify the doubt you mentioned about \\tilde{b}, we do sample \\tilde{b}_t during training from p_{\\psi}(\\tilde{b}_t | z_t), and feed it to h_t, where z_t is sampled from q_\\phi (z_t | h_{t-1}, b_t).  This process of inferring \\tilde{b}_t and feeding to h_t however is implicitly captured in the term p(x_{t+1} | h_t ). The only \\tilde{b}_t dependent term that appears in the objective is p_{\\psi}(b_t | z_t ), which (to be precise) maximizes p_{\\psi}( \\tilde{b}_t = b_t | z_t ).\n\nRegarding your comment “both \\tilde{b} and \\tilde{h} should be presented as *observed* random variables during *training* and latent at inference time”, that’s true. \n\nRegarding your comment about not setting alpha to anything less than 1, we are not sure if we understand your concern correctly. We treat alpha as a hyperparameter, so its value should be chosen based on validation set. The positive value of alpha governs how much weightage is given to the reconstruction of b_t vs the rest of the terms in the cost.\n\nAs suggested by the reviewer, here is a brief comparison between our model and the papers cited by the reviewer: \nIn krishnan et al, the data x_t at each time step t is modeled using a VAE with hidden state z, where the approximate posterior q_{\\phi} (z | x) is a function of the forward and backward hidden states, and the KL divergence minimizes the difference between this approximate posterior and the prior over z. The key difference between their model and ours is that their model learns a VAE on the data space, i.e., the reconstruction error is on the data itself, such that the latent variable z of the VAE is a function of the Bi-RNN's hidden states. In our model on the other hand, the VAE is learned on the Bi-LSTM's hidden state, i.e., the reconstruction error is on the forward and backward LSTM's hidden states h_t and b_t which share the latent variable z_t. In Gao et al, the approximate prior at time step t is modeled as q_{\\phi} (z_t | z_{t-1}, x_t ), which factorizes as q_{\\phi} (z_t | z_{t-1} ) . q_{\\phi} (z_t | x_t ). Each of the latter two functions are modeled as Gaussians with mean and variance as a non-linear function of z_{t-1} and x_t respectively. Thus this model does not make use of recurrent neural networks in modeling the data. Secondly, similar to Krishnan et al, this model learns to reconstruct data instead of a hidden space, as in our model.\n\nWe did try experiments without modeling \\tilde{h}_t but found the results to be slightly worse. We believe it acts as a regularizer on the activation h_t learned by the model. But in general the coefficient \\beta used for the reconstruction loss of h_t is a hyperparameter and so it should be chosen using the validation set. \nIndeed, in the ablation studies, we report that the KL term is not useful in the case of PTB dataset because the KL term is small and performance remains unaffected when not including it in the objective. But performance drops in the case of the other datasets if the KL term is removed since for these datasets the KL term is large. \n\nOnce again, we apologize for the lack of clarity. We have made the text clearer in the latest version of our paper which can be found at the anonymous link (https://anonfile.com/W6i9bad3b4/ICLR18_VLM.pdf).\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513865237796,"tcdate":1513865237796,"number":5,"cdate":1513865237796,"id":"BJRGgSFff","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"HJg6l0FxM","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Clarifications","comment":"Thank you for your constructive comments. You are right. We have corrected Eq 1 in our latest version (https://anonfile.com/W6i9bad3b4/ICLR18_VLM.pdf). Please note that our model implementation was not affected by these writing mistakes.\n\nTo answer why it is beneficial to make h_t dependent on \\tilde{b}_t, note that forward and backward LSTMs model data sequences independently in different ways in a traditional Bi-LSTM. Creating the dependence from \\tilde{b}_t to h_t is important to make the forward LSTM use of information from the backward LSTM and thus learn a richer representation. This representation is useful in tasks like next step prediction where only the forward LSTM is used during inference and hence the structure captured by the backward LSTM is lost in the case of a traditional Bi-LSTM. In our model this structure is utilized.\n\nWe did experiments where we remove the connection from \\tilde{b}_t to h_t and found that only using \\tilde{b}_t in the reconstruction cost (as a regularizer) does not produce as good results as our model where both the reconstruction and feeding \\tilde{b}_t to h_t is used. Thus feeding \\tilde{b}_t to h_t helps the forward model during inference. On the flip side, we do not pass \\tilde{h}_t to b_t because we do not use the backward LSTM during inference, and so it may not benefit us.\n\nYes, in the no reconstruction loss experiments we do sample \\tilde{b}_t.\n\nWe have uploaded the Blizzard results in Figure 2 with no reconstruction loss + full backprop that you asked for to this anonymous link https://anonfile.com/j3nbo0dbbd/blz_rec_sdc_full.png It can be seen that reconstruction loss with stochastic backprop yields the best performance compared to all other alternatives.\n\nRegarding setting \\beta=0, we treat it as a hyperparameter and so it is chosen using the validation set. We do not have any explanation why having it zero is better sometimes. \n\nWe have made the description of skip gradient clearer in the latest version. The idea is to stochastically skip gradients of the auxiliary reconstruction costs with respect to the recurrent\nunits from back-propagating through time. To achieve this, at each time step, a mask drawn from a Bernoulli distribution which governs whether to skip the gradient or to back-propagate it for each data sample."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513862261634,"tcdate":1513862261634,"number":4,"cdate":1513862261634,"id":"ByA_NNKzz","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"Hy7uO8PlG","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Clarifications and additional experimental results","comment":"Thank you for your detailed comments. The major concern of the reviewer seems to be the lack of a clear contrast between our proposed model and Z-Forcing, which is closely related to our model. We will try to make this distinction clearer. In order to showcase the differences between Z-Forcing and our model in terms of the differences between how our model is trained, rather than just the architectural differences (as stressed by the reviewer in their comments), we additionally conducted the following experiment which incrementally adds the additional optimization changes to Z-Forcing (that we added to train our model). Specifically we run experiments to see the effects of stochastic backprop on Z-Forcing. We also add a reconstruction cost on h_t in the Z-Forcing model as another separate experiment. So for a detailed comparison, we show the evolution of Bits Per Character (BPC) on PTB for four cases:\n1. Z-forcing\n2. Z-forcing + stochastic backprop (on the auxiliary cost)\n3. Z-forcing + stochastic backprop (on the auxiliary cost) + reconstruction/auxiliary loss\n4. Variational Bi-LSTM\nThe plot can be found in this anonymous link https://anonfile.com/HdFdcadbb6/ptb_sdc_zf_rec.png . As can be seen there is a gradual improvement from model 1 to model 4. \nFurther, we also have the following ablation studies in the latest version (https://anonfile.com/W6i9bad3b4/ICLR18_VLM.pdf) of our paper that the\n Reconstruction loss on h_t  vs activity regularization on h_t-- here we show how the auxiliary reconstruction loss on h_t performs between compared with simply using an l2 regularization on h_t. \n Use of parametric encoder prior vs. fixed (standard VAE) Gaussian prior-- here we discuss the importance of the VAE prior we propose (which is conditional over h_t) compared to a fixed Gaussian that is usually used in VAEs.\n Effectiveness of auxiliary costs and stochastic back-propagation-- here we show that stochastic backpropagation helps during optimization.\n Importance of sampling from VAE prior during training-- here we show that sampling z_t during training has a regularization effect on the model.\n\nTo address the reviewer’s concern regarding additional qualitative analysis of the data generated by our mode, here are some of the samples generated by our model on the IMDB dataset:\nit was very well directed by the critics and critics who have n't seen it .\ni did n't want to see this movie .\nbut the movie does have a few laughs .\nthe action is also very well acted but it has a great story .\nit 's just a bit too slow and the ending is very good .\nthis film is not as bad as you 've heard .\nit 's also quite a good film with a great cast and great story lines .\nand what the movie was nominated for is a great cast .\nit 's a good film and you ca n't miss it .\n\nRegarding your concern for the need of a new regularizer, our reasoning is as follows. In the paper we already mention that the forward and backward LSTM capture different aspects of a temporal sequence, and this is the reason why (for traditional Bi-LSTMs) concatenating the hidden representations from the two LSTMs leads to better performance in tasks where such a concatenation is possible. However, this concatenation is not possible in the next step prediction tasks (Eg. language generation) where only the forward LSTM must be used during inference. Hence the information captured by the backward LSTM in a Bi-LSTM trained separate from the forward LSTM is lost. For this reason, an objective/regularization that jointly optimizes the two LSTMs in a Bi-LSTM is needed. Other examples of such joint optimization are Z-Forcing and twin networks that we cite in our paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642386169,"tcdate":1511818269428,"number":3,"cdate":1511818269428,"id":"H1BQNZqgf","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Review","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Propose explicitly modeling the hidden state of the backward RNN used for inference in a sequential deep generative model. Good idea, good empirical performance; the writing, however, is confusing.","rating":"6: Marginally above acceptance threshold","review":"This paper builds a sequential deep generative model with (1) an inference network parameterized by an RNN running from the future to the past and (2) an explicit representation of the hidden state of the backward RNN in the generative model. The model is validated on held-out likelihood via the ELBO on text, handwriting, speech and images. It presents good emprical results and works at par with or better than many other baselines considered.\n\nThe main source of novelty here the choice made in the transition function of z_t to also incorporate an explicit variable to models the hidden state of the backward RNN at inference time and use that random variable in the generative process. This is a choice of structural prior for the transition function of the generative model that I think lends it more expressivity realizing the empirical gains obtained.\n\nI found the presentation of both the model and learning objective to be confusing and had a hard time following it. The source of my confusion is is that \\tilde{b} (the following argument applies equivalently to \\tilde{h}) is argued to be a latent variable. Yet it is not inferred (via a variational distribution) during training.\n\nPlease correct me if I'm wrong but I believe that an easier to understand way to explain the model is as follows: both \\tilde{b} and \\tilde{h} should be presented as *observed* random variables during *training* and latent at inference time. Training then comprises maximizing the marginal likelihood of the data *and* maximizing the conditional likelihood of the two observed variables(via p_psi and p_eta; conditioned on z_t). Under this view, setting beta to 0 simply corresponds to not observing \\tilde{h_t}. alpha can be annealed but should never be set to anything less than 1 without breaking the semantics of the learned generative model.\n\nConsider Figure 1(b). It seems that the core difference between this work and [Chung et. al] is that this work parameterizes q(Z_t) using x_t....x_T (via a backward RNN). This choice of inference network can be motivated from the point of view of building a better approximation to the structure of the posterior distribution of Z_t under the generative model. Both [Fracarro et. al] and [Krishnan et. al] (https://arxiv.org/pdf/1609.09869.pdf) use RNNs from x_T to x_1 to train sequential state space models. [Gao et. al] (https://arxiv.org/pdf/1605.08454.pdf) derive an inference network with a block-diagonal structure motivated by correlations in the posterior distribution. Incorporating a discussion around this idea would provide useful context for where this work stands amongst the many sequential deep generative models in the\nliterature.\n\nQuestions for the authors:\n* How important is modeling \\tilde{h_t} in TIMIT, Blizzard and IMDB?\n* Did you try annealing the KL divergence in the PTB experiment. Based on the KL divergence you report it seems the latent variable is not necessary.\n\nOverall, I find the model to be interesting and it performs well empirically. However, the text of the paper lacks a bit of context and clarity that makes understanding it challenging to understand in its current form.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642386212,"tcdate":1511805112035,"number":2,"cdate":1511805112035,"id":"HJg6l0FxM","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Review","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting paper, but not the clearest presentation.","rating":"7: Good paper, accept","review":"This paper proposes a particular form of variational RNN that uses a forward likelihood and a backwards posterior.  Additional regularization terms are also added to encourage the model to encode longer term dependencies in its latent distributions.\n\nMy first concern with this paper is that the derivation in Eq. 1 does not seem to be correct.  There is a p(z_1:T) term that should appear in the integrand.\n\nIt is not clear to me why h_t should depend on \\tilde{b}_t.  All paths from input to output through \\tilde{b}_t also pass through z_t so I don't see how this could be adding information.  It may add capacity to the decoder in the form of extra weights, but the same could be achieved by making z_t larger. Why not treat \\tilde{b}_t symmetrically to \\tilde{h}_t, and use it only as a regularizer?  \n\nIn the no reconstruction loss experiments do you still sample \\tilde{b}_t in the generative part?  Baselines where the \\tilde{b}_t -> h_t edge is removed would be very nice.\n\nIt seems the Blizzard results in Figure 2 are missing no reconstruction loss + full backprop.\n\nI don't understand the description of the \"Skip Gradient\" trick.  Exactly which gradients are you skipping at random?\n\nDo you have any intuition for why it is sometimes necessary to set beta=0?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642386256,"tcdate":1511643242576,"number":1,"cdate":1511643242576,"id":"Hy7uO8PlG","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Review","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference/Paper1125/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting ideas; paper could be improved by more ablation experiments, theoretical justifications, and evaluation methods","rating":"4: Ok but not good enough - rejection","review":"*Quality*\n\nThe paper is easy to parse, with clear diagrams and derivations at the start. The problem context is clearly stated, as is the proposed model.\n\nThe improvements in terms of average log-likelihood are clear. The model does improve over state-of-the-art in some cases, but not all.\n\nBased on the presented findings, it is difficult to determine the quality of the learned models overall, since they are only evaluated in terms of average log likelihood. It is also difficult to determine whether the improvements are due to the model change, or some difference in how the models themselves were trained (particularly in the case of Z-Forcing, a closely related technique). I would like to see more exploration of this point, as the section titled “ablation studies” is short and does not sufficiently address the issue of what component of the model is contributing to the observed improvements in average log-likelihood.\n\nHence, I have assigned a score of \"4\" for the following reasons: the quality of the generated models is unclear; the paper does not clearly distinguish itself from the closely-related Z-Forcing concept (published at NIPS 2017); and the reasons for the improvements shown in average log-likelihood are not explored sufficiently, that is, the ablation studies don't eliminate key parts of the model that could provide this information.\n\nMore information on this decision is given in the remainder.\n\n*Clarity*\n\nA lack of generated samples in the Experimental Results section makes it difficult to evaluate the performance of the models; log-likelihood alone can be an inadequate measure of performance without some care in how it is calculated and interpreted (refer, e.g., to Theis et al. 2016, “A Note on the Evaluation of Generative Models”).\n\nThere are some typos and organizational issues. For example, VAEs are reintroduced in the Related Works section, only to provide an explanation for an unrelated optimization challenge with the use of RNNs as encoders and decoders.\n\nI also find the motivations for the proposed model itself a little unclear. It seems unnatural to introduce a side-channel-cum-regularizer between a sequence moving forward in time and the same sequence moving backwards, through a variational distribution. In the introduction, improved regularization for LSTM models is cited as a primary motivation for introducing and learning two approximate distributions for latent variables between the forward and backward paths of a bi-LSTM. Is there a serious need for new regularization in such models? The need for this particular regularization choice is not particularly clear based on this explanation, nor are the improvements state-of-the-art in all cases. This weakens a possible theoretical contribution of the paper.\n\n*Originality*\n\nThe proposed modification appears to amount to a regularizer for bi-LSTMs which bears close similarity to Z-Forcing (cited in the paper). I recommend a more careful comparison between the two methods. Without such a comparison, they are a little hard to distinguish, and the originality of this paper is hard to evaluate. Both appear to employ the same core idea of regularizing an LSTM using a learned variational distributions. The differences *seem* to be in the small details, and these details appear to provide better performance in terms of average log-likelihood on all tasks compared to Z-Forcing--but, crucially, not compared to other models in all cases.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511408670479,"tcdate":1511408670479,"number":3,"cdate":1511408670479,"id":"rJU7Epmef","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"H1RmckMeM","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Further experiments for clarification","comment":"The analysis of our method with and without stochastic backprop as well as with and without reconstruction losses are provided in the ablation studies section (figure 2). The text around this figure is unfortunately missing in the submitted version but can be found in the latest (anonymous) version we have linked in our previous reply. This analysis shows how our model benefits from stochastic backprop.\n\nWe had also run experiments to see the effects of stochastic backprop on Z-forcing. We additionally also add a reconstruction cost on h_t in the Z-forcing model as another separate experiment. So for a detailed comparison, we show the evolution of BPC on PTB for four models:\n1. Z-forcing\n2. Z-forcing + stochastic backprop (on the auxiliary cost)\n3. Z-forcing + stochastic backprop (on the auxiliary cost) + reconstruction/auxiliary loss\n4. Variational Bi-LSTM\nThe plot can be found in this anonymous link https://anonfile.com/HdFdcadbb6/ptb_sdc_zf_rec.png . As can be seen there is a gradual improvement from model 1 to model 4. \n\nWe agree with your suggestion of exploring the usefulness of the latent variable z and we ourselves had given thought to it. However, this is not the focus of our work, and this analysis applies to all models that make use of a latent variable in LSTMs (including Z-forcing). So we leave this as separate future work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511408228310,"tcdate":1511408228310,"number":2,"cdate":1511408228310,"id":"SknDf6mxz","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"S1-LoAGxM","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Clarification for Eq. 10","comment":"We thank you for raising this question. Your first doubt regarding \\tilde{b}_t is because of ambiguity in our notation. We do sample \\tilde{b}_t from p_psi. These samples are used in the third term-- alpha log p_psi(b_t | z_t). To remove ambiguity, this term should be read as-- alpha log p_psi(\\tilde{b}_t = b_t | z_t).\n\nRegarding your second question about terms that should relate \\tilde{b}_t and h_t, we believe the notations are correct. Imagine if we were to write the objective for a simple LSTM, then this objective would simply contain a summation of terms p(x_{t+1} | h_t) over time steps t. The dependence of h_t on the previous time steps are implicit. Similarly, in our objective, the term p(x_{t+1} | h_t) implicitly contains the dependence on \\tilde{b}_t, z_t and the previous time step variables."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511349486857,"tcdate":1511349064998,"number":3,"cdate":1511349064998,"id":"S1-LoAGxM","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Public_Comment","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some questions about Eq. 10 in latest version ","comment":"Dear authors,\n\nit's an interesting research, but I still have some questions about the objective, and I hope you can give me a help!\n\nIn Eq.10, the objective, \\tilde{b}_t is sampled from p_psi. However, there is no \\tilde{b}_t in the inside term, so it seems that there is no need to sample \\tilde{b}_t? Is it just a typo?\n\nNext, based on figure 1(a) and your answer, I think there may be some terms to stand for the direct connection between h_t and \\tilde{b}_t. However, it seems that, in Eq. 10 , there is no term to stand for the directly conditional dependence between h_t and \\tilde{b}_t( or b_t ). I guess maybe the term p(x_{t+1} | b_t) includes relations like p(x_{t+1} | h_t)p(h_t | b_t), is it true?\nThanks for your help!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511327904737,"tcdate":1511287334065,"number":2,"cdate":1511287334065,"id":"H1RmckMeM","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Public_Comment","forum":"SkqV-XZRZ","replyto":"rkK4-0bgG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for clarification!","comment":"\"Another possible difference in your implementation could be that we suggest using stochastic back-propagation through the auxiliary costs. This entails that the gradients through the auxiliary cost be stochastically dropped during training\"\n\nI was not dropping these gradients, this is the only difference I could think of. Though, this raises the point, is all the benefit actually coming from this \"stochastic back-propagation\" over Z-Forcing ? As without using this stochastic back-propagation, results seems more or less same to Z-Forcing (https://arxiv.org/abs/1711.05411)\n\nI'd encourage the authors to add the results with/without \"stochastic back-propagation\"  and compare themselves to the results which Z-Forcing paper reports.\n\nAnother thing which would make this submission strong,  is to analyze how useful the latents (learned z's) are. For ex. may be for some classification task.   "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511280944639,"tcdate":1511280944639,"number":1,"cdate":1511280944639,"id":"rkK4-0bgG","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Official_Comment","forum":"SkqV-XZRZ","replyto":"ByTDAogxG","signatures":["ICLR.cc/2018/Conference/Paper1125/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1125/Authors"],"content":{"title":"Probable causes of your issue","comment":"We appreciate your interest in our paper and your effort to reproduce our results. \nWe apologize for the lack of clarity in the submitted version. We have improved the model description in our current version. We would like to point out that while our model is similar in spirit to Z-forcing, there are notable differences in the derivation of the variational lower bound and the auxiliary costs that provide improvement in performance because the forward LSTM is implicitly directly fed the backward LSTM's state which is in contrast with Z-forcing.\n\nFrom your comment, it seems to us that you add a reconstruction cost on h_t on top of the Z-forcing objective. If this is true, then we would like to clarify that in addition to adding the reconstruction cost and feeding z_t to h_t, we also pass \\tilde{b}_t to h_t. Amongst other differences, this is a crucial difference between Z-forcing and our model. In other words, during training, we sample \\tilde{b}_t for a sampled z_t, and encourage this \\tilde{b}_t to be similar to b_t, and also feed this \\tilde{b}_t to h_t. In this way, our model learns to implicitly use b_t during training as an input to h_t. This is different from Z-forcing where the model passes z_t to h_t while minimizing the KL divergence difference between the prior and posterior over z_t.\n\nAnother possible difference in your implementation could be that we suggest using stochastic backpropagation through the auxiliary costs. This entails that the gradients through the auxiliary cost be stochastically dropped during training.\n\nWe hope these suggestions help in reproducing the results we report in our paper.\n\nFor further clarification, we have uploaded an anonymous copy of the latest version of our paper here: https://anonfile.com/W6i9bad3b4/ICLR18_VLM.pdf."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511213200479,"tcdate":1511206501364,"number":1,"cdate":1511206501364,"id":"ByTDAogxG","invitation":"ICLR.cc/2018/Conference/-/Paper1125/Public_Comment","forum":"SkqV-XZRZ","replyto":"SkqV-XZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Difference to Z-Forcing ?","comment":"Hello Authors, \n\nVery interesting work! \n\nI have been trying to reproduce your experiments. As far as I understand it is straightforward extension to Z-Forcing(https://arxiv.org/abs/1711.05411). I tried to replicate your results using the Z-Forcing code(https://github.com/sordonia/zforcing) so far I have not been able to replicate your results. Adding the reconstruction cost (in the forward RNN, which was also missing from Z-Forcing) does not seem to have any impact on results. \n\nSo are you doing something which is not mentioned in the paper?\n\n    "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092380311,"tcdate":1509138743610,"number":1125,"cdate":1510092359696,"id":"SkqV-XZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkqV-XZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variational Bi-LSTMs","abstract":"Recurrent neural networks like long short-term memory (LSTM) are important architectures for sequential prediction tasks. LSTMs (and RNNs in general) model sequences along the forward time direction. Bidirectional LSTMs (Bi-LSTMs), which model sequences along both forward and backward directions, generally perform better at such tasks because they capture a richer representation of the data. In the training of Bi-LSTMs, the forward and backward paths are learned independently. We propose a variant of the Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a dependence between the two paths (during training, but which may be omitted during inference). Our model acts as a regularizer and encourages the two networks to inform each other in making their respective predictions using distinct information. We perform ablation studies to better understand the different components of our model and evaluate the method on various benchmarks, showing state-of-the-art performance.","pdf":"/pdf/4324fa39868648281fcca9536b21bab92f264995.pdf","paperhash":"anonymous|variational_bilstms","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Bi-LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkqV-XZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1125/Authors"],"keywords":[]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}