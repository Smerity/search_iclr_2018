{"notes":[{"tddate":null,"ddate":null,"tmdate":1516418225102,"tcdate":1516418225102,"number":9,"cdate":1516418225102,"id":"BytnN4xSz","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"S1Nj--xSG","signatures":["~Jeasine_Ma1"],"readers":["everyone"],"writers":["~Jeasine_Ma1"],"content":{"title":"Response to the author's response","comment":"Thanks for your response! Here I still got 2 questions:\n1. Although it is ok to implicitly learned the partition function Z, however, Z is a function of s, and since it only depends on s, it will not hurt your conclusion on state-only reward recovery, while the bias still exists. The experiments do shows some empirical evidence on whether this bias will affect the result, but I think a comparative \n experiment with an unbiased reward(at least without the logZ bias) is needed. What is your opinion on that?\n2. (Sorry, this question is proposed with assumption that the comparative experiment in Q1 is needed, however you may think that it is useless ;)) A simple but efficient method of the experiment can be training the state value function with a biased reward and TD(0) method first, then use vanilla method to compute the advantages(A = \\sum\\limits_t{r_t} - V(s_t)), since V is optimal, bias in advantage can be eliminated. Then this advantage can be used for any advantage-baed method including TRPO. Do you have any other method for this experiment?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516406257352,"tcdate":1516406257352,"number":7,"cdate":1516406257352,"id":"HyKxU-erf","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"B1c_e2Fzf","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"No performance guarantees, but may work well in practice depending on the environment","comment":"If the ground truth reward depends on both states and actions, the algorithm cannot represent the true reward and thus the performance of the policy will not match that of the experts (we have included new experiments in Section 7.3 for this case). The results will likely depend on the task - in our experiments the performance was not much worse than the experts, but the only action-dependent term in the reward for OpenAI Gym locomotion tasks is a control penalty for actions with large magnitude.\n\nHowever, we also argue that no IRL algorithm which operates over arbitrary reward function classes will be able to recover ground truth rewards in this case, since we cannot avoid reward shaping (section 5). In order to remove shaping, we need to manually restrict the class of reward functions such that shaping is not possible. An alternative approach is to adopt a multi-task IRL paradigm to generalize across different dynamics.\n\nThe state definition for most OpenAI Gym locomotion tasks (including the ones used in this paper) contains velocities - thus we can still represent the ground truth reward. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516405148022,"tcdate":1516405148022,"number":6,"cdate":1516405148022,"id":"S1Nj--xSG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"BJ-3TanEM","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"The partition function is learned as part of f(\\tau)","comment":"Section 3.1 of Finn 2016 (http://arxiv.org/abs/1611.03852) is incorrect in regard to learning the partition function on the bias of the last sigmoid layer. We can't uniquely separate the bias term from the rest of the function approximator. For example, the cost function approximator c_\\theta(tau) could incorporate the log Z term and we could set the learned bias term to 0. Thus, there is no point in explicitly adding a separate learned bias term to capture the partition function as in Finn16 - we simply learn a function f(\\tau) which implicitly learns the partition function, although we cannot extract it."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516201859250,"tcdate":1516195241228,"number":8,"cdate":1516195241228,"id":"BJ-3TanEM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["~Jeasine_Ma1"],"readers":["everyone"],"writers":["~Jeasine_Ma1"],"content":{"title":"Where is the partition function Z?","comment":"Under the assumption of MaxEnt IRL, demonstration can be seen as trajectories drawn from (1/Z)exp(-c(\\tau)), however, in eq(2) the partition function disappears(compared to Sec 3.1 in http://arxiv.org/abs/1611.03852).  Correct me if I am wrong,  as the authors propose f*(\\tau) = R*(\\tau) + const(R is an entropy regularized reward, that's ok), do they mean by the logZ item(if the partition function exists in eq(2)) can be a constant?  And refer to Appendix A.4, f* == log\\pi_E == A*(s,a), the second part of this equation comes from soft Q-learning because of the entropy regularized reward R, the the first part holds because it eliminates the partition function Z, so I wonder if it still holds even when the partition function is added? \n\nAnd there is another minor typo(?) The inline equation under eq(2): R(\\tau) = log(1-D(\\tau)) - log(D(\\tau)) -> R(\\tau) = log(D(\\tau)) - log(1-D(\\tau)), according to Appendix A.3 "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515811817649,"tcdate":1515811817649,"number":5,"cdate":1515811817649,"id":"SJfeNePVM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"BJz9VDTQG","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/AnonReviewer2"],"content":{"title":"After rebuttal","comment":"Thank you for your reply and the revision of the paper. I briefly gone through the revised paper. My concerns have been addressed (but I should say that I have not verified the math closely)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515185290203,"tcdate":1515185290203,"number":4,"cdate":1515185290203,"id":"BJz9VDTQG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"ryZzenclz","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thank you for the constructive feedback. We’ve incorporated your comments and clarified certain points of the paper below. Please let us know if there are other additional issues which need clarification.\n\n> The MaxEnt IRL formulation of this work, which assumes that p_theta(tau) is proportional to exp( r_theta (tau) ), comes from\n[Ziebart et al., 2008] and assumes a deterministic dynamics. Ziebart’s PhD dissertation [Ziebart, 2010] or the following paper show that the formulation is different for stochastic dynamics.\nIs it still a reasonable thing to develop based on this earlier, an inaccurate, formulation?\n\nWe have updated the background (section 3) and appendix (section A) to use the maximum causal entropy framework rather than the earlier maximum entropy framework of [Ziebart 08]. Our algorithm requires no changes since the causal entropy framework more accurately describes what we were doing in the first place (our old derivations were valid in the deterministic case, where MaxEnt and MaxCausalEnt are identical, but in the stochastic case, our approach in fact matches MaxCausalEnt).\n\n> * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants.\nAlso please discuss how ergodicity leads to the conclusion that spaces of s’ and s are identical. What does “space of s” mean? Do you mean the support of s? Please make the argument more rigorous.\n* Please make the argument of Section 5.1 more rigorous.\n\nWe’ve provided more formal proofs for Section 5 and the appendix. In order to fix the statements, we’ve changed the condition on the dynamics - a major component is that it requires that each state be reachable from >1 other state within one step. Ergodicity is neither a sufficient nor necessary condition on the dynamics, but special cases such as an ergodic MDP with self-transitions at each state satisfies the new condition (though the minimum necessary conditions are less restrictive).\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515185254426,"tcdate":1515185254426,"number":3,"cdate":1515185254426,"id":"SJAvEPa7f","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"Hyn6kL_xG","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thank you for the detailed feedback. We have included all of the typo corrections and clarifications, as well as included state-only runs in the imitation learning experiments (Section 7.3). As detailed below, we believe that we have addressed all of the issues raised in your review, but we would appreciate any further feedback you might offer.\n\n> The need for an \"adversarial\" approach is not justified as fully, but perhaps is a consequence of recent work.\n\nAdversarial approaches are an inherent consequence of using sampling-based methods for training energy-based models, and we’ve edited Section 2, paragraph 2 to make this more clear. There is in fact no other (known) choice for doing this: any method that does maxent IRL and generates samples (rather than assuming known dynamics) must be adversarial in nature, as shown by Finn16a. Traditional methods like tabular MaxEnt IRL [Ziebart 08] have an adversarial nature as they must alternate between an inner-loop RL problem (the sampler) and updating the reward function (the discriminator).\n\n> Although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak.\n\nWe’ve slightly modified the paragraph before section 7.1 to make this connection more clear. We use environments where a reward function is available for the purpose of easily collecting demonstrations (otherwise we would need to resort to motion capture or teleoperation). However the experimental setup after demo collection is exactly the same as one would encounter while using IRL when a ground truth reward is not available.\n\n> Amin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.)\n\nAmin, Jian & Singh’s work is indeed relevant and we have also included it in the related work section.\n\n> Perhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare.\n\nWe’ve included these in the experiments. State-only runs perform slightly worse as expected, since the true reward has torque penalty terms which depend on the action, and cannot be captured by the model. However the performance isn’t so bad that the agent fails to solve the task.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515185172185,"tcdate":1515185172185,"number":2,"cdate":1515185172185,"id":"SJnzNvaXz","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"ryyF8NyZM","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Thank you for the thoughtful feedback. We’ve incorporated the suggestions to the best of our ability, and clarified portions of the paper, as described below.\n\n> \"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" This is not an assumption, it's the form of the solution we get by maximizing the entropy (for regularization).\n\nWe’ve modified Section 3 to remove this ambiguity (note that we’ve also modified the section to use the causal entropy framework as requested by another reviewer). This statement was referring to the fact that we are assuming the expert is drawing samples from the distribution p(tau), not the fact that p(tau) \\propto exp(r(tau)).\n\n> \"The paper lacks a little bit in novelty since it is basically a variant of GAN-GCL, but it makes it up with the inclusion of  a shaping term in the rewards and with the related formal arguments.\"\n\nIn regard to GAN-GCL, we would note that, although the method draws heavily on the theory in this workshop paper, it is unpublished and does not describe an implementation of any actual algorithm -- the GAN-GCL paper simply describes a theoretical connection between GANs and IRL. Our implementation of the algorithm that is closest to the one suggested by the theory in the GAN-GCL workshop paper does not perform very well in practice (Section 7.3).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1514518581578,"tcdate":1513894001795,"number":7,"cdate":1513894001795,"id":"B1c_e2Fzf","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"What if the groundtruth reward is a function of both the state and the action?","comment":"In figure 1, the authors show an example of state only reward recovers the groundtruth. However, the groundtruth reward here is a function of only state. What if the groundtruth reward is a function of both the state and the action, can we still apply this method? \n\nIn the continuous control experiment 2, if the ant achieves reward by moving forward. It is obvious that the reward depends on both s and s', I'm confused how a reward solely depending on s is able to recover the groundtruth."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1513397917123,"tcdate":1513397917123,"number":6,"cdate":1513397917123,"id":"S1HsAfGzf","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"ICLR 2018 Reproducibility Review","comment":"We reproduce the results from the submitted ICLR paper: \"Learning Robust Rewards with Adversarial Inverse Reinforcement Learning\" where we reproduce the previous state of the art results, namely the Generative Adversarial Network - Guided Cost Learning (Finn et. al 2016), the Generative Adversarial Imitation Learning (Ho & Ermon 2016) and make a comparison to the non-robust version of the AIRL algorithm (a previous iteration of the robust version of the AIRL algorithm) methods on the pendulum, custom designed ant and pointmass openAI gym environments. \n\nThis paper introduces an inverse reinforcement learning technique that utilizes a Generative Adversarial Network (GAN) (Goodfellow et. al 2014) to generate its rewards called Adversarial Inverse Reinforcement Learning (AIRL) (Anonymous, 2018). The algorithm updates the discriminator by training with expert trajectories, and then updating the policy in an attempt to confuse the discriminator. The paper makes two main claims about the functionality of the algorithm: 1) AIRL can learn robust rewards that make it the optimal algorithm for transfer learning tasks. 2) That AIRL is scalable up to high-dimensional tasks. This paper goes on to further claim that AIRL can perform competitively in imitation learning environments when compared to previous state-of-the-art algorithms such as generative adversarial imitation learning (GAIL) (Ho & Ermon 2016), but when it comes to transfer learning tasks, AIRL performs significantly better compared to those same algorithms.\n\nWhile we could not implement the robust AIRL algorithm, we made the effort to do as many experiments with the baseline algorithms. We believe that our inability to reproduce the full robust AIRL algorithm is not a statement on the reproducibility of this paper, but simply due to our lack of technical expertise. The results of these methods are as follows: \n\nTransfer Learning Experiments:\n\n        Method & Pointmass-Maze & Disabled Ant      \n        GAN-GCL & -61.8  & -79.201\n        AIRL & -51.2  & -92.578\n        GAIL & -40.2  & -70.5668\n        TRPO (Expert Policy) & -17.1 & 150.7\n\nImitation Learning Experiments:\n\n        Method & Pendulum & Ant \n        GAN-GCL & -242.5 & 467.7\n        AIRL & -210.7 & 983.7\n        GAIL & -198.2 & 1501.3\n        TRPO (Expert Policy) & -128.4 & 2000.6 \n\nAs can be seen from these tables, our results and those found in the paper seem to be fairly similar. Our lower results with the AIRL algorithm is to be expected as we implemented the non-robust version whereas the paper shows results for the robust version. The variance in our results could be due to the unspecified n_iteration parameter, where higher/lower values of the n_iteration could contribute to higher/lower scores respectively. \n\nOur choice of hyperparameters were effectively those found in the report and the default hyperparameters found in the code provided to us by the authors. We increased the number of iterations to 1500 for the ant and pointmaze tasks, and increased it to 500 for the pendulum task to increase the chance of convergence. \n\nOur choice of selecting these environments was to test the claims that AIRL can not only be effective in transfer learning tasks, but to also scale up to high dimensional environments. Our empirical observations suggested that the ant/disabled ant task provided the highest dimensional environment for which we could test the scalability. Running on 7th Generation Intel® Core™ i7-7700HQ Quad Core Processor at 2.6GHz took 1 hour and 30 minutes to run 1000 iterations of the non-robust AIRL algorithm on the disabled ant task. \n\nAs far as the level of reproducibility is concerned. Having been provided the code from the authors went a long way to helping us reproduce the experiment. Within the code, the custom environments and the baseline algorithms were all provided which helped ensure that we were conducting the experiments in a fairly similar environment. Even though we could not reproduce the robust version of the AIRL algorithm, the mathematical foundations for the algorithm, along with pseudocode of the algorithm itself, is stated very clearly in the paper. As a result, our conjecture is that anyone who is more experienced in implementing code in RLLAB and openAIgym should have relatively little difficulty in implementing the robust version of the AIRL algorithm, but we do not have the expertise to state this for fact. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512272393756,"tcdate":1512272393756,"number":5,"cdate":1512272393756,"id":"rkGMGe-bG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"ryZzenclz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some discussion for this review","comment":"Yes, you missed an important fact for f*(s,a) = log_\\pi_E(a|s) = A*(s,a), this equality holds only under the policy are updated with a max entropy regularization(you can refer to this article: http://arxiv.org/abs/1702.08165). And under this context, Q and V are not just the the reward sum but with an extra entropy item. So it may be not correct to use such a simple example. In addition, since the methods in this paper are all with the max entropy item, it is ok for the authors to use this form of result."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512270602822,"tcdate":1512270602822,"number":4,"cdate":1512270602822,"id":"r17fiybWM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"ryyF8NyZM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some discussion for the last sentence of this review","comment":"I think \"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" is right. Since we sampled from expert policy or just made some demonstrations, and we were unaware to this max entropy items when we did this, we can only assume that these trajs obeying the boltzman distribution. In fact, in the context of MaxEnt IRL, only the optimal trajs distribution output by the model(by maximizing the max entropy item under the feature expectaton constraints) has a closed form of \\propto exp(r(tau))."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642476378,"tcdate":1512158838653,"number":3,"cdate":1512158838653,"id":"ryyF8NyZM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Review","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A variante of the GAN-GCL for Inverse RL (IRL) is presented and evaluated. The difference with the original algorithm is the fact that the sampling happens at the level of stat-actions instead of full trajectories, to reduce its variance. Empirical results clearly show the advantage of this method.","rating":"7: Good paper, accept","review":"This paper revisits the generative adversarial network guided cost learning (GAN-GCL)  algorithm presented last year. The authors argue learning rewards from sampled trajectories has a high variance. Instead, they propose to learn a generative model wherein actions are sampled as a function of states. The same energy model is used for sampling actions: the probability of an action is proportional to the exponential of its reward. To avoid overfitting the expert's demonstrations (by mimicking the actions directly instead of learning a reward that can be generalized to different dynamics), the authors propose to learn rewards that depend only on states, and not on actions. Also, the proposed reward function includes a shaping term, in order to cover all possible transformations of the reward function that could have been behind the expert's actions. The authors argue formally that this is necessary to disentangle the reward function from the dynamics. Th paper also demonstrates this argument empirically (e.g. Figure 1).\n\nThis paper is well-written and technically sound. The empirical evaluations seem to be supporting the main claims of the paper. The paper lacks a little bit in novelty since it is basically a variante of GAN-GCL, but it makes it up with the inclusion of  a shaping term in the rewards and with the related formal arguments. The empirical evaluations could also be strengthened with experiments in higher-dimensional systems (like video games). \n\n\"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" This is not an assumption, it's the form of the solution we get by maximizing the entropy (for regularization).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642476447,"tcdate":1511862280640,"number":2,"cdate":1511862280640,"id":"ryZzenclz","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Review","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Using the deterministic-MDP formulation of MaxEnt IRL is a concern","rating":"6: Marginally above acceptance threshold","review":"SUMMARY:\nThis paper considers the Inverse Reinforcement Learning (IRL) problem, and particularly suggests a method that obtains a reward function that is robust to the change of dynamics of the MDP.\n\nIt starts from formulating the problem within the MaxEnt IRL framework of Ziebart et al. (2008). The challenge of MaxEnt IRL is the computation of a partition function. Guided Cost Learning (GCL) of Finn et al. (2016b) is an approximation of MaxEnt IRL that uses an adaptive importance sampler to estimate the partition function. This can be shown to be a form of GAN, obtained by using a specific discriminator [Finn et al. (2016a)].\n\nIf the discriminator directly works with trajectories tau, the result would be GAN-GCL. But this leads to high variance estimates, so the paper suggests using a single state-action formulation, in which the discriminator f_theta(s,a) is a function of (s,a) instead of the trajectory. The optimal solution of this discriminator is to have f(s,a) = A(s,a) — the advantage function.\nThe paper, however, argues that the advantage function is “entangled” with the dynamics, and this is undesirable. So it modified the discriminator to learn a function that is a combination of two terms, one only depends on state-action and the other depends on state, and has the form of shaped reward transformation.\n\n\nEVALUATION:\n\nThis is an interesting paper with good empirical results. As I am not very familiar with the work of Finn et al. (2016a) and Finn et al. (2016b), I have not verified the detail of derivations of this new paper very closely. That being said, I have some comments and questions:\n\n\n* The MaxEnt IRL formulation of this work, which assumes that p_theta(tau) is proportional to exp( r_theta (tau) ), comes from\n[Ziebart et al., 2008] and assumes a deterministic dynamics. Ziebart’s PhD dissertation [Ziebart, 2010] or the following paper show that the formulation is different for stochastic dynamics:\n\nZiebart, Bagnell, Dey, “The Principle of Maximum Causal Entropy for Estimating Interacting Processes,” IEEE Trans. on IT, 2013.\n\nIs it still a reasonable thing to develop based on this earlier, an inaccurate, formulation?\n\n\n* I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants.\nIt is suggested that since the only items on both sides of the equation on top of p. 13 depend on s’ are h* and V, they should be equal.\nThis would be true if s’ could be chosen arbitrararily. But s’ would be uniquely determined by s for a deterministic dynamics. In that case, this conclusion is not obvious anymore.\n\nConsider the state space to be integers 0, 1, 2, 3, … .\nSuppose the dynamics is that whenever we are at state s (which is an integer), at the next time step the state decreases toward 1, that is s’ = phi(s,a) = s - 1; unless s = 0, which we just stay at s’ = s = 0. This is independent of actions.\nAlso define r(s) = 1/s for s>=1 and r(0) = 0.\nSuppose the discount factor is gamma = 1 (note that in Appendix B.1, the undiscounted case is studied, so I assume gamma = 1 is acceptable).\n\nWith this choices, the value function V(s) = 1/s + 1/(s-1) + … + 1/1 = H_s, i.e., the Harmonic function.\nThe advantage function is zero. So we can choose g*(s) = 0, and h*(s) = h*(s’) = 1.\nThis is in contrast to the conclusion that h*(s’) = V(s’) + c, which would be H_s + c, and g*(s) = r(s) = 1/s.\n(In fact, nothing is special about this choice of reward and dynamics.)\n\nAm I missing something obvious here?\n\nAlso please discuss how ergodicity leads to the conclusion that spaces of s’ and s are identical. What does “space of s” mean? Do you mean the support of s? Please make the argument more rigorous.\n\n\n* Please make the argument of Section 5.1 more rigorous.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642476487,"tcdate":1511706564444,"number":1,"cdate":1511706564444,"id":"Hyn6kL_xG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Review","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Learns environment-independent rewards; reasonable next step in adversarial IRL","rating":"6: Marginally above acceptance threshold","review":"The paper provides an approach to learning reward functions in high-dimensional domains, showing that it performs comparably to other recent approaches to this problem in the imitation-learning setting. It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state-action or state-action-state. It uses this property to produce \"disentangled rewards\", demonstrating that they transfer well to the same task under different transition dynamics.\n\nThe need for \"state-only\" rewards is a useful insight and is covered fairly well in the paper. The need for an \"adversarial\" approach is not justified as fully, but perhaps is a consequence of recent work. The experiments are thorough, although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak.\n\nDetailed feedback:\n\n\"deployed in at test-time on environments\" -> \"deployed at test time in environments\"?\n\n\"which can effectively recover disentangle the goals\" -> \"which can effectively disentangle the goals\"?\n\n\"it allows for sub-optimality in demonstrations, and removes ambiguity between demonstrations and the expert policy\": I am not certain what is being described here and it doesn't appear to come up again in the paper. Perhaps remove it?\n\n\"r high-dimensional (Finn et al., 2016b) Wulfmeier\" -> \"r high-dimensional (Finn et al., 2016b). Wulfmeier\".\n\n\"also consider learning cost function with\" -> \"also consider learning cost functions with\"?\n\n\"o learn nonlinear cost function have\" -> \"o learn nonlinear cost functions have\".\n\n\" are not robust the environment changes\" -> \" are not robust to environment changes\"?\n\n\"We present a short proof sketch\": It is unclear to me what is being proven here. Please state the theorem.\n\n\"In the method presented in Section 4, we cannot learn a state-only reward function\": I'm not seeing that. Or, maybe I'm confused between rewards depending on s vs. s,a vs. s,a,s'. Again, an explicit theorem statement might remove some confusion here.\n\n\"AIRLperforms\" -> \"AIRL performs\".\n\nFigure 2: The blue and green colors look very similar to me. I'd recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret.\n\n\"must reach to goal\" -> \"must reach the goal\"?\n\n\"pointmass\" -> \"point mass\". (Multiple times.)\n\nAmin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.)\n\nPerhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1511638137399,"tcdate":1511638137399,"number":3,"cdate":1511638137399,"id":"SJbt4SDez","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["~David_Anthony_Venuto1"],"readers":["everyone"],"writers":["~David_Anthony_Venuto1"],"content":{"title":"Code Release for ICLR Reproducibility Workshop","comment":"Hello,\n\nWe are also a group of 3 students from McGill University, who are participating in the ICLR 2018 Reproducibility Research.  We are also interested in the AIRL algorithm proposed in your paper and validating the results found in your paper. We noticed that you released a portion of your code, namely the non-robust version of your AIRL algorithm to other participants in this reproducibility challenge. We were wondering if you could also provide us with that link as it will go a long way to assisting us in reproducing your results.\n\nFor your convenience, we have provided our emails below.\n\nBest Regards,\n\nIsaac (isaac.chan@mail.mcgill.ca)\nDavid (david.venuto@mail.mcgill.ca)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510612752227,"tcdate":1510612752227,"number":2,"cdate":1510612752227,"id":"HJOGkoD1z","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"H11JStDyG","signatures":["~Max_Morrison1"],"readers":["everyone"],"writers":["~Max_Morrison1"],"content":{"title":"Contact Info","comment":"Hello authors,\n\nThat would be an excellent starting point. Our email addresses are as follows:\n\n  - Jin (wyjin@umich.edu)\n  - Max (morrimax@umich.edu)\n  - Sam (samtenka@umich.edu)\n\nThank you for your help,\nJin, Max, and Sam"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510606038665,"tcdate":1510606038665,"number":1,"cdate":1510606038665,"id":"H11JStDyG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"Skxih5LyM","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"Code is partially released (will fully release after acceptance)","comment":"Hi Jin, Max, and Sam\n\nWe won't release the full code until after acceptance, but we have already released a publicly available implementation of the baselines + the \"non-robust\" version of AIRL. This should be a very good starting point for reproducibility. If you can provide an email address, we can send you a link (so as to not break anonymity on OpenReview)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510546584600,"tcdate":1510546584600,"number":1,"cdate":1510546584600,"id":"Skxih5LyM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["~Max_Morrison1"],"readers":["everyone"],"writers":["~Max_Morrison1"],"content":{"title":"ICLR 2018 Reproducibility Workshop Inquiry","comment":"Hello authors,\n\nWe are three students at the University of Michigan working together to submit to the ICLR 2018 Reproducibility Workshop.  We are all very interested in the AIRL algorithm described in your paper “Learning Robust Rewards with Adversarial Inverse Reinforcement Learning” and would like to focus on it for our submission.  Specifically, we hope to both reproduce your results as well as conduct further experiments and hyperparameter tuning.  Our team would greatly benefit from working with your implementation.  Your paper mentioned that you intend to post the implementation of AIRL.  Do you happen to have a schedule for when the code will be available for us to use?\n\nThank you for your time and your work,\nJin, Max, and Sam\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515180483187,"tcdate":1509127901045,"number":595,"cdate":1509739209028,"id":"rkHywl-A-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/9a80f78161ba5b142cee920e26e43f42a11318f6.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]},"nonreaders":[],"replyCount":19,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}