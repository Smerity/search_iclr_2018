{"notes":[{"tddate":null,"ddate":null,"tmdate":1512272393756,"tcdate":1512272393756,"number":5,"cdate":1512272393756,"id":"rkGMGe-bG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"ryZzenclz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some discussion for this review","comment":"Yes, you missed an important fact for f*(s,a) = log_\\pi_E(a|s) = A*(s,a), this equality holds only under the policy are updated with a max entropy regularization(you can refer to this article: http://arxiv.org/abs/1702.08165). And under this context, Q and V are not just the the reward sum but with an extra entropy item. So it may be not correct to use such a simple example. In addition, since the methods in this paper are all with the max entropy item, it is ok for the authors to use this form of result."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512270602822,"tcdate":1512270602822,"number":4,"cdate":1512270602822,"id":"r17fiybWM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"ryyF8NyZM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some discussion for the last sentence of this review","comment":"I think \"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" is right. Since we sampled from expert policy or just made some demonstrations, and we were unaware to this max entropy items when we did this, we can only assume that these trajs obeying the boltzman distribution. In fact, in the context of MaxEnt IRL, only the optimal trajs distribution output by the model(by maximizing the max entropy item under the feature expectaton constraints) has a closed form of \\propto exp(r(tau))."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222695566,"tcdate":1512158838653,"number":3,"cdate":1512158838653,"id":"ryyF8NyZM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Review","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A variante of the GAN-GCL for Inverse RL (IRL) is presented and evaluated. The difference with the original algorithm is the fact that the sampling happens at the level of stat-actions instead of full trajectories, to reduce its variance. Empirical results clearly show the advantage of this method.","rating":"7: Good paper, accept","review":"This paper revisits the generative adversarial network guided cost learning (GAN-GCL)  algorithm presented last year. The authors argue learning rewards from sampled trajectories has a high variance. Instead, they propose to learn a generative model wherein actions are sampled as a function of states. The same energy model is used for sampling actions: the probability of an action is proportional to the exponential of its reward. To avoid overfitting the expert's demonstrations (by mimicking the actions directly instead of learning a reward that can be generalized to different dynamics), the authors propose to learn rewards that depend only on states, and not on actions. Also, the proposed reward function includes a shaping term, in order to cover all possible transformations of the reward function that could have been behind the expert's actions. The authors argue formally that this is necessary to disentangle the reward function from the dynamics. Th paper also demonstrates this argument empirically (e.g. Figure 1).\n\nThis paper is well-written and technically sound. The empirical evaluations seem to be supporting the main claims of the paper. The paper lacks a little bit in novelty since it is basically a variante of GAN-GCL, but it makes it up with the inclusion of  a shaping term in the rewards and with the related formal arguments. The empirical evaluations could also be strengthened with experiments in higher-dimensional systems (like video games). \n\n\"Under maximum entropy IRL, we assume the demonstrations are drawn from an optimal policy p(\\tau) \\propto exp(r(tau))\" This is not an assumption, it's the form of the solution we get by maximizing the entropy (for regularization).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222695602,"tcdate":1511862280640,"number":2,"cdate":1511862280640,"id":"ryZzenclz","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Review","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Using the deterministic-MDP formulation of MaxEnt IRL is a concern","rating":"6: Marginally above acceptance threshold","review":"SUMMARY:\nThis paper considers the Inverse Reinforcement Learning (IRL) problem, and particularly suggests a method that obtains a reward function that is robust to the change of dynamics of the MDP.\n\nIt starts from formulating the problem within the MaxEnt IRL framework of Ziebart et al. (2008). The challenge of MaxEnt IRL is the computation of a partition function. Guided Cost Learning (GCL) of Finn et al. (2016b) is an approximation of MaxEnt IRL that uses an adaptive importance sampler to estimate the partition function. This can be shown to be a form of GAN, obtained by using a specific discriminator [Finn et al. (2016a)].\n\nIf the discriminator directly works with trajectories tau, the result would be GAN-GCL. But this leads to high variance estimates, so the paper suggests using a single state-action formulation, in which the discriminator f_theta(s,a) is a function of (s,a) instead of the trajectory. The optimal solution of this discriminator is to have f(s,a) = A(s,a) — the advantage function.\nThe paper, however, argues that the advantage function is “entangled” with the dynamics, and this is undesirable. So it modified the discriminator to learn a function that is a combination of two terms, one only depends on state-action and the other depends on state, and has the form of shaped reward transformation.\n\n\nEVALUATION:\n\nThis is an interesting paper with good empirical results. As I am not very familiar with the work of Finn et al. (2016a) and Finn et al. (2016b), I have not verified the detail of derivations of this new paper very closely. That being said, I have some comments and questions:\n\n\n* The MaxEnt IRL formulation of this work, which assumes that p_theta(tau) is proportional to exp( r_theta (tau) ), comes from\n[Ziebart et al., 2008] and assumes a deterministic dynamics. Ziebart’s PhD dissertation [Ziebart, 2010] or the following paper show that the formulation is different for stochastic dynamics:\n\nZiebart, Bagnell, Dey, “The Principle of Maximum Causal Entropy for Estimating Interacting Processes,” IEEE Trans. on IT, 2013.\n\nIs it still a reasonable thing to develop based on this earlier, an inaccurate, formulation?\n\n\n* I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants.\nIt is suggested that since the only items on both sides of the equation on top of p. 13 depend on s’ are h* and V, they should be equal.\nThis would be true if s’ could be chosen arbitrararily. But s’ would be uniquely determined by s for a deterministic dynamics. In that case, this conclusion is not obvious anymore.\n\nConsider the state space to be integers 0, 1, 2, 3, … .\nSuppose the dynamics is that whenever we are at state s (which is an integer), at the next time step the state decreases toward 1, that is s’ = phi(s,a) = s - 1; unless s = 0, which we just stay at s’ = s = 0. This is independent of actions.\nAlso define r(s) = 1/s for s>=1 and r(0) = 0.\nSuppose the discount factor is gamma = 1 (note that in Appendix B.1, the undiscounted case is studied, so I assume gamma = 1 is acceptable).\n\nWith this choices, the value function V(s) = 1/s + 1/(s-1) + … + 1/1 = H_s, i.e., the Harmonic function.\nThe advantage function is zero. So we can choose g*(s) = 0, and h*(s) = h*(s’) = 1.\nThis is in contrast to the conclusion that h*(s’) = V(s’) + c, which would be H_s + c, and g*(s) = r(s) = 1/s.\n(In fact, nothing is special about this choice of reward and dynamics.)\n\nAm I missing something obvious here?\n\nAlso please discuss how ergodicity leads to the conclusion that spaces of s’ and s are identical. What does “space of s” mean? Do you mean the support of s? Please make the argument more rigorous.\n\n\n* Please make the argument of Section 5.1 more rigorous.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222695641,"tcdate":1511706564444,"number":1,"cdate":1511706564444,"id":"Hyn6kL_xG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Review","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference/Paper595/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Learns environment-independent rewards; reasonable next step in adversarial IRL","rating":"6: Marginally above acceptance threshold","review":"The paper provides an approach to learning reward functions in high-dimensional domains, showing that it performs comparably to other recent approaches to this problem in the imitation-learning setting. It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state-action or state-action-state. It uses this property to produce \"disentangled rewards\", demonstrating that they transfer well to the same task under different transition dynamics.\n\nThe need for \"state-only\" rewards is a useful insight and is covered fairly well in the paper. The need for an \"adversarial\" approach is not justified as fully, but perhaps is a consequence of recent work. The experiments are thorough, although the connection to the motivation in the abstract (wanting to avoid reward engineering) is weak.\n\nDetailed feedback:\n\n\"deployed in at test-time on environments\" -> \"deployed at test time in environments\"?\n\n\"which can effectively recover disentangle the goals\" -> \"which can effectively disentangle the goals\"?\n\n\"it allows for sub-optimality in demonstrations, and removes ambiguity between demonstrations and the expert policy\": I am not certain what is being described here and it doesn't appear to come up again in the paper. Perhaps remove it?\n\n\"r high-dimensional (Finn et al., 2016b) Wulfmeier\" -> \"r high-dimensional (Finn et al., 2016b). Wulfmeier\".\n\n\"also consider learning cost function with\" -> \"also consider learning cost functions with\"?\n\n\"o learn nonlinear cost function have\" -> \"o learn nonlinear cost functions have\".\n\n\" are not robust the environment changes\" -> \" are not robust to environment changes\"?\n\n\"We present a short proof sketch\": It is unclear to me what is being proven here. Please state the theorem.\n\n\"In the method presented in Section 4, we cannot learn a state-only reward function\": I'm not seeing that. Or, maybe I'm confused between rewards depending on s vs. s,a vs. s,a,s'. Again, an explicit theorem statement might remove some confusion here.\n\n\"AIRLperforms\" -> \"AIRL performs\".\n\nFigure 2: The blue and green colors look very similar to me. I'd recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret.\n\n\"must reach to goal\" -> \"must reach the goal\"?\n\n\"pointmass\" -> \"point mass\". (Multiple times.)\n\nAmin, Jiang, and Singh's work on efficiently learning a transferable reward function seems relevant here. (Although, it might not be published yet: https://arxiv.org/pdf/1705.05427.pdf.)\n\nPerhaps the final experiment should have included state-only runs. I'm guessing that they didn't work out too well, but it would still be good to know how they compare.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1511638137399,"tcdate":1511638137399,"number":3,"cdate":1511638137399,"id":"SJbt4SDez","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["~David_Anthony_Venuto1"],"readers":["everyone"],"writers":["~David_Anthony_Venuto1"],"content":{"title":"Code Release for ICLR Reproducibility Workshop","comment":"Hello,\n\nWe are also a group of 3 students from McGill University, who are participating in the ICLR 2018 Reproducibility Research.  We are also interested in the AIRL algorithm proposed in your paper and validating the results found in your paper. We noticed that you released a portion of your code, namely the non-robust version of your AIRL algorithm to other participants in this reproducibility challenge. We were wondering if you could also provide us with that link as it will go a long way to assisting us in reproducing your results.\n\nFor your convenience, we have provided our emails below.\n\nBest Regards,\n\nIsaac (isaac.chan@mail.mcgill.ca)\nDavid (david.venuto@mail.mcgill.ca)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510612752227,"tcdate":1510612752227,"number":2,"cdate":1510612752227,"id":"HJOGkoD1z","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"H11JStDyG","signatures":["~Max_Morrison1"],"readers":["everyone"],"writers":["~Max_Morrison1"],"content":{"title":"Contact Info","comment":"Hello authors,\n\nThat would be an excellent starting point. Our email addresses are as follows:\n\n  - Jin (wyjin@umich.edu)\n  - Max (morrimax@umich.edu)\n  - Sam (samtenka@umich.edu)\n\nThank you for your help,\nJin, Max, and Sam"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510606038665,"tcdate":1510606038665,"number":1,"cdate":1510606038665,"id":"H11JStDyG","invitation":"ICLR.cc/2018/Conference/-/Paper595/Official_Comment","forum":"rkHywl-A-","replyto":"Skxih5LyM","signatures":["ICLR.cc/2018/Conference/Paper595/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper595/Authors"],"content":{"title":"Code is partially released (will fully release after acceptance)","comment":"Hi Jin, Max, and Sam\n\nWe won't release the full code until after acceptance, but we have already released a publicly available implementation of the baselines + the \"non-robust\" version of AIRL. This should be a very good starting point for reproducibility. If you can provide an email address, we can send you a link (so as to not break anonymity on OpenReview)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510546584600,"tcdate":1510546584600,"number":1,"cdate":1510546584600,"id":"Skxih5LyM","invitation":"ICLR.cc/2018/Conference/-/Paper595/Public_Comment","forum":"rkHywl-A-","replyto":"rkHywl-A-","signatures":["~Max_Morrison1"],"readers":["everyone"],"writers":["~Max_Morrison1"],"content":{"title":"ICLR 2018 Reproducibility Workshop Inquiry","comment":"Hello authors,\n\nWe are three students at the University of Michigan working together to submit to the ICLR 2018 Reproducibility Workshop.  We are all very interested in the AIRL algorithm described in your paper “Learning Robust Rewards with Adversarial Inverse Reinforcement Learning” and would like to focus on it for our submission.  Specifically, we hope to both reproduce your results as well as conduct further experiments and hyperparameter tuning.  Our team would greatly benefit from working with your implementation.  Your paper mentioned that you intend to post the implementation of AIRL.  Do you happen to have a schedule for when the code will be available for us to use?\n\nThank you for your time and your work,\nJin, Max, and Sam\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739211684,"tcdate":1509127901045,"number":595,"cdate":1509739209028,"id":"rkHywl-A-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkHywl-A-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Robust Rewards with Adverserial Inverse Reinforcement Learning","abstract":"Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning methods\ncan remove the need for explicit engineering of policy or value features, but\nstill require a manually specified reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifficult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learning\nalgorithm based on an adversarial reward learning formulation that is competitive\nwith direct imitation learning algorithms. Additionally, we show that AIRL is\nable to recover portable reward functions that are robust to changes in dynamics,\nenabling us to learn policies even under significant variation in the environment\nseen during training.","pdf":"/pdf/12819f47f4af3a62409ea77ca53f67892dbb8b93.pdf","TL;DR":"We propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.","paperhash":"anonymous|learning_robust_rewards_with_adverserial_inverse_reinforcement_learning","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Robust Rewards with Adverserial Inverse Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHywl-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper595/Authors"],"keywords":["inverse reinforcement learning","deep reinforcement learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}