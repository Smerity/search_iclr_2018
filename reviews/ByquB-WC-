{"notes":[{"tddate":null,"ddate":null,"tmdate":1514201051812,"tcdate":1514201051812,"number":4,"cdate":1514201051812,"id":"B1NJePCzM","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Comment","forum":"ByquB-WC-","replyto":"SyuT1isxG","signatures":["ICLR.cc/2018/Conference/Paper674/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper674/Authors"],"content":{"title":"Response to Reviewer3","comment":"Thank you for your review. Based on the points you mentioned, I revised the paper.\nBelow is your review and my answer to that.\n\n\"I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks.”\n— Our response ))\nIn the past, this point has not been clarified, so we have revised paper to emphasize on how RMN is related to relation network.\nOur model is a new text-based reasoning model based on the Memory Network framework.\nIn text-based reasoning, the most important thing is to select supporting sentences from large memory, which is performed through attention mechanism. \nWe found that the performance increases with more complex attention mechanisms.\nAs RN is one of the models that reasons well, we analyzed RN from the perspective of Memory Network.\nWe found out that the g of RN examines the relatedness of object pair and question very well.\nMotivated from it, we also used the MLP to focus on the supporting sentences examined from the relatedness of object and question in the memory network framework.\nAs a result, we were motivated by the fact that MLP was effective to examine the relatedness rather than the modeling structure of the RN that use object pair combination.\n\n\n\"Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)?”\n— Our response ))\nThis is because our model is a new model for text-based reasoning based on Memory Network. \nI also thought about evaluating our model on images.\nHowever, since it is Memory Network based reasoning model, I wanted to verify the performance of the model for text, first.\n\n\n\"I'd encourage the authors to do a more detailed experimental study with more tasks.\"\n— Our response ))\nWe added the experimental results of the RN to the bAbI dialog-based dataset and discussed it on the paper.\nIn addition, we compared training time and performance of RN to our model in a large memory condition.\n\n\n\n“ \"we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?\"\n— Our response ))\nExtrinsic attention computation refers to inner product and absolute difference performed by MemN2N, GMemN2N, DMN+ when relatedness of question and sentence is calculated.\nOn the other hand, there is no computation conducted in RN and RMN because they use simple concatenation.\n\n\n\"writing could be cleaned up for spelling / grammar (e.g., \"last 70 stories\" instead of \"last 70 sentences”)\"\n— Our response ))\nI have reviewed a number of times but have not been able to catch them.\nThank you for pointing out and I removed such content from the new paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1514142124506,"tcdate":1514142124506,"number":3,"cdate":1514142124506,"id":"ByV3YdaGf","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Comment","forum":"ByquB-WC-","replyto":"rk-hlXcez","signatures":["ICLR.cc/2018/Conference/Paper674/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper674/Authors"],"content":{"title":"Response to Reviewer1","comment":"We thank the reviewer for the points of clarification and correction.\nWe have modified the paper to address these points, and include detailed answers about each question below.\n\n\"how widespread is this problem across other models or are you simply addressing a point problem for RN?”\n\n— Our response ))\nIt seems to have asked this question because the issue of the submitted paper was unclear (We revised it to be clear) .\nIn fact, our paper suggests a new framework suitable for text-based reasoning rather than solving the problems of RN.\nWhile suggesting RMN, it shows the possibility of replacing the pair-wise interaction of RN.\n\n\n\"It would not be fair to claim superiority over RN since you only evaluate on bAbI while RN also demonstrated results on other tasks. For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion.”\n\n— Our response ))\nAs RMN is a new framework for text-based reasoning, we didn’t perform additional experiment over text, like image.\nRather, we conducted experiments on bAbI dialog-based QA dataset for rich discussion.\n\n\n\"RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights. Showing results with multiple hops (1,2,..) would be useful here.\"\n\n— Our response )\nI’m not sure that I understood you comment as you intended to.\nPlease let me know if my response is not enough to this question.\nYou said that RN is allowed to model more complex interactions than just two hops, however, at least in text-based QA dataset, it is revealed not always true.\nIf we take a closer look at our model, RMN is also able to model complex interactions.\nWhen hop 1 result, r1, is concatenated with updated memory, it is similar to the object pair that RN is dealing with.\nTherefore RMN is able to handle complicate interaction as much as RN is.\n\nAlso we add the result with multiple hops and it reveals that the number of hops is correlated with the number of relation.\n\n\n\"More details are needed about Figure 3. Is this on bAbi as well? How did you generate these stories with so many sentences?\"\n\n— Our response ))\nSorry for the unclear description about Figure 3.\nIt was tested on the bAbI story based QA dataset because it has 320 sentences on one story at maximum.\nTraditionally, Memory Network based models test their performance on 130 sentences at maximum for task 3 and 70 sentences at maximum for the others.\nRN’s experiment was a special case that tested on 20 sentences.\nHowever our revised paper does not include this figure anymore.\n\n\n\"Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops. what are the savings from reducing this time complexity?\"\n\n— Our response ))\nWe changed our comparison with RN to model accuracy and training time when memory size is large and small.\nOur reduction in the time complexity leads to shorter training time than RN when memory size is large.\nIn addition, when memory is large, RN’s reasoning ability is decreased while RMN still shows good reasoning ability.\n\n\n\"Another clarification is the bAbI performance over Entnet which claims to solve all tasks.”\n\n— Our response ))\nIncluding most of other models, such as MemN2N, DMN+, and RN, we also conducted experiment in jointly rather than task-wise. On the other hand, EntNet's results which claims to solve all tasks, are the results of task-wise condition. For fair comparison, we used the jointly trained results of the EntNet which is described in the Appendix of EntNet paper.\nTo make clear, I add this to footnote."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1514135702506,"tcdate":1514133900569,"number":2,"cdate":1514133900569,"id":"HyH5K8Tzf","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Comment","forum":"ByquB-WC-","replyto":"r1Z9q7Ygf","signatures":["ICLR.cc/2018/Conference/Paper674/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper674/Authors"],"content":{"title":"Response to Reviewer2","comment":"Thank you for your review. You raise a good point that help us clarify and improve the paper. \nIt took us quite a long time to do some additional experiments on the point you pointed out.\n\n\n\"While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network. Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N.\"\n\n--- Our response))\nSince the proposed model (RMN) follows the framework of Memory Network, it has a similar structure to MemN2N and GMemN2N which are also Memory Network based models.\nThe reason for mentioning RN is that the MLP-based attention mechanism of the RMN is motivated by the RN's g.\nWhen analyzing the structure of the RN from the viewpoint of the memory network, it can be seen that the g of the RN plays the same role as the output feature map which takes charge of the attention mechanism among the components of the memory network.\nWe re-described this on the paper to clarify and added a table comparing MemN2N, RN, and RMN. \n\n\"What is the exact contribution of the paper with respect to MemN2N and GMemN2N?\"\n\n--- Our response))\nWe think it is the most critical question, and I acknowledge that I have written the paper unclear.\nWe have revised the paper so that we can answer this question.\n\nFor your question, I would like to say that all models based on Memory Network (MemN2N, GMemN2N etc) show small differences.\nFor example, GMemN2N only added gate operation compared to MemN2N.\nIn this respect, our model’s contribution is the MLP-based attention mechanism.\nWe thought that the reasoning ability of the model depends on how well the relevant sentence are found in the memory.\nThe performance of MemN2N, GMemN2N, and DMN + was improved in the order of the attention mechanism becoming complex.\nTherefore, RMN is designed to have an overall simple structure while having MLP-based attention mechanism to catch the complicate relation.\nTo validate this effect, we added model analysis as a subsection to the discussion and conducted an ablation study to compare the results according to the approach of the attention mechanism.\nAlso, we designed an updating component that fits well on our attention component. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1514136658956,"tcdate":1514133430168,"number":1,"cdate":1514133430168,"id":"HyCnDI6Gz","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Comment","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper674/Authors"],"content":{"title":"Revised paper","comment":"We’ve uploaded a new version of the paper that addresses much of the reviewers’ comments and questions. \nAlso we included new experiments that throws a light on the modeling effect of RMN. \nThe additional experiments are as follows.\n1) RN's result on bAbI dialog based QA dataset.\n2) RN's result on bAbI story based QA dataset.\n3) Ablation study on RMN where attention mechanism is changed.\n4) Result of RN and RMN where memory size is varied.\n5) Result of RMN according to the number of hops\n\nThe results show that RMN is better at text-based reasoning compared to MemN2N, GMemN2N, and other Memory Network based models.\nIn addition, when compared to Relation Network, RMN's strong reasoning ability is revealed when memory size in large.\nWe also found the correlation between the number of hops and the number of relations.\n\nComments from reviewers have helped to clarify the paper, and we have revised it to talk more clearly about the contribution of our model."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1515642490203,"tcdate":1511923648169,"number":3,"cdate":1511923648169,"id":"SyuT1isxG","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Review","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input. The model achieves good results on bAbI compared to memory networks and the relation network model. From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question). This average is then fed to a softmax layer for answer prediction. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)? The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments. I'd encourage the authors to do a more detailed experimental study with more tasks, but I can't recommend this paper's acceptance in its current form.\n\nother questions / comments:\n- \"we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?\n- writing could be cleaned up for spelling / grammar (e.g., \"last 70 stories\" instead of \"last 70 sentences\"), currently the paper is very hard to read and it took me a while to understand the model","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1515642490249,"tcdate":1511825576865,"number":2,"cdate":1511825576865,"id":"rk-hlXcez","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Review","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Finding ReMO review","rating":"4: Ok but not good enough - rejection","review":"This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN). RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017). RMN reduces the complexity to linear time for the bAbi dataset. RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning. RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time.\n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning. However, how widespread is this problem across other models or are you simply addressing a point problem for RN? For example, Entnet is able to reason as the input is fed in and the decoding costs are low. Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply. \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP). It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks. For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion. More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights. Showing results with multiple hops (1,2,..) would be useful here.\n\nMore details are needed about Figure 3. Is this on bAbi as well? How did you generate these stories with so many sentences? Another clarification is the bAbi performance over Entnet which claims to solve all tasks. Your results show 4 failed tasks, is this your reproduction of Entnet?\n\nFinally, what are the savings from reducing this time complexity? Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.\n\nOverall, this paper feels like a small improvement over RN. Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement. One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1515642490288,"tcdate":1511762568693,"number":1,"cdate":1511762568693,"id":"r1Z9q7Ygf","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Review","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Not sure what is novel","rating":"4: Ok but not good enough - rejection","review":"The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer. The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task.\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model.\n- The paper is clearly written.\n\nCons:\n- I am not sure what is novel in the proposed model. While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network. Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N. Please tell me if I am missing something, but I am not sure of the contribution of the paper. Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful.\n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1514132476693,"tcdate":1509131634536,"number":674,"cdate":1509739164941,"id":"ByquB-WC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/965f210d0b3c89fc880ada0ef003a5beb0354c74.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}