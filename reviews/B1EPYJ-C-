{"notes":[{"tddate":null,"ddate":null,"tmdate":1513363084620,"tcdate":1513363084620,"number":4,"cdate":1513363084620,"id":"ByrcIc-GM","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Comment","forum":"B1EPYJ-C-","replyto":"Hk_LRZ5gG","signatures":["ICLR.cc/2018/Conference/Paper500/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper500/Authors"],"content":{"title":"Response","comment":"Thank you for your feedback, helping us see which parts are not communicated clearly enough. Please see also our response to all reviewers above.\n\nDifference between Random Mask and Subsampling - These are techniques presented in Sections 2 and 3, respectively. For Random Mask, we compute and apply the gradient only to pre-selected coordinates. For subsampling, we compute and apply gradients without constraint, and subsample at the end. If we were to run the local optimization for just a single gradient update, these updates/gradients would be identical; however, the subsequent gradients computed locally before communicating, would already be different as they would be computed in different points.\nWe did not continue with Random Mask experiments further, as it is not straightforward to use this jointly with the other techniques, such as structured random rotation. The most important gains were obtained as a combination of these multiple techniques. If we trained the Random Mask update in the rotated space, we would make the training procedure significantly more expensive, as applying the rotation would be necessary for every gradient computation. However, applying structured random rotation only once at the end is negligible compared to total cost of training.\nWe will make these points more clear in the submission.\n\nCIFAR vs. Reddit data\nWe don’t intend to emphasize the CIFAR data too much, as it is relatively small, and artificially partitioned by us to fit the setting of FL. The Reddit dataset comes with natural user partition and is much more reflective of actual application in practice. The HD rotation do actually improve performance significantly - this is perhaps more clearly visible in Figure 5 where we experiment with more clients per round, and can compress very aggressively - 1% subsampling and 1 bit quantization! We will stress the Reddit experiments more in final version.\n\nPointer to other ICLR submissions (Variance-based Gradient Compression for Efficient Distributed Deep Learning) - Note there is also another submission in similar spirit (Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training)\nIn both of these works, the central part of proposed techniques keeps track of compression/quantization error incurred in previous rounds, and adds this in the current round before applying compression/quantization. This is not applicable in the setting of Federated Learning, as we cannot remember such errors - think of the billions of eligible phones in the world, but selecting only thousands to participate in a given round."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513363001513,"tcdate":1513363001513,"number":3,"cdate":1513363001513,"id":"rkZHL9ZMz","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Comment","forum":"B1EPYJ-C-","replyto":"BJhrvHcgf","signatures":["ICLR.cc/2018/Conference/Paper500/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper500/Authors"],"content":{"title":"Response","comment":"Thank you for your encouraging review. Below are remarks and responses to your highlighted concerns.\n\nYou remark that we achieve up to 32x reduction in communication. We would like to stress that we can achieve a lot more - with combining subsampling, rotations, and quantization without impacting convergence speed. See the extreme in Figure 5 where we subsample 1% of the elements and then quantize to 1bit (3200x on the compressed layers, although with a drop in performance).\n\n[See also response to all reviewers above for comparison with other methods]\nWe also experimented with various adaptive methods which overall provided slightly worse results, before we were aware of the mentioned works. Nevertheless, our very recent preliminary experiment suggests that performance of QSGD improves when we use it with the subsampling and structured random rotation proposed in our work, and is roughly on par with the experiments we present.\n\nSparsity: Note that if the updates are sparse, it is possible to use a sparse representation first, and then apply the presented techniques to compress list of nonzero values of the sparse representation. It is not ideal, but QSGD degrades in a similar way, as the gaps between non-zero values encoded using Elias coding are no longer necessarily small numbers, making the whole compression slightly weaker.\n\nWe agree that it should be possible to do better than averaging within the Federated Averaging of McMahan et al. However, this problem is clearly out of scope for this work, and probably worth a separate paper altogether."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513362823467,"tcdate":1513362823467,"number":2,"cdate":1513362823467,"id":"HkJcS5WGz","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Comment","forum":"B1EPYJ-C-","replyto":"BkWhzt0ez","signatures":["ICLR.cc/2018/Conference/Paper500/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper500/Authors"],"content":{"title":"Reponse","comment":"Thank you for your review, highlighting good motivation and organization of the work. Let us address the three specific issues you highlighted.\n\nThe remark on our proposed procedure being heuristic being an issue is in our opinion misplaced. \nThe learning procedure (Federated Averaging) is in the first place not the contribution of our paper - it was proposed in McMahan et al., it was shown to work well for large-scale problems and has been successfully deployed in production environment by Google (see McMahan and Ramage), and we build on top of it. This is in line with other optimization techniques for deep learning - they usually have very well understood parallels in the convex setting, but are not really understood in the landscape of deep learning - only empirically observed to typically still work. This procedure is also an extension of existing techniques which are properly analysed in the convex setting - see Ma et al. and Reddi et al. The central part of our contribution does have a proper theoretical justification - see Suresh et al.\n\nWhile individual building blocks have been used in various works, we are not aware of some of them being used in the context of reducing update size in deep learning. Please see also response to all reviewers above for why some of them are not practical in the standard data-center training.\n\nWe have tested our method on the large-scale Reddit dataset, which is highly representative of the types of problems suited to federated learning (unlike ImageNet). The CIFAR experiment can be seen as proof-of-concept but we had to artificially split the dataset into “clients”, and hence does not reflect the practical setting well. The same would be true for ImageNet. The Reddit dataset comes with natural user-based partitioning, and in terms of number of datapoints, is actually much larger than ImageNet.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513362593711,"tcdate":1513362593711,"number":1,"cdate":1513362593711,"id":"H15iEq-MM","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Comment","forum":"B1EPYJ-C-","replyto":"B1EPYJ-C-","signatures":["ICLR.cc/2018/Conference/Paper500/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper500/Authors"],"content":{"title":"Response to all reviewers","comment":"We would like to thank all reviewers for their feedback. The following is response relevant to all reviewers, and explains a particular point we will stress more clearly in the submission.\n\nIn the best technique we used (subsampling + rotation + quantization), the related recently proposed methods such as QSGD or TernGrad are an alternative to the quantization part, not for the whole procedure. If used separately, they yield a significantly weaker result. Note that the results in QSGD paper, authors generally use more than 1bit per element on average. (see also Corollary 3.3 which promises ~2.8 bits per element asymptotically). In contrast we reduced the communication significantly below 1bit per element.\n\nOur technique yields sparse objects whose sparsity pattern is independent of the objects we are trying to compress. This lets us to communicate only the quantized values, and not the indices those values correspond to - those can be recovered from a shared random seed. Further, applying structured random rotation improves the performance of quantization. These are however more computationally expensive operations (especially rotation), which makes it impractical in the setting of the above mentioned methods (MPI-based GPU-to-GPU communication on small minibatches). Nevertheless, this is a component that significantly improves our performance, and it could actually now become practical also in data-center training, together with the trend shifting to large-batch training (see for instance works on training ImageNet in 1hour, 24 min, 15 mins...)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642457479,"tcdate":1512112808729,"number":3,"cdate":1512112808729,"id":"BkWhzt0ez","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Review","forum":"B1EPYJ-C-","replyto":"B1EPYJ-C-","signatures":["ICLR.cc/2018/Conference/Paper500/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The studied problem seems to be interesting, but there exist several major issues in the paper.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a new learning method, called federated learning, to train a centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. Experiments on both convolutional and recurrent networks are used for evaluation. \n\nThe studied problem in this paper seems to be interesting, and with potential application in real settings like mobile phone-based learning. Furthermore, the paper is easy to read with good organization. \n\nHowever, there exist several major issues which are listed as follows:\n\nFirstly, in federated learning, each client independently computes an update to the current model based on its local data, and then communicates this update to a central server where the client-side updates are aggregated to compute a new global model. This learning procedure is heuristic, and there is no theoretical guarantee about the correctness (convergence) of this learning procedure. The authors do not provide any analysis about what can be learned from this learning procedure. \n\nSecondly, both structured update and sketched update methods adopted by this paper are some standard techniques which have been widely used in existing works. Hence, the novelty of this paper is limited. \n\nThirdly, experiments on larger datasets, such as ImageNet, will improve the convincingness. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642457521,"tcdate":1511835460391,"number":2,"cdate":1511835460391,"id":"BJhrvHcgf","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Review","forum":"B1EPYJ-C-","replyto":"B1EPYJ-C-","signatures":["ICLR.cc/2018/Conference/Paper500/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper examines techniques to lower the communication of distributed model updates in a federated setup. The authors focus on low-rank, sparsified, and quantized updates. There are several interesting experiments, but comparisons with state-of-the-art quantization techniques are missing.","rating":"7: Good paper, accept","review":"\nThe authors examine several techniques that lead to low communication updates during distributed training in the context of Federated learning (FL). Under the setup of FL, it is assumed that training takes place over edge-device like compute nodes that have access to subsets of data (potentially of different size), and each node can potentially be of different computational power. Most importantly, in the FL setup, communication is the bottleneck. Eg a global model is to be trained by local updates that occur on mobile phones, and communication cost is high due to slow up-link.\n\nThe authors present techniques that are of similar flavor to quantized+sparsified updates. They distinguish theirs approaches into 1) structured updates and 2) sketched updates. For 1) they examine a low-rank version of distributed SGD where instead of communicating full-rank model updates, the updates are factored into two low rank components, and only one of them is optimized at each iteration, while the other can be randomly sampled.\nThey also examine random masking, eg a sparsification of the updates, that retains a random subset of the entries of the gradient update (eg by zero-ing out a random subset of elements). This latter technique is similar to randomized coordinate descent.\n\nUnder the theme of sketched updates, they examine quantized and sparsified updates with the property that in expectation they are identical to the true updates. The authors specifically examine random subsampling (which is the same as random masking, with different weights) and probabilistic quantization, where each element of a gradient update is randomly quantized to b bits. \n\nThe major contribution of this paper is their experimental section, where the authors show the effects of training with structured, or sketched updates, in terms of reduced communication cost, and the effect on the training accuracy. They present experiments on several data sets, and observe that among all the techniques, random quantization can have a significant reduction of up to 32x in communication with minimal loss in accuracy.\n\nMy main concern about this paper is that although the presented techniques work well in practice, some of the algorithms tested are similar algorithms that have already been proven to work well in practice. For example, it is unclear how the performance of the presented quantization algorithms compares to say  QSGD [1] and Terngrad [2]. Although the authors cite QSGD, they do not directly compare against it in experiments.\n\nAs a matter of fact, one of the issues of the presented quantized techniques (the fact that random rotations might be needed when the dynamic range of elements is large, or when the updates are nearly sparse) is easily resolved by algorithms like QSGD and Terngrad that respect (and promote) sparsity in the updates. \n\nA more minor comment is that it is unclear that averaging is the right way to combine locally trained models for nonconvex problems. Recently, it has been shown that averaging can be suboptimal for nonconvex problems, eg a better averaging scheme can be used in place [3]. However, I would not worry too much about that issue, as the same techniques presented in this paper apply to any weighted linear averaging algorithm.\n\nAnother minor comment: The legends in the figures are tiny, and really hard to read.\n\nOverall this paper examines interesting structured and randomized low communication updates for distributed FL, but lacks some important experimental comparisons.\n\n\n[1] QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks https://arxiv.org/abs/1610.02132\n[2] TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning\nhttps://arxiv.org/abs/1705.07878\n[3] Parallel SGD: When does averaging help? \nhttps://arxiv.org/abs/1606.07365\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642457567,"tcdate":1511820879583,"number":1,"cdate":1511820879583,"id":"Hk_LRZ5gG","invitation":"ICLR.cc/2018/Conference/-/Paper500/Official_Review","forum":"B1EPYJ-C-","replyto":"B1EPYJ-C-","signatures":["ICLR.cc/2018/Conference/Paper500/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Reducing uplink communication in distributed setting: a strategy that works well, could use more clarity and insight.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance.  The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server.  Experiments are based on the federated averaging algorithm.  The work is valuable, but has room for improvement.\n\nThe paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms.  This is not a criticism, however, it is difficult to see the reason for including the structured low-rank experiments in the paper (itAs a reader, I found it difficult to understand the actual procedures used.  For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)?  How is the structured update \"learned\"?  It would be very helpful to include algorithms.\n\nIt seems like a good strategy is to subsample, perform Hadamard rotation, then quantise.    For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data.  It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo.  \n\nIf convenient, could the authors comment on a similarly motivated paper under review at iclr 2018:\nVARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING\n\npros:\n\n- good use of intuition to guide algorithm choices\n- good compression with little loss of accuracy on best strategy\n- good problem for FA algorithm / well motivated\n- \n\ncons:\n\n- some experiment choices do not appear well motivated / inclusion is not best choice\n- explanations of algos / lack of 'algorithms' adds to confusion\n\na useful reference:\n\nStrom, Nikko. \"Scalable distributed dnn training using commodity gpu cloud computing.\" Sixteenth Annual Conference of the International Speech Communication Association. 2015.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739268996,"tcdate":1509124443624,"number":500,"cdate":1509739266320,"id":"B1EPYJ-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1EPYJ-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Federated Learning: Strategies for Improving Communication Efficiency","abstract":"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.  We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.  The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. \n\nIn this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.","pdf":"/pdf/1713e61b6075fc7ee1fab2987a6366e0c4c623d2.pdf","paperhash":"anonymous|federated_learning_strategies_for_improving_communication_efficiency","_bibtex":"@article{\n  anonymous2018federated,\n  title={Federated Learning: Strategies for Improving Communication Efficiency},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1EPYJ-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper500/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}