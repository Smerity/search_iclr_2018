{"notes":[{"tddate":null,"ddate":null,"tmdate":1511741884697,"tcdate":1511741884697,"number":3,"cdate":1511741884697,"id":"ByHaYRulf","invitation":"ICLR.cc/2018/Conference/-/Paper826/Official_Comment","forum":"HJBhEMbRb","replyto":"HJ815-Lxz","signatures":["ICLR.cc/2018/Conference/Paper826/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper826/Authors"],"content":{"title":"Numerical experiments support theoretical results","comment":"“This class seems to have been defined by reverse engineering the proof. Any interpretations on class F_\\phi_\\lapha?”\n\nF_\\phi_\\lapha is the set of 2-layer neural nets (ReLU-type activation) with bounded group path norm. That’s why in our earlier response we called the generalization result in Theorem 3 “a path norm-based generalization bound.”\n\n“at the cost of loosing accuracy (by 5% in the paper's experiments) on real labels”\n\nAs we stated in our previous response, “due to computational constraints, we could only test a small set of lambda values for each regularization strategy.” By considering larger validation sets for lambda, we can tune lambda to get closer validation accuracy to the original accuracy.\n\n“But many authors share their code via anonymous github accounts.”\n\nWe will check this option. Let us repeat that we will make the code publicly available on our Github account after the anonymous review process is complete.\n\n“From Fig 2b the authors conclude that random labels will have a higher Fourier L1-norm.”\n\nWe have not concluded “random labels will have a higher Fourier L1-norm” from figure 2b. The only conclusions made from figure 2b in our manuscript (section 7.1) are “Figures 2b and 2c confirm that both Fourier L1-norm and bandwidth consistently increase with training” and “This suggests that, as implied by the theory above, regularizing Fourier L1-norm and bandwidth could improve generalizability of the final learned model.” \n\n“I know that proofs in the appendix of the paper requires approximation in L1 sense thats not at all clear in the introduction.”\n\nWe will make this clearer in the text."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222776690,"tcdate":1511735385981,"number":3,"cdate":1511735385981,"id":"H1Mweadlz","invitation":"ICLR.cc/2018/Conference/-/Paper826/Official_Review","forum":"HJBhEMbRb","replyto":"HJBhEMbRb","signatures":["ICLR.cc/2018/Conference/Paper826/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Official Review","rating":"4: Ok but not good enough - rejection","review":"This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis. Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017). However, I am not satisfied with the results in the current version.\n\n1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions. \n\n2) Even if for sin activation functions, the analysis is NOT complete. The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima. However, Corollary 3 is only a concentration bound on the gradient. There is a gap that how this corollary implies generalization. The paragraph below this corollary is only a high level intuition. \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222776730,"tcdate":1511732695063,"number":2,"cdate":1511732695063,"id":"SJk182OlM","invitation":"ICLR.cc/2018/Conference/-/Paper826/Official_Review","forum":"HJBhEMbRb","replyto":"HJBhEMbRb","signatures":["ICLR.cc/2018/Conference/Paper826/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"\nThis work proposes to study the generalization of learning neural networks via the Fourier-based method. It first gives a Fourier-based generalization bound, showing that Rademacher complexity of functions with small bandwidth and Fourier l_1 norm will be small. This leads to generalization for 2-layer networks with appropriate bounded size. For 2-layer networks with sine activation functions, assuming that the data distribution has nice spectral property (ie bounded bandwidth), it shows that the local minimum of the population risk (if with isolated component condition) will have small size, and also shows that the gradient of the empirical risk is close to that of the population risk. Empirical results show that the size of the networks learned on random labels are larger than those learned on true labels, and shows that a regularizer implied by their Fourier-based generalization bound can effectively reduce the generalization gap on random labels. \n\nThe idea of applying the Fourier-based method to generalization is interesting. However, the theoretical results are not very satisfactory. \n-- How do the bounds here compared to those obtained by directly applying Rademacher complexity to the neural network functions? \n-- How to interpret the isolated components condition in Theorem 4? Basically, it means that B(P_X) should be a small constant. What type of distributions of X will be a good example? \n-- It is not easy to put together the conclusions in Section 6.1 and 6.2. Suppose SGD leads to a local minimum of the empirical loss. One can claim that this is an approximate local minimum (ie, small gradient) by Corollary 3. But to apply Theorem 4, one will need a version of Theorem 4 for approximate local minima. Also, one needs to argue that the local minimum obtained by SGD will satisfy the isolated component condition. The argument in Section 8.6 is not convincing, ie, there is potentially a large approximation error in (41) and one cannot claim that Lemma 1 and Theorem 4 are still valid without the isolated component condition. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1511557597701,"tcdate":1511557597701,"number":3,"cdate":1511557597701,"id":"HJ815-Lxz","invitation":"ICLR.cc/2018/Conference/-/Paper826/Public_Comment","forum":"HJBhEMbRb","replyto":"HJ1KekUef","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Points not addressed satisfactorily.","comment":"\"Theorem 3 proves a path norm-based generalization bound for ReLU activation by considering the Fourier representation of ReLU function.\" - The theorem is restricted to class of NNs -  F_\\phi_\\lapha. This class  seems to have been defined by reverse engineering the proof. Any interpretations  on class F_\\phi_\\lapha?\n\n\"Yes, but for L2-norm if some lambda coefficient closes the generalization gap for random labels, that lambda will lead to a considerable drop in test accuracy for true labels.\"\nAgreed. However, changing the regularization only to improve the generalization of random labels at the cost of loosing accuracy (by 5% in the paper's experiments) on real labels seems unpractical. \nIdeally one should compare the GE of NNs with path-norm vs l2 on random labels such that the NNs have same accuracy (or GE) on real labels.\n\n\"Our code is much longer than the 5000 character limit of openreview comments\"\nSure. But many authors share their code via anonymous github accounts.\n\n\n\"This is a sufficient condition\" - I see. But since we empirically know that small batch gradient descent outperforms large-batch, this condition might not the right condition to look to prove the observation by Keskar et al's mentioned in the introduction.\n\n\n“From Fig 2b the authors conclude that random labels will have a higher path-norm.” \nYou are right, it was a typo. I meant \"Fourier L1-norm\".  Thus my comment changes to - From Fig 2b the authors conclude that random labels will have a higher Fourier L1-norm.\n\n\"Hence the function cannot be arbitrarily well approximated by bandlimited functions\" - I meant approximation in L2 sense (not L1) which is commonly used in Fourier analysis since it has a physical meaning of energy (My conclusions are based on Parseval's theorem). I know that proofs in the appendix of the paper requires approximation in L1 sense but thats not at all clear in the introduction."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1511546998643,"tcdate":1511546998643,"number":2,"cdate":1511546998643,"id":"HJ1KekUef","invitation":"ICLR.cc/2018/Conference/-/Paper826/Official_Comment","forum":"HJBhEMbRb","replyto":"ry3-q3Xez","signatures":["ICLR.cc/2018/Conference/Paper826/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper826/Authors"],"content":{"title":"Numerical experiments support theoretical results","comment":"“Simplifying ReLU by sinusoidal function doesn't seem like a good idea and seems very forced.”\n\nIn fact, our Fourier-based approach suggests the opposite is true. Theorem 3 proves a path norm-based generalization bound for ReLU activation by considering the Fourier representation of ReLU function. Note that Fourier transform is a function’s representation in the sinusoidal basis.\n\n“one could keep decreasing the training accuracy (to 0.1) by increasing the value of lambda in l_2 norm”\n\nYes, but for L2-norm if some lambda coefficient closes the generalization gap for random labels, that lambda will lead to a considerable drop in test accuracy for true labels. On the other hand, for path norms we numerically showed that the same lambda coefficient can close the generalization gap for both true and random labels without compromising test accuracy for true labels (Figures 1B and 3).\n\n“Corollary 3 says that gradients via large-batch will have a better generalization than gradients via small-batch”\n\nWe emphasize that Corollary 3 only claims the generalization of the GRADIENT of the empirical risk to the GRADIENT of the population risk. This is a sufficient condition, which holds only for large-batch gradient descent, for applying Theorem 4 to guarantee generalization for the local minima found by large-batch gradient descent. \n\n“Can the code be shared on anonymously?”\n\nOur code is much longer than the 5000 character limit of openreview comments. We will make the code publicly available on Github after the anonymous review process is complete. \n\n“5) Your comment is unclear here.” \n\nYour original comment was “From Fig 2b the authors conclude that random labels will have a higher path-norm.” However, figure 2b shows Fourier L1-norm which is completely different from path norms.\n\n“Restricting ourselves to Borel measurable functions for now, any function in L_2 can be arbitrarily 'approximated' by band-limited functions”\n\nThis statement is incorrect. For example, consider the density function of a uniform random variable over [-0.5,0.5]. While this function is in L_2, its Fourier transform (the sinc function) is not absolutely integrable. Hence the function cannot be arbitrarily well approximated by bandlimited functions, since the absolute integral of its Fourier transform over [B,infty) is infinite for any value B.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1511406084993,"tcdate":1511406084993,"number":2,"cdate":1511406084993,"id":"ry3-q3Xez","invitation":"ICLR.cc/2018/Conference/-/Paper826/Public_Comment","forum":"HJBhEMbRb","replyto":"r1I4--WxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Points not addressed satisfactorily.","comment":"1) Simplifying ReLU by sinusoidal function doesn't seem like a good idea and seems very forced.\n\n2) Comment (2) directly contradicts the numbers in comment (4). Refer to my point 4.\n\n3) Since large-batch descent have a larger value of n, Corollary 3 says that gradients via large-batch will have a better generalization than gradients via small-batch. Since the whole paper is centered around the theme - \"good generalization ==> better performance\", it does contradict Keskar et al.’s results.\n\n3) Well, the paper initially didn't say anything about any validation set for choosing values of lambda. Can the code be shared on anonymously?\n\n4) For true labels, the decrease in training accuracy (.021) and the decrease in generalizing error is (.007) are comparable. If 0.021 is not 'really compromising accuracy', then 0.007 is not really decreasing generalizing error.\n\nAlso, for random labels the test accuracy will always be around .1 and one could keep decreasing the training accuracy (to 0.1) by increasing the value of lambda in l_2 norm (all the way upto infinity). Hence I don't see why one should use the norm proposed in this paper over l_2.\n\n5) Your comment is unclear here.\n\nMinor points\n\n1) Technically Gaussian is still not a band-limited function (by the definition stated in the paper). Restricting ourselves to Borel measurable functions for now, any function in L_2 can be arbitrarily 'approximated' by band-limited functions.\n\n2,3) But still, what is the interpretation of such an (approximate) assumption?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1511227893594,"tcdate":1511227693999,"number":1,"cdate":1511227693999,"id":"r1I4--WxG","invitation":"ICLR.cc/2018/Conference/-/Paper826/Official_Comment","forum":"HJBhEMbRb","replyto":"rJOFw1u1z","signatures":["ICLR.cc/2018/Conference/Paper826/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper826/Authors"],"content":{"title":"Numerical experiments support theoretical results","comment":"The following addresses the points made in the conclusion:\n\n1) Sinusoidal activation is not proposed as a replacement for RELU, but as an analytical simplification to illustrate that gradient-based methods converge to generalizable local minima.\n  \n2) Our numerical results in Figures 1B and 3 indicate that group path norms can close the generalization gap for both random and true labels without compromising test accuracy for true labels, while L2_norm cannot.\n\n3) Corollary 3 only applies to large-batch gradient descent and it does not explain the difference between the generalization performance of small and large batch gradient-descent.  Hence it does not contradict Keskar et al.’s results.\n\nHere is the response to the other concerns:\n\n3) We have a typo in this sentence and the word “test” should be replaced with “validation.” To fairly compare different regularization strategies, we tested about 5 lambda values for each strategy and then reported the performance on the test set for the lambda value that resulted in the best performance on the validation set. Good performance here means low generalization gap with comparable validation accuracy for the true labels.\n\n4) Due to computational constraints, we could only test a small set of lambda values for each regularization strategy. For each strategy, we chose the largest lambda coefficient which did not result in more than a 5% decrease in the validation accuracy for true labels. For the X2-group path norm (Figure 1B3), this resulted in test and train accuracies of 0.519 and 0.632 for the true labels and 0.099 and 0.104 for the random labels. For the L2-norm (Figure 1B2), this resulted in test and train accuracies of 0.540 and 0.659 for the true labels and 0.096 and 0.285 for the random labels. Therefore in comparison to the L2 norm, the X2-group path norm achieves smaller generalization gap for both true and random labels (significantly for random labels) without really compromising test accuracy. \n\n5) The point of Figure 2b is to validate our Fourier-based generalization bound, and not to examine path norms which is irrelevant to the plots in Figure 2. Figure 2b demonstrates how the Fourier L1-norm, which is different from path norm, of a neural net with sine activation changes during training. The hypotheses fitting random and true labels have comparable Fourier L1 norm (slightly larger for random labels), but as shown in Figure 2c the bandwidth achieved when fitting random labels is > 1.5 times larger than the bandwidth achieved when fitting true labels. Our generalization result in Theorem 2 depends on both Fourier L1-norm and bandwidth, which correctly predicts that the generalization risk should be larger for random labels.\n\nResponse to the minor points:\n\n1) The Fourier transform of a Gaussian function has a Gaussian shape. Therefore, a Gaussian function’s Fourier transform is concentrated around the origin in a ball with radius inversely proportional to the standard deviation of that Gaussian function and can be arbitrarily well approximated by a bandlimited function. \n\n2,3) This is a technical assumption to obtain the exact value instead of an approximation of a convolution integral. As we have discussed in the Appendix (section 8.6), Theorem 4 (shown through Lemma 1) remains approximately valid even without the isolated components assumption. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222776769,"tcdate":1510752722492,"number":1,"cdate":1510752722492,"id":"rkcAW6tJG","invitation":"ICLR.cc/2018/Conference/-/Paper826/Official_Review","forum":"HJBhEMbRb","replyto":"HJBhEMbRb","signatures":["ICLR.cc/2018/Conference/Paper826/AnonReviewer2"],"readers":["everyone"],"content":{"title":"application domain seems restricted","rating":"6: Marginally above acceptance threshold","review":"Deep neural networks have found great success in various applications. This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach. Specifically, the authors develop a Fourier-based generalization bound. Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions. Numerical experimental results are also presented to verify the theory.\n\n(1) The scope is a bit limited. The paper only considers 2-layer NNs. Is there an essential difficulty in extending the result here to NNs with more layers? Also, the analysis for gradient-based method in section 6  is only for squared-error loss, SINE activation and a deterministic target variable. What would happen if Y is random or the activation is ReLU?\n(2) The generalization bound in Corollary 3 is only for the gradient w.r.t. \\alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W. It would be intersting to present some analysis regarding the gradient w.r.t. W.\n(3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function. However, no comparison is clearly made. It would be better if the authors could explain this more?\n\nIn summary, the application domain of the theoretical results seems a bit restricted.\n\nMinor comments:\nEq. (1): d\\xi should be dx\nLemma 2: one \\hat{g} should be \\hat{f}","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1510631296072,"tcdate":1510631296072,"number":1,"cdate":1510631296072,"id":"rJOFw1u1z","invitation":"ICLR.cc/2018/Conference/-/Paper826/Public_Comment","forum":"HJBhEMbRb","replyto":"HJBhEMbRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Theoretical results and experiments aren't consistent (sometimes contradicting).","comment":"Summary: The authors propose 1) New class of activation functions which have bounded bandlimit (in the foruier domain) 2) and 'nice' l1-norm in the fourier domain. They evaluate the generalization bound derived by Bartlett and Mendelson (2002) for bandlimited functions and get  tighter gaurantees than just using lipschitz continuity. They also extend the analysis to the gradients of the loss functions. They also have a few experiments trying to support their claims.\n\nA few concerns -\n\n1) The authors propose using sin function as an activation function over ReLu (or sigmoid). However, they haven't directly compared the accuracy of NNs using ReLu with NNs using sin function. To be more specific, Figure 2 evaluate NNs with sin-activation function using MSE whereas Figure 3 evaluates NNs with ReLu-activation functions using prediction accuracy.\n\n2) In the introduction, Keskar et. al. are cited claiming - 'SGD has been empirically shown to outperform large-batch gradient descent'. However, the Corollary 3 seems to say that the difference between sample-gradient and population-gradient is smaller for large values of samples and hence large-batch gradient descent should outperform SGD. The previous two statements are in direct contradiction.\n\n3) The experimental section states that - \"We note that while we tested multiple values of lambda for each regularization technique, we always chose lambda the that resulted in the smallest generalization gap with comparable test performance.\"\nSuch techniques are classic examples on how to overfit. The standard practice in the community is to choose the lambda via cross-validation.\n\n4) In figure 1b the authors claim that path-norm penalty reduces the generalization error (compared l2 penalty). However, on close observation I see that using path-norm penalty also reduced the accuracy (which is not mentioned in the description). It is well known that reducing the size of the class of functions (to optimize over), reduces both training accuracy and generalization error. Hence it most likely that using path norm is 'effectively' reducing the class of functions and hence decreasing the generalization error (along with accuracy). One could quite possibly acheive the same effect by using l2 norm with a higher value of lambda.\n\n5) From Fig 2b the authors conclude that random labels will have a higher path-norm. However, this is not very convincing to me as it looks like the path-norm for true labels might be equal to the path-norm for the random labels if the experiments were run for a few more epocs.\n\n\nFew minor points\n1) The introduction claims gaussian activation function as bandlimited. Can you explain this?\n2) The assumption in lemma 1 are used only to prove the result and hence seems a little artificial. What is the interpretation of such an assumption?\n3) Lemma 1 requires ||w_j|| to be larger the bandwith of P_X. Large values of W would make the sin function a highly non-monotonous activation function (with high frequency). I don't think such a highly variable non-monotonous functions is a good candidate for an activation functions.\n\n\nConclusion: There is huge disconnect between theretical conclusions and experiments results in this paper.\n1) The theory proposes using sin-activation activation function - However, the sin-activation is not directly compared to ReLu in any experiments.\n2) The theory proposes superiority of path-norm over l2 norm - The experiments are inconclusive (point 5).\n3) The paper claims the gradient methods that accurately estimate the population gradient are expected to have better performance - Keskar et. al. (https://arxiv.org/pdf/1609.04836.pdf) have the exact opposite conclusion."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]}},{"tddate":null,"ddate":null,"tmdate":1509739080527,"tcdate":1509135532936,"number":826,"cdate":1509739077868,"id":"HJBhEMbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJBhEMbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Spectral Approach to Generalization and Optimization in Neural Networks","abstract":"The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.","pdf":"/pdf/94245797792d1608920162a79198d04496e7fad8.pdf","paperhash":"anonymous|a_spectral_approach_to_generalization_and_optimization_in_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Spectral Approach to Generalization and Optimization in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJBhEMbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper826/Authors"],"keywords":["Generalization","Neural Networks","Fourier Analysis"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}