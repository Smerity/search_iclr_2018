{"notes":[{"tddate":null,"ddate":null,"tmdate":1515684326234,"tcdate":1515379305480,"number":6,"cdate":1515379305480,"id":"BJWd98x4z","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Comment","forum":"HyXBcYg0b","replyto":"HycHDznMM","signatures":["ICLR.cc/2018/Conference/Paper340/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper340/Authors"],"content":{"title":"Authors","comment":"We would like to thank the referee for her/his time reviewing the revised paper and for improving her/his evaluation score. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514051393721,"tcdate":1514051393721,"number":5,"cdate":1514051393721,"id":"HycHDznMM","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Comment","forum":"HyXBcYg0b","replyto":"ryFFX8Yxf","signatures":["ICLR.cc/2018/Conference/Paper340/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper340/Authors"],"content":{"title":"Authors feedback","comment":"We are thankful to the reviewer for her/his comments and time. We hope our answers will clarify the importance of this work and the referee will be inclined to improve her/his evaluation score. \n\nQ: Compare learning based approaches to variational ones\nA: We solved the combinatorial Dirichlet problem with labeled and unlabelled data using [Grady’06, Random walks for image segmentation, Eq. 11, Section B]. The average accuracy (over 100 experiments) for this variational technique is 45.37% (we remind that only 1 label per class is used, and random choice is around 5-15%), while the performance of the best learning technique is 82%. Learning techniques produce better performances with a different paradigm as they use training data with ground truth, while variational techniques do not use such information. The downside is the need to see 2000 training graphs to get to 82%. However, when the training is done, the test complexity of these learning techniques is O(E), where E is the number of edges in the graph. This is an advantage over the variational Dirichlet model that solves a sparse linear system of equations with complexity O(E^1.5) [Lipton-Rose-Tarjan’79]. We thank the referee for this useful comment. We added this comment in the paper. \n\nQ: Abstract, conclusion should be revised\nA: We revised the abstract and conclusion. \n\nQ: The authors should clarify which problem they are dealing with\nA: The general problem we want to solve is learning meaningful representations of graphs with variable length using either ConvNet or RNN architectures. These graph representations can be applied to different tasks such as vertex classification (in this paper for graph matching and graph clustering) and also graph classification, graph regression, graph visualization, graph generative model, etc. We added this comment in the paper.\n\nQ: Give some numerical results\nA: Here is the summary of the results:\n1. Sub-graph matching:\n  (a) Accuracy of shallow graph NNs is 79% for RNNs and 67% for the proposed ConvNet.\n  (b) Accuracy of deep graph NNs (L=10) is 87% for RNNs and 90% for the proposed ConvNet.\n2. Semi-supervised graph clustering:\n  (a) Accuracy of shallow graph NNs is 69% for RNNs and 41% for the proposed ConvNet.\n  (b) Accuracy of deep graph NNs (L=10) is 65% for RNNs and 82% for the proposed ConvNet.\n3. Computational times for graph RNNs is 1.5-4x slower than the proposed ConvNet.\nWe added these results in the abstract.\n\nQ: Minor comments\nA: Thank you. We revised the paper accordingly.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514051337845,"tcdate":1514051337845,"number":4,"cdate":1514051337845,"id":"SyffDMnff","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Comment","forum":"HyXBcYg0b","replyto":"rJ_yvM3fz","signatures":["ICLR.cc/2018/Conference/Paper340/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper340/Authors"],"content":{"title":"Authors feedback 2","comment":"Q: Taking original Scarselli and replacing the RNN by LSTM gates\nA: Yes, this is what we did and explained in Section 3. We do not claim any major contribution for graph LSTM. Our goal was to compare all graph RNN architectures (GRU and LSTM) vs. graph ConvNet architectures. As graph LSTM was not available in the literature, we simply used Scarselli-etal and Tai-etal to extend LSTM to arbitrary graphs. \n\nQ: A second concern is the presentation of the paper, which can be confusing at some points. \nA: We improved the abstract, conclusion and revised some parts of the paper in view of the reviewer’s questions.\n\nQ: After (2), the important point to raise is the fact that the dependence graph of the model is not a DAG anymore\nA: Agreed - we added this comment in the paper. \n\nQ: How is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?\nA: The output is a simple fully connected layer from the convolutional graph features. We added this comment in the paper.\n\nQ: The length of a graph is not defined\nA: Beginning of Sections 4.1 and 4.2 explained how the graph size is designed for each experiment. For graph matching, the size varies randomly between 170 and 270 nodes, and for graph clustering the length is between 50 and 250. \n\nQ: At the beginning of section 2.1 I do not understand the reference to word prediction and NLP. \nA: Similar to the beginning of Section 2.2, the beginning of section 2.1 uses the most well-known example of RNN tasks (word prediction in NLP) and ConvNet task (feature extraction in computer vision) to define the notion of neighbourhood for these architectures. This is simply didactic - these examples are used as a first step to understand the extension of neighbourhood from regular grid (1D for NLP and 2D for computer vision) to arbitrary graphs (brain networks, social networks, etc) for RNNs and ConvNets. \n\nQ: ConvNets are more pruned to deep networks than RNNs\nA: It simply means that graph ConvNets performance (with residuality) scales better than graph RNNs (with residuality). \n\nQ: What are \"heterogeneous graph domains\"?\nA: Homogeneous graph domains refer to regular lattices and heterogeneous graph domains refer to graphs with complex variable structures like proteins, brain connectivity, gene regulatory network, etc. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514051359893,"tcdate":1514051296082,"number":3,"cdate":1514051296082,"id":"rJ_yvM3fz","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Comment","forum":"HyXBcYg0b","replyto":"Sy7QPPYxM","signatures":["ICLR.cc/2018/Conference/Paper340/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper340/Authors"],"content":{"title":"Authors feedback 1","comment":"We thank the reviewer for her/his time and comments. We provide below specific answers to the questions. We hope the reviewer will update positively her/his decision in view of our answers. \n\nQ: My biggest concern is novelty\nA: Several techniques for graph NNs have been published in the last two years. None of the existing works compare with rigorous numerical experiments which type of architectures (RNNs or ConvNets) should be used for graphs with variable length. The main contribution and novelty of this work is to answer this fundamental question, and give the reader the winning architecture. By running controlled numerical experiments on two basic graph analysis tasks, sub-graph matching and semi-supervised clustering, we reached the conclusion that ConvNets architectures should be used, and the best formulation of graph ConvNets uses edge gates and residuality. We believe such result to be important for future models in this domain (and also a bit controversial) as most graph NNs published in the literature focused on RNN architectures. \n\nQ: Adding stacking and residuality are now standard operations\nA: When we started this work, we doubted that stacking and residuality were helpful for the class of graphs with variable length. Graphs are different data than images: arbitrary graph structures are irregular (e.g. molecule graphs or gene networks), graph convolutional operations are not shift-invariant, and multi-scale structures depend on graph topology. Our original motivation was to numerically study the stacking and residuality properties for graph RNNs and ConvNets, and see if it would be useful. \nWe found out the important result that without residuality, *none* of the existing graph NNs can stack more than 2 layers. They simply do not work; they are not able to learn good representations to solve the matching and clustering tasks. Hence, although residuality is quite common in computer vision tasks, our experiments showed that this property is even *more* important for graphs than for images. Quantitatively, we got a boost by at least 10% of accuracy when we stacked more than 6 layers. So, it seemed to us and to other researchers to be a useful result for future research in this domain (that includes applications in chemistry, physics, neuroscience). \n\nQ: The model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality.\nA: Residuality plays indeed an essential role for graph learning tasks. Without residuality, the existing techniques such as Li-etal, Sukhbaatar-etal, Marcheggiani-Titov are far behind (more than 10% - they actually do not benefit much from multiple layers) than the proposed gated graph residual model. Note that in the experiments, we *did* upgrade the existing techniques with residuality. We could have simply reported the (lower) performances of the original methods, which would have been more impressive on the plots for our model but also not informative. \nThe proposed graph ConvNet actually offers a slight improvement compared to Sukhbaatar-etal and Marcheggiani-Titov *when* these models are upgraded with residuality. However, the paper is not an application paper (we do not claim any SOTA on any benchmark dataset), but rather an investigation paper where we want to convey the message that, after rigorous numerical experiments, graph ConvNet architectures should be preferred when we want to design deep learning techniques on arbitrary graphs such as for drugs design. \n\nQ: Graph ConvNets and Graph RNNs are the same thing, equations (1) and (6) are equivalent.\nA: We disagree with this comment - equations 1 and 6 are as different as standard RNNs are distinct from standard ConvNets. The purpose of the mathematical formulations 1 and 6 is to generalize standard ConvNets and RNNs not only to image domains but to arbitrary graph domains (1 and 6 reduce to original RNNs and ConvNets for regular grids). Figure 1 illustrates the fundamental difference between both graph architectures. \n\nQ: An introduction of the problem is missing. What kind of problems should be solved? What are the inputs, the outputs?\nA: The general problem we want to solve is learning meaningful representations of graphs with variable length using either ConvNet or RNN architectures. These graph representations can be applied to different tasks such as vertex classification (for graph matching and clustering in this work) and also graph classification, graph regression, graph visualization, and graph generative model. \nIn this work, inputs are graphs with variable size and outputs are vertex classification vectors of input graphs. We added this answer in the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514051251143,"tcdate":1514051251143,"number":2,"cdate":1514051251143,"id":"HyshLz3fz","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Comment","forum":"HyXBcYg0b","replyto":"r1-tSfqeG","signatures":["ICLR.cc/2018/Conference/Paper340/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper340/Authors"],"content":{"title":"Author feedback","comment":"We thank the reviewer for her/his time and valuable comments. We hope to clarify any misunderstanding below and show the importance of this work in the field of deep learning on graphs. \n\nQ: Relationship between the two models\nA: There is no direct relationship between the proposed graph LSTM and the proposed graph ConvNet. We simply wanted to compare the best possible graph RNN architectures vs. the best graph ConvNets to find out what type of graph NNs should be used when dealing with problems involving graphs of variable length. As graph LSTM was not available in the literature, we simply used Scarselli-etal and Tai-etal to extend LSTM to arbitrary graphs. For the proposed graph ConvNet, we merged Sukhbaatar-etal and Marcheggiani-Titov, and added residuality to define the most possible generic ConvNet architecture for arbitrary graphs. Then, we performed several numerical experiments on graph matching and graph clustering to reach the conclusion that graph ConvNets should be preferred over RNN models for the class of variable graphs (such as molecules in quantum chemistry, gene regulatory networks for genetic disorders and particle physics for jet constituents). \n \nQ: Experiments on the merits of Graph ConvNets over Graph LSTM\nA: The most important advantage of graph ConvNets over graph LSTM is the multi-scale property. Graph ConvNet architectures have a monotonous increase of performance/accuracy when the network gets deeper, unlike RNN architectures for which performance decreases for a large number of layers. This property is illustrated in both graph experiments, see Figures 3 and 5 middle row. This makes ConvNet architectures more robust w.r.t. network design than RNN systems: Hyper-parameters such as L (nb of layers) and T (nb of inner RNN iterations, Fig4) must be carefully selected for graph RNNs, unlike graph ConvNets. Besides, RNN architectures are 1.5-4x slower than ConvNets (right column Figs 3 and 5) and they converge slower, Fig6. \n\nQ: Analysis why the ConvNet-like architecture is better \nA: We do agree such analysis would be important and we would like to carry it out in a future work. However, it is a challenging analysis as the data domain does not hold a nice mathematical structure like Euclidean lattices for images. This will require time and new analysis tools to develop such theory (given also that the standard theory for regular grids/images is still open). \nIn the meantime, we hope the reviewer considers the rigorous numerical experiments - two fundamental graph experiments with controlled analytical settings (stochastic block models for the graph distributions) that offer a clear conclusion about graph ConvNets, which can be leveraged to build better NNs in the fast-emerging domain of deep learning on graphs. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1515642434616,"tcdate":1511822713456,"number":3,"cdate":1511822713456,"id":"r1-tSfqeG","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Review","forum":"HyXBcYg0b","replyto":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference/Paper340/AnonReviewer3"],"readers":["everyone"],"content":{"title":"the relation between the two proposed models is not very clear","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets. There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture. The Related work part of this paper makes a good description of both topics. \n\nI would expect the paper elaborate more (at least in a more explicit way) about the relationship between the two models (the proposed graph LSTM and the proposed Gated Graph ConvNets). The authors claim that the innovative of the graph Residual ConvNets architecture, but experiments and the model section do not clearly explain the merits of Gated Graph ConvNets over Graph LSTM. The presentation may raise some misunderstanding. A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting. \n\nIn the section of experiments, they compare 5 different methods on two graph mining tasks. These two proposed neural network models seem performing well empirically. \n\nIn my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length, \nand both models worth future stuies for speicific problems. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1515642434654,"tcdate":1511778075312,"number":2,"cdate":1511778075312,"id":"Sy7QPPYxM","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Review","forum":"HyXBcYg0b","replyto":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference/Paper340/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Residual Gated Graph ConvNets","rating":"3: Clear rejection","review":"The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community. In particular, a tree structured LSTM is taken and modified. The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality.\n\nMy biggest concern is novelty, as the modifications are minor. In particular, the formulation can be seen in a different way. As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate). This is a minor modification. Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper.\n\nA second concern is the presentation of the paper, which can be confusing at some points. A major example is the mathematical description of the methods. When reading the description as given, one should actually infer that Graph ConvNets and Graph RNNs are the same thing, which can be seen by the fact that equations (1) and (6) are equivalent.\n\nAnother example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies. \n\nGenerally, a clear introduction of the problem is also missing. What are the inputs, what are the outputs, what kind of problems should be solved? The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?\n\nThe model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality.\n\nA couple of details :\n\n- the length of a graph is not defined. The size of the set of nodes might be meant.\n\n- at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing. RNNs are not restricted to NLP and I think there is no need to introduce an application at this point.\n\n- It is unclear what does the following sentence means: \"ConvNets are more pruned to deep networks than RNNs\"?\n\n- What are \"heterogeneous graph domains\"?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1515642434694,"tcdate":1511773057124,"number":1,"cdate":1511773057124,"id":"ryFFX8Yxf","invitation":"ICLR.cc/2018/Conference/-/Paper340/Official_Review","forum":"HyXBcYg0b","replyto":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference/Paper340/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting approach that should be better presented ","rating":"7: Good paper, accept","review":"The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version.\n\nSummary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs. It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees. The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms. They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering.  \n\nMain comments:\nIt would strengthen the paper to also compare all these network learning based approaches to variational ones. For instance, to a spectral clustering method for the semi supervised clustering, or\nsolving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006.\n\nThe abstract and the conclusion should be revised, they are very vague.\n- The abstract should be self contained and should not contain citations.\n- The authors should clarify which problem they are dealing with.\n- instead of the \"numerical result show the performance of the new model\", give some numerical results here, otherwise, this sentence is useless.\n- we propose ... as propose -> unclear: what do you propose?\n \n\nMinor comments:\n- You should make sentences when using references with the author names format. Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997)\n- As Eq 2 -> As the minimization of Eq 2 (same with eq 4)\n- Don't start sentences with And, or But\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514050970606,"tcdate":1509100090864,"number":340,"cdate":1509739352856,"id":"HyXBcYg0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyXBcYg0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Residual Gated Graph ConvNets","abstract":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures.  Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","pdf":"/pdf/4ae1c5290208ac5496c0f2bae3944b8e4ce7f7ac.pdf","TL;DR":"We compare graph RNNs and graph ConvNets, and we consider the most generic class of graph ConvNets with residuality.","paperhash":"anonymous|residual_gated_graph_convnets","_bibtex":"@article{\n  anonymous2018residual,\n  title={Residual Gated Graph ConvNets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyXBcYg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper340/Authors"],"keywords":["graph neural networks","ConvNets","RNNs","pattern matching","semi-supervised clustering"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}