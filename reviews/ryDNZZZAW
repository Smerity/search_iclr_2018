{"notes":[{"tddate":null,"ddate":null,"tmdate":1512413152291,"tcdate":1512413152291,"number":4,"cdate":1512413152291,"id":"Hkuk_f7ZG","invitation":"ICLR.cc/2018/Conference/-/Paper651/Official_Review","forum":"ryDNZZZAW","replyto":"ryDNZZZAW","signatures":["ICLR.cc/2018/Conference/Paper651/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An extension of domain adaptation using adversarial learning to multiple source domains.","rating":"6: Marginally above acceptance threshold","review":"This work presents a bound to learn from multiple source domains for domain adaptation using adversarial learning. This is a simple extension to the previous work based on a single source domain. The adversarial learning aspect is not new.\n\nThe proposed method (MDAN) was evaluated on 3 known data sets. Overall, the improvements from using MDAN were consistent and promising.\n\nThe bound used in the paper accounts for the worst case scenario, which may not be a tight bound when some of the source domains are very different from the target domain. Therefore, it does not completely address the problem of learning from multiple source domains. The fact that soft-max performs better than hard-max suggest that some form of domain selection or weighting might lead to a better solution. The empirical results in the third experiment (Table 4) also suggest that the proposed solution does not generalize well to domains that are less similar to the target domain.\n\nSome minor comments:\n- Section 2: \"h disagrees with h\" -> \"h disagrees with f\".\n- Theorem 3.1: move the \\lambda term to the end to be consistent with equation 1.\n- Last line of Section 3: \"losses functions\" -> \"loss functions\".\n- Table 1 and 2: shorthands sDANN, cDANN, H-Max and S-Max are used here are not consistent with those used in subsequence experiments.  It's good to be consistent.\n- In Section 5.2, it was conjectured that the poorer performance of MDAN on SVHN is due to its dissimilarity to the other domains. However, given that the best-single results are close to the target only results, SVHN should be similar to one or more of the source domains. MDAN is probably hurt by the worst case bound.\n- In Table 4, the DANN performance for S=6 and T=A is off compared to the rest. Any idea?\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multiple Source Domain Adaptation with Adversarial Learning","abstract":"While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. We propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting. \n","pdf":"/pdf/89ab463ecc6682929a6f2460adf56b078d47a425.pdf","paperhash":"anonymous|multiple_source_domain_adaptation_with_adversarial_learning","_bibtex":"@article{\n  anonymous2018multiple,\n  title={Multiple Source Domain Adaptation with Adversarial Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDNZZZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper651/Authors"],"keywords":["adversarial learning","domain adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1512314939470,"tcdate":1512314939470,"number":3,"cdate":1512314939470,"id":"SkmS_5--z","invitation":"ICLR.cc/2018/Conference/-/Paper651/Official_Review","forum":"ryDNZZZAW","replyto":"ryDNZZZAW","signatures":["ICLR.cc/2018/Conference/Paper651/AnonReviewer4"],"readers":["everyone"],"content":{"title":"A new generalization bound and an extension of using adversarial learning for multiple source domain adaptation","rating":"6: Marginally above acceptance threshold","review":"The generalization bounds proposed in this paper is an extension of Blitzer et al. 2007. The previous bounds was proposed for single domain single target setting, and this paper extends it to multiple source domain setting. \n\nThe proposed bound is presented in Theorem 3.4, showing some interesting observations, such as the performance on the target domain depends on the worst empirical error among multiple source domains.  The proposed bound reduces to Blitzer et al. 2007â€™s when there is only single source domain. \n\nPros \n+ The proposed bound is of some interest.\n+ The bound leads to an efficient learning strategy using adversarial neural networks.\n\nCons:\n- My major concern is that the baselines evaluated in the experiments are quite limited. There are other publications working on the multi-source-domain setting, which were not mentioned/compared in the submission.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multiple Source Domain Adaptation with Adversarial Learning","abstract":"While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. We propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting. \n","pdf":"/pdf/89ab463ecc6682929a6f2460adf56b078d47a425.pdf","paperhash":"anonymous|multiple_source_domain_adaptation_with_adversarial_learning","_bibtex":"@article{\n  anonymous2018multiple,\n  title={Multiple Source Domain Adaptation with Adversarial Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDNZZZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper651/Authors"],"keywords":["adversarial learning","domain adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1512222709106,"tcdate":1511772231990,"number":2,"cdate":1511772231990,"id":"HyxIlUFlz","invitation":"ICLR.cc/2018/Conference/-/Paper651/Official_Review","forum":"ryDNZZZAW","replyto":"ryDNZZZAW","signatures":["ICLR.cc/2018/Conference/Paper651/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A modest but honest extension of single source adversarial network to multi-source domain adaptation","rating":"6: Marginally above acceptance threshold","review":"The paper builds on the previous work of Ganin et al. (2015, 2016), that introduced a domain adversarial neural network (DANN) for single source domain adaptation. Whereas Ganin et al. (2016) were building directly on the (single source) domain adaptation theorem of Ben-David et al., the authors prove a similar result for the multiple sources case. \n\nThis new result appears to be a simple extension of the single source theorem. A similar result to Theorem 3.1 can be obtained by considering the maximum over the k bounds obtained by considering the k pairs source-target one by one, using Theorem 2.1 of Blitzer et al. (2008). In fact, the latter bound might even be tighter, as Theorem 3.1 considers the maximum over the three components of the domain adaptation bound separately (the source error, the  discrepancy and the lambda term). The same observation holds for Theorem 3.4, which is very similar to Theorem 1 of Blitzer et al. (2008).  This made me doubt that the derived theorem is studying multi-source domain adaptation in an optimal way. \nThat being said, the authors show in their experiments that their multiple sources network (named MDAN), which is based on their theoretical study, generally achieves better accuracy than the best single source DANN algorithm. This succeeds in convincing me that the proposed approach is of interest. At least, these empirical results could be used as non-trivial benchmarks for further development. \n\nNote that the fact that the \"smoothed version\" of MDAN performs better than the \"hard version\", while the latter is directly backed by the theory, also suggests that something is not captured by the theorem. The authors suggest that it can be a question of \"data-efficiency performance\": \"We argue that with more training iterations, the performance of Hard-Max can be further improved\" (page 8). This appears to me to be the weakest claim of the paper, since it is not backed by an empirical or a theoretical study. \n\nPros:\n- Tackle an important problem that is not studied as it deserves.\n- Based on a theoretical study of the multi-source domain adaptation problem.\n- The empirical study is exhaustive enough to show that the proposed algorithm actually works.\n- May be used as a benchmark for further multi-source domain adaptation research.\n\nCons:\n- The soft-max version of the algorithm - obtaining the best empirical study - is not backed by the theory.\n- It is not obvious that the theoretical study and the proposed algorithm is actually the right thing to do.\n\nMinor comment:\n- Section 5: It seems that the benchmarks named \"sDANN\" and \"cDANN\" in 5.1 are the same as \"best-Single-DANN\" and \"Combine-DANN\" in 5.2. If I am right, the nomenclature must be uniformized. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multiple Source Domain Adaptation with Adversarial Learning","abstract":"While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. We propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting. \n","pdf":"/pdf/89ab463ecc6682929a6f2460adf56b078d47a425.pdf","paperhash":"anonymous|multiple_source_domain_adaptation_with_adversarial_learning","_bibtex":"@article{\n  anonymous2018multiple,\n  title={Multiple Source Domain Adaptation with Adversarial Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDNZZZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper651/Authors"],"keywords":["adversarial learning","domain adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1512222709143,"tcdate":1511730423318,"number":1,"cdate":1511730423318,"id":"S1yZTj_lz","invitation":"ICLR.cc/2018/Conference/-/Paper651/Official_Review","forum":"ryDNZZZAW","replyto":"ryDNZZZAW","signatures":["ICLR.cc/2018/Conference/Paper651/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The theoretical analysis is new and interesting but for me its worst case aspect does not reflect real application of multisource learning and the experimental results are good but lack of comparisons with other multi-source approaches.","rating":"5: Marginally below acceptance threshold","review":"Quality:\nThe paper appears to be correct.\n\nClarity:\nThe paper is very clear\n\nOriginality:\nThe theoretical contribution extends the seminal work of Ben-David et al., the idea of using adversarial learning is not new, the novelty is mediaum\n\nSignificance:\nThe theoretical analysis is interested but for me limited, the idea of the algorithm is not new but as far as I know the first explicitly presented for multi-source. \n\nPros:\n-new theoretical analysis for multisource problem\n-paper clear\n-smoothed version is interesting\nCons\n-Learning bounds with worst case standpoint is probably not the best analysis for multisource learning\n-experimental evaluation limited in the sense that similar algorithms in the literature are not compared\n-Extension a bit direct from the seminal work of Ben-David et al.\n\n\nSummary:\nThis paper presents a multiple source domain adaptation approach based on adversarial learning.\nThe setting considered contains multiple source domains with labeled instances and one target domain with unlabeled instances. The authors propose learning bounds in this context that extend seminal work of Ben-David and co-authors(2007,2010) where they essentially consider the max source error and the max divergence between target and source with the presence of empirical estimate.\nThen, they propose an adversarial algorithm to optimize this bound, with another version optimizing a smoothed version, following the approach of Ganin et al.(2016). \nAn experimental evaluation on 3 known tasks is presented.\n\nComments:\nComments:\n\n-I am not particularly convinced that the proposed theory explains best multi-source learning. In multi-source, you expect that one source may compensate the others when needed for classification of particular instances. The paper considers a kind of worst case by taking the max error over the sources and the max divergence between target and source, but not really representative of what happens for real problems in the sense that you do not take into account how the different sources interact.\nThe experimental results confirm this aspect actually.\nMaybe, the authors could propose a learning bound that correspond to the smoothed version proposed in the paper and that works best.\n\nThe Hard version of the algorithm seems here to comply with the bound, while the algorithm that is really interesting is the smoothed version.\n\n-Experimental evaluation is a bit limited, there is no comparison with other (deep learning methods) tackling multi-source scenarios (or equivalent), while I think it is easy to find related approaches :\n-E. Tzeng, J. Hoffman, T. Darrell, K. Saenko. Simultaneous Deep Transfer Across Domains and Tasks. ICCV 2015.\n-I-H Jhuo, D Liu, D.T. Lee, and S-Fu. Chang. Robust visual domain adaptation with low-rank reconstruction. In IEEE CVPR, 2012.\n-Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In IEEE International Conference on Computer Vision (ICCV), 2015.\n-Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source domain generalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n-R. Gopalan,R. Li,and R. Chellappa. Unsupervised Adaptation Across Domain shifts by generating intermediate data representations. PAMI, 36(11), 2014.\n\nNote also this paper at CVPR'17: about Domain adversarial adaptation.\nE. Tzeng, J. Hoffman, K. Saenko, T. Darrell.  Adversarial Discriminative Domain Adaptation, CVPR 2017.\n\n\n-Nothing is said about the complexity of applying the algorithm on the different datasets (convergence, tuning, ...)\nFor the smoothed version, it could be interesting to see if the weights w_i associated to each source are related to each (original) source error and see how the sources are complementary. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multiple Source Domain Adaptation with Adversarial Learning","abstract":"While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. We propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting. \n","pdf":"/pdf/89ab463ecc6682929a6f2460adf56b078d47a425.pdf","paperhash":"anonymous|multiple_source_domain_adaptation_with_adversarial_learning","_bibtex":"@article{\n  anonymous2018multiple,\n  title={Multiple Source Domain Adaptation with Adversarial Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDNZZZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper651/Authors"],"keywords":["adversarial learning","domain adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1509739180312,"tcdate":1509130543077,"number":651,"cdate":1509739177641,"id":"ryDNZZZAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryDNZZZAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multiple Source Domain Adaptation with Adversarial Learning","abstract":"While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. We propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting. \n","pdf":"/pdf/89ab463ecc6682929a6f2460adf56b078d47a425.pdf","paperhash":"anonymous|multiple_source_domain_adaptation_with_adversarial_learning","_bibtex":"@article{\n  anonymous2018multiple,\n  title={Multiple Source Domain Adaptation with Adversarial Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDNZZZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper651/Authors"],"keywords":["adversarial learning","domain adaptation"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}