{"notes":[{"tddate":null,"ddate":null,"tmdate":1515385521439,"tcdate":1515385521439,"number":5,"cdate":1515385521439,"id":"HkKhMOgEM","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Comment","forum":"rJSr0GZR-","replyto":"Hk2EakRmf","signatures":["ICLR.cc/2018/Conference/Paper1047/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1047/Authors"],"content":{"title":"Response to Reviewer 3","comment":"Dear Reviewer 3,\n\n\"- Contributions; this paper propose an improvement over a existing model. However, neither the idea/insights it brought can be applied onto other generative models, nor the improvement bring a significant improvement over the-state-of-the-arts. I am wondering what the community will learn from this paper, or what the author would like to claim as significant contributions. \"\n\nThanks for your comments. \n\nWith the changes we have made so far, we believe our contributions include\n1)\tWe replace the simple prior with a learned prior by training the code generator to output latent variables that will minimize an adversarial loss in data space.\n2)\tWe employ a learned similarity metric (Larsen et al., 2015) in place of the default squared error in data space for training the autoencoder.\n3)\tWe maximize the mutual information between part of the code generator input and the decoder output for supervised and unsupervised training using a variational technique introduced in InfoGAN (Chen et al., 2016).\n\nExtensive experiments confirm its effectiveness of generating better quality images and learning better disentangled representations than AAE in both supervised and unsupervised settings, particularly on complicated datasets. In addition, to the best of our knowledge, this is one of the first few works that attempt to introduce a learned prior for AAE.\n\nWe have re-written extensively the entire manuscript, presenting more experimental results and analyses as requested. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515385599477,"tcdate":1515220641930,"number":4,"cdate":1515220641930,"id":"H1csCkCmz","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Comment","forum":"rJSr0GZR-","replyto":"SkCykNYCb","signatures":["ICLR.cc/2018/Conference/Paper1047/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1047/Authors"],"content":{"title":"Changes we've done in the revised manuscript","comment":"Dear Thanh Tung Hoang,\n\n\"AAE with code generator can produce much better images but suffer from mode collapse. It seems that the improvement in the image quality is due to the fact that the network has remembered some of the input. In other words, the mode collapse problem makes generated images look better. I would love to see the result without mode collapse problem. For example, you could try Wasserstein GAN which suffer less from mode collapse problem. I am also interested in the learned prior distribution. If you could provide some analysis on the learned prior then your paper could be much better.\"\n\nSince receiving the review comments, we have improved our model in several significant ways, including\n1)\tIntroducing a pair of more capable encoder and decoder with ResNets. (See appendix for the implementation details)\n2)\tEmploying a learned similarity metric in place of the default squared error in data space to improve the convergence of the decoder. (See Section 3 Learning Priors for the reasons)\n3)\tIntroducing the variational technique in InfoGAN for training the decoder and code generator when it is necessary to generate images conditionally on an input variable, as in our supervised and unsupervised learning tasks. (See Section 3 Learning Priors for the reasons)\n\nWith these changes, our model can now produce much better images without incurring obvious mode collapse.\n\nWe have re-written extensively the entire manuscript, presenting more experimental results and analyses. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515385571873,"tcdate":1515220474406,"number":3,"cdate":1515220474406,"id":"ryz-A1CXz","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Comment","forum":"rJSr0GZR-","replyto":"ByzPtktlM","signatures":["ICLR.cc/2018/Conference/Paper1047/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1047/Authors"],"content":{"title":"Response to Reviewer 1","comment":"Dear Reviewer 1,\n\n\"This paper proposes an interesting idea--to learn a flexible prior from data by maximizing data likelihood. It seems that in the prior improvement stage, what you do is training a GAN with CG+dec as the generator while D_I as the discriminator (since you also update dec at the prior improvement stage). So it can also be regarded as GAN trained with an additional enc and D_c, and additional objective. In my opinion, this may explain why your model can generate sharper images. \nThe experiments do demonstrate the power of their model compared to AAE. However, only the qualitative analysis may not persuade me and more thorough analysis is needed. \"\n\nThanks for your suggestions. We have provided more analysis results including comparison of inception scores and visualization of learned code space in the revised manuscript. \n\n\"1. About the latent space for z. The motivation in AAE is to impose aggregated posterior regularization $D(q(z),p(z))$ where $p(z)$ is chosen as a simple one, e.g., Gaussian. I'm curious how the geometry of the latent space will be, when the code generator is introduced. Maybe some visualization like t-sne will be helpful. \n\n2. Any quantitative analysis? Doing a likelihood analysis like that in the AAE paper will be very informative. \"\n\nThanks for your suggestion. For quantitative evaluation, we have compared the inception score of the proposed method with other generative models in Table I. We also have visualized the learned priors with t-SNE in Figs. 9 and 12 for the supervised and unsupervised learning tasks. The text in Section 4.2.1 and Section 4.2.2 have been modified accordingly to include the discussions (see the last paragraphs in these sections). \n\nIn addition, since receiving the review comments, we have improved our model in several significant ways, including \n1) Introducing a pair of more capable encoder and decoder with ResNets. (See appendix for the implementation details) \n2) Employing a learned similarity metric in place of the default squared error in data space to improve the convergence of the decoder. (See Section 3 Learning The Prior for the reasons) \n3) Introducing the variational technique in InfoGAN for training the decoder and code generator when it is necessary to generate images conditionally on an input variable, as in our supervised and unsupervised learning tasks. (See Section 3 Learning The Prior for the reasons) With these changes, our model can now produce much better images without incurring obvious mode collapse.\n\nWe have re-written extensively the entire manuscript, presenting more experimental results and analyses as requested. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515385455339,"tcdate":1515220276471,"number":2,"cdate":1515220276471,"id":"Hk2EakRmf","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Comment","forum":"rJSr0GZR-","replyto":"BkD44d9gM","signatures":["ICLR.cc/2018/Conference/Paper1047/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1047/Authors"],"content":{"title":"Response to Reviewer 3","comment":"Dear Reviewer 3,\n\n\"This paper propose a simple extension of the adversarial auto-encoders for (conditional) image generation. The general idea is that instead of using Gaussian prior, the propose algorithm uses a \"code generator\" network to warp the gaussian distribution, such that the internal prior of the latent encoding space is more expressive and complicated. \n\nPros: \n- The proposed idea is simple and easy to implement \n- The results show improvement in terms of visual quality \n\nCons: \n- I agree that the proposed prior should better capture the data distribution. However, incorporating a generic prior over the latent space plays a vital role as regularisation, this helps avoid model collapse. \nAdding a complicated code generation network brings too much flexibility for the prior part. This makes the prior and posterior learnable, which makes it easier to fool the regularisation discriminator (think about the latent code and prior code collapsed to two different points). As a result, this weakens the regularisation over the latent encoder space. \n- The above mentioned could be verified through qualitative results. As shown in Fig. 5. I believe this is a result due to the fact that the adversarial loss in the regularisation phase does not a significant influence there. \"\n\nThanks for your comments. I agree that generic priors may help avoid mode collapse. However, it also risks overly regularizing the model, consequently decreasing its expressiveness. \n\nThis work, like few other similar attempts for VAE, aims to learn a prior through a code generation network so that the resulting model can better explain the data distribution. Unlike the prior works, which are mostly based on maximizing the data log-likelihood, ours tries to learn the prior by minimizing an adversarial loss in data space. \n\nSince receiving the review comments, we have improved our model in several significant ways, including\n1)\tIntroducing a pair of more capable encoder and decoder with ResNets. (See appendix for the implementation details)\n2)\tEmploying a learned similarity metric in place of the default squared error in data space to improve the convergence of the decoder. (See Section 3 Learning The prior for the reasons)\n3)\tIntroducing the variational technique in InfoGAN for training the decoder and code generator when it is necessary to generate images conditionally on an input variable, as in our supervised and unsupervised learning tasks. (See Section 3 Learning The Prior for the reasons)\n\nWith these changes, our model can now produce much better images without incurring obvious mode collapse. Furthermore, as shown in our visualization of latent code space in supervised and unsupervised tasks (see Figs 9 and 12), the code generator does exert a regularization effect while producing better images.  \n\n\"- I have some doubts over why AAE works so poorly when the latent dimension is 2000. How to make sure it's not a problem of implementation or the model wasn't trapped into a bad local optima / saddle points. Could you justify this?\"\n\nThanks for pointing out this. We have implemented a pair of more capable encoder and decoder with ResNets. AAE now performs reasonably well (see Figs. 5 and 6). But, still when the latent dimension is increased to 100-D or 2000-D, the simple Gaussian prior may overly regularize the model. Imagine that the latent codes generated by the encoder may occupy only a tiny portion of the high dimensional code space specified by the prior. In this case, the limited training data can hardly ensure that every random sample drawn from the prior would produce a good decoded image.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515385239918,"tcdate":1515220031344,"number":1,"cdate":1515220031344,"id":"SJvH3J0XM","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Comment","forum":"rJSr0GZR-","replyto":"ByjrTO5ef","signatures":["ICLR.cc/2018/Conference/Paper1047/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1047/Authors"],"content":{"title":"Response to Reviewer 2","comment":"Dear Reviewer2, \n\n\"Recently some interesting work on a role of prior in deep generative models has been presented. The choice of prior may have an impact on the expressiveness of the model [Hoffman and Johnson, 2016]. A few existing work presents methods for learning priors from data for variational autoencoders [Goyal et al., 2017][Tomczak and Welling, 2017]. The work, \"VAE with a VampPrior,\" [Tomczak and Welling, 2017] is missing in references. \"\n\nThanks for your suggestion. We have cited this work in Introduction and provided a description in Related Work. \n\n\"The current work focuses on adversarial autoencoder (AAE) and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution. Adversarial loss is used to train the code generator network, allowing the output of the network could be any distribution. I think the method is quite simple but interesting approach to improve AAEs without hurting the reconstruction. The paper is well written and is easy to read. The method is well described. However, what is missing in this paper is an analysis of learned priors, which help us to better understand its behavior. The model is evaluated qualitatively only. What about quantitative evaluation? \"\n\nThanks for your suggestion. For quantitative evaluation, we have compared the inception score of the proposed method with other generative models in Table I. We also have visualized the learned priors with t-SNE in Figs. 9 and 12 for the supervised and unsupervised learning tasks. The text in Section 4.2.1 and Section 4.2.2 have been modified accordingly to include the discussions (see the last paragraphs in these sections).\n\nIn addition, since receiving the review comments, we have improved our model in several significant ways, including \n1) Introducing a pair of more capable encoder and decoder with ResNets. (See appendix for the implementation details) \n2) Employing a learned similarity metric in place of the default squared error in data space to improve the convergence of the decoder. (See Section 3 Learning The Prior for the reasons) \n3) Introducing the variational technique in InfoGAN for training the decoder and code generator when it is necessary to generate images conditionally on an input variable, as in our supervised and unsupervised learning tasks. (See Section 3 Learning The Prior for the reasons) With these changes, our model can now produce much better images without incurring obvious mode collapse.\n\nWe have re-written extensively the entire manuscript, presenting more experimental results and analyses as requested. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642379441,"tcdate":1511849283178,"number":3,"cdate":1511849283178,"id":"ByjrTO5ef","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Review","forum":"rJSr0GZR-","replyto":"rJSr0GZR-","signatures":["ICLR.cc/2018/Conference/Paper1047/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple idea to improve adversarial autoencoders by learning priors","rating":"6: Marginally above acceptance threshold","review":"Recently some interesting work on a role of prior in deep generative models has been presented. The choice of prior may have an impact on the expressiveness of the model [Hoffman and Johnson, 2016]. A few existing work presents methods for learning priors from data for variational autoencoders [Goyal et al., 2017][Tomczak and Welling, 2017].  The work, \"VAE with a VampPrior,\" [Tomczak and Welling, 2017] is missing in references.\n\nThe current work focuses on adversarial autoencoder (AAE) and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution. Adversarial loss is used to train the code generator network, allowing the output of the network could be any distribution. I think the method is quite simple but interesting approach to improve AAEs without hurting the reconstruction. The paper is well written and is easy to read. The method is well described. However, what is missing in this paper is an analysis of learned priors, which help us to better understand its behavior. \n\nThe model is evaluated qualitatively only. What about quantitative evaluation? \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642379480,"tcdate":1511846958833,"number":2,"cdate":1511846958833,"id":"BkD44d9gM","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Review","forum":"rJSr0GZR-","replyto":"rJSr0GZR-","signatures":["ICLR.cc/2018/Conference/Paper1047/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Improving AAE by warping the Gaussian prior using deep networks","rating":"5: Marginally below acceptance threshold","review":"This paper propose a simple extension of the adversarial auto-encoders for (conditional) image generation. The general idea is that instead of using Gaussian prior, the propose algorithm uses a \"code generator\" network  to warp the gaussian distribution, such that the internal prior of the latent encoding space is more expressive and complicated. \n\nPros:\n- The proposed idea is simple and easy to implement\n- The results show improvement in terms of visual quality\n\nCons:\n- I agree that the proposed prior should better capture the data distribution. However, incorporating a generic prior over the latent space plays a vital role as regularisation, this helps avoid model collapse. Adding a complicated code generation network brings too much flexibility for the prior part. This makes the prior and posterior learnable, which makes it easier to fool the regularisation discriminator (think about the latent code and prior code collapsed to two different points). As a result, this weakens the regularisation over the latent encoder space.  \n- The above mentioned could be verified through qualitative results. As shown in Fig. 5. I believe this is a result due to the fact that the adversarial loss in the regularisation phase does not a significant influence there. \n- I have some doubts over why AAE works so poorly when the latent dimension is 2000. How to make sure it's not a problem of implementation or the model wasn't trapped into a bad local optima / saddle points. Could you justify this?\n- Contributions; this paper propose an improvement over a existing model. However, neither the idea/insights it brought can be applied onto other generative models, nor the improvement bring a significant improvement over the-state-of-the-arts. I am wondering what the community will learn from this paper, or what the author would like to claim as significant contributions. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642379525,"tcdate":1511745882321,"number":1,"cdate":1511745882321,"id":"ByzPtktlM","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Official_Review","forum":"rJSr0GZR-","replyto":"rJSr0GZR-","signatures":["ICLR.cc/2018/Conference/Paper1047/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea, but more thorough analysis is needed.","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an interesting idea--to learn a flexible prior from data by maximizing data likelihood.\n\nIt seems that in the prior improvement stage, what you do is training a GAN with CG+dec as the generator while D_I as the discriminator (since you also update dec at the prior improvement stage). So it can also be regarded as GAN trained with an additional enc and D_c, and additional objective. In my opinion, this may explain why your model can generate sharper images.\n\nThe experiments do demonstrate the power of their model compared to AAE. However, only the qualitative analysis may not persuade me and more thorough analysis is needed.\n\n1. About the latent space for z. The motivation in AAE is to impose aggregated posterior regularization $D(q(z),p(z))$ where $p(z)$ is chosen as a simple one, e.g., Gaussian. I'm curious how the geometry of the latent space will be, when the code generator is introduced. Maybe some visualization like t-sne will be helpful.\n2. Any quantitative analysis? Doing a likelihood analysis like that in the AAE paper will be very informative. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1509666534384,"tcdate":1509666534384,"number":1,"cdate":1509666534384,"id":"SkCykNYCb","invitation":"ICLR.cc/2018/Conference/-/Paper1047/Public_Comment","forum":"rJSr0GZR-","replyto":"rJSr0GZR-","signatures":["~Thanh_Tung_Hoang1"],"readers":["everyone"],"writers":["~Thanh_Tung_Hoang1"],"content":{"title":"Wasserstein GAN could improve the mode collapse problem","comment":"AAE with code generator can produce much better images but suffer from mode collapse. It seems that the improvement in the image quality is due to the fact that the network has remembered some of the input. In other words, the mode collapse problem makes generated images look better. I would love to see the result without mode collapse problem. For example, you could try Wasserstein GAN which suffer less from mode collapse problem. I am also interested in the learned prior distribution. If you could provide some analysis on the learned prior then your paper could be much better."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1515468443275,"tcdate":1509138011404,"number":1047,"cdate":1510092360392,"id":"rJSr0GZR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJSr0GZR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Priors for Adversarial Autoencoders","abstract":"Most deep latent factor models choose simple priors for simplicity, tractability\nor not knowing what prior to use. Recent studies show that the choice of\nthe prior may have a profound effect on the expressiveness of the model,\nespecially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders\n(AAEs). We introduce the notion of code generators to transform manually selected\nsimple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than\nAAEs in both supervised and unsupervised settings. Lastly, we present its\nability to do cross-domain translation in a  text-to-image synthesis task.","pdf":"/pdf/bb0a3113e804f56b3a47935923d39ac626f36536.pdf","TL;DR":"Learning Priors for Adversarial Autoencoders","paperhash":"anonymous|learning_priors_for_adversarial_autoencoders","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Priors for Adversarial Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJSr0GZR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1047/Authors"],"keywords":["deep learning","computer vision","generative adversarial networks"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}