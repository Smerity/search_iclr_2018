{"notes":[{"tddate":null,"ddate":null,"tmdate":1511821921030,"tcdate":1511821921030,"number":6,"cdate":1511821921030,"id":"HktDzGqgf","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Comment","forum":"H1xJjlbAZ","replyto":"BkgXE1qef","signatures":["ICLR.cc/2018/Conference/Paper618/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper618/Authors"],"content":{"title":"Our operational definition of fragility","comment":"Thanks for your comment.\n\nOur operational definition of fragility is that the interpretation is fragile if, given a fixed network, two similar inputs with the same prediction have very different ‘interpretations’. All of our results support the claim that interpretations are fragile by this metric. \n\nIt seems like you are basically questioning whether this is a good metric of fragility and you might have a different metric in mind. We think it’s certainly interesting to explore other metrics, and our paper opens the door for that. There are two main motivations for our metric/definition of fragility. \n\n1. Our definition is well-motivated by practice. Suppose you have a pathology image that is predicted to be a malignant tumor, as an example. The clinician might then use some saliency map to interpret which part of the image is the most informative. The reliability of this interpretation exactly maps onto our fragility metric: the input image always have measurement errors and our fragility metric quantifies whether the same parts of the image would light up as informative across different measurement errors. This motivates defining fragility as we do. \n\n2.  Our definition is consistent with other notions of fragility to adversarial attacks. \n\nYou said that the perturbation to the input changes the function. Yes, that’s exactly why the interpretation is fragile by our metric. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1512260606335,"tcdate":1511819881459,"number":3,"cdate":1511819881459,"id":"Sk-uqZ9xG","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Review","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["ICLR.cc/2018/Conference/Paper618/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting work, but wrong conclusions","rating":"5: Marginally below acceptance threshold","review":"The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs). The authors perturb input images and create explanations using different methods. Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps. This is true even for random perturbations. \n\nThe images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind. Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change.\nFor one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability. Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation. In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer.\n\nIt can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent. The explanation maps mirror for the considered methods the model's reaction to the input. They are thus not meaningless, but are a measure to model reaction instead of an independent process. The excellent Figure 2 supports this point. Not the interpretation itself is fragile, but the model.\nAdding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane. The fragility of DNN models to marginally perturbed inputs themselves is well known. \nThis especially true for adversial perturbations, which have been used as test cases in this work. The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches.\n\nThe authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps. In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method.\nFurther do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model. It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input.\n\nFurther remarks:\nIt would be interesting to see the size and position of the center of mass attacks in the appendix. The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations. The reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the DeepLift method. Can bugs and suboptimal configurations be ruled out during the experiments? The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point). However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis.\n\nHave the authors considered training the net with small random perturbations added to the samples, to compare the \"vanilla\" model to the more robust one, which has seen noisy samples, and compared explanations? \nWhy not train (finetune) the considered models using softplus activations instead of exchanging activation nodes?\nAppendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation\n\nConclusion:\nThe paper follows an interesting approach, but ultimately takes the wrong view point:\nThe authors try to attribute fragility to explaining methods, which visualize/measure the reaction of the model to the perturbed inputs. A major rework should be considered.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1512222702283,"tcdate":1511814697355,"number":2,"cdate":1511814697355,"id":"S1Z4Lgqez","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Review","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["ICLR.cc/2018/Conference/Paper618/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The key point of the paper is that it is possible to get different salience maps for interpretability while retaining correct labeling. ","rating":"4: Ok but not good enough - rejection","review":"The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output.    Thus the authors claim that one has to be careful about using feature importance maps.\n\nPro:  The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.\n\nCons:\nThe main problem I have with the paper is that there is no precise definition of what constitutes the stable feature importance map.  The examples in the paper seem to be cherry picked to illustrate dramatic effects.   The experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs. The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1511810072381,"tcdate":1511810072381,"number":9,"cdate":1511810072381,"id":"BkgXE1qef","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Interesting observation but the conclusions drawn from the observation are incorrect.","comment":"The paper describes an interesting effect but it attributes the observation incorrectly to interpretability method.\nTherefore the title and conclusions drawn in the paper are not correct.\n\nTo be precise the paper claims that interpretability methods are fragile and that the experiments support this.\nTo claim this it is necessary to have two aspects.\nA) A change is made such that the network is explained in a different way\nB) This change should be controlled such that we are guaranteed that the explanation MUST be the same.\nIn this paper effect (A) is present, however part (B) is NOT present since the function the network computes changes.\nIf the network makes a different computation it is possible that the explanation needs to change as well.\nTherefore the paper does not show that interpretation is fragile.\n\nTo understand why point (B) is missing we have to look at what is done and what is being interpreted.\n\nIt is shown that it is possible to imperceptibly change an image such that the gradient w.r.t. to the input changes, but without changing the classifier’s output.\nSince most interpretability methods are based on the gradient this changes their interpretation.\nThis is where the paper makes a mistake by ignoring the meaning of the gradient or saliency map (as defined by Simonyan).\n\nAs noted in the Saliency Map paper by Simonyan, the gradient of the output (logit) w.r.t. the input \nis a local linear approximation of the network’s function in the neighborhood of the datapoint, i.e. a Taylor series according to said paper.\nFor relu and max-pooling networks this approximation is actually exact given correct treatment of the bias.\nNow when the gradient changes after an input change, this shows that the network computes a different function.\n\nThe approach in this paper assures that the most probable class does not change. \nIt does not control how this decision was made.\nSince the function might be different, the explanation should not necessarily be the same. \nIt might very well be possible that the network’s decision needs to be interpreted differently now.\nIntuitively, this can be understood by realizing that for many problems the same decision can be made based on different subsets of evidence.\n\nSince there is no certainty that the decision was made in an identical manner for the original datapoint and the modified one,\n It is NOT possible to conclude that interpretability methods are fragile.\n\nAdditionally, the theoretical analysis also make an incorrect claim. \nThe paper states that a logistic regression network is susceptible to the adversarial attack on the interpretability.\nThis is not correct.\nIn the case that the logistic model (two or multi-class) is analyzed from the logit (as is done commonly in the papers by Bach et al, Simonyan et al ,…..) the gradient w.r.t. the input is always the weight vector. \nIn a two class setting where a sigmoid is added, the gradient becomes a scaled version of the weight vector. \nFor this reason interpretability is not fragile here either.\nTherefore at least a multi class logistic regression is needed and the methods have to be applied to the output post-softmax.\nPlease not that this is not how the original papers describe the approach how to use these methods.\n\nTo conclude, I think the manuscript shows an interesting effect but it draws the wrong conclusions.\nThe fact that the network’s function can change drastically due to a small change in the input without affecting the prediction is surprising.\nThis is an effect similar to the original observation of adversarial examples.\nHowever it does NOT show fragility of interpretability maps.\nFor this reason, I believe that the paper needs to be updated before it can be accepted.\nThe observed effect does warrant further investigation and is an interesting finding."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1511730726288,"tcdate":1511390748798,"number":7,"cdate":1511390748798,"id":"SJBmRuQxG","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"S1UARBXgG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Deflected answers.","comment":"1) \"All three saliency methods, including the simple gradient method, have drawbacks and are unstable\"\n           Thus the conclusion \"Interpretation of Neural networks is fragile\" is based on unstable metrics and hence it requires much more evidence (stable metrics) than shown in the paper.\n\n2) \"most of them led to very small changes in the prediction confidence\"\n            Thus the figure 1 is not a good representative of the experimental results (since 1.a, 1.c have a huge change in their prediction confidence)\n\n3) Sure.  So the authors agree that the statement \"importance of individual features or training examples is highly fragile to even small random perturbations to the input image\" is after all over-emphasized. As figure 3 illustrates, for small perturbations L_\\infty = 1, random perturbations are around  3-10 times weaker than adversarial perturbations.\n\n4) Why was spearman ranking chosen over L2? Can you share results for other distance measures anonymously? Many other submissions in ICLR do this.\n\n5) Sure."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1511378638039,"tcdate":1511378638039,"number":5,"cdate":1511378638039,"id":"S1UARBXgG","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Comment","forum":"H1xJjlbAZ","replyto":"HkJSs4Xlf","signatures":["ICLR.cc/2018/Conference/Paper618/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper618/Authors"],"content":{"title":"Yes JvN would have agreed that the interpretation is fragile.","comment":"Thanks for your interest. \n\nAll three saliency methods, including the simple gradient method, have drawbacks and are fragile. The best way to see this is the systematic experiments in Fig. 3. \n\nOne of our findings is that, in general, perturbations can substantially change the interpretation (saliency) without changing the predicted label. In the systematic experiments of Sec. 4, ALL of the perturbations preserved the original label, and most of them led to very small changes in the prediction confidence. \n\nOur results show that our targeted perturbations are much more effective than random perturbations. We also demonstrate that the saliency of individual features (e.g. particular pixels) is fragile to even random perturbations. Fig. 3 top row shows that there is a ~80% turnover in the top features under random perturbation. These two results are complementary. \n\nYes, we have used other similarity metrics such as intersection of the top features, L2 distance, etc. and the same results hold. \n\nAs is common for most of the ICLR submissions, we plan make all the code public soon.  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1511373623190,"tcdate":1511373623190,"number":6,"cdate":1511373623190,"id":"HkJSs4Xlf","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk - John von Neumann","comment":"Dear Authors,\n\n1) In Figure 8 the three saliency maps differ from each other. This implies that at least 2 of the saliency maps are incorrect/irrelevant to this problem (may be all the 3 are). \n    To corroborate the above point:\n    a) In Figure 9, the DEEPLift saliency map differs from the other two \n    b) In Figure 7, the Integrated saliency map differs from the other two. \n    From this, one would think that the simple gradient method is the most reliable of the three methods but Section 2.1 contradictingly states that (Shrikumar, Sundarajan) \"simple gradient method\" has drawbacks.\n\n2) In Figure 1, the confidence of the output of the network has increased in all the three of your examples. Is this a general phenomenon or are these hand picked examples?\n\n3) The introduction stresses equally on\n    a) \"importance of individual features or training examples is highly fragile to even small random perturbations to the input image\". \n    b) \"we show how targeted perturbations can lead to different global interpretations\".\n    However, surprisingly Figure 1 doesn't have any examples based on random perturbation attack. \n    On further searching, I was able to find an example in the appendix, where the changes in the saliency map due to random pertubation is nothing compared to changes in the saliency map due to attacked perturbation. I feel the case for random permutation attack is over-emphasized.\n\n4) In section 2.2, each test image has a vector v of size |train_images|, where the element v[i] is the importance of training image i for the classification of the test image. For high dimensional vectors v1, v2, the spearman rank correlation is very prone to small noise.\nTo overcome this, the most 'natural' way to quantify the similarity between two different test images would be to take a dot product of their corresponding vectors v1 and v2. One could also using other similarity measures such as 2 -d(v1,v2), where d(v1,v2) is a distance metric designed to overcome the noise in high dimensions.\n\n5) I was not able to find the public code of the experiments, even though this paper heavily relies on the experiments. Is the code public?\n\nThanks"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1512222702326,"tcdate":1511281101652,"number":1,"cdate":1511281101652,"id":"HJIRZCbeG","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Review","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["ICLR.cc/2018/Conference/Paper618/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interpretation of Neural Network is Fragile","rating":"6: Marginally above acceptance threshold","review":"The authors study cases where interpretation of deep learning predictions is extremely fragile. They systematically characterize the fragility of several widely-used feature-importance interpretation methods. In general, questioning the reliability of the visualization techniques is interesting. Regarding the technical details, the reviewer has the following comments: \n\n- What's the limitation of this attack method?\n\n- How reliable are the interpretations? \n\n- The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity. \n\n- Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510927442466,"tcdate":1510900395196,"number":4,"cdate":1510900395196,"id":"Sk7hGWhkM","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Comment","forum":"H1xJjlbAZ","replyto":"SJtR6e3kz","signatures":["ICLR.cc/2018/Conference/Paper618/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper618/Authors"],"content":{"title":"Straight Forward Implementation","comment":"We will release our code soon to show how the algorithm is implemented. The gradients can be implemented in Tensorflow like we described in the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510899153490,"tcdate":1510899153490,"number":5,"cdate":1510899153490,"id":"SJtR6e3kz","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"BJhbOjiJG","signatures":["~yang_zhang1"],"readers":["everyone"],"writers":["~yang_zhang1"],"content":{"title":"Is \"tf.gradients(tf.gradients(maxlogit, input), input)\" legal? ","comment":"Thanks for your reply.\n\nI think you would agree that to apply your attack algorithm in the paper, you cannot avoid calculating the above gradient in the title, i.e the gradient of the saliency map w.r.t the input. However, this expression itself is not legal. Notice that the first gradient inside is actually the saliency map. We do can calculate it because we know the forward pass. However, for the gradient outside, we will have a problem because now we don't know what's the gradient for the pooling layers. I don't think Softplus will solve this issue. \n\nI was wondering if you could release your code. Maybe you have a smart way to get around based on some math derivations. \n\nThank you again."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510877188346,"tcdate":1510877188346,"number":3,"cdate":1510877188346,"id":"BJhbOjiJG","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Comment","forum":"H1xJjlbAZ","replyto":"HyNgEDoJM","signatures":["ICLR.cc/2018/Conference/Paper618/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper618/Authors"],"content":{"title":"The method is applicable to a networks with pooling layers","comment":"The interpretation attacks in our paper do work for NNs with pooling layers. In fact, our experimental results on ImageNet (Figs 1 and 3) were all produced for Sqeezenet, which has maxpooling and average pooling layers. The reason this works is as follows: when computing the attack direction, we replace ReLU with Softplus so we have non-zero second gradients. Max-pooling simply picks out a particular neuron to pass through, so it will have non-zero second order gradients as long as the activation has non-zero second gradient  (except on a set of measure zero).  The second order chain rule has two terms, one of which, is zero as the second order gradient of maxpooling is zero. The other term is non-zero as long as the activation functions of the network have non-zero second order gradients. For more clarification please look at Faà di Bruno's formula for second order derivative.  (You can empirically confirm it by building a simple network in Tensroflow with softplus activation and a maxpool layer and take the second order gradient).\nNote that we use this Softplus network only for the purpose of finding an attack direction, we still compute saliency with respect to the original ReLU network, as we discussed in the paper. This means that our attacks can be applied to any ReLU network with or without pooling.  \nYou are correct that the main message of our paper is that reliability of neural network interpretation is fragile, and we developed approaches to systematically quantify this effect for the first time. We expect that there are many ideas to improve upon the specific attacks we proposed, and this opens up an interesting research direction."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510859756517,"tcdate":1510859756517,"number":4,"cdate":1510859756517,"id":"HyNgEDoJM","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["~yang_zhang1"],"readers":["everyone"],"writers":["~yang_zhang1"],"content":{"title":"This attack method is not applicable to any network with pooling layers","comment":"To apply the attack method, we cannot avoid calculating the gradient of the saliency map w.r.t to the input. However, notice that the saliency map itself is a gradient of the maxlogit w.r.t the input. The gradient for a neural network is usually calculated by using the backpropagation, which depends on the forward pass. So, for any network with pooling layers, the gradient of the saliency map w.r.t the input is not calculable, because the pooling layers, when you calculate the gradient for the second time, are not differentiable. In the paper, you only point out that for the widely used activation function Relu, in order to avoid the zero gradient problem, we can replace it with a softplus activation. \n\nOne possible way to solve this problem is to stop the gradient of D w.r.t the input x at the saliency map level. However, this fix doesn't make sense mathematically, though it might can still be an attack method. \n\nConsidering that most of the networks will contain pooling layers, I think the application of this attack method is limited. But questioning the reliability of the visualization techniques is a good point in general. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510343713783,"tcdate":1510343656381,"number":3,"cdate":1510343656381,"id":"SylgNFXJf","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"ryEWYqGJM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Feature importance methods","comment":"My question is more about identifying important features rather than finding influential training images. I wonder if feature importance methods produce similar good results on training and test images? "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510283516181,"tcdate":1510283516181,"number":2,"cdate":1510283516181,"id":"ryEWYqGJM","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Comment","forum":"H1xJjlbAZ","replyto":"HJAUq8CA-","signatures":["ICLR.cc/2018/Conference/Paper618/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper618/Authors"],"content":{"title":"Re: Interpretation on Training vs Test Data","comment":"Generally speaking, we find similar performance between interpretability on training images and test images. Of course, if a training image is used at test time, influence functions will return the training image itself as the most influential image."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510096921898,"tcdate":1510005333972,"number":2,"cdate":1510005333972,"id":"HJAUq8CA-","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"r1g2UGCRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Interpretation on Training vs Test Data","comment":"Thanks for your thorough reply. Another question: regarding subjective measure of interpretation, do feature importance methods perform similarly on training and test data?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1510092426898,"tcdate":1509988008540,"number":1,"cdate":1509988008540,"id":"r1g2UGCRZ","invitation":"ICLR.cc/2018/Conference/-/Paper618/Official_Comment","forum":"H1xJjlbAZ","replyto":"HyA39DpR-","signatures":["ICLR.cc/2018/Conference/Paper618/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper618/Authors"],"content":{"title":"Reliability of Interpretations on Original Images","comment":"Very good question. \n    To have a sense of reliability of saliency methods, one can argue with both subjective and objective measures. The most prevalent objective measure in the literature has been weakly-supervised object localization which is basically trying to localize the classified object using the most salient input dimensions(pixels). Simonyan et al discussed this measure in their original work for the simple gradient method and reported less than 50% error. DeepLIFT and Integrated Gradients methods have not been yet applied to the task of localization to the best of our knowledge.\n    As a subjective comment, however, I have to say that all of the three mentioned feature importance methods are successful (nearly all the time) in pointing to the region of important pixels (in other words the region of image containing the classified object); what makes them different in performance, is how noisy the saliency map is. In other words, how many non-important pixels(subjectively) are pointed out by the feature importance method to be important or how many missing important pixels one can detect in the salient part of the heat map. It could be said that Integrated Gradients results in the best subjective saliency map and also DeepLIFT has acceptable performance. Examples of both could be found in https://github.com/ankurtaly/Integrated-Gradients and https://github.com/kundajelab/deeplift, respectively. Also, the recent \"SmoothGrad: removing noise by adding noise\" paper has tried to solve the problem of noisy saliency maps and they have reported convincing results.\n    Understanding whether influence functions provide meaningful explanations is more challenging since training examples can influence the prediction of a test image in different ways. For example, a very similar-looking training image from the same class may help the classifier identify the test image, but so can a very different-looking training image from a different class. As such, we find that the training examples that are identified as the most helpful by influence functions are not generally the same as those images that are visually similar to the test image. However, regardless of the meaningfulness of influence functions on a particular test image, we show that across the test images, we are able to perturb the ordering of the training image influences."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1509943989957,"tcdate":1509943989957,"number":1,"cdate":1509943989957,"id":"HyA39DpR-","invitation":"ICLR.cc/2018/Conference/-/Paper618/Public_Comment","forum":"H1xJjlbAZ","replyto":"H1xJjlbAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reliability of Interpretations on Original Images","comment":"I was wondering how reliable are the interpretations? i.e., for what fraction of original images do they generate a \"meaningful\" interpretation? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]}},{"tddate":null,"ddate":null,"tmdate":1509739198561,"tcdate":1509128920232,"number":618,"cdate":1509739195884,"id":"H1xJjlbAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1xJjlbAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"INTERPRETATION OF NEURAL NETWORK IS FRAGILE","abstract":"In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research.  A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense:  two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","pdf":"/pdf/2a78460adc318589991dd9b17b5139c956ff5d59.pdf","TL;DR":"Can we trust a neural network's explanation for its prediction? We examine the robustness of several popular notions of interpretability of neural networks including saliency maps and influence functions and design adversarial examples against them.","paperhash":"anonymous|interpretation_of_neural_network_is_fragile","_bibtex":"@article{\n  anonymous2018interpretation,\n  title={INTERPRETATION OF NEURAL NETWORK IS FRAGILE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1xJjlbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper618/Authors"],"keywords":["Adversarial Attack","Interpretability","Saliency Map","Influence Function","Robustness","Machine Learning","Deep Learning","Neural Network"]},"nonreaders":[],"replyCount":17,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}