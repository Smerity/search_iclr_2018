{"notes":[{"tddate":null,"ddate":null,"tmdate":1515078584816,"tcdate":1515078584816,"number":12,"cdate":1515078584816,"id":"r1S3XTsmG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"Hyla24jmz","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"comment":":)","title":"Thanks!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515044064855,"tcdate":1515044024043,"number":11,"cdate":1515044024043,"id":"Hyla24jmz","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"rJnTD8VXM","signatures":["ICLR.cc/2018/Conference/Paper492/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/AnonReviewer1"],"content":{"title":"Thank you!","comment":"Thanks a lot for the very detailed response to all our points. It was worth waiting. Though a few of your responses are frustrating (mainly when you don't follow our advices because of the lack of space), I believe you did a great job and I'm completely satisfied. It is not a so common practice to give such detailed responses in the context of a conference, and I think it is a good practice. I hope your paper will get accepted, you have significantly improved it and you deserve it! ;)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515015018172,"tcdate":1515015018172,"number":10,"cdate":1515015018172,"id":"B1Gdj6qQM","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"BylvIk5lG","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"That's a useful connection","comment":"Dear Leon, you make some interesting connections!\n\n1) Yes, our estimator does look a lot like a doubly-robust estimator.  In fact, estimating gradients through discrete random variables seems a lot like counterfactual modeling, where we happen to know all the confounders.  The motivation seems a bit different, though, in that the doubly-robust estimator is about correcting for model misspecification and removing bias, where we start with an unbiased estimator.  But the overall idea is indeed similar.  Also, the fact that these different weighting schemes both give unbiased estimates raises the question: what is the family of weighting schemes that we could be looking at?\n\n2) I see what you mean - the special case where c = f recovers the reparameterization trick, and the weighted estimates cancel out.  One thread I'd like to think about further is, can we do even better?  We actually have results on toy problems where LAX achieves lower variance than the reparameterization trick.  Of course, there are sometimes many possible reparameterizations, each with different variance.\n\n3) I think the \"Generalized Reparameterization Gradient\" paper looked at questions very similar to this one.  Also, one future direction we're thinking about is learning the reparameterization as well as the surrogate.\n\n\nThanks again for the interesting points.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515010892505,"tcdate":1515010892505,"number":9,"cdate":1515010892505,"id":"r148oh9QG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"rkaDiETZf","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"thanks!","comment":"Thank you for your comment. A sentence has been added to section 7 to mention the application these works could have to our method. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514995850335,"tcdate":1514750642298,"number":8,"cdate":1514750642298,"id":"Bk9nMTL7f","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"HyFP5nE1M","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"Response to reviewer 2","comment":"Dear reviewer 2,\n\nThank you for your kind words and detailed feedback.  Some small changes have been made to address your comments. I will address your comments in the order they were written. Your original comments will appear in [brackets] and my responses will follow.\n\n[+ Is there a good way to initialize c_phi prior to optimization? Given that c_phi must be a proxy for f(), maybe you can take advantage of this observation to find a good initialization for phi?]\n\nGood question. This was touched upon in section 3.3 where we use the concrete relaxation to initialize our control variate and then learn an offset parameterized by a neural network. However this approach is only applicable when f is known.  We also did not experiment with different structures for the control variate beyond different neural network architectures. This is an interesting idea which we leave to further work to explore. \n\n\n[+ I was confused with the Bernoulli example in Appendix B. Consider the case theta=0.5. Then, b=H(z) takes value 1 if z>0, and 0 otherwise. Thus, p(z|b,theta) should assign mass zero to values z>0 when b=0, which does not seem to be the case with the proposed sampling scheme in page 11, since v*theta=0.5*v, which gives values in [0,0.5]. And similarly for the case b=1.]\n\nThis issue was due to a typo in Appendix B where theta was swapped for (1 - theta). This has been fixed in the new version. \n\n\n[+ Why is the method called LAX? What does it stand for?]\n\nWe first coined “RELAX” as an alternative to REBAR that learned the continuous “relax”-ation.  We then developed LAX, and since it was a simpler version of relax, we chose a simpler name.  We realize that these aren’t particularly descriptive names, and welcome any suggestions for naming these estimators.\n\n[+ In Section 3.3, it is unclear to me why rho!=phi. Given that c_phi(z)=f(sigma_lambda(z))+r_rho(z), with lambda being a temperature parameter, why isn't rho renamed as phi? (the first term doesn't seem to have any parameters). In general, this section was a little bit unclear if you are not familiar with the REBAR method; consider adding more details.]\n\nThe paper has been updated to specify that phi = {rho, lambda}\n\n\n[+ Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach.]\n\nThe paper mentions that REBAR can be viewed as a special case of the RELAX method where the concrete relaxation is used as the control variate. For brevity’s sake, a full explanation of REBAR was left out.\n\n\n[+ In the abstract, consider adding two of the main ideas that the estimator relies on: control variates and reparameterization gradients. This would probably be more clear than \"based on gradients of a learned function.\"]\n\nThe abstract has been slightly changed to mention that the method involves control variates.\n\n\n[+ In the first paragraph of Section 3, the sentence \"f is not differentiable or not computable\" may be misleading, because it is unclear what \"not computable\" means (one may think that it cannot be evaluated). Consider replacing with \"not analytically computable.\"]\n\nGood point. ”not computable” has been removed from that sentence. \n\n\n[+ In Section 3.3, it reads \"differentiable function of discrete random variables,\" which does not make sense.]\n\nBy this, it was meant that the function f is differentiable, but we may be evaluating it only on a discrete input. An example of such a function would be f(x) = x^2, where x = {0, 1}. Here f is differentiable when its domain is the real numbers but we are evaluating it restricted to {0, 1}. This wording was removed to avoid confusion.\n\n\n[+ Before Eq. 11, it reads \"where epsilon_t does not depend on theta\". I think it should be the distribution over epsilon_t what doesn't depend on theta.]\n\nThis section has been reworded to make this more clear\n\n[+ In Section 6.1, it was unclear to me why t=.499 is a more challenging setting.]\n\nThe closer that t gets to .5 means that the values of f(0) and f(1) get closer together. This means a Monte Carlo estimator of the gradient will require more samples to converge to the correct value.\n\n\n[+ The header of Section 6.3.1 should be removed, as Section 6.3 is short.]\n\nSection 6.3.1 has been removed and consolidated into 6.3\n\n\n[+ In Section 6.3.1, there is a broken reference to a figure]\n\nGood catch.  The broken reference has been fixed in section 6.3\n\n[+ Please avoid contractions (doesn't, we'll, it's, etc.)]\n\nContractions have been removed.\n\n[+ There were some other typos; please read carefully the paper and double-check the writing. In particular, I found some missing commas, some proper nouns that are not capitalized in Section 5, and others (e.g., \"an learned,\" \"gradient decent\").]\n\nThese typos have been fixed.  Thank you for your attention to detail, it has improved the text considerably.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514995836345,"tcdate":1514750397542,"number":7,"cdate":1514750397542,"id":"rkSpZaImf","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"SJLiWTImf","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"Local comments part 2","comment":"[About Double Inverted Pendulum, Appendix E3 mentions 50 million frames, but the figure shows 4 millions steps. Where is the truth?]\n\nThis was due to a typo in the appendix. The figure is correct- it was run for 5 million steps, and we corrected the appendix to reflect this. \n\n\n\n[Why do you give steps for the reward, and episodes for log-variance? The caption mentions \"variance (log-scale)\", but saying \"log-variance\" would be more adequate.]\n\nThe variance was estimated at every episode since we needed to run a full episode to compute the policy gradient. After every training episode, 100 episodes were run to generate samples from our gradient estimator which were used to estimate the variance of the estimator. We run the algorithms for a fixed number of steps to be consistent with previous work. \n\n\n\n[p9: the optimal control variate: what is this exactly? How do you compare a control variate over another? This may be explained in Section 2.]\n\nThe optimal control variate is the control variate which produces a gradient estimator with the lowest possible variance. \n\n\n\n[GAE (Kimura, 2000). I'm glad you refer to former work (there is a very annoying tendency those days to refer only to very recent papers from a small set of people who do not correctly refer themselves to previous work), but you may nevertheless refer to John Schulman's paper about GAEs anyways... ;)]\n\nThanks for the pointer, this reference was added.\n\n\n[Appendix E.1 could be reorganized, with a common hat and then E.1.1 for one layer model(s?) and E.1.2 for the two layer model(s?)]\n\nThe appendix was reorganized as per your suggestions. \n\n\n\n[A sensitivity analysis wrt to your hyper-parameters would be welcome, this is true for all empirical studies.]\n\nIn general, we did not find the algorithm to be very sensitive to hyperparameters other than the learning rate.  We agree that adding a sensitivity analysis would be an improvement. We have added this to the experimental details in the Appendix along with a sentence which presents the best performing hyperparameters.\n\n\n[In E2, is the output layer linear? You just say it is not ReLU…]\n\nThe Appendix was changed to note that the output layers were linear.\n\n\n[The networks used in E2 are very small (a standard would be 300 and 400 neurons in hidden layers). Do you have a constraint on this?]\n\nThere was no hard constraint on network size. These networks were chosen because they worked well with the baseline A2C algorithm, and is a standard choice in the literature, and the OpenAI baselines.\n\n\n[\"As our control variate does not have the same interpretation as the value function of A2C, it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model. We leave the task of incorporating these and other variance reduction techniques to future work.\"\nFirst, this is important, so if this is true I would move this to the main text (not in appendix).\nBut also, it seems to me that the first sentence of E3 contradicts this, so where is the truth?]\n\n\nWe moved this section to the main text, and clarified why we don’t use reward bootstrapping for discrete, but use it for continuous experiments. We do so with the continuous control RL tasks by structuring the control variate as C = V(s) + c(a, s), where V was trained as the value function in A2C. \n\n\n[{0.01,0.003,0.001} I don't believe you just tried these values. Most probably, you played with other values before deciding to perform grid search on these, right?\nThe same for 25 in E3.]\n\nFor all the experiments, the hyperparameter values used for grid search mentioned in the appendices (E1, E2, E3) were the only ones tried. No other values were tried because of computational constraints. These values were not chosen because they gave us better result, but because we thought they would make a comprehensive grid.\n\n\n[Globally, you experimental part is rather weak, we would expect a stronger methodology, more experiments also with more difficult benchmarks (half-cheetah and the whole gym zoo ;)), more detailed analyses of the results, but to me the value of your paper is more didactical and conceptual than experimental, which I really appreciate, so I will support your paper despite these weaknesses.]\n\nYes, we believe that further experimentation is warranted, but feel that our current results demonstrate the effectiveness of our method. Thank you for your support.  \n\nTypos:\n\nWe have fixed all of the typos that you noticed. Thanks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514995824850,"tcdate":1514750366129,"number":6,"cdate":1514750366129,"id":"SJLiWTImf","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"H1Cjea8mG","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"continued response: Local comments part 1","comment":"Local Comments:\n\n[Backpropagation through the void: I don't understand why this title. I'm not a native english speaker, I'm probably missing a reference to something, I would be glad to get it.]\n\nThe title alludes to the scope of the method.  Normally we refer to backprop “through” something.  If we use our method to estimate gradients through an unknown or non-existent computation graph, we could say we are backpropagating through “the void”.  However, I would guess that you are not the only one confused by the title.\n\n[Figure 1 right. Caption states variance, but it is log variance. Why does it oscillate so much with RELAX?]\n\nThe caption has been changed to log-variance. We guess that the oscillation is due to interactions that arise as the control variate is trained. Since the parameters of the distribution p are constantly changing, the control variate is consistently a step behind p, which may account for these oscillations. We are aware of this phenomenon but have left it for further work to analyze in more detail.\n\n[Beginning of 3.1: you may state more clearly that optimizing $c_\\phi$ the way you do it will also \"minimize\" the variance, and explain better why (\"we require the gradient of the variance of our gradient estimator\"...). It took me a while to get it.]\n\nWe believe it is clear that by minimizing a Monte Carlo estimate of the variance, we will be minimizing the variance of the estimator. The first sentence of the second paragraph in section 3.1 has been slightly changed to avoid confusion. The title of the section has also been changed to be more clear.\n\n\n\n[In 3.1.1 a weighting based on $\\d/\\d\\theta log p(b)$ => shouldn't you write $... log p(b|\\theta)$ as before? ]\n\nThe paper has been updated to include the dependence on theta in the distribution. \n\n\n[Figure 2 is mentioned in p.3, it should appear much sooner than p6.]\n\nDue to length restrictions, it was difficult to place this figure elsewhere in the paper. \n\n\n\n[In Figure 2, there is nothing about the REINFORCE PART. Why?]\n\nWe wanted to focus on comparing the learned control variates of REBAR and RELAX.\n\n\n[In 3.4 you alternate sums over an infinite horizon and sums over T time steps. You should stick to the T horizon case, as you mention the case T=1 later.]\n\nGood point, we changed our notation to use a finite horizon of T.\n\n\n[p6 Related work]\n\nCan you clarify what you mean here?\n\n\n[The link to the work of Salimans 2017 is far from obvious, I would be glad to know more…]\n\nWhen applied to RL, our algorithm is an extension of policy-gradient optimization. We felt it reasonable to refer to alternative approaches such as that of Salimans et al. to inform the reader of concurrent, but orthogonal work.\n\n\n\n[Q-prop (Haarnoja et al.,2017): this is not the adequate reference to Q-prop, it should be (Gu et al. 2016), you have it correct later ;)]\n\nThanks, we corrected this.\n\n[Figure 3: why do you stop after so few epochs? I wondered how expensive is the computation of your estimator, but since in the RL case you go up to 50 millions (or 4 millions?), it's probably not the issue. I would be glad to see another horizontal lowest validation error for your RELAX estimator (so you need to run more epochs).\n\"ELBO\" should be explained here (it is only explained in the appendices).]\n\nIn our VAE experiments, we were interested in comparing with the baseline of REBAR. For that reason, we followed their experimental setup which, runs for 2 million iterations.  We added a note that ELBO is shorthand for evidence lower-bound. \n\n\n[6.2, Table 1: Best obtained training objective: what does this mean? Should it be small or large? You need to explain better. How much is the modest improvement (rather give relative improvement in the text?)? To me, you should not defer Table 3 to an appendix (nor Table 4).]\n\nGood points.  We change “best objective”to “highest obtained ELBO” to indicate that higher ELBOs are better. We couldn’t move the tables up because of space constraints.\n\n[Figure 4: Any idea why A2C oscillates so much on inverted pendulum? Any idea why variance starts to decrease after 500 episodes using RELAX? Isn't related to the combination of regression and optimization, as suggested above?]\n\nRegarding the inverted pendulum, we believe this is due to the larger variance of the baseline gradient estimator. Regarding the variance on the double pendulum, we do not fully understand this phenomenon, thus we do not make any claims about it. \n\n(continued in next comment)\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514995813956,"tcdate":1514750118368,"number":5,"cdate":1514750118368,"id":"H1Cjea8mG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"S1JPkCzlG","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"Global Comments","comment":"Dear reviewer 1,\n\nThank you for your detailed comments and positive reception of the work. I will address your comments in the order in which you wrote them. For clarity, I have placed your comments in [brackets] and my responses will follow in plain text.\n\nGlobal Comments:\n\n[When reading (6), it is clear that the framework performs regression of $c_\\phi$ towards the unknown $f$ simultaneously with optimization over $c_\\phi$.\nTaking this perspective, I would be glad to see how the regression part performs with respect to standard least square regression,\ni.e. just using $||f(b)-c_\\phi(b)||^2$ as loss function. You may compare the speed of convergence of $c_\\phi$ towards $f$ using (6) and the least squared error.\nYou may also investigate the role of this regression part into the global g_LAX optimization by studying the evolution of the components of (6).]\n\nWe did not experiment with the L2 loss directly. I do believe this should be explored in further work as the L2 loss has less computational overhead than the Monte Carlo variance estimate that was used in this work. This was tested in the concurrently submitted “Sample-efficient Policy Optimization with Stein Control Variate“ and in that work, the Monte Carlo variance estimate was found to produce better results in some settings. We ourselves are curious about the relative performance of minimizing the L2 loss vs the variance, but will probably leave that for further work.\n\n[Related to the above comment, in Algo. 1, you mention \"f(.)\" as given to the algo. Actually, the algo does not know f itself, otherwise it would not be blackbox optimization. So you may mean different things. In a batch setting, you may give a batch of [x,f(x) (,cost(x)?)] points to the algo. You more probably mean here that you have an \"oracle\" that, given some x, tells you f(x) on demand. But the way you are sampling x is not specified clearly.] \n\nYou are correct in your understanding the algorithm. For clarity of notation, we have decided to keep the algorithm box as it is.\n\n[This becomes more striking when you move to reinforcement learning problems, which is my main interest. The RL algorithm itself is not much specified. Does it use a replay buffer (probably not)? Is it on-policy or off-policy (probably on-policy)? What about the exploration policy? I want to know more... Probably you just replace (10) with (11) in A2C, but this is not clearly specified.]\n\nWe do not use a reply-buffer, our method is on-policy. We simply replace (10) with (11) as you mentioned. Since we mention the similarities between our algorithm and A2C, we have decided to not elaborate further. Exploration is encouraged due to entropy regularization which is commonly used in policy-gradient methods. A note has been added to the experimental details section in the Appendix to explain this. \n\n[In Section 4, can you explain why, in the RL case, you must introduce stochasticity to the inputs? Is this related to the exploration issue (see above)?]\n\nWe were trying to explain that none of these methods can be used without modification to optimize parameters of a deterministic discrete function, since there would be no exploration.  In the RL case, stochasticity is introduced by using a stochastic policy and exploration is encouraged due to entropy regularization. For brevity and clarity, this paragraph has been removed.\n\n[Last sentence of conclusion: you are too allusive about the relationship between your learned control variate and the Q-function. I don't get it, and I want to know more…]\n\nHere we are simply mentioning the potential of training the control variate using off policy data as is done in the Q-prop paper. We believe further theoretical work must be done to better understand the relationship between the optimal control variate and the optimal Q-function, so we do not make any claims about this relationship.  \n\n(continued in next comment)\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1514995792944,"tcdate":1514748878525,"number":4,"cdate":1514748878525,"id":"r1U0in8XG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Comment","forum":"SyzKd1bCW","replyto":"S1a0antgG","signatures":["ICLR.cc/2018/Conference/Paper492/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper492/Authors"],"content":{"title":"Response to reviewer 3","comment":"Dear Reviewer 3,\n\nThank you for your overall positive review of the work.  To respond to your specific criticisms:\n\n1) “Given Tucker et al, its contribution is somehow incremental”.  We did build closely on Tucker et. al., but we generalized their method, and expanded its scope substantially.  REBAR was only applicable to known computation graphs with discrete random variables, making it inapplicable to reinforcement learning or continuous random variables.\n\n2) “ they only showed three quite simple experiments (I guess they need to focus one setting; RL or VAE)”.  Our first experiment was deliberately simple, so we could visualize the learned control variate.  Regarding our VAE experiments, achieving state of the art on discrete variational autoencoders may constitute a “simple” experiment, but it’s not clear that this is a problem.  We included the RL experiments to demonstrate the breadth of problems to which our method can be applied, since this is one of the main advantages of RELAX over REBAR.\n\n3) “it would be good to actually show if the variation of g_hat is much smaller than other standard methods”.  Figures 1 and 4 both have plots labeled ‘log variance’, showing exactly this.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1513081044861,"tcdate":1513077604852,"number":2,"cdate":1513077604852,"id":"rkaDiETZf","invitation":"ICLR.cc/2018/Conference/-/Paper492/Public_Comment","forum":"SyzKd1bCW","replyto":"SyzKd1bCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Missing related work?","comment":"Other recent work on generalizing reparameterization gradients seems to be missing:\nRuiz et al., The Generalized Reparameterization Gradient, NIPS, 2016.\nNaesseth et al., Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms, AISTATS, 2017.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1511810648286,"tcdate":1511810648286,"number":1,"cdate":1511810648286,"id":"BylvIk5lG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Public_Comment","forum":"SyzKd1bCW","replyto":"SyzKd1bCW","signatures":["~Leon_Bottou1"],"readers":["everyone"],"writers":["~Leon_Bottou1"],"content":{"title":"Relation with doubly robust estimation","comment":"First of all, this is a very nicely written paper on an important topic.  \n\nI only write this comment because I would like to hear the comments of the author(s) about the following remarks. \n\n    1)  To me equation (7) expresses the doubly robust estimation of the gradient in question. This is a bit different from the usual robust estimation because the importance sampling weights, dlog(P(b,theta))/dtheta are signed, but this remains fundamentally similar.\n\n    2)  One very interesting aspect of the paper might therefore be a clarification of the power of the reparametrization trick.  It now looks like doubly robust estimation of the gradient using the knowledge that the predictor function is exact (that makes it simply robust in fact ;-)  When it is not exact, one needs to correct with a pair of weighted estimates.\n\n    3) How much of the power of the reparametrization trick comes from the fact that the reparametrization is assumed exact, as opposed to needing to be corrected by a difference of weighted estimates? \n\nMany thanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642456287,"tcdate":1511800276966,"number":3,"cdate":1511800276966,"id":"S1a0antgG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Review","forum":"SyzKd1bCW","replyto":"SyzKd1bCW","signatures":["ICLR.cc/2018/Conference/Paper492/AnonReviewer3"],"readers":["everyone"],"content":{"title":"I think the paper provides interesting idea for optimizing black-box functions of random variables. ","rating":"6: Marginally above acceptance threshold","review":"The paper considers the problem of choosing the parameters of distribution to maximize an expectation over that distribution. This setting has attracted a huge interest in ML communities (that is related to learning policy in RL as well as variational inference with hidden variables). The paper provides a framework for such optimization, by interestingly combining three standard ways. \n\nGiven Tucker et al, its contribution is somehow incremental, but I think it is an interesting idea to use neural networks for control variate to handle the case where f is unknown.  \n\nThe main issue of this paper seems to be in limited experimental results; they only showed three quite simple experiments (I guess they need to focus one setting; RL or VAE). Moreover, it would be good to actually show if the variation of g_hat is much smaller than other standard methods.\n   \nThere is missing reference at page 8.\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642456330,"tcdate":1511346007009,"number":2,"cdate":1511346007009,"id":"S1JPkCzlG","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Review","forum":"SyzKd1bCW","replyto":"SyzKd1bCW","signatures":["ICLR.cc/2018/Conference/Paper492/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A good conceptual contribution with a clear background, a not so good empirical study, but I like it much","rating":"7: Good paper, accept","review":"This paper suggests a new approach to performing gradient descent for blackbox optimization or training discrete latent variable models. The paper gives a very clear account of existing gradient estimators and finds a way to combine them so as to construct and optimize a differentiable surrogate function. The resulting new gradient estimator is then studied both theoretically and empirically. The empirical study shows the benefits of the new estimator for training discrete variational autoencoders and for performing deep reinforcement learning.\n\nTo me, the main strengths of the paper is the very clear account of existing gradient estimators (among other things it helped me understand obscurities of the Q-prop paper) and a nice conceptual idea. The empirical study itself is more limited and the paper suffers from a few mistakes and missing information, but to me the good points are enough to warrant publication of the paper in a good conference like ICLR.\n\nBelow are my comments for the authors.\n\n---------------------------------\nGeneral, conceptual comments:\n\nWhen reading (6), it is clear that the framework performs regression of $c_\\phi$ towards the unknown $f$ simultaneously with optimization over $c_\\phi$.\nTaking this perspective, I would be glad to see how the regression part performs with respect to standard least square regression,\ni.e. just using $||f(b)-c_\\phi(b)||^2$ as loss function. You may compare the speed of convergence of $c_\\phi$ towards $f$ using (6) and the least squared error.\nYou may also investigate the role of this regression part into the global g_LAX optimization by studying the evolution of the components of (6).\n\nRelated to the above comment, in Algo. 1, you mention \"f(.)\" as given to the algo. Actually, the algo does not know f itself, otherwise it would not be blackbox optimization. So you may mean different things. In a batch setting, you may give a batch of [x,f(x) (,cost(x)?)] points to the algo. You more probably mean here that you have an \"oracle\" that, given some x, tells you f(x) on demand. But the way you are sampling x is not specified clearly.\n\nThis becomes more striking when you move to reinforcement learning problems, which is my main interest. The RL algorithm itself is not much specified. Does it use a replay buffer (probably not)? Is it on-policy or off-policy (probably on-policy)? What about the exploration policy? I want to know more... Probably you just replace (10) with (11) in A2C, but this is not clearly specified.\n\nIn Section 4, can you explain why, in the RL case, you must introduce stochasticity to the inputs? Is this related to the exploration issue (see above)?\n\nLast sentence of conclusion: you are too allusive about the relationship between your learned control variate and the Q-function. I don't get it, and I want to know more...\n\n-----------------------------------\nLocal comments:\n\nBackpropagation through the void: I don't understand why this title. I'm not a native english speaker, I'm probably missing a reference to something, I would be glad to get it.\n\nFigure 1 right. Caption states variance, but it is log variance. Why does it oscillate so much with RELAX?\n\nBeginning of 3.1: you may state more clearly that optimizing $c_\\phi$ the way you do it will also \"minimize\" the variance, and explain better why (\"we require the gradient of the variance of our gradient estimator\"...). It took me a while to get it.\n\nIn 3.1.1 a weighting based on $\\d/\\d\\theta log p(b)$ => shouldn't you write $... log p(b|\\theta)$ as before?\n\nFigure 2 is mentioned in p.3, it should appear much sooner than p6.\n\nIn Figure 2, there is nothing about the REINFORCE PART. Why?\n\nIn 3.4 you alternate sums over an infinite horizon and sums over T time steps. You should stick to the T horizon case, as you mention the case T=1 later.\n\np6 Related work\n\nThe link to the work of Salimans 2017 is far from obvious, I would be glad to know more...\n\nQ-prop (Haarnoja et al.,2017): this is not the adequate reference to Q-prop, it should be (Gu et al. 2016), you have it correct later ;)\n\nFigure 3: why do you stop after so few epochs? I wondered how expensive is the computation of your estimator, but since in the RL case you go up to 50 millions (or 4 millions?), it's probably not the issue. I would be glad to see another horizontal lowest validation error for your RELAX estimator (so you need to run more epochs).\n\"ELBO\" should be explained here (it is only explained in the appendices).\n\n6.2, Table 1: Best obtained training objective: what does this mean? Should it be small or large? You need to explain better. How much is the modest improvement (rather give relative improvement in the text?)? To me, you should not defer Table 3 to an appendix (nor Table 4).\n\nFigure 4: Any idea why A2C oscillates so much on inverted pendulum? Any idea why variance starts to decrease after 500 episodes using RELAX? Isn't related to the combination of regression and optimization, as suggested above?\n\nAbout Double Inverted Pendulum, Appendix E3 mentions 50 million frames, but the figure shows 4 millions steps. Where is the truth?\n\nWhy do you give steps for the reward, and episodes for log-variance? The caption mentions \"variance (log-scale)\", but saying \"log-variance\" would be more adequate.\n\np9: the optimal control variate: what is this exactly? How do you compare a control variate over another? This may be explained in Section 2.\n\nGAE (Kimura, 2000). I'm glad you refer to former work (there is a very annoying tendency those days to refer only to very recent papers from a small set of people who do not correctly refer themselves to previous work), but you may nevertheless refer to John Schulman's paper about GAEs anyways... ;)\n\nAppendix E.1 could be reorganized, with a common hat and then E.1.1 for one layer model(s?) and E.1.2 for the two layer model(s?)\n\nA sensitivity analysis wrt to your hyper-parameters would be welcome, this is true for all empirical studies.\n\nIn E2, is the output layer linear? You just say it is not ReLU...\n\nThe networks used in E2 are very small (a standard would be 300 and 400 neurons in hidden layers). Do you have a constraint on this?\n\n\"As our control variate does not have the same interpretation as the value function of A2C, it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model. We leave the task of incorporating these and other variance reduction techniques to future work.\"\nFirst, this is important, so if this is true I would move this to the main text (not in appendix).\nBut also, it seems to me that the first sentence of E3 contradicts this, so where is the truth?\n\n{0.01,0.003,0.001} I don't believe you just tried these values. Most probably, you played with other values before deciding to perform grid search on these, right?\nThe same for 25 in E3.\n\nGlobally, you experimental part is rather weak, we would expect a stronger methodology, more experiments also with more difficult benchmarks (half-cheetah and the whole gym zoo ;)), more detailed analyses of the results, but to me the value of your paper is more didactical and conceptual than experimental, which I really appreciate, so I will support your paper despite these weaknesses.\n\nGood luck! :)\n\n---------------------------------------\nTypos:\n\np5\nmonte-carlo => Monte(-)Carlo (no - later...)\ntaylor => Taylor\nyou should always capitalize Section, equation, table, figure, appendix, ...\n\ngradient decent => descent (twice)\n\np11: probabalistic\n\np15 ELU(Djork-... => missing space\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642456371,"tcdate":1510423137046,"number":1,"cdate":1510423137046,"id":"HyFP5nE1M","invitation":"ICLR.cc/2018/Conference/-/Paper492/Official_Review","forum":"SyzKd1bCW","replyto":"SyzKd1bCW","signatures":["ICLR.cc/2018/Conference/Paper492/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good paper, convincing experiments, writing is clear but there are quite a few typos","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper introduces LAX/RELAX, a method to reduce the variance of the REINFORCE gradient estimator. The method builds on and is directly inspired by REBAR. Similarly to REBAR, RELAX is an unbiased estimator, and the idea is to introduce a control variate that leverages the reparameterization gradient. In contrast to REBAR, RELAX learns a free-from control variate, which allows for low-variance gradient estimates for both discrete and continuous random variables. The method is evaluated on a toy experiment, as well as the discrete VAE and reinforcement learning. It effectively reduces the variance of state-of-the-art methods (namely, REBAR and actor-critic).\n\nOverall, I enjoyed reading the paper. I think it is a neat idea that can be of interest for researchers in the field. The paper is clearly explained, and I found the experiments convincing. I have minor comments only.\n\n+ Is there a good way to initialize c_phi prior to optimization? Given that c_phi must be a proxy for f(), maybe you can take advantage of this observation to find a good initialization for phi?\n\n+ I was confused with the Bernoulli example in Appendix B. Consider the case theta=0.5. Then, b=H(z) takes value 1 if z>0, and 0 otherwise. Thus, p(z|b,theta) should assign mass zero to values z>0 when b=0, which does not seem to be the case with the proposed sampling scheme in page 11, since v*theta=0.5*v, which gives values in [0,0.5]. And similarly for the case b=1.\n\n+ Why is the method called LAX? What does it stand for?\n\n+ In Section 3.3, it is unclear to me why rho!=phi. Given that c_phi(z)=f(sigma_lambda(z))+r_rho(z), with lambda being a temperature parameter, why isn't rho renamed as phi? (the first term doesn't seem to have any parameters). In general, this section was a little bit unclear if you are not familiar with the REBAR method; consider adding more details.\n\n+ Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach.\n\n+ In the abstract, consider adding two of the main ideas that the estimator relies on: control variates and reparameterization gradients. This would probably be more clear than \"based on gradients of a learned function.\"\n\n+ In the first paragraph of Section 3, the sentence \"f is not differentiable or not computable\" may be misleading, because it is unclear what \"not computable\" means (one may think that it cannot be evaluated). Consider replacing with \"not analytically computable.\"\n\n+ In Section 3.3, it reads \"differentiable function of discrete random variables,\" which does not make sense.\n\n+ Before Eq. 11, it reads \"where epsilon_t does not depend on theta\". I think it should be the distribution over epsilon_t what doesn't depend on theta.\n\n+ In Section 6.1, it was unclear to me why t=.499 is a more challenging setting.\n\n+ The header of Section 6.3.1 should be removed, as Section 6.3 is short.\n\n+ In Section 6.3.1, there is a broken reference to a figure.\n\n+ Please avoid contractions (doesn't, we'll, it's, etc.)\n\n+ There were some other typos; please read carefully the paper and double-check the writing. In particular, I found some missing commas, some proper nouns that are not capitalized in Section 5, and others (e.g., \"an learned,\" \"gradient decent\").","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515010843084,"tcdate":1509124217799,"number":492,"cdate":1509739270727,"id":"SyzKd1bCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyzKd1bCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Backpropagation through the Void: Optimizing control variates for black-box gradient estimation","abstract":"Gradient-based optimization is the foundation of deep learning and reinforcement learning.\nEven when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function.\nThese estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.","pdf":"/pdf/3be2d39c16701db7e2ee473c752f5f50c9bce8ae.pdf","TL;DR":"We present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning. ","paperhash":"anonymous|backpropagation_through_the_void_optimizing_control_variates_for_blackbox_gradient_estimation","_bibtex":"@article{\n  anonymous2018backpropagation,\n  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyzKd1bCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper492/Authors"],"keywords":["optimization","machine learning","variational inference","reinforcement learning","gradient estimation","deep learning","discrete optimization"]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}