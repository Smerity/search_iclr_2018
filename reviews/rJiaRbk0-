{"notes":[{"tddate":null,"ddate":null,"tmdate":1515040888253,"tcdate":1515040796829,"number":4,"cdate":1515040796829,"id":"rJBQl4j7z","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Comment","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper118/Authors"],"content":{"title":"Revision to the paper (to all reviewers) : updates on experimental results","comment":"Thanks all reviewers for their valuable comments, we updated a new version of the paper by including the following results:\n\n1. We make discussion about the sharpening sigmoid method proposed by the reviewers, and add the algorithm as one of the baselines in the experiments. The experimental results still show that our proposed method achieves the best performance in all tasks.\n\n2. We update the experimental results on language modelling task which achieves the best performance (52.1) as far as we know without using any hyperparameter search method.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1514377291890,"tcdate":1514377291890,"number":3,"cdate":1514377291890,"id":"rJ4UxzbQz","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Comment","forum":"rJiaRbk0-","replyto":"S15OPlugz","signatures":["ICLR.cc/2018/Conference/Paper118/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper118/Authors"],"content":{"title":"We respectfully disagree with the comment about the novelty/experimental results of the paper. ","comment":"\n[Regarding the experiment]\n\nWe are afraid that the reviewer makes a wrong judgement to the performance results, our model is much better than the baseline on two tasks. \n\nFor machine translation, we achieved the SOTA performance on German->English task and the improvement is significate (+ about 1 point) in the field of translation, not to mention that our model is much better than some other submissions https://openreview.net/forum?id=HktJec1RZ. \n\nFor language model, by leveraging several tricks in literature, we significantly improve the performance from 77.4 to 52.1 (the best number as far as we know). This number is achieved without using any hyperparameter search method, we reported the detail in the paper. \n\n[Regarding the motivation]\n\nWe have discussed in section 2.1 that there are a bunch of work empirically and theoretically studying the relationship between flat loss surface and generalization, not to mention that there are some continuous study and verification in ICLR 2018 submissions, e.g., https://openreview.net/forum?id=HkmaTz-0W . Thus our method is well motivated: by pushing the softmax operator towards its flat region will lead to better generalization. \n\n[Regarding the novelty of the paper]\n\nWe are regretful to see the reviewer claims that there is little novelty in the paper. First, we are the first to apply Gumbel-softmax trick for robust training of LSTM by pushing the value of the gate to the boundary.  We empirically show that our method achieves better accuracy even achieves the SOTA performance in some tasks. Second, we show that by different low-precision/low-rank compressions, our model is even still comparable to the baseline models before compressions. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1515039729782,"tcdate":1514376995142,"number":2,"cdate":1514376995142,"id":"BJj7JzW7G","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Comment","forum":"rJiaRbk0-","replyto":"Syo-smqgf","signatures":["ICLR.cc/2018/Conference/Paper118/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper118/Authors"],"content":{"title":"Thanks for the relevant comments. We have improved PTB results according to the suggestions.","comment":"\n[Regarding the computation of function G]\n\nDuring training, the output of the gate is computed directly by function G, while the function G contains some random noise U.\n\n[Regarding the sharpened sigmoid function experiment]\n\nThanks for figure this out. First, we want to point out that theoretically it doesn’t help: Simply consider function f_{W,b}(x) =sigmoid((Wx+b)/tau), where tau is the temperature, it is computationally equivalent to f_{W’,b’}(x) =sigmoid(W’x+b’) by setting W’=W/tau and b’ = b/tau. Then using a small temperature is equivalent to rescale the initial parameter as well as gradient to a larger range. Usually, setting an initial point in a larger range with a larger learning rate will harm the optimization process.\n\nWe also did a set of experiments and updated the paper to show it doesn’t help in practice.\n\n[Regarding the significance of experimental results]\n\nFor machine translation, we achieved the SOTA performance on German->English task and the improvement is significate (+ about 1 point) in the field of translation, not to mention that our model is much better than some other submissions https://openreview.net/forum?id=HktJec1RZ. For English->German task, we noticed that “Attention is all you need” is the state of the art but it is not LSTM-based; thus we didn’t list that result in the paper.\n\nFor language model, thanks for the reference, we have studied the papers. By leveraging several tricks in literature, we significantly improve the performance from 77.4 to 52.1 (the best number as far as we know) without using any hyperparameter search method, we reported the detail in the paper. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1515039702155,"tcdate":1514376846821,"number":1,"cdate":1514376846821,"id":"BJvcAWbQG","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Comment","forum":"rJiaRbk0-","replyto":"HyA3jBqgG","signatures":["ICLR.cc/2018/Conference/Paper118/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper118/Authors"],"content":{"title":"Thanks for the relevant comments. Here are the responses to the questions.","comment":"[Regarding the small temperature experiment]\n\nThanks for figure this out. First, we want to point out that theoretically it doesn’t help: Simply consider function f_{W,b}(x) =sigmoid((Wx+b)/tau), where tau is the temperature, it is computationally equivalent to f_{W’,b’}(x) =sigmoid(W’x+b’) by setting W’=W/tau and b’ = b/tau. Then using a small temperature is equivalent to rescale the initial parameter as well as gradient to a larger range. Usually, setting an initial point in a larger range with a larger learning rate will harm the optimization process.\n\nWe also did a set of experiments and updated the paper to show it doesn’t help in practice.\n\n[Regarding the binary net]\n\nDespite the different between the model structure (gate-based LSTM v.s. CNN), the main difference is that we regularize the output of the activation of the gates to binary value only, but not to regularize the weights. One should notice that the accuracy of Binary Net is usually much worse than the baseline model. However, we show that (1) Our models generalize well among different tasks. (2) The accuracy of the models after low-rank/low-precision compression using our method is competitive to (or even better than) the baseline. Besides, our techniques can also be applied to binarynet training.\n\n[Regarding apply rounding / low-rank for all the parameters]\n\nWe will do the experiment but as our proposed method is focusing on LSTM unit. We are not sure whether the performance will drop a lot when we apply rounding/low-rank to embedding and attention."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1516156376382,"tcdate":1511836598499,"number":3,"cdate":1511836598499,"id":"HyA3jBqgG","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Review","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper propose a new \"gate\" function for LSTM to enable the values of the gates towards 0 or 1. The motivation behind is a  flat region of the loss surface is likely to generalize well. It shows the experimental results are comparable or better than vanilla LSTM and much more robust to low-precision approximation and low-rank approximation.\n\nIn section 3.2, the paper claimed using a smaller temperature cannot guarantee the outputs to be close to the boundary. Is there any experimental evidence to show it's not working? It also claimed pushing output gate to 0/1 will drop the performance. It actually quite interesting because there are bunch of paper claimed output gate is not important for language modeling, e.g. https://openreview.net/pdf?id=HJOQ7MgAW . \n\nIn the sensitive analysis, what if apply rounding / low-rank for all the parameters? \n\nHow was this approach compare to binarynet https://arxiv.org/abs/1602.02830 ? Applying the same idea, but only for forget gate/ input gate. Also, can we apply this idea to the binarynet? \n\nOverall, I think it's an interesting paper but I feel it should compare with some simple baseline to binarized the gate function.  \n\nUpdates: Thanks a lot for all the clarification. It do improve the paper quality but I'm still thinking it's higher than \"6\" but lower than \"7\". To me, improve ppl from \"52.8\" to \"52.1\" isn't very significant. For WMT, it improve on DE->EN but not for EN->DE (although it improve both for the author's own baseline). So I'm not fully convinced this approach could improve the generalization. But I feel this work can have many other applications such as \"binarynet\". ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1515642395174,"tcdate":1511828226587,"number":2,"cdate":1511828226587,"id":"Syo-smqgf","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Review","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, but not impressive","rating":"6: Marginally above acceptance threshold","review":"The paper argues for pushing the input and forget gate’s output toward 0 or 1, i.e., the LSTM tends to reside in flat region of surface loss, which is likely to generalize well. To achieve that, the sigmoid function in the original LSTM is replaced by a function G that is continuous and differentiable with respect to the parameters (by applying the Gumbel-Softmax trick). As a result, the model is still differentiable while the output gate is approximately binarized.  \n\nPros:\n-\tThe paper is clearly written\n-\tThe method is new and somehow theoretically guaranteed by the proof of the Proposition 1\n-\tThe experiments are clearly explained with detailed configurations\n-\tThe performance of the method in the model compression task is promising \n\nCons:\n-\tThe “simple deduction” which states that pushing the gate values toward 0 or 1 correspond to the region of the overall loss surface may need more theoretical analysis\n-\tIt is confusing whether the output of the gate is sampled based on or computed directly by the function G  \n-\tThe experiments lack many recent baselines on the same dataset (Penn Treebank: Melis et al. (2017) – On the State of the Art of Evaluation in Neural Language Models; WMT: Ashish et.al. (2017) – Attention Is All You Need) \n-\tThe experiment’s result is only slightly better than the baseline’s\n-\tTo be more persuasive, the author should include in the baselines other method that can “binerize” the gate values such as the one sharpening the sigmoid function. \n\n\nIn short, this work is worth a read. Although the experimental results are not quite persuasive, the method is nice and promising. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1515642395214,"tcdate":1511683954491,"number":1,"cdate":1511683954491,"id":"S15OPlugz","invitation":"ICLR.cc/2018/Conference/-/Paper118/Official_Review","forum":"rJiaRbk0-","replyto":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference/Paper118/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The technical novelty is limited and experiments do not show much benefits of the proposed model.","rating":"4: Ok but not good enough - rejection","review":"This paper aims to push the LSTM gates to be binary. To achieve this, the paper proposes to employ the recent Gumbel-Softmax trick to obtain end-to-end trainable categorical distribution (taking 0 or 1 value). The resulted G2-LSTM is applied for language model and machine translation in the experiments. \n\nThe novelty of this paper is limited. Just directly apply the Gumbel-Softmax trick. \n\nThe motivation is not explained clearly and convincingly. Why need to pursue binary gates? According to the paper, it may give better generalization performance. But there is no theoretical or experimental evidence provided by this paper to support this argument. \n\nThe results of the new G2-LSTM are not significantly better than baselines in the experiments.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]}},{"tddate":null,"ddate":null,"tmdate":1515040400055,"tcdate":1509002946844,"number":118,"cdate":1509739471595,"id":"rJiaRbk0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJiaRbk0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards Binary-Valued Gates for Robust LSTM Training ","abstract":"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. Its goal is to use gates to control the information flow (e.g., whether to skip some information/transformation or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal and is easy to overfit. In this paper, we propose a new way for LSTM training, which pushes the values of the gates towards 0 or 1. By doing so, we can (1) better control the information flow: the gates are mostly open or closed, instead of in a middle state; and (2) avoid overfitting to certain extent: the gates operate at their flat regions, which is shown to correspond to better generalization ability. However, learning towards discrete values of the gates is generally difficult. To tackle this challenge, we leverage the recently developed Gumbel-Softmax trick from the field of variational methods, and make the model trainable with standard backpropagation. Experimental results on language modeling and machine translation show that (1) the values of the gates generated by our method are more reasonable and intuitively interpretable, and (2) our proposed method generalizes better and achieves better accuracy on test sets in all tasks. Moreover, the learnt models are not sensitive to low-precision approximation and low-rank approximation of the gate parameters due to the flat loss surface.","pdf":"/pdf/b28ed5b2b788c9183a845a9079ffb55546d9addb.pdf","TL;DR":"We propose a new algorithm for LSTM training by learning towards binary-valued gates which we shown has many nice properties.","paperhash":"anonymous|towards_binaryvalued_gates_for_robust_lstm_training","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Binary-Valued Gates for Robust LSTM Training },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJiaRbk0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper118/Authors"],"keywords":["recurrent neural network","LSTM","long-short term memory network","machine translation","generalization"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}