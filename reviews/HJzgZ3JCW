{"notes":[{"tddate":null,"ddate":null,"tmdate":1515777446554,"tcdate":1515777446554,"number":6,"cdate":1515777446554,"id":"Hk0i6DUVz","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Comment","forum":"HJzgZ3JCW","replyto":"BkOMrBeGM","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"],"content":{"title":"Good work","comment":"Thanks for the clarifications! I think the score is still appropriate."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1515536625435,"tcdate":1515536625435,"number":5,"cdate":1515536625435,"id":"rktxb6fNM","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Comment","forum":"HJzgZ3JCW","replyto":"ryvYuSgzf","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"],"content":{"title":"Thanks for the response. ","comment":"Thanks for the additional comments. I keep the rating. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1513976032181,"tcdate":1513976032181,"number":4,"cdate":1513976032181,"id":"BJdkWgiGz","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Comment","forum":"HJzgZ3JCW","replyto":"B171DHlfG","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"],"content":{"title":"Thanks for the response","comment":"Thanks for the response. I hold a positive opinion on this paper. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1513277567422,"tcdate":1513277567422,"number":3,"cdate":1513277567422,"id":"ryvYuSgzf","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Comment","forum":"HJzgZ3JCW","replyto":"SyMeSO8ef","signatures":["ICLR.cc/2018/Conference/Paper168/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper168/Authors"],"content":{"title":"Response","comment":"Thanks for your comments. \nWe agree that placing activation functions in other domains (e.g. Fourier) could hold more promise than we've uncovered so far.\nAs far as accuracy before re-training, since we used iterative pruning and re-training, we provide top-5 accuracy drop for each pruning step of Winograd-ReLU CNN on ImageNet:\n\noriginal density | pruned density | original accuracy | pruned accuracy (without re-training)\n100%                  |                           | 87.43%                 | \n70%                    | 60%                   | 87.456%               | 87.338% \n60%                    | 50%                   | 87.424%               | 87.202% \n50%                    | 40%                   | 87.406%               | 86.672%\n40%                    | 35%                   | 87.406%               | 86.784%\n35%                    | 30%                   | 87.358%               | 86.286%\n30%                    | 25%                   | 87.228%               | 85.692%\n25%                    | 20%                   | 86.898%               | 84.466%\n20%                    | 15%                   | 86.570%               | 80.430%\n15%                    | 12%                   | 86.246%               | 79.246%\n12%                    | 10%                   | 85.916%               | 77.128%\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1513277147419,"tcdate":1513277147419,"number":2,"cdate":1513277147419,"id":"B171DHlfG","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Comment","forum":"HJzgZ3JCW","replyto":"rJMLjDqeM","signatures":["ICLR.cc/2018/Conference/Paper168/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper168/Authors"],"content":{"title":"Response","comment":"We appreciate your comments and questions; thank you. Let us address each in turn:\n1) We agree, more work is warranted for deeper networks; we plan to explore this in the future.\n2) It is true that the Winograd-ReLU CNN network architecture is not equivalent to an ordinary Winograd CNN. However, training a Winograd-ReLU network from scratch is a fairly simple solution. In fact there's no transformation from ordinary CNN weights to Winograd-ReLU CNN weights: the ReLU layer sizes are different. This cannot be compensated by any weight transformation.\n3) While a reduction in wall clock time is the eventual goal, we focus here on a novel network type that reduces the theoretical number of operations needed, rather than the systems work needed to accelerate it.  This will need careful design with attention to architecture optimizations and tradeoffs, and we leave this as future work.\n4) We'll try to find a clear way to present both density and workload in these figures, thanks for the suggestion."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1513276688869,"tcdate":1513276688869,"number":1,"cdate":1513276688869,"id":"BkOMrBeGM","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Comment","forum":"HJzgZ3JCW","replyto":"HJ8UsZ6gM","signatures":["ICLR.cc/2018/Conference/Paper168/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper168/Authors"],"content":{"title":"Response","comment":"Thank you for your questions and comments; please allow us to address them here.\n1) You are right to be confused - this should have been \"p-specific,\" meaning the values of B, G, and A depend on p.  We'll correct this in a future version.\n2) In general, our approach can be used wherever general Winograd convolutions can be used.  B, G, and A will be different for different patch sizes and filter sizes, and of course, we leave finding these and experimenting with larger sizes as future work.\n3) Quantization approaches could fit well in the introduction; we'll try to find a way to make it clear that it may be orthogonal to pruning and Winograd convolutions.\n4) Thanks for catching this typo.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1515642401806,"tcdate":1512016718176,"number":3,"cdate":1512016718176,"id":"HJ8UsZ6gM","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Review","forum":"HJzgZ3JCW","replyto":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well-written paper introducing a novel method of reducing multiplications in CNNs with very minor loss in accuracy","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary: \nThe paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. \nThis modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way:\n- weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity\n- the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations\n\nThis way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers.\n\nReview:\nThe paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. \nI only have a couple of questions/comments:\n1) I’m not familiar with the term m-specific (“Matrices B, G and A are m-specific.”) and didn’t find anything that seemed related in a very quick google search. Maybe it would make sense to add at least an informal description.\n2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs?\n3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015)\n4) Figure 5 caption has a typo: “acrruacy”\n\nReferences:\nCourbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \"Binaryconnect: Training deep neural networks with binary weights during propagations.\" In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015.\nLin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. \"Neural networks with few multiplications.\" arXiv preprint arXiv:1510.03009 (2015).\nRastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1515642401849,"tcdate":1511844682292,"number":2,"cdate":1511844682292,"id":"rJMLjDqeM","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Review","forum":"HJzgZ3JCW","replyto":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A promising Method, though with some limitations","rating":"7: Good paper, accept","review":"This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. The resultant CNN can achieve ~10x theoretical speedup with little performance loss.\n\nThe paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting.  \n\nThe results on Cifar-10 and ImageNet are promising. In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup. \nThe results on ImageNet using ResNet-18 architecture are also promising. However, no results are provided for deeper networks, so it is unclear how this method can benefit the computation of very deep neural networks \n\nA general limitation of the proposed method is the network architecture inconsistency with the ordinary CNNs. Due to the location change of ReLUs, it is unclear how to transform a pretrained ordinary CNNs to the new architectures accurately. It seems training from scratch using the transformed architectures is the simplest solution. \n\nThe paper does not report the actual speedup in the wall clock time. The actual implementation is what matters in the end. \n\nIt will be more informative to present Figure 2,3,4 with respect to the workload in addition to the weight density. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1515642402945,"tcdate":1511585002087,"number":1,"cdate":1511585002087,"id":"SyMeSO8ef","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Review","forum":"HJzgZ3JCW","replyto":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good paper with thorough experiments","rating":"7: Good paper, accept","review":"This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand. The resulting Winograd-ReLU\nCNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool-CNN-C, and ImageNEt with ResNet-18). The proposed method seems to improve over the two baseline approaches (Winograd and sparsity, respectively).\n\nOverall, the paper is well-written and the experiments seems to be quite thorough and clear. Note that I am not an expert in this field and I might miss important references along this direction. I am leaving it to other reviewers to determine its novelty. \n\nPutting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration. Also, I am curious about the performance after weight pruning but before retraining).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1509739448354,"tcdate":1509044457680,"number":168,"cdate":1509739445697,"id":"HJzgZ3JCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}