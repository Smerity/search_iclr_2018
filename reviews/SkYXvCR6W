{"notes":[{"tddate":null,"ddate":null,"tmdate":1512345571855,"tcdate":1512345571855,"number":1,"cdate":1512345571855,"id":"HJFJeMf-G","invitation":"ICLR.cc/2018/Conference/-/Paper107/Official_Comment","forum":"SkYXvCR6W","replyto":"rkHO_abWG","signatures":["ICLR.cc/2018/Conference/Paper107/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper107/Authors"],"content":{"title":"Table 6","comment":"Hello\n\nThank you for you interest in this approach.\n\nTable 6 results are testing error, or 1- accuracy*100. \n\nTheses results came from Table 4 in Zhang, Zhao, LeCun paper: https://arxiv.org/pdf/1509.01626.pdf .  We just compare their results with ours. \n\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification","abstract":"This paper reports the empirical exploration of a novel approach to encode words using compressing inspired techniques for use on convolutional neural networks at character-level. This approach reduces drastically the number of parameters to be optimized using deep learning, resulting in competitive accuracies in only a fraction of the time spent by characters level convolutional neural networks,  enabling training in simpler hardware.","pdf":"/pdf/e0811d0f0c80db70241966ea65c36c29dc0128a6.pdf","TL;DR":"Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation","paperhash":"anonymous|compressing_encoding_of_words_for_use_in_characterlevel_convolutional_networks_for_text_classification","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkYXvCR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper107/Authors"],"keywords":["Character Level Convolutional Networks","Text Classification","Word Compressing"]}},{"tddate":null,"ddate":null,"tmdate":1512327277473,"tcdate":1512327277473,"number":1,"cdate":1512327277473,"id":"rkHO_abWG","invitation":"ICLR.cc/2018/Conference/-/Paper107/Public_Comment","forum":"SkYXvCR6W","replyto":"SkYXvCR6W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"question table 6","comment":"Hello,\n\nI am part of a team at McGill University participating in the ICLR 2018 reproducibility challenge.\nWe are currently trying to reproduce results from your paper.\n\nWe have a few questions and would be very thankful if you could answer them:\n1) what is the loss function used to compare models on table 6 ?\n2) can you provide more details about your implementation of the traditional models described in table 6?\n\nThank you in advance."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification","abstract":"This paper reports the empirical exploration of a novel approach to encode words using compressing inspired techniques for use on convolutional neural networks at character-level. This approach reduces drastically the number of parameters to be optimized using deep learning, resulting in competitive accuracies in only a fraction of the time spent by characters level convolutional neural networks,  enabling training in simpler hardware.","pdf":"/pdf/e0811d0f0c80db70241966ea65c36c29dc0128a6.pdf","TL;DR":"Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation","paperhash":"anonymous|compressing_encoding_of_words_for_use_in_characterlevel_convolutional_networks_for_text_classification","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkYXvCR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper107/Authors"],"keywords":["Character Level Convolutional Networks","Text Classification","Word Compressing"]}},{"tddate":null,"ddate":null,"tmdate":1512222548106,"tcdate":1511866764691,"number":3,"cdate":1511866764691,"id":"rkB5-TcgG","invitation":"ICLR.cc/2018/Conference/-/Paper107/Official_Review","forum":"SkYXvCR6W","replyto":"SkYXvCR6W","signatures":["ICLR.cc/2018/Conference/Paper107/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"2: Strong rejection","review":"This paper proposes a new character encoding scheme for use with character-convolutional language models. This is a poor quality paper, is unclear in the results (what metric is even reported in Table 6), and has little significance (though this may highlight the opportunity to revisit the encoding scheme for characters).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification","abstract":"This paper reports the empirical exploration of a novel approach to encode words using compressing inspired techniques for use on convolutional neural networks at character-level. This approach reduces drastically the number of parameters to be optimized using deep learning, resulting in competitive accuracies in only a fraction of the time spent by characters level convolutional neural networks,  enabling training in simpler hardware.","pdf":"/pdf/e0811d0f0c80db70241966ea65c36c29dc0128a6.pdf","TL;DR":"Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation","paperhash":"anonymous|compressing_encoding_of_words_for_use_in_characterlevel_convolutional_networks_for_text_classification","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkYXvCR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper107/Authors"],"keywords":["Character Level Convolutional Networks","Text Classification","Word Compressing"]}},{"tddate":null,"ddate":null,"tmdate":1512222548148,"tcdate":1511790343542,"number":2,"cdate":1511790343542,"id":"BJ1fw5Flf","invitation":"ICLR.cc/2018/Conference/-/Paper107/Official_Review","forum":"SkYXvCR6W","replyto":"SkYXvCR6W","signatures":["ICLR.cc/2018/Conference/Paper107/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The manuscript needs updating. The current state is not good enough.","rating":"3: Clear rejection","review":"The manuscript proposed to use prefix codes to compress the input to a neural network for text classification. It builds upon the work by Zhang & LeCun (2015) where the same tasks are used.\n\n\nThere are several issues with the paper and I cannot recommend acceptance of the paper in the current state. \n- It looks like it is not finished.\n- the datasets are not described properly. \n- It is not clear to me where the baseline results come from.\n They do not match up to the Zhang paper (I have tried to find the matching accuracies there).\n- It is not clear to me what the baselines actually are or how I can found more info on those.\n- the results are not remarkable. \n\nBecause of this, the paper needs to be updated and cleaned up before it can be properly reviewed. \n\nOn top of this, I do not enjoy the style the paper is written in, the language is convoluted. \nFor example: “The effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification “\nI do not know which message the paper tries to get across here. \nAs a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive.\nThe acknowledgements should not be included here either. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification","abstract":"This paper reports the empirical exploration of a novel approach to encode words using compressing inspired techniques for use on convolutional neural networks at character-level. This approach reduces drastically the number of parameters to be optimized using deep learning, resulting in competitive accuracies in only a fraction of the time spent by characters level convolutional neural networks,  enabling training in simpler hardware.","pdf":"/pdf/e0811d0f0c80db70241966ea65c36c29dc0128a6.pdf","TL;DR":"Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation","paperhash":"anonymous|compressing_encoding_of_words_for_use_in_characterlevel_convolutional_networks_for_text_classification","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkYXvCR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper107/Authors"],"keywords":["Character Level Convolutional Networks","Text Classification","Word Compressing"]}},{"tddate":null,"ddate":null,"tmdate":1512222548193,"tcdate":1510390403187,"number":1,"cdate":1510390403187,"id":"rkst5E4Jz","invitation":"ICLR.cc/2018/Conference/-/Paper107/Official_Review","forum":"SkYXvCR6W","replyto":"SkYXvCR6W","signatures":["ICLR.cc/2018/Conference/Paper107/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Main idea lacks significance","rating":"4: Ok but not good enough - rejection","review":"The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row. The idea is interesting, and overall introduction is clear.\n\nHowever, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015. The results using this particular encoding are not better than any previous work.\n\nThe network architecture seems to be arbitrary and unusual. It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels. The depth of the network is only 5, even with many layers listed in table 5.\n\nIt uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features. This does not seem to be reasonable.\n\nOverall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification","abstract":"This paper reports the empirical exploration of a novel approach to encode words using compressing inspired techniques for use on convolutional neural networks at character-level. This approach reduces drastically the number of parameters to be optimized using deep learning, resulting in competitive accuracies in only a fraction of the time spent by characters level convolutional neural networks,  enabling training in simpler hardware.","pdf":"/pdf/e0811d0f0c80db70241966ea65c36c29dc0128a6.pdf","TL;DR":"Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation","paperhash":"anonymous|compressing_encoding_of_words_for_use_in_characterlevel_convolutional_networks_for_text_classification","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkYXvCR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper107/Authors"],"keywords":["Character Level Convolutional Networks","Text Classification","Word Compressing"]}},{"tddate":null,"ddate":null,"tmdate":1509739480649,"tcdate":1508988705529,"number":107,"cdate":1509739477993,"id":"SkYXvCR6W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkYXvCR6W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification","abstract":"This paper reports the empirical exploration of a novel approach to encode words using compressing inspired techniques for use on convolutional neural networks at character-level. This approach reduces drastically the number of parameters to be optimized using deep learning, resulting in competitive accuracies in only a fraction of the time spent by characters level convolutional neural networks,  enabling training in simpler hardware.","pdf":"/pdf/e0811d0f0c80db70241966ea65c36c29dc0128a6.pdf","TL;DR":"Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation","paperhash":"anonymous|compressing_encoding_of_words_for_use_in_characterlevel_convolutional_networks_for_text_classification","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Encoding of Words for use in Character-level Convolutional Networks for Text Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkYXvCR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper107/Authors"],"keywords":["Character Level Convolutional Networks","Text Classification","Word Compressing"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}