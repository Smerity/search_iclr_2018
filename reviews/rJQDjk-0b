{"notes":[{"tddate":null,"ddate":null,"tmdate":1515969146176,"tcdate":1515969146176,"number":1,"cdate":1515969146176,"id":"S1zt98K4z","invitation":"ICLR.cc/2018/Conference/-/Paper514/Public_Comment","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["~Anirudh_Goyal1"],"readers":["everyone"],"writers":["~Anirudh_Goyal1"],"content":{"title":"Interesting Work! ","comment":"I just wanted to point out (for general readers) that this paper tries to address a very interesting research problem i.e training RNN's in an online fashion , an important open problem that has been severely under-explored in machine learning community.  Even though, the presented results are preliminary, this paper proposes a principled way to approach this problem.   \n\nRegarding the paper, I think reviewers did good job asking \"right\" questions, and I feel satisfied by the authors response!  I am really excited to see how can we extend this!  Good work! :-)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515019005574,"tcdate":1515018475467,"number":4,"cdate":1515018475467,"id":"rk7gF0cXG","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Comment","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper514/Authors"],"content":{"title":"Paper revision","comment":"The paper has been revised following the reviewers' advice. Two sections focusing on the evolution of the variance of the gradient approximation, both with respect to the length of the input sequence and to the size of the network have been added, along with corresponding experiments."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515017540633,"tcdate":1515017540633,"number":3,"cdate":1515017540633,"id":"SyaSrR5Xf","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Comment","forum":"rJQDjk-0b","replyto":"B1Ud2yqgz","signatures":["ICLR.cc/2018/Conference/Paper514/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper514/Authors"],"content":{"title":"Answer to reviewer 1","comment":"Thank you for your comments and suggestions.\n\n1/ Regarding comparison to other online methods such as NoBackTrack and Echo State Networks. For plain, fully connected RNNs, NoBackTrack and UORO turn out to be mathematically identical (though implemented quite differently), so they will perform the same. On the contrary, for LSTMs, NoBackTrack is extremely difficult to implement (to our knowledge, it has never been done); this was one of the motivations for UORO, but it makes the comparison difficult.\n\nFor Echo State Networks: ESNs amount in great part to not learning the internal weights, only the output weights (together with a carefully tuned initialization). As much as we are aware, they are not known to fare particularly well on the kind of task we consider, but we may have missed relevant references.\n\n2/ We have included a few more tasks and tests, although this remains relatively small-scale.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515017405462,"tcdate":1515017405462,"number":2,"cdate":1515017405462,"id":"SJHpNAqQG","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Comment","forum":"rJQDjk-0b","replyto":"B1QPFb5eG","signatures":["ICLR.cc/2018/Conference/Paper514/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper514/Authors"],"content":{"title":"Answer to reviewer 3","comment":"Thank you for your insights, questions and suggestions. We have tried to attend your concerns in the revised version of the paper. \n\n1/ As you pointed out, the results are indeed preliminary. As pointed out in the answer to Reviewer 2, it is difficult to obtain results competitive with BPTT on large scale benchmarks given the additional constraints on UORO (namely, no storage of past data, and good convergence properties, which is not the case of truncated backpropagation if dependencies exceed its truncation range).\n\n2/ About the variance of UORO for large networks: We have added an experiment to test this. The variance of UORO does increase with netowrk size (probably sublinearly), and larger networks will require smaller learning rates.\n\n3/ About the effect of length on the quality of the approximation: We have added an experiment to test the evolution of UORO variance when time increases along the input sequence. The variance of UORO does not explode over time, and is stationary.  A key point of UORO is the whole renormalization process (variable rho), designed precisely for this. An independent, theoretical proof for the similar case of NoBackTrack is in (Masse 2017).  Thus UORO is applicable to unbounded sequences (notably, in the experiments, datasets are fed as a single sequence, containing 10^6-10^7 characters). \n\n4/ About the stochastic gradient descent argument: indeed one has to be careful. If UORO is used to process a number of finite training sequences, and gradient steps are performed at the end of each sequence only, then this is a fully standard SGD argument: UORO computes, in a streaming fashion, an unbiased estimate of the same gradient as BPTT for each training sequence.  However, if the gradients steps are performed at every time step, as we do here, then you are right that an additional argument is needed. The difference between applying gradients at each step and applying gradients only at the end of each sequence is at *second order* in the learning rate: if the learning rate is small, applying gradients at each time does not change the computations too much, and the SGD argument applies up to second-order terms.  This is fully formalized in (Masse 2017). If moreover, only one infinite training sequence is provided, then an additional assumption of ergodicity (decay of correlations) is needed. But in any case unbiasedness is the central property.\n\n5/ About the optima reached by UORO vs BPTT: in the limit of very small learning rates, UORO, RTRL, and BPTT with increasing truncation lengths will all produce the same limit trajectories. The theory from (Masse 2017) proves local convergence to the *same* set of local optima for RTRL and UORO (if starting close enough to the local optimum). On the other hand, for large learning rates, we are not aware of theoretical results for any recurrent algorithm.\n\n6/ Regarding reference (Masse 2017): this reference is now publically\navailable and we provide a link in the bibliography. We were indeed aware of Masse's work a bit before it was put online, but that still covers many people, so we do not believe this breaks anonymity. Our paper is disjoint from (Masse 2017), as can be directly checked by comparing the texts.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515017161458,"tcdate":1515017137433,"number":1,"cdate":1515017137433,"id":"H1thm0c7G","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Comment","forum":"rJQDjk-0b","replyto":"r11STCqxG","signatures":["ICLR.cc/2018/Conference/Paper514/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper514/Authors"],"content":{"title":"Answer to reviewer 2","comment":"Thank you for the constructive feedback. At the moment, we haven't\nsucceeded in scaling UORO up to the state of the art with results competitive with backpropagation on large scale benchmarks. This may be due to the additional constraints borne by UORO, namely, both\nmemorylessness and unbiasedness at all time scales.  Such datasets (notably next-character or next-word predictions) contain difficult short-term dependencies: Truncated BPTT with relatively small truncation is expected to learn those dependencies better than an algorithm like UORO, which must consider all time ranges at once.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642459306,"tcdate":1511873847098,"number":3,"cdate":1511873847098,"id":"r11STCqxG","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Review","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clever trick for making general memory-efficient online unbiased RNN learning possible","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents a generic unbiased low-rank stochastic approximation to full rank matrices that makes it possible to do online RNN training without the O(n^3) overhead of real-time recurrent learning (RTRL). This is an important and long-sought-after goal of connectionist learning and this paper presents a clear and concise description of why their method is a natural way of achieving that goal, along with experiments on classic toy RNN tasks with medium-range time dependencies for which other low-memory-overhead RNN training heuristics fail. My only major complaint with the paper is that it does not extend the method to large-scale problems on real data, for instance work from the last decade on sequence generation, speech recognition or any of the other RNN success stories that have led to their wide adoption (eg Graves 2013, Sutskever, Martens and Hinton 2011 or Graves, Mohamed and Hinton 2013). However, if the paper does achieve what it claims to achieve, I am sure that many people will soon try out UORO to see if the results are in any way comparable.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516399073184,"tcdate":1511819610849,"number":2,"cdate":1511819610849,"id":"B1QPFb5eG","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Review","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very interesting paper that approaches online training of RNNs in a principled way, although more experiments would make it more convincing","rating":"7: Good paper, accept","review":"Post-rebuttal update:\nI am happy with the rebuttal and therefore I will keep the score of 7.\n\nThis is a very interesting paper. Training RNN's in an online fashion (with no backpropagation through time) is one of those problems which are not well explored in the research community. And I think, this paper approaches this problem in a very principled manner. The authors proposes to use forward approach for the calculation of the gradients. The author proposes to modify RTRL by maintaining a rank one approximation of jacobian matrix (derivative of state w.r.t parameters)  which was done in NoBackTrack Paper. The way I think this paper is different from NoBackTrack Paper is that this version can be implemented in a black box fashion and hence easy to implement using current DL libraries like Pytorch. \n\nPros.\n\n- Its an interesting paper, very easy to follow, and with proper literature survey.\n\nCons:\n\n- The results are quite preliminary. I'll note that this is a very difficult problem.\n- \"The proof of UORO’s convergence to a local optimum is soon to be published Masse & Ollivier (To appear).\"  I think, paper violates the anonymity.  So, I'd encourage the authors to remove this. \n\nSome Points: \n\n- I find the argument of stochastic gradient descent wrong (I could be wrong though). RNN's follow the markov property (wrt hidden states from previous time step and the current input) so from time step t to t+1, if you change the parameters, the hidden state at time t (and all the time steps before) would carry stale information unless until you're using something like eligibility traces from RL literature. I also don't know how to overcome this issue. \n\n- I'd be worried about the variance in the estimate of rank one approximation. All the experiments carried out by the authors are small scale (hidden size = 64). I'm curious if authors tried experimenting with larger networks, I'd guess it wont perform well due to the high variance in the approximation. I'd like to see an experiment with hidden size  = 128/256/512/1024. My intuition is that because of high variance it would be difficult to train this network, but I could be wrong. I'm curious what the authors had to say about this. \n\n- If the variance of the approximation is indeed high, can we use something to control the dynamics of the network which can result in less variance. Have authors thought about this ? \n\n- I'd also like to see experiments on copying task/adding task (as these are standard experiments which are done for analysis of long term dependencies) \n\n- I'd also like to see what effect the length of sequence has on the approximation. As small errors in approximation on each step can compound giving rise to chaotic dynamics. (small change in input => large change in output)\n\n- I'd also like to know how using UORO changes the optimization as compared to Back-propagation through time in the sense, does the two approaches would reach same local minimum ? or is there a possibility that the former can reach \"less\" number of potential local minimas as compared to BPTT. \n\n\nI'm tempted to give high score for this paper( Score - 7) , as it is unexplored direction in our research community, and I think this paper makes a very useful contribution to tackle this problem in a very principled way.  But I'd like some more experiments to be done (which I have mentioned above), failing to do those experiments, I'd be forced to reduce the score (to score - 5) ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516575799547,"tcdate":1511812206743,"number":1,"cdate":1511812206743,"id":"B1Ud2yqgz","invitation":"ICLR.cc/2018/Conference/-/Paper514/Official_Review","forum":"rJQDjk-0b","replyto":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference/Paper514/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The authors introduce a novel approach to online learning of the parameters of recurrent neural networks from long sequences that overcomes the limitation of truncated backpropagation through time (BPTT) of providing biased gradient estimates.\n\nThe idea is to use a forward computation of the gradient as in Williams and Zipser (1989) with an unbiased approximation of Delta s_t/Delta theta to reduce the memory and computational cost.\n\nThe proposed approach, called UORO, is tested on a few artificial datasets.\n\nThe approach is interesting and could potentially be very useful. However, the paper lacks in providing a substantial experimental evaluation and comparison with other methods.\nRather than with truncated BPTT with smaller truncation than required, which is easy to outperform, I would have expected a comparison with some of the other methods mentioned in the Related Work Section, such as NBT, ESNs, Decoupled Neural Interfaces, etc. Also the evaluation should be extended to other challenging tasks. \n\nI have increased the score to 6 based on the comments and revisions from the authors.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515016776249,"tcdate":1509124955034,"number":514,"cdate":1509739258525,"id":"rJQDjk-0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJQDjk-0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unbiased Online Recurrent Optimization","abstract":"The novel \\emph{Unbiased Online Recurrent Optimization} (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is computationally as costly as \\emph{Truncated Backpropagation Through Time} (truncated BPTT), a widespread algorithm for online learning of recurrent networks \\cite{jaeger2002tutorial}.  UORO is a modification of \\emph{NoBackTrack} \\cite{DBLP:journals/corr/OllivierC15} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models.  Like NoBackTrack, UORO provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated BPTT does not provide this property, leading to possible divergence.  On synthetic tasks where truncated BPTT is shown to diverge, UORO converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated BPTT diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.\n","pdf":"/pdf/8964d44e2753e9fc7eca75dbeb87fbaeac16009a.pdf","TL;DR":"Introduces an online, unbiased and easily implementable gradient estimate for recurrent models.","paperhash":"anonymous|unbiased_online_recurrent_optimization","_bibtex":"@article{\n  anonymous2018unbiased,\n  title={Unbiased Online Recurrent Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJQDjk-0b}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper514/Authors"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}