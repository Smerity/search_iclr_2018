{"notes":[{"tddate":null,"ddate":null,"tmdate":1515810008492,"tcdate":1515793817393,"number":9,"cdate":1515793817393,"id":"H1ZsTiUEf","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Comment","forum":"rkTBjG-AZ","replyto":"B1BvqGB4f","signatures":["ICLR.cc/2018/Conference/Paper926/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper926/Authors"],"content":{"title":"re: re: final remarks","comment":"Saying that a DSL is not interesting for this community is your subjective opinion. Work on code, frameworks, and APIs has been published at top ML conferences and/or been hugely influential in ML; for example: \n- http://download.tensorflow.org/paper/whitepaper2015.pdf\n- https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf\n- https://papers.nips.cc/paper/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs.pdf,\n- https://openreview.net/forum?id=ryrGawqex\n- https://arxiv.org/pdf/1701.03980.pdf\n- https://papers.nips.cc/paper/6256-a-credit-assignment-compiler-for-joint-prediction.pdf).\n\nYou can always take the position that something is not interesting because it does not let you do anything new. Using that line of thought, high-level programming languages are not interesting because you already can accomplish the same in low-level programming languages; Tensorflow, PyTorch, and similar framework are not interesting because you can write your own deep learning code from scratch in C++ or Python. \n\nMuch of the recent progress in ML was greatly facilitated by the existence of high-level tools such Tensorflow and Pytorch. One can only wonder how much more far behind as a community we would be if everyone was still writing their own backpropagation implementations. While Tensorflow or Pytorch do not allow you to do things that could not be done in C++ or Python, they make expressing interesting ML programs drastically easier, and as a result, researchers are able to think about and approach problems differently than they would if there were no such deep learning frameworks. \n\nOur work allows us to think about architecture search differently. Namely, splitting the problem of architecture search into three parts (model search space specification language, model search algorithm, model evaluation algorithm) is a useful perspective as the different parts can be changed and/or improved independently (e.g., a more expressive model search space specification language, a more sample efficient model search algorithm, a different model evaluation algorithm). Additionally, our model search space specification language has interesting ideas on how to induce these complex compositional architecture search spaces (which end up being in complex implicit hyperparameter spaces) by writing expressions in a DSL. The fact that all this could be expressed differently is of limited relevance. The truth is that our DSL allows researchers to think differently about the problem and gives them interesting tools to write expressive search spaces over architectures. That being said, we believe that our paper introduces a sufficient number of new and interesting ideas to be of value to this community.\n\nCan you clarify (e.g., give an example) of what you mean by \"directed acyclic graph\" in this context (i.e., in the context of hyperparameters spaces). \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515690588780,"tcdate":1515690588780,"number":7,"cdate":1515690588780,"id":"B1BvqGB4f","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Comment","forum":"rkTBjG-AZ","replyto":"BJS0iIpQG","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper926/AnonReviewer2"],"content":{"title":"re: final remarks","comment":"A domain specific language would be more appreciated by a different audience, where writing in Python is seen as a drawback and where the DSL is more exciting. As a reviewer for ICLR I am looking for something new that it lets me do. I don't see it yet, and random search wouldn't be compelling.\n\nAlso, there has been some mis-characterization of Hyperopt here: Hyperopt has always supported graph-structured (rather than just tree-structured) compositions in the computational graph / search space input to fmin. The \"Tree of Parzen Windows\" is something of a misnomer in this regard, as the implementation of that algorithm in Hyperopt works on directed acyclic graphs."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515187043090,"tcdate":1515186934323,"number":5,"cdate":1515186934323,"id":"r1RljwaQz","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Comment","forum":"rkTBjG-AZ","replyto":"Skp1e-zgM","signatures":["ICLR.cc/2018/Conference/Paper926/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper926/Authors"],"content":{"title":"The reviewer misrepresents our work","comment":"Our work is novel for the reasons elaborated in our other replies. \n\nSince this work has been made available on ArXiv, its ideas have been used or suggested future work on architecture search. For example, the ideas of compositionality for the representation of a search space are used in Hierarchical Representations for Efficient Architecture Search (https://openreview.net/forum?id=BJQRKzbA-); using MCTS to do model search is used in \"Finding Competitive Network Architectures Within a Day Using UCT\"; using SMBO to do model search is used in \"Progressive Neural Architecture Search\", among others.\n\nThe framework set forth by this paper provides a foundation for thinking about architecture search. Progress in architecture search can be made by developing a better model search space representation language, giving more expressive tools to a deep learning expert to represent search spaces over models. Progress can also be made by developing better model search algorithms that search spaces of models more efficiently. The fact that this framework is modular is a big advantage, as research can be focused on each of the components rather that having to having to develop a monolithic system from scratch each time. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515185178213,"tcdate":1515183348704,"number":4,"cdate":1515183348704,"id":"rk6l68T7G","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Comment","forum":"rkTBjG-AZ","replyto":"Sy4q4vBgf","signatures":["ICLR.cc/2018/Conference/Paper926/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper926/Authors"],"content":{"title":"The decomposition of the framework in the three parts: model search space specification language, model search algorithm, and model evaluation algorithm allows us to reason clearly about architecture search. The model search space specification is modular, compositional, and extensible.","comment":"The reviewer presents the following criticism:\n1. The decomposition of the framework into model search space specification language, model search algorithm, and model evaluation algorithm is interesting, but there is concern about its importance in practice.\n2. There is concern that the framework may be restrictive for large-scale problems.\n\nResponse: \n1. The decomposition into these three components allows us to think clearly about each of them rather than dealing with all aspects simultaneously. For example, future contributions may focus on extending the model search space specification language or on developing better model search algorithms. These components interact only through a very minimal interface. This decomposition of the problem will be useful for future research on architecture search.\n\n2. There is no fundamental reason why this framework should be restrictive in the way the reviewer is concerned. We discuss in Section 4 the properties required by a module. These are quite general, and therefore we can easily introduce useful new basic and composite modules. See also Appendix B for examples of modules that we defined. As the different components in our framework are highly extensible and modular, our work will be very useful in approaching future problems in architecture search. Namely, the model search space specification language is expressive enough to capture many relevant high-performance search spaces, as discussed in paragraph 4 of section 4.1 and in appendix D.\n\nWe do not have the resources available to conduct experiments on the scale of some other recent papers (e.g., those coming out of major corporate research labs such as Google). Nonetheless, our smaller scale experiments are enough to support our claims: expressive search spaces over architectures can be represented easily by writing expressions in our model search space specification language (see Appendix A, Figure 4, and Figure 5); the search spaces induced can be effectively searched by random search; using model search algorithms attuned to the structure of the search space results in improved search performance.\n\nOne of the main focus of this work is the representation power of the model search space specification language. We also note that due to the flexibility of the search space specification language, architecture search can be easily integrated in a ML workflow, as the expert only has to design a reasonable search space and provide a way of evaluating models.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515185118640,"tcdate":1515183052808,"number":3,"cdate":1515183052808,"id":"BJS0iIpQG","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Comment","forum":"rkTBjG-AZ","replyto":"rJHFv8pXG","signatures":["ICLR.cc/2018/Conference/Paper926/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper926/Authors"],"content":{"title":"Final remarks on the previous reply","comment":"Final remarks: \nThe introduction of the model search space specification language along with just random search experiments would by itself be interesting enough to warrant publication. This domain-specific language is extensible and compositional, allowing the user to easily represent search spaces over architectures and compile them to computational graphs. The model search algorithms proposed, while simple, are well-suited to the resulting search spaces and are a good start to design more complex and performant ones for this setting. This is, to the best of our knowledge, the first work to propose such a DSL for architecture search and explore its benefits; as such, it provides an extensible platform for future research on architecture search. To support this benefit, we have made all code available and will continue to extend it.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515185086306,"tcdate":1515181948808,"number":2,"cdate":1515181948808,"id":"rJHFv8pXG","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Comment","forum":"rkTBjG-AZ","replyto":"ByV24asxM","signatures":["ICLR.cc/2018/Conference/Paper926/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper926/Authors"],"content":{"title":"Our goal is to develop a framework for architecture search in deep learning around the model search space specification language introduced. Our goal is not to propose MCTS as an algorithm for general purpose hyperparameter optimization; rather we propose random, MCTS, and SMBO as simple baseline algorithms for model search that are well-suited to the structures arising from the search spaces induced by our model search space specification language. ","comment":"The reviewer presents the following criticism:\n1. TPE/SMAC also allows us to describe search spaces with conditional structure. \n2. “Making a science of model search …” uses TPE to search over simple convolutional architectures.\n3. The performance of MCTS for hyperparameter optimization would be better evaluated in the benchmarks pointed out by the reviewer.\n\nResponse:\n1. We do not claim to be the first to present a method that works with conditional structure. We clearly state in our paper that there are general-purpose hyperparameter optimization algorithms such as TPE, however these are harder to use for architecture search because they require the user to write more code and single out what are the hyperparameters to search over. In contrast, in our, writing an expression in our DSL (domain-specific language) automatically induces the search space. Furthermore, this language allows us to directly compile the resulting model to the corresponding computational graph. Note that our work in focused on architecture search for deep learning and not general hyperparameter optimization. We are in the same line of work as Zoph and Le (ICLR 2017). \n\nExpressions written in our model search space specification language encode trees; paths through the encoding correspond to fully specified models that can be compiled to computational graphs. Note that this tree is implicit;  we only require functionality to traverse the tree, and not a full explicit representation. This is important when there are exponentially many paths from the root to leaves. This contrasts with TPE: e.g., \"Making a science of model search\" uses a simple representation, and therefore is constrained to simple trees. For example, the following (toy) search space is hard to represent in Hyperopt, but poses no problem in our language: \n\n(Repeat \n  (Optional \n    (Repeat \n      (Affine [32, 64]) \n     [1, 2, 4])\n  )\n[1, 2, 4]) \n\nThe problem arises from the interaction between Optional and Repeat. These problems are exacerbated by deeper nesting. Representing this search space in Hyperopt would require writing a cumbersome cases expression for each of the different combinations of hyperparameter values for Repeat and Optional modules. See https://github.com/jaberg/hyperopt/wiki/FMin for information on the cases construct in Hyperopt: hp.choice. By contrast, our language imposes no such burden on the user.\n\nDue to these significant differences, it is incorrect to say that our DSL for specifying search spaces over architectures is not novel when compared to something such as Hyperopt. \n\nWe explore how the introduction of the search space specification language allows us to construct a integrated framework for architecture search. The main focus of this paper is not to propose new hyperparameter optimization algorithms in current general settings. The model search algorithms that we propose are adjusted to the structures that arise from our model search space specification language. The experiments study the potential of different model search algorithms with structures of this type.\n\n2. The paper mentioned does indeed search over simple convolutional architectures on CIFAR-10, nonetheless, the search space is hard-coded and does not make use of the compositionality resulting from the model search space specification language. This point is addressed in the related work section (e.g., paragraph 6 of section 2). One important aspect of our framework is that it allows the user to easily write search spaces over architectures, functioning as a tool to support model discovery. Much of the recent progress in deep learning was supported by the existence of tools that allow experts rapid experimentation and exploration (e.g., Tensorflow, Pytorch). We need to build these tools for architecture search in deep learning, i.e., specific tools for architecture search rather than existing general-purpose hyperparameter optimization tools.\n\n3. Our work focus on the development of a framework for architecture search in deep learning. Currently, there are no standard benchmarks for architecture search. In particular, due to the focus on architecture search, the suggested generic datasets for pipeline tuning (e.g., auto-sklearn, autoweka) would fit well the message of this paper (e.g., from paragraph 3 of section 1 to the end of that section). Our goal is not to propose MCTS as an algorithm for general purpose hyperparameter optimization; rather we propose random, MCTS, and SMBO as simple baseline algorithms for model search that are well-suited to the structures arising from the search spaces induced by our model search space specification language."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515642531683,"tcdate":1511933099936,"number":3,"cdate":1511933099936,"id":"ByV24asxM","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Review","forum":"rkTBjG-AZ","replyto":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer2"],"readers":["everyone"],"content":{"title":"MCTS is promising, but should be evaluated in a more standard way","rating":"4: Ok but not good enough - rejection","review":"Monte-Carlo Tree Search is a reasonable and promising approach to hyperparameter optimization or algorithm configuration in search spaces that involve conditional structure.\n\nThis paper must acknowledge more explicitly that it is not the first to take a graph-search approach. The cited work related to SMAC and Hyperopt / TPE addresses this problem similarly. The technique of separating a description language from the optimization algorithm is also used in both of these projects / lines of research. The [mis-cited] paper titled “Making a science of model search …” is about using TPE to configure 1, 2, and 3 layer convnets for several datasets, including CIFAR-10. SMAC and Hyperopt have been used to search large search spaces involving pre-processing and classification algorithms (e.g. auto-sklearn, autoweka, hyperopt-sklearn). There have been near-annual workshops on AutoML and Bayesian optimization at NIPS and ICML (see e.g. automl.org).\n\nThere is a benchmark suite of hyperparameter optimization problems that would be a better way to evaluate MCTS as a hyperparameter optimization algorithm: http://www.ml4aad.org/automl/hpolib/","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1515642531722,"tcdate":1511515276341,"number":2,"cdate":1511515276341,"id":"Sy4q4vBgf","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Review","forum":"rkTBjG-AZ","replyto":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors propose to automatically design and train deep architectures. ","rating":"5: Marginally below acceptance threshold","review":"This paper introduces a DeepArchitect framework to build and train deep models automatically. Specifically, the authors proposes three components, i.e., model search specification language, model search algorithm, model evaluation algorithm. The paper is written well, and the proposed framework provides us with a systematical way to design deep models.\n\nHowever, my concern is mainly about its importance in practice. The experiments and computational modules are basic and small-scale, i.e., it may be restricted for large-scale computer vision problems. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1516293136255,"tcdate":1511292901250,"number":1,"cdate":1511292901250,"id":"Skp1e-zgM","invitation":"ICLR.cc/2018/Conference/-/Paper926/Official_Review","forum":"rkTBjG-AZ","replyto":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference/Paper926/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The novelty in this paper is below what is expected for a publication at ICLR. I recommend rejection.","rating":"4: Ok but not good enough - rejection","review":"The author present a language for expressing hyperparameters (HP) of a network. This language allows to define a tree structure search space to cover the case where some HP variable exists only if some previous HP variable took some specific value. Using this tool, they explore the depth of the network, when to apply batch-normalization, when to apply dropout and some optimization variables. They compare the search performance of random search, monte carlo tree search and a basic implementation of a Sequential Model Based Search. \n\nThe novelty in this paper is below what is expected for a publication at ICLR. I recommend rejection.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]}},{"tddate":null,"ddate":null,"tmdate":1510092385799,"tcdate":1509137221794,"number":926,"cdate":1510092362372,"id":"rkTBjG-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkTBjG-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DeepArchitect: Automatically Designing and Training Deep Architectures","abstract":"In deep learning, performance is strongly affected by the choice of architecture\nand hyperparameters. While there has been extensive work on automatic hyperpa-\nrameter optimization for simple spaces, complex spaces such as the space of deep\narchitectures remain largely unexplored. As a result, the choice of architecture is\ndone manually by the human expert through a slow trial and error process guided\nmainly by intuition. In this paper we describe a framework for automatically\ndesigning and training deep models. We propose an extensible and modular lan-\nguage that allows the human expert to compactly represent complex search spaces\nover architectures and their hyperparameters. The resulting search spaces are tree-\nstructured and therefore easy to traverse. Models can be automatically compiled to\ncomputational graphs once values for all hyperparameters have been chosen. We\ncan leverage the structure of the search space to introduce different model search\nalgorithms, such as random search, Monte Carlo tree search (MCTS), and sequen-\ntial model-based optimization (SMBO). We present experiments comparing the\ndifferent algorithms on CIFAR-10 and show that MCTS and SMBO outperform\nrandom search. We also present experiments on MNIST, showing that the same\nsearch space achieves near state-of-the-art performance with a few samples. These\nexperiments show that our framework can be used effectively for model discov-\nery, as it is possible to describe expressive search spaces and discover competitive\nmodels without much effort from the human expert. Code for our framework and\nexperiments has been made publicly available","pdf":"/pdf/1b4b11e6581fddc7fc91f511087335cef3ad7fb1.pdf","TL;DR":"We describe a modular and composable language for describing expressive search spaces over architectures and simple model search algorithms applied to these search spaces. ","paperhash":"anonymous|deeparchitect_automatically_designing_and_training_deep_architectures","_bibtex":"@article{\n  anonymous2018deeparchitect:,\n  title={DeepArchitect: Automatically Designing and Training Deep Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkTBjG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper926/Authors"],"keywords":["architecture search","deep learning","hyperparameter tuning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}