{"notes":[{"tddate":null,"ddate":null,"tmdate":1513596319520,"tcdate":1513596319520,"number":9,"cdate":1513596319520,"id":"rJwjH7HzM","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Uploaded revision 2","comment":"Changelist:\n\n- Better NAS results with more tuning on Wikitext-2 and Enwik8. The story is the same, still lagging other models.\n- Tiny adjustments related to down-projections that hopefully clarify things."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1513541916290,"tcdate":1513541916290,"number":8,"cdate":1513541916290,"id":"SkNXbLNzf","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["~William_Chan1"],"readers":["everyone"],"writers":["~William_Chan1"],"content":{"title":"Hyperparams","comment":"I wish/encourage the authors would post the hyperparams, it would make reproducing the results much easier, especially for the academic community which may not have the resources to run full hyperparameter searches (even if the scripts are released)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1513003024469,"tcdate":1513003024469,"number":8,"cdate":1513003024469,"id":"HJOf_f2Wz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"S1Mw8jBef","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: a useful exercise","comment":"We thank AnonReviewer1 for their review.\n\nWe would like to point out that the state-of-the-art results and model comparisons are only part of the message. More importantly, we argue that the way model evaluation is performed is often unsatisfactory. Evaluation at a single hyperparameter setting, failing to control for dominant sources of variation make results unreliable and slow down progress.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1513002986120,"tcdate":1513002986120,"number":7,"cdate":1513002986120,"id":"HJfg_G2bz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"rJTcBCtxG","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: With extensive tuning, LSTM beats other new models","comment":"We feel that AnonReviewer3 might have missed that the main message of the paper was that evaluation - as it's generally performed - is unreliable. Our results suggest that state-of-the-art results are only superficially considered, and variance and parameter sensitivity are likewise given short shrift.\n\nThe main criticism seems to center on evaluating models on datasets that are too small which increases evaluation variance and the results are thus not trustworthy. That is a very good summary of the main message of the paper! We agree that small datasets are problematic, but one cannot refute previous results that were obtained on small datasets using large datasets. Furthermore, we do hyperparameter tuning and a careful analysis of the variance. Furthermore, the third dataset (enwik8) is a large character based corpus and we still improve previously reported LSTM results by a substantial margin.\n\nFinally, to do this kind of study we chose language modelling because of its relevance to all kinds recurrent neural models while being simpler than machine translation and speech recognition models. We have demonstrated evaluation problems in this simple and relevant setting. It is unclear why the reviewer requests results on MT and ASR.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1513002890349,"tcdate":1513002890349,"number":6,"cdate":1513002890349,"id":"BJM9Df2WM","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HkGW8A2gG","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Important big-picture work in a fast-moving field","comment":"We thank AnonReviewer2 for the thoughtful and detailed review, let us address the points brought up one by one in the original order (we will likewise clarify these points in the paper):\n\n- Some hyperparameters were indeed left at \"default\" values because our tuner cannot efficiently tune a large set of hyperparameters. Still we did tuning studies with lower and higher BPTT lengths, batch sizes and including Adam parameters (beta1, beta2, epsilon) and with other optimizers to make sure that our intuition about what hyperparameters are most important is correct. We did a tuning study with all hyperparameters (about 40 hyperparameters in total) to catch any unexpected parameter combinations even if it was a long shot due to the aforementioned tuner inefficiency.\n\n- Yes, the down-projection is simply projecting down from the LSTM hidden size to the embedding size. The ratio of the embedding size and cell size is a tuneable. The cell and embedding sizes are computed from the budget and this input_embedding_ratio hyperparameter. As the paper puts it: \"The tuner is given control over the presence and size of the down-projection, and thus over the tradeoff between the number of embedding vs. recurrent cell parameters. Consequently, the cells’ hidden size and the embedding size is determined by the actual parameter budget, depth and the input embedding ratio hyperparameter.\"\n\n- Yes, we didn't find a very different cell with promising results in the literature.\n\n- No comment.\n\n- We are working on factoring out the code from a larger system and providing training scripts with the tuned hyperparameters.\n\n- The Multiplicative LSTM is indeed interesting. We did some preliminary investigation and could not make it perform very well. In the end, it was excluded to avoid adding further multipliers to our already very high resource consumption.\n\n- We used shared masks because of implementation convenience and for computational considerations.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512993098808,"tcdate":1512993098808,"number":5,"cdate":1512993098808,"id":"BJXUZl2Wf","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"SJeVnLdbG","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Strong recommend accept","comment":"Thank you for taking the time to write the review.\n\nThe down-projection is indeed the former version: it projects the output of the top LSTM (plus skip connections) to output_embedding_size. We didn't try the suggested variant.\n\nYes, depth 1 and 2 LSTMs did not need skip connections but depth 4 suffered without them according to preliminary experiments. Alas, we have no further insight on this."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512758311578,"tcdate":1512758311578,"number":7,"cdate":1512758311578,"id":"SJeVnLdbG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["~Stephen_Merity1"],"readers":["everyone"],"writers":["~Stephen_Merity1"],"content":{"title":"Strong recommend accept ","comment":"I would strongly recommend this paper be accepted for publication. It tackles and uncovers many important discussions regarding our models, our datasets, their sensitivity to hyperparameters, and the process of thoroughly comparing models and searching for hyper parameters. This helps inform a broader discussion about how we can ensure that the scientific process for our field is best followed. Thoroughly analyzing and forcing a reconsideration of the impact of proposed RNN model architectures (LSTM, RHN, NASCell) on these tasks is worth the price of admission by itself, let alone the many other learnings and investigations.\n\nInformal Rating: 8\nConfidence: 5\n(work directly in this field and have recreated many aspects of the results since publication)\n\n= Hyper parameters =\n\nI'll reply to an earlier comment (search for \"we have been asked for the hyperparameter settings on numerous occasions\") as I'm interested in continuing discussion regarding your hesitance to release hyper parameters. Overall I am glad that you decided that you will release training scripts with the tuned hyperparameters as I genuinely think this will benefit the community going forward.\n\n= Down projection =\n\nTo clarify, is this a specific and separate layer (a dense layer that takes the output of the LSTM, h, and projects it from |h| to the embedding size |e|) or is the final LSTM accepting an input of size |h| and internally down projecting it to an output of size |e|? I imagine it would be the former given that your single layer LSTMs appear to still use down projection, hence having a larger |h| than |e|? Did you experiment with modifying the last LSTM layer's sizings (which may breaking your skip connections but would be more \"parameter efficient\")?\n\n= Skip connections =\n\nDid you investigate models that didn't use skip connections for the RNNs? We have found in our work that such skip connections did not appear to be required, especially for models that are under four or so layers (though we have trained 6 or 7 layer models too - it just gets finicky at that stage). You and your team might have insights on this that we do not?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512403548417,"tcdate":1512403548417,"number":4,"cdate":1512403548417,"id":"H14DMe7WG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HkGngSGZz","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Reproducible? Hyper-parameters?","comment":"Indeed we have been asked for the hyperparameter settings on numerous occasions. Originally, we did not provide these details as the main message of the paper was not about the state of the results but model evaluation, but there is another, more fundemental reason too: any single hyperparameter setting would make it easy to compare a derivative work to our well tuned baseline, but at best that could prove that the new model is better (it could never prove that it's worse). More, two new models each evaluated with those hyperparameters would still be incomparable.\n\nFor these reasons, we think that presently there is no way around tuning, and there is limited utility in publishing hyperparameter settings.\n\nThat said, we are working on factoring out the code from a larger system and providing training scripts with the tuned hyperparameters."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512358193128,"tcdate":1512358058535,"number":6,"cdate":1512358058535,"id":"HkGngSGZz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reproducible? Hyper-parameters?","comment":"The main contribution of this paper is to show with extensive hyper-parameter tuning, LSTM can achieve a state of the art result for language modeling. However, it seems that the authors didn't give the concrete hyper-parameters for reproducing their results. This paper is lack of technical novelty and totally of a experimental work; thus the experiment setting is very important. Only showing HP tuning can get state of the art results if not enough, since it is a common sense that hyper-parameter tuning can improve performance for any machine learning models. I think the paper should at least describe their hyper-parameters setting (for example, learning rate, hidden size, dropout ratio, weight decay, etc) for getting their results or releasing the code for the community if possible."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512198994405,"tcdate":1512198994405,"number":5,"cdate":1512198994405,"id":"BkqLXA1Zz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"rJTcBCtxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Too harsh","comment":"PTB is very much a standard baseline in the space that people routinely publish perf-based papers on, so I think it's a little harsh to knock people for publishing on it, especially when they make the effort to get results on multiple larger datasets, such as Wikitext-2.\n\nThis is not a paper about the utility of hyperparameter optimization, the fact that that works has been well established. It's a paper about how hyperparameter optimization wasn't properly used on a bunch of standard benchmarks in this space, which has already proven very valuable. I really don't think it's necessary to request results on MT or ASR "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512038083994,"tcdate":1512038083994,"number":3,"cdate":1512038083994,"id":"r1naAI6eG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"Sk5vVdhez","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Question about Transfer Experiment","comment":"The hyperparameters differ only in boring ways: Wikitext-2 needs a bit less intra layer and state dropout. This is very likely to be due to corpus size. Down-projection sizes are also a bit different due to the vocabulary size mismatch (when there is a down-projection at all).\n\nI'm not sure there are hyperparameters that work well on both, but yes, we could tune for combined (in whatever way) performance on a number of datasets. By doing this, we could learn more about how hyperparameters are best specified so that they are reasonably independent from datasets and also from other hyperparameters."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1515642446389,"tcdate":1512003065685,"number":3,"cdate":1512003065685,"id":"HkGW8A2gG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Review","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Important big-picture work in a fast-moving field","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space. Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks.\nThis is a significant result in language modeling and a milestone in deep learning reproducibility research. The paper is clearly motivated and authoritative in its conclusions but it's somewhat lacking in detailed model or experiment descriptions.\n\nSome further points:\n\n- There are several hyperparameters set to the \"standard\" or \"default\" value, like Adam's beta parameter and the batch size/BPTT length. Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified.\n\n- The description of the model is ambiguous on at least two points. First, it wasn't completely clear to me what the down-projection is (if it's simply projecting down from the LSTM hidden size to the embedding size, it wouldn't represent a hyperparameter the tuner can set, so I'm assuming it's separate and prior to the conventional output projection). Second, the phrase \"additive skip connections combining outputs of all layers\" has a couple possible interpretations (e.g., skip connections that jump from each layer to the last layer or (my assumption) skip connections between every pair of layers?).\n\n- Fully evaluating the \"claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation\" would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization.\n\n- The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance.\n\n- The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table. The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces. For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters.\n\n- The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most \"standard\" or \"conventional\" LSTM implementation (e.g., as provided in optimized GPU libraries). In addition to further discussion on this point, this result also suggests evaluating other recently proposed \"minor changes\" to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016)\n\n- It would also have been nice to see a comparison between the variational/recurrent dropout parameterization \"in which there is further sharing of masks between gates\" and the one with \"independent noise for the gates,\" as described in the footnote. There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511990674089,"tcdate":1511990674089,"number":2,"cdate":1511990674089,"id":"r19qHo2gM","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HJV5juheG","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Re","comment":"This is exactly what we did. The presence and size of the down-projection was a tuned hyperparameter. Section 7.1 discusses for which models it was useful and for which it wasn't. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511979917246,"tcdate":1511979917246,"number":4,"cdate":1511979917246,"id":"HJV5juheG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"HkATqf5xz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re","comment":"4-layer LSTM 24M is not the universal best setting for both PTB and Wikitext-2...so I guess the lesson to be learned here is that we need to consider down-projection as a hyper-parameter, and this bottleneck-structure may or may not be useful."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511978082354,"tcdate":1511978082354,"number":3,"cdate":1511978082354,"id":"Sk5vVdhez","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["~Slav_Petrov1"],"readers":["everyone"],"writers":["~Slav_Petrov1"],"content":{"title":"Question about Transfer Experiment","comment":"Very nice paper!\n\nI was wondering whether you could provide more details on the transfer experiment where you tuned the hyperparameters of the LSTM on the PTB and then used those parameters to train a model on Wikitext-2. Do the tuned hyperparameters differ in interesting ways? Are different parameters needed because the vocabulary size is different or because the corpus size is different? Are there parameters that give decent performance across both tasks? Could you tune on both tasks simultaneously?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511824126406,"tcdate":1511824069768,"number":1,"cdate":1511824069768,"id":"HkATqf5xz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HksEzG5gf","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Whether to use Down-projection?","comment":"Section 7.1 discusses the effect of down-projection in general for various models (depth/budget). Section 7.3 uses a 4-layer LSTM with 24M weights as an example, for which the down-projection is universally suboptimal.\n\nWe agree that Section 7.3 is not very clear on this."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511821874979,"tcdate":1511821874979,"number":2,"cdate":1511821874979,"id":"HksEzG5gf","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Whether to use Down-projection?","comment":"I found two confusing statements from the paper:\n\nSection 7.1:\nDown-projection was found to be very beneficial by the tuner for some depth/budget combinations.\nOn Penn Treebank, it improved results by about 2–5 perplexity points at depths 1 and 2 at 10M, and\ndepth 1 at 24M\n\nSection 7.3:\nOmitting input embedding ratio because\nthe tuner found having a down-projection suboptimal almost non-conditionally for this model\n\nI think the first one is suggesting down-projection to be beneficial and the second one is suggesting that tuner finds it suboptimal..."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1515642446428,"tcdate":1511806356899,"number":2,"cdate":1511806356899,"id":"rJTcBCtxG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Review","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/AnonReviewer3"],"readers":["everyone"],"content":{"title":"With extensive tuning, LSTM beats other new models","rating":"5: Marginally below acceptance threshold","review":"The authors did extensive tuning of the parameters for several recurrent neural architectures. The results are interesting. However the corpus the authors choose are quite small, the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn.\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens. This will use significant resources and is much more difficult, but it's also really valuable, because it's much more close to real world usage of language models. And less tuning is needed for these larger datasets. \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1515642446463,"tcdate":1511532122319,"number":1,"cdate":1511532122319,"id":"S1Mw8jBef","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Review","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/AnonReviewer1"],"readers":["everyone"],"content":{"title":"a useful exercise","rating":"7: Good paper, accept","review":"The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs.  This type of study is important to give perspective to non-standardized performance scores reported across separate publications, and indeed the results here are interesting as they favour relatively simpler structures.\n\nI have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1513595981946,"tcdate":1509115959482,"number":420,"cdate":1509739310608,"id":"ByJHuTgA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/6424ba8e6d85843ae72f1648c3f32362b67a6da2.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]},"nonreaders":[],"replyCount":19,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}