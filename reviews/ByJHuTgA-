{"notes":[{"tddate":null,"ddate":null,"tmdate":1512403548417,"tcdate":1512403548417,"number":4,"cdate":1512403548417,"id":"H14DMe7WG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HkGngSGZz","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Reproducible? Hyper-parameters?","comment":"Indeed we have been asked for the hyperparameter settings on numerous occasions. Originally, we did not provide these details as the main message of the paper was not about the state of the results but model evaluation, but there is another, more fundemental reason too: any single hyperparameter setting would make it easy to compare a derivative work to our well tuned baseline, but at best that could prove that the new model is better (it could never prove that it's worse). More, two new models each evaluated with those hyperparameters would still be incomparable.\n\nFor these reasons, we think that presently there is no way around tuning, and there is limited utility in publishing hyperparameter settings.\n\nThat said, we are working on factoring out the code from a larger system and providing training scripts with the tuned hyperparameters."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512358193128,"tcdate":1512358058535,"number":6,"cdate":1512358058535,"id":"HkGngSGZz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reproducible? Hyper-parameters?","comment":"The main contribution of this paper is to show with extensive hyper-parameter tuning, LSTM can achieve a state of the art result for language modeling. However, it seems that the authors didn't give the concrete hyper-parameters for reproducing their results. This paper is lack of technical novelty and totally of a experimental work; thus the experiment setting is very important. Only showing HP tuning can get state of the art results if not enough, since it is a common sense that hyper-parameter tuning can improve performance for any machine learning models. I think the paper should at least describe their hyper-parameters setting (for example, learning rate, hidden size, dropout ratio, weight decay, etc) for getting their results or releasing the code for the community if possible."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512198994405,"tcdate":1512198994405,"number":5,"cdate":1512198994405,"id":"BkqLXA1Zz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"rJTcBCtxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Too harsh","comment":"PTB is very much a standard baseline in the space that people routinely publish perf-based papers on, so I think it's a little harsh to knock people for publishing on it, especially when they make the effort to get results on multiple larger datasets, such as Wikitext-2.\n\nThis is not a paper about the utility of hyperparameter optimization, the fact that that works has been well established. It's a paper about how hyperparameter optimization wasn't properly used on a bunch of standard benchmarks in this space, which has already proven very valuable. I really don't think it's necessary to request results on MT or ASR "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512038083994,"tcdate":1512038083994,"number":3,"cdate":1512038083994,"id":"r1naAI6eG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"Sk5vVdhez","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Question about Transfer Experiment","comment":"The hyperparameters differ only in boring ways: Wikitext-2 needs a bit less intra layer and state dropout. This is very likely to be due to corpus size. Down-projection sizes are also a bit different due to the vocabulary size mismatch (when there is a down-projection at all).\n\nI'm not sure there are hyperparameters that work well on both, but yes, we could tune for combined (in whatever way) performance on a number of datasets. By doing this, we could learn more about how hyperparameters are best specified so that they are reasonably independent from datasets and also from other hyperparameters."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512222650580,"tcdate":1512003065685,"number":3,"cdate":1512003065685,"id":"HkGW8A2gG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Review","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Important big-picture work in a fast-moving field","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space. Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks.\nThis is a significant result in language modeling and a milestone in deep learning reproducibility research. The paper is clearly motivated and authoritative in its conclusions but it's somewhat lacking in detailed model or experiment descriptions.\n\nSome further points:\n\n- There are several hyperparameters set to the \"standard\" or \"default\" value, like Adam's beta parameter and the batch size/BPTT length. Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified.\n\n- The description of the model is ambiguous on at least two points. First, it wasn't completely clear to me what the down-projection is (if it's simply projecting down from the LSTM hidden size to the embedding size, it wouldn't represent a hyperparameter the tuner can set, so I'm assuming it's separate and prior to the conventional output projection). Second, the phrase \"additive skip connections combining outputs of all layers\" has a couple possible interpretations (e.g., skip connections that jump from each layer to the last layer or (my assumption) skip connections between every pair of layers?).\n\n- Fully evaluating the \"claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation\" would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization.\n\n- The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance.\n\n- The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table. The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces. For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters.\n\n- The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most \"standard\" or \"conventional\" LSTM implementation (e.g., as provided in optimized GPU libraries). In addition to further discussion on this point, this result also suggests evaluating other recently proposed \"minor changes\" to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016)\n\n- It would also have been nice to see a comparison between the variational/recurrent dropout parameterization \"in which there is further sharing of masks between gates\" and the one with \"independent noise for the gates,\" as described in the footnote. There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511990674089,"tcdate":1511990674089,"number":2,"cdate":1511990674089,"id":"r19qHo2gM","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HJV5juheG","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Re","comment":"This is exactly what we did. The presence and size of the down-projection was a tuned hyperparameter. Section 7.1 discusses for which models it was useful and for which it wasn't. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511979917246,"tcdate":1511979917246,"number":4,"cdate":1511979917246,"id":"HJV5juheG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"HkATqf5xz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re","comment":"4-layer LSTM 24M is not the universal best setting for both PTB and Wikitext-2...so I guess the lesson to be learned here is that we need to consider down-projection as a hyper-parameter, and this bottleneck-structure may or may not be useful."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511978082354,"tcdate":1511978082354,"number":3,"cdate":1511978082354,"id":"Sk5vVdhez","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["~Slav_Petrov1"],"readers":["everyone"],"writers":["~Slav_Petrov1"],"content":{"title":"Question about Transfer Experiment","comment":"Very nice paper!\n\nI was wondering whether you could provide more details on the transfer experiment where you tuned the hyperparameters of the LSTM on the PTB and then used those parameters to train a model on Wikitext-2. Do the tuned hyperparameters differ in interesting ways? Are different parameters needed because the vocabulary size is different or because the corpus size is different? Are there parameters that give decent performance across both tasks? Could you tune on both tasks simultaneously?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511824126406,"tcdate":1511824069768,"number":1,"cdate":1511824069768,"id":"HkATqf5xz","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Comment","forum":"ByJHuTgA-","replyto":"HksEzG5gf","signatures":["ICLR.cc/2018/Conference/Paper420/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper420/Authors"],"content":{"title":"Re: Whether to use Down-projection?","comment":"Section 7.1 discusses the effect of down-projection in general for various models (depth/budget). Section 7.3 uses a 4-layer LSTM with 24M weights as an example, for which the down-projection is universally suboptimal.\n\nWe agree that Section 7.3 is not very clear on this."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1511821874979,"tcdate":1511821874979,"number":2,"cdate":1511821874979,"id":"HksEzG5gf","invitation":"ICLR.cc/2018/Conference/-/Paper420/Public_Comment","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Whether to use Down-projection?","comment":"I found two confusing statements from the paper:\n\nSection 7.1:\nDown-projection was found to be very beneficial by the tuner for some depth/budget combinations.\nOn Penn Treebank, it improved results by about 2–5 perplexity points at depths 1 and 2 at 10M, and\ndepth 1 at 24M\n\nSection 7.3:\nOmitting input embedding ratio because\nthe tuner found having a down-projection suboptimal almost non-conditionally for this model\n\nI think the first one is suggesting down-projection to be beneficial and the second one is suggesting that tuner finds it suboptimal..."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512222650622,"tcdate":1511806356899,"number":2,"cdate":1511806356899,"id":"rJTcBCtxG","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Review","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/AnonReviewer3"],"readers":["everyone"],"content":{"title":"With extensive tuning, LSTM beats other new models","rating":"5: Marginally below acceptance threshold","review":"The authors did extensive tuning of the parameters for several recurrent neural architectures. The results are interesting. However the corpus the authors choose are quite small, the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn.\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens. This will use significant resources and is much more difficult, but it's also really valuable, because it's much more close to real world usage of language models. And less tuning is needed for these larger datasets. \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1512222650668,"tcdate":1511532122319,"number":1,"cdate":1511532122319,"id":"S1Mw8jBef","invitation":"ICLR.cc/2018/Conference/-/Paper420/Official_Review","forum":"ByJHuTgA-","replyto":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference/Paper420/AnonReviewer1"],"readers":["everyone"],"content":{"title":"a useful exercise","rating":"7: Good paper, accept","review":"The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs.  This type of study is important to give perspective to non-standardized performance scores reported across separate publications, and indeed the results here are interesting as they favour relatively simpler structures.\n\nI have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]}},{"tddate":null,"ddate":null,"tmdate":1509739313267,"tcdate":1509115959482,"number":420,"cdate":1509739310608,"id":"ByJHuTgA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByJHuTgA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the State of the Art of Evaluation in Neural Language Models","abstract":"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n","pdf":"/pdf/cda1b570cd391b757a732796ff3d269ae5609459.pdf","TL;DR":"Show that LSTMs are as good or better than recent innovations for LM and that model evaluation is often unreliable.","paperhash":"anonymous|on_the_state_of_the_art_of_evaluation_in_neural_language_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the State of the Art of Evaluation in Neural Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByJHuTgA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper420/Authors"],"keywords":["rnn","language modelling"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}