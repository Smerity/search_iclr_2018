{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222536053,"tcdate":1511836046337,"number":3,"cdate":1511836046337,"id":"r1L5FHqeG","invitation":"ICLR.cc/2018/Conference/-/Paper100/Official_Review","forum":"S1lN69AT-","replyto":"S1lN69AT-","signatures":["ICLR.cc/2018/Conference/Paper100/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper analyzes the effectiveness of model pruning for deployment in resource constrained environments. The contribution is marginal but interesting as a summary","rating":"5: Marginally below acceptance threshold","review":"This paper analyzes the effectiveness of model pruning for deployment in resource constrained environments. The contribution is marginal but interesting as a summary\n\n\nThis paper analyzes the effectiveness of model pruning for deployment in resource constrained environments. Contrary to other approaches, this paper assumes there is a computational budget to be meet and the pruning approach should result in a model that fits within that budget.\n\nAccording to the paper there is a contribution of a pruning scheme. To the best of my understanding, the proposal / contribution is minimal or not clearly detailed. My understanding is the approach is equivalent to a L1 pruning where the threshold for pruning is updated over time / training process rather than pushing weights down towards zero (as it is usually done). \nThen, there is a schedule for minimizing the impact of modifying the weights although this has been discussed in related works (see Alvarez and Salzmann 2016). \n\n\nGiven this setup, the paper present a number of comparisons and experimental validations. \n\nThere are several steps that are not clear to me. \n\n1) how does this compare to the low-rank or group sparsity approaches referred in the related work section?\n2) The key here is modifying the thresholds as the training progresses up to a certain point which seems to me quite equivalent to L1 pruning where the regularization term is also affected by the learning rate (therefore having less influence as the training progresses). In this paper though there are heuristics to stop pruning when certain constraints are met. Which is interesting (as pruning will affect the quality and capacity of the network) but also applicable to other methods. Also, as suggested in related works, the pruning becomes negligible after certain number of epochs (therefore there is no real need to stop the process). Any discussion here would be interesting.\n\n3) For me, it is interesting the fact that pruning in an initial stage is too aggressive. However, it also limits the capacity of the network by pruning too much at the begining. I think there are contrary messages in the paper that would be nice to clarify: pruning rapidly at the beginning where redundant connections are abundant and then, there is the need to have a large learning rate to recover from the pruning.\n\n4) I missed a discussion on the Inception model in the experimental settings. \n\n5) If this is based on masks for pruning and performing sparse operations I wonder how does this benefit at inference time since many operations will be faster in a dense matrix multiplication manner. That is why I think would be interesting to do at group level as proposed in some related methods.\n\n6) Tables showing comparisons are not complete. I do not understand why measuring the non-zero parameters if, in the baseline, there is no analysis on how many of these parameters can be actually set to 0 by pruning as a postprocessing step. Please, add explanations on why / how non-zeros are measured in the baseline.\n\n7) More importantly, I think the comparison sparsity vs width is not fair. This is comparing the training process of a model with limited capacity vs a model where the capacity is progressively limited (the pruned). Training regimes should be detailed and properly analyzed for this to be fair. Nevertheless, results are consistent with other approaches listed in the state of the art (pruning while training is a good thing).\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression","abstract":"Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.","pdf":"/pdf/793106e5bf16f84f2cd73b7446a90b6b428d2369.pdf","TL;DR":"We demonstrate that large, but pruned models (large-sparse) outperform their smaller, but dense (small-dense) counterparts with identical memory footprint.","paperhash":"anonymous|to_prune_or_not_to_prune_exploring_the_efficacy_of_pruning_for_model_compression","_bibtex":"@article{\n  anonymous2018to,\n  title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1lN69AT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper100/Authors"],"keywords":["pruning","model sparsity","model compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222536108,"tcdate":1511819947654,"number":2,"cdate":1511819947654,"id":"BkE3cW5gG","invitation":"ICLR.cc/2018/Conference/-/Paper100/Official_Review","forum":"S1lN69AT-","replyto":"S1lN69AT-","signatures":["ICLR.cc/2018/Conference/Paper100/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Informative experiments, unsurprising results.","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThis paper presents a thorough examination of the effects of pruning on model performance.  Importantly, they compare the performance of \"large-sparse\" models (large models that underwent pruning in order to reduce memory footprint of model) and \"small-dense\" models, showing that \"large-sparse\" models typically perform better than the \"small-dense\" models of comparable size (in terms of number of non-zero parameters, and/or memory footprint).  They present results across a number of domains (computer vision, language modelling, and neural machine translation) and model types (CNNs, LSTMs).  They also propose a way of performing pruning with a pre-defined sparsity schedule, simplifying the pruning process in a way which works across domains.  They are able to show convincingly that pruning is an effective way of trading off accuracy for model size (more effective than simply reducing the size of model architecture), although there does come a point where too much sparsity degrades the model performance considerably; this suggests that pruning a medium size model to 80%-90% sparsity is likely better than pruning a larger model to >= 95% sparsity.\n\nReview:\nQuality: The quality of the work is high --- the experiments are extensive and thorough.  I would have liked to see \"small-dense\" vs. \"large-sparse\" comparisons on Inception (only large-sparse results are reported).\n\nClarity: The paper is clearly written, though there is room for improvement. For example, many of the results are presented in a redundant manner (in both tables and figures, where the table and figure are often not next to each other in the document).  Also, it is not clear in several cases exactly which training/heldout/test sets are used, and on which partition of the data the accuracies/BLEU scores/perplexities presented correspond to. A small section (before \"Methods\") describing the datasets/features in detail would be helpful.  Also, it would have probably been nice to explain all of the tasks and datasets early on, and then present all the results at once (NIT: include the plots in paper, and move the tables to an appendix).\n\nOriginality: Although the experiments are informative, the work as a whole is not very original.  The method proposed of using a sparsity schedule to perform pruning is simple and effective, but is a rather incremental contribution. The primary contribution of this paper is its experiments, which for the most part compare known methods.\n\nSignificance: The paper makes a nice contribution, though it is not particularly significant or surprising.  The primary observations are:\n(1) large-sparse is typically better than small-dense, for a fixed number of non-zero parameters and/or memory footprint.\n(2) There is a point at which increasing the sparsity percentage severely degrades the performance of the model, which suggests that there is a \"sweet-spot\" when it comes to choosing the model architecture and sparsity percentage which give the best performance (for a fixed memory footprint).\n\nResult #1 is not very surprising, given that Han et al (2016) were able to show significant compression without loss in accuracy; thus, because one would expect a smaller dense model to perform worse than the large dense model, it would also perform worse than the large sparse model.\nResult #2 had already been seen in Han et al (2016) (for example, in Figure 6).\n\nPros:\n- Very thorough experiments across a number of domains\n\nCons:\n- Methodological contributions are minor.\n- Results are not surprising, and are in line with previous papers.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression","abstract":"Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.","pdf":"/pdf/793106e5bf16f84f2cd73b7446a90b6b428d2369.pdf","TL;DR":"We demonstrate that large, but pruned models (large-sparse) outperform their smaller, but dense (small-dense) counterparts with identical memory footprint.","paperhash":"anonymous|to_prune_or_not_to_prune_exploring_the_efficacy_of_pruning_for_model_compression","_bibtex":"@article{\n  anonymous2018to,\n  title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1lN69AT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper100/Authors"],"keywords":["pruning","model sparsity","model compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222536166,"tcdate":1511802895148,"number":1,"cdate":1511802895148,"id":"rJPMO6YxG","invitation":"ICLR.cc/2018/Conference/-/Paper100/Official_Review","forum":"S1lN69AT-","replyto":"S1lN69AT-","signatures":["ICLR.cc/2018/Conference/Paper100/AnonReviewer3"],"readers":["everyone"],"content":{"title":"pruning efficacy in deep learning","rating":"5: Marginally below acceptance threshold","review":"This paper presents a comparison of model sizes and accuracy variation for pruned version of over-parameterized deep networks and smaller but dense models of the same size. It also presents an algorithm for gradual pruning of small magnitude weight to achieve a pre-determined level of sparsity. The paper demonstrates that pruning of large over-parameterized models leads to better classification compared to smaller dense models of relatively same size. This pruning technique is demonstrated as a modification to TensorFlow on MobileNet, LSTM for PTB dataset and NMT for seq2seq modeling.\n\nThe paper seems mainly a comparison of impact of pruning a large model for various tasks. The novelty in the work seems quite limited mainly in terms of tensorflow implementation of the network pruning using a binary mask. The weights which are masked in the forward pass don't get updated in the backward pass. The fact that most deep networks are inherently over-parametrized seems to be known for quite sometime.\n\nThe experiments are missing comparison with the threshold based pruning proposed by Han etal. to ascertain if the gradual method is indeed better. A computational complexity comparison is also important if the proposed pruning method is indeed effective. In Section 1, the paper claims to arrive at \"the most accurate model\". However, the validation of the claim is mostly empirical and shows that there lies a range of values for increase in sparsity and decrease in prediction accuracy is better compared to other values.\n\nOverall, the paper seems to perform experimental validation of some of the known beliefs in deep learning. The novelty in terms of ideas and insights seems quite limited.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression","abstract":"Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.","pdf":"/pdf/793106e5bf16f84f2cd73b7446a90b6b428d2369.pdf","TL;DR":"We demonstrate that large, but pruned models (large-sparse) outperform their smaller, but dense (small-dense) counterparts with identical memory footprint.","paperhash":"anonymous|to_prune_or_not_to_prune_exploring_the_efficacy_of_pruning_for_model_compression","_bibtex":"@article{\n  anonymous2018to,\n  title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1lN69AT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper100/Authors"],"keywords":["pruning","model sparsity","model compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509559122985,"tcdate":1509559086903,"number":1,"cdate":1509559086903,"id":"ryDNsFvAb","invitation":"ICLR.cc/2018/Conference/-/Paper100/Public_Comment","forum":"S1lN69AT-","replyto":"S1lN69AT-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Supplementary comments on learning structured sparsity in DNNs","comment":"Regarding \" Such techniques perform coarse-grain pruning and depend critically on the structure of the convolutional layers, and may not be directly extensible to other neural network architectures that lack such structural properties (LSTMs for instance)\",  the category of learning structured sparsity [1][2] in DNNs is a more general way than we thought. It is more challenging for DNNs with more sophisticated structures, but it might be possible to use it in LSTMs [3] and to even learn to reduce layers in ResNets [2], if we could figure out the kind of structures we want to learn. \n\n[1] https://arxiv.org/abs/1608.08710\n[2] http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf\n[3] https://arxiv.org/abs/1709.05027"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression","abstract":"Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.","pdf":"/pdf/793106e5bf16f84f2cd73b7446a90b6b428d2369.pdf","TL;DR":"We demonstrate that large, but pruned models (large-sparse) outperform their smaller, but dense (small-dense) counterparts with identical memory footprint.","paperhash":"anonymous|to_prune_or_not_to_prune_exploring_the_efficacy_of_pruning_for_model_compression","_bibtex":"@article{\n  anonymous2018to,\n  title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1lN69AT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper100/Authors"],"keywords":["pruning","model sparsity","model compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739484538,"tcdate":1508973864182,"number":100,"cdate":1509739481880,"id":"S1lN69AT-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1lN69AT-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression","abstract":"Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.","pdf":"/pdf/793106e5bf16f84f2cd73b7446a90b6b428d2369.pdf","TL;DR":"We demonstrate that large, but pruned models (large-sparse) outperform their smaller, but dense (small-dense) counterparts with identical memory footprint.","paperhash":"anonymous|to_prune_or_not_to_prune_exploring_the_efficacy_of_pruning_for_model_compression","_bibtex":"@article{\n  anonymous2018to,\n  title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1lN69AT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper100/Authors"],"keywords":["pruning","model sparsity","model compression","deep learning"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}