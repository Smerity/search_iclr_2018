{"notes":[{"tddate":null,"ddate":null,"tmdate":1512279567397,"tcdate":1512279567397,"number":3,"cdate":1512279567397,"id":"S1vzCb-bz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Review","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["ICLR.cc/2018/Conference/Paper164/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Well written paper, lack of novelty","rating":"5: Marginally below acceptance threshold","review":"This paper has proposed a new method for classifying nodes of a graph. Their method can be used in both semi-supervised scenarios where the label of some of the nodes of the same graph as the graph in training is missing (Transductive) and in the scenario that the test is on a completely new graph (Inductive).\nEach layer of the network consists of feature representations for all of the nodes in the Graph. A linear transformation is applied to all the features in one layer and the output of the layer is the weighted sum of the transformed neighbours (including the node). The attention logit between node i and its neighbour k is calculated by a one layer fully connected network on top of the concatenation of the transformed representation of node i and transformed representation of the neighbour k. They also can incorporate the multi-head attention mechanism and average/concatenate the output of each head.\n\nOriginality:\nAuthors improve upon GraphSAGE by replacing the aggregate and sampling function at each layer with an attention mechanism. However, the significance of the attention mechanism has not been studied in the experiments. For example by reporting the results when attention is turned off (1/|N_i| for every node) and only a 0-1 mask for neighbours is used. They have compared with GraphSAGE only on PPI dataset. I would change my rating if they show that the 33% gain is mainly due to the attention in compare to other hyper-parameters. \nAlso, in page 4 authors claim that GraphSAGE is limited because it samples a neighbourhood of each node and doesn't aggregate over all the neighbours in order to keep its computational footprint consistent. However, the current implementation of the proposed method is computationally equal to using all the vertices in GraphSAGE.\n\nPros:\n- Interesting combination of attention and local graph representation learning. \n- Well written paper. It conveys the idea clearly.\n- State-of-the-art results on three datasets.\n\nCons:\n- When comparing with spectral methods it would be better to mention that the depth of embedding propagation in this method is upper-bounded by the depth of the network. Therefore, limiting its adaptability to broader class of graph datasets. \n- Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1512222583046,"tcdate":1511951873050,"number":2,"cdate":1511951873050,"id":"ryFW0bhlM","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Review","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["ICLR.cc/2018/Conference/Paper164/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good basic idea with several weaknesses in the technical exposition and the experiments","rating":"5: Marginally below acceptance threshold","review":"This is a paper about learning vector representations for the nodes of a graph. These embeddings can be used in downstream tasks the most common of which is node classification.\n\nSeveral existing approaches have been proposed in recent years. The authors provide a fair and almost comprehensive  discussion of state of the art approaches. There are a couple of exception that have already been mentioned in a comment from Thomas Kipf and Michael Bronstein. A more precise discussion of the differences between existing approaches (especially MoNets) should be a crucial addition to the paper. You provide such a  comparison in your answer to Michael's comment. To me, the comparison makes sense but it also shows that the ideas presented here are less novel than they might initially seem. The proposed method introduces two forms of (simple) attention. Nothing groundbreaking here but still interesting enough and well explained. It might also be a good idea to compare your method to something like LLE (locally linear embedding). LLE also learns a weight for each of neighbors of a node and computes the embedding as a weighted average of the neighbor embeddings according to these weights. Your approach is different since it is learned end-to-end (not in two separate steps) and because it is applicable to arbitrary graphs (not just graphs where every node has exactly k neighbors as in LLE). Still, something to relate to. \n\nPlease take a look at the comment by Fabian Jansen. I think he is on to something. It seems that the attention weight (from i to j) in the end is only a normalization operation that doesn't take the embedding of node i into account.  \n\nThere are two  issues with the experiments.\n\nFirst, you don't report results on Pubmed because your method didn't scale. Considering that Pubmed has less than 20,000 nodes this shows a clear weakness of your approach. You write (in an answer to a comment) that it *should* be parallelizable but somehow you didn't make it work. We have to, however, evaluate the approach on what it is able to do at the moment. Having a complexity that is quadratic in the number of nodes is terrible and one of the major reasons learning with graphs has moved from kernels to neural approaches. While it is great that you acknowledge this openly as a weakness, it is currently not possible to claim that your method scales to even moderately sized graphs. \n\nSecond, the experimental set-up on the Cora and Citeseer data sets should be properly randomized. As Thomas pointed out, for graph data the variance can be quite high. For some split the method might perform really well and less well for others. In your answer titled \"Requested clarifications\" to a different comment you provide numbers randomized over 10 runs. Did you randomize the parameter initialization only or also the the train/val/test splits? If you did the latter, this seems reasonable. In Kipf et al.'s GCN paper this is what was done (not over 100 splits as some other commenter claimed. The average over 100 runs  pertained to the ICA method only.) ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1512222583093,"tcdate":1511772080755,"number":1,"cdate":1511772080755,"id":"BJth1UKlf","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Review","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["ICLR.cc/2018/Conference/Paper164/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Very interesting work, but the graph structure is not fully exploited","rating":"7: Good paper, accept","review":"The paper introduces a neural network architecture to operate on graph-structured\ndata named Graph Attention Networks.\nKey components are an attention layer and the possibility to learn how to\nweight different nodes in the neighborhood without requiring spectral decompositions\nwhich are costly to be computed.\n\nI found the paper clearly written and very well presented. I want to thank\nthe author for actively participating in the discussions and in clarifying already\nmany of the details that I was missing.\n\nAs also reported in the comments by T. Kipf I found the lack of comparison to previous\nworks on attention and on constructions of NN for graph data are missing.\nIn particular MoNet seems a more general framework, using features to compute node\nsimilarity is another way to specify the \"coordinate system\" for convolution.\nI would argue that in many cases the graph is given and that one would have\nto exploit its structure rather than the simple first order neighbors structure.\n\nI feel, in fact, that the paper deals mainly with \"localized metric-learning\" rather than\nusing the information in the graph itself. There is no\nexplicit usage of the graph beyond the selection of the local neighborhood.\nIn many ways when I first read it I though it would be a modified version of\nmemory networks (which have not been cited). Sec. 2.1 is basically describing\na way to learn a matrix W so that the attention layer produces the weights to be\nused for convolution, or the relative coordinate system, which is to me a\nmemory network like construction, where the memory is given by the neighborhood.\n\nI find the idea to use the multi-head attention very interesting, but one should\nconsider the increase in number of parameters in the experimental section.\n\nI agree that the proposed method is computationally efficient but the authors\nshould keep in mind that parallelizing across all edges involves lot of redundant\ncopies (e.g. in a distributed system) as the neighborhoods highly overlap, at\nleast for interesting graphs.\n\nThe advantage with respect to methods that try to use LSTM in this domain\nin a naive manner is clear, however the similarity function (attention) in this\nwork could be interpreted as the variable dictating the visit ordering.\n\nThe authors seem to emphasize the use of GPU as the best way to scale their work\nbut I tend to think that when nodes have varying degrees they would be highly\nunused. Main reason why they are widely used now is due to the structure in the\nrepresentation of convolutional operations.\nAlso in case of sparse data GPUs are not the best alternative.\n\nExperiments are very well described and performed, however as explained earlier\nsome comparisons are needed.\nAn interesting experiment could be to use the attention weights as adjacency\nmatrix for GCN.\n\nOverall I liked the paper and the presentation, I think it is a simple yet\neffective way of dealing with graph structure data. However, I think that in\nmany interesting cases the graph structure is relevant and cannot be used\njust to get the neighboring nodes (e.g. in social network analysis).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1511521004713,"tcdate":1511521004713,"number":8,"cdate":1511521004713,"id":"Hyresurlz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["~Fabian_Jansen1"],"readers":["everyone"],"writers":["~Fabian_Jansen1"],"content":{"title":"Spurious weights","comment":"In equation 3 the coefficients are calculated as a softmax. However, it appears that the first half of the weight vector \"a\" beloning to the node \"i\" under consideration drops out of the equation and is thus not used nor trained.\n\nFrom equation 3:\nalpha(i,j) = exp(a * [W*h(i)||W*h(j)]) / Sum(k) exp(a * [W*h(i)||W*h(j)])\n\nWriting vector a explicitly in two parts as a = [a(1)||a(2)]:\n\nalpha(i,j) = exp([a(1)||a(2)] * [W*h(i)||W*h(j)]) / Sum(k) exp([a(1)||a(2)] * [W*h(i)||W*h(j)])\n                 = exp(a(1)*W*h(i) + a(2)*W*h(j) ) / Sum(k) exp(a(1)*W*h(i) + a(2)*W*h(k) ) \n                 = exp(a(1)*W*h(i) ) exp(a(2)*W*h(j) ) / Sum(k) exp(a(1)*W*h(i)) exp(a(2)*W*h(k) ) \n                 = exp(a(2)*W*h(j) ) / Sum(k) exp(a(2)*W*h(k) ) \n\nThe a(1) part drops out"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1511378337846,"tcdate":1511376393587,"number":6,"cdate":1511376393587,"id":"ryMGLHXxz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Comment","forum":"rJXMpikCZ","replyto":"Sy2YY-MlM","signatures":["ICLR.cc/2018/Conference/Paper164/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper164/Authors"],"content":{"title":"Further experimental results, and clarification of complexity","comment":"Thank you for your comments and queries on the complexity and experimental setup!\n\nWe fully agree that 100-run performance would give the fairest comparison to the baselines. Accordingly, results after 100 runs of our model largely follow the trend of the 10-run result:\nCora: 83.0 +- 0.7 (maximum 84.3%)\nCiteseer: 72.6 +- 0.8 (maximum 74.2%)\n\nTo highlight: we have used a 1-run result (without any additional runs), rather than the best result, in the original writeup as submitted. Our N-run results already showed it is possible to achieve better single-run results than 83.3% and 74.0%, respectively. \n\nOur particular choice of attentional mechanism, a, is explicitly written out in Equation (3), clarified by the text immediately preceding this Equation, and illustrated by Figure 1 (left). It may be expressed as:\n\na(x, y) = a^T[x||y]\nwhere a is a learnable weight vector, || is concatenation, and ^T is transposition.\n\nThat is, it corresponds to a simple, linear, single-layer MLP with a single output neuron,  acting on the concatenated features of the two nodes to compute the attention coefficient---largely similar to the original attention mechanism of Bahdanau et al.\n\nTheoretically, our model needs to compute the attentional coefficients e_{i, j} only across the edges of the graph, i.e. O(|E|) computations of a single-layer MLP overall, which are independent, and thus can be parallelised. This is on par with other baseline techniques (such as GCNs or Chebyshev Nets). Taking into account that we need to perform a matrix multiplication on each node's features (to transform the feature space from F to F' features), we may express the overall computational complexity of a single attention head's computations as O(|V| x F x F' + |E| x F'), where F is the input feature count, and F' the output feature count---keeping in mind that many of these computations are trivially parallelisable on a GPU.\n\nThe P(N, 2) or C(N, 2) values mentioned in your comment would correspond to a dense graph (E ~ V^2), where O(V^2) complexity is unavoidable regardless of which graph technique is selected.\n\nUnfortunately, even though the softmax computation on every node should be trivially parallelisable, we were unable to make advantage of our tensor manipulation framework to achieve this parallelisation, while retaining a favourable storage complexity (as its softmax function is optimised for same-sized vectors). This implied that we had to reuse the technique from the self-attention paper of Vaswani et al., wherein attention is computed over all pairs of nodes, with a bias value of -inf inserted into non-connected pairs of nodes. This required a storage complexity of O(V^2), and caused OOM errors on our GPU when the Pubmed dataset was provided---which is the reason for the lack of results on Pubmed. \n\nWe were, however, able to run our model on the PPI dataset, which has 3x the number of nodes and 18x the number of edges of Pubmed. We were able to do this as PPI is split into 24 disjoint graphs (with test graphs entirely unseen), allowing us to effectively batch the softmax operation. This should still demonstrate evidence of the fact our model is capable of retaining competitive performance when scaling up to larger graphs (and, perhaps more critically, that it is capable of inductive, as well as transductive, generalisation).\n\nWe thank you once again for your comments, and will be sure to include some  aspects of the above discussion in a revised version of the paper!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1511295364243,"tcdate":1511295364243,"number":6,"cdate":1511295364243,"id":"Sy2YY-MlM","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Computational Complexity and Experimental Results","comment":"The proposed GAT needs to compute e_{i,j} for arbitrary i, j in the graph. Thus the total number of e_{i,j} is P(N,2) for directed graph and C(N,2) for undirected graph. When the graph size (N) increases, the computational complexity increases quickly. Can the author show the computational complexity and compare it with existing methods? Also, what is the definition of attentional mechanism \"a\" in eqn (1)?\n\nIn Kipf's GCN paper, the performance is claimed being the results of an average of 100 random initializations. However, this paper using the best performance to compare with others' average performance is not reasonable. From the last comment we are informed that the average performance for 10 runs of the same model with different random seeds are (with highlighted standard deviations): Cora: 83.0 +- 0.6 (with a maximum of 83.9%) Citeseer: 72.7 +- 0.7 (with a maximum of 74.2%). For a fair comparison with the baseline, the authors may provide the average performance for 100 runs.\n\nAlso, I am curious of the reason why the author did not show the Pubmed dataset results, which is used with Cora and Citeseer togethor in existing graph CNN works. Pubmed's graph size is much larger than the other two, so it is an important dataset to test the proposed method and compare with the baselines."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510962714468,"tcdate":1510962714468,"number":5,"cdate":1510962714468,"id":"SkMm8gaJM","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Comment","forum":"rJXMpikCZ","replyto":"HyL2UVDJG","signatures":["ICLR.cc/2018/Conference/Paper164/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper164/Authors"],"content":{"title":"Further baseline experimental results","comment":"First of all, thank you very much for your thorough comment and thoughts on the experimental setup! \n\nWe directly quoted back the baseline results originally reported, under the assumption that appropriate hyperparameter optimisation had already been performed on them. However, we have now performed further experiments on the baseline techniques, in line with some of your recommendations, and the results of this study still point to an outperformance by GAT models. We focused on the experiments that were easily runnable without significantly modifying the codebases at https://github.com/tkipf/gcn and https://github.com/williamleif/GraphSAGE. Our findings can be summarised as follows, and will be highlighted in an updated version of the paper:\n\nCora/Citeseer: We have trained the GCN and Chebyshev (K = 2 and K = 3) models, with a hidden size of 64, with ReLU and ELU activations, for 10 runs each. Note that we did not need to add an additional input linear layer (as suggested by the comment), given that the code at https://github.com/tkipf/gcn/blob/master/gcn/layers.py#L176 already does this.\n\nThe best-performing models achieved the following mean +- std results:\n\nCora: 81.5 +- 0.7 (Cheby2 ReLU)\nCiteseer: 71.0 +- 0.3 (Cheby3 ELU and GCN ReLU)\n\nThese results are still outperformed by both our model's single-run performance (as in our paper) and 10-run performance (as in our reply to a previous comment below).\n\nPPI: Firstly, we would like to note that our model actually considers three-hop neighbourhoods (rather than four), and that the GraphSAGE models feature skip connections---in fact, our model only has one skip connection in total whereas GraphSAGE has a skip connection for every aggregation layer (the concat operation in Line 5 of Algorithm 1 in https://arxiv.org/abs/1706.02216). The authors of GraphSAGE have, in fact, highlighted that this skip connection was critical to their performance gains.\n\nIn line with this, we have tested a wide range of larger GraphSAGE models with three aggregation layers, with both ReLU and ELU activations, spanning feature counts up to 1024. Specially, for the third layer we focused on feature counts of 121 and 726, as our GAT model’s final aggregation layer also acts as a classification layer, computing 6 * 121 features which are then pointwise-averaged. Some of these combinations resulted in OOM errors, with the best performing one being a GraphSAGE-LSTM model computing [512, 512, 726] features, with 128 features being used for aggregating neighbourhoods, using the ELU activation. This approach achieved a micro-F1 score of 0.648 on PPI. We have found it beneficial to let the model train for more epochs compared to the original authors' work, and were able to reach a maximal test micro-F1 score of 0.768 after doing so.\n\nThis is still outperformed by a significant margin by both our single-model result (reported in the paper) and our 10-run result (reported in a reply to a previous comment below).\n\nFinally, as pointed out by the comment, we report that, for a pre-trained GAT model on Cora, the mean attentional coefficients in the hidden layer (across all eight attention heads) are 0.275 for the self-edge and 0.185 for the neighbourhood edges."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510586030159,"tcdate":1510586030159,"number":5,"cdate":1510586030159,"id":"HyL2UVDJG","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Results are inconclusive without sufficient baseline experiments","comment":"The experiment section is not clearly indicative of what attributed to the improvement in results. \n\nFor experiments on Cora and Citeseer datasets, the authors have used the same train/test/val split as used in Kipf&Welling, ICLR'17.  Though the authors have used the same split, it is not sufficient to compare them on the original reported results from the baseline papers to indicate that the attention mechanism alone is providing improved results without analysing the \ndifferences in architecture and experiment results. The proposed model besides the proposed attention mechanism has additional learning capacity as the model introduces an additional linear layer for input projection and has more number of hidden units per layer. Kipf's GCN has reported results with hidden units set to 16 whereas the size of the attention feature size (8*8) is 64. It is not clear how much improvement does the increased hidden size provides. It would be clear if the authors report results for the GCN and Chebyshev model with and additional input linear layer and hidden size set to 64. And also report the effect of use of elu activation functions instead of RELU as previously mentioned by Thomas Kipf in his comment.\n\nSimilarly in the inductive learning task the attention feature size is 1024 whereas the max feature size for the GraphSage models are 256. The GraphSage results are reported with partial neighborhood information from 2 hop neighbors. Whereas, in this paper the authors have used skip connections (also used in GCN) and the complete neighborhood information from 4-hop neighbors. No analysis of the effect of these two components are mentioned. It is not clear how the proposed model would perform under the same setting as in GraphSage (2-hop and partial neighborhood) or how much improvement would GraphSage obtain with skip connections and 4-hop information. On a side note, as GraphSage is not efficient to work with complete neighborhood, the authors can use Kipf's implementation of GCN and Chebyshev to report results on PPI with 4-hop complete neighborhood information and skip-connection. With these additional experiments, it would be clear how much improvement does the proposed attention mechanism exactly provides.\n\nFurther, I'm surprised that attention mechanism can provide improved results especially with Cora and Citeseer where the average degree is less than 2. These two datasets are highly homophilous. It would be useful if the authors report the mean attention score for the self edge and its neighbors.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510188071937,"tcdate":1510188071937,"number":4,"cdate":1510188071937,"id":"BJg4NXZyz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Comment","forum":"rJXMpikCZ","replyto":"HJk72Fekz","signatures":["ICLR.cc/2018/Conference/Paper164/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper164/Authors"],"content":{"title":"Utilised splits, and a comment on ELU","comment":"Thank you for the kind feedback, the plethora of useful related work, and the queries!\n\nWe have already noted the relationship of our work to MoNets and VAIN (as given in our replies to the authors below). The work on Neighbourhood attention is also relevant, and will also be cited appropriately alongside the related work by Santoro et al. (which we already cited in the original version). Also, the improved neighbourhood attention might hold interesting future work avenues (such as introducing an edge-wise 'message passing' network whose outputs one can attend over).\n\nWe have utilised exactly the same training/validation/testing splits for Cora and Citeseer as the ones used in Kipf & Welling. This information should be already highlighted in the description of our experimental setup. In fact, for extracting the dataset we use exactly the code provided at: https://github.com/tkipf/gcn/blob/master/gcn/utils.py\n\nWe have found early on in our experiments that the properties of the ELU function are convenient for simplifying the optimisation process of our method - reducing the amount of effort invested in our hyperparameter search."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510162222756,"tcdate":1510161882511,"number":3,"cdate":1510161882511,"id":"HJMkC2xyz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Comment","forum":"rJXMpikCZ","replyto":"r1KPYM0Cb","signatures":["ICLR.cc/2018/Conference/Paper164/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper164/Authors"],"content":{"title":"Relationship to MoNets","comment":"Thank you very much for your comment, and pointing us to this work! MoNets are definitely a highly relevant piece of related work to ours, and therefore they will receive appropriate treatment and a citation in the subsequent revision of our paper.\n\nWe find that our work can indeed be reformulated as a particular case of the MoNet framework. Namely, setting the pseudo-coordinate function to be \n\nu(x, y) = f(x) || f(y)\n(where f(x) represent (potentially MLP-transformed) features of node x, and || is concatenation) \n\nand the weight function to be \n\nw_j(u) = softmax(MLP(u))\n(with the softmax performed over the entire neighbourhood of a node)\n\nwould make the patch operator similar to ours. \n\nThis could be interpreted as a way of integrating the ideas of self-attentional interfaces (such as the work of Vaswani et al.: https://arxiv.org/abs/1706.03762 ) into the patch-operator framework presented by MoNet. Specially, and in comparison to the previously specified MoNet frameworks, our model uses node features for similarity computations, rather than the node's structural properties (such as their degrees in the graph). This, in combination with using a multilayer perceptron for computing the attention coefficients, allows the network more freedom in the way it chooses to express similarities between different nodes in the graph, irrespective of the local topological properties. The addition of the softmax function ensures that these coefficients will be well-behaved (and potentially probabilistically interpretable).\n\nLastly, our work also features a few stabilising additions to the attention model (to better cope with the smaller training set sizes), such as applying dropout on the computed attention coefficients, exposing the network to a stochastically sampled neighbourhood on every iteration. Such regularisation techniques might be harder to interpret or justify when structural properties are used as pseudo-coordinates, as stochastically dropping neighbours changes e.g. the node degrees.\n\nTo avoid any potential confusion for other readers of this discussion, we would like to also highlight that the arXiv link for MoNets that we referred to is: https://arxiv.org/pdf/1611.08402.pdf "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510160886457,"tcdate":1510160886457,"number":2,"cdate":1510160886457,"id":"H1Cechgkz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Comment","forum":"rJXMpikCZ","replyto":"ryGdFlCCb","signatures":["ICLR.cc/2018/Conference/Paper164/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper164/Authors"],"content":{"title":"Relationship to VAIN","comment":"Thank you for the positive feedback, as well as bringing your paper to our attention! We have found it to be very interesting related work, and will be sure to cite it in a subsequent version of our paper (most likely alongside our existing citation of the work of Santoro et al.: https://arxiv.org/abs/1706.01427 ). We highlight a few comparisons between our approaches that are worth mentioning below.\n\nWe compute attention coefficients using an edge-wise mechanism, rather than a node-wise mechanism followed by an edge-wise distance metric. This is suitable for a graph setting (with neighbourhoods specified by the graph structure), because we can only evaluate this mechanism across the edges that are in the graph (easing the computational load). In a multi-agent setting (as the one explored by your paper), there may not be an immediately-obvious such structure, and this is why one has to resort to specifying interactions across all pairs of agents (at least initially, before the kind of pruning by way of k-NN could be performed). As we focus on making per-node predictions in graphs, we also found it useful for a node to attend over its own features, which your proposed model explicitly disallows. Our work also features a few stabilising additions to the attention model (to better cope with the smaller training set sizes), such as multi-head attention and dropout on the computed attention coefficients."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510151411058,"tcdate":1510149143115,"number":4,"cdate":1510149143115,"id":"HJk72Fekz","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["~Thomas_N._Kipf1"],"readers":["everyone"],"writers":["~Thomas_N._Kipf1"],"content":{"title":"Effect of ELU activation function and further references","comment":"Very nicely presented work.\n\nI was wondering how much influence the ELU activation function had on your results? It looks like all baseline models make use of ReLU for easier comparison. \n\nIn terms of datasets: did you use the same splits for Cora and Citeseer as in previous work (e.g. Kipf&Welling, ICLR2017), or did you merely use the same size of split and resample? In my experience, the choice of train/val/test splits can have a very significant impact on test performance (it is possible to get up to 84% accuracy on Cora using a lucky train/val/test split with earlier models as well).\n\nAs mentioned by others, you might want to refer to earlier work on attention mechanisms for graph neural networks or using multiple basis functions (\"attention heads\"), such as in the MoNet paper. Here are some references:\n\nhttps://arxiv.org/abs/1611.08402  - MoNets: looks like your model is a special case of theirs, they also compare on the same kinds of tasks but avoid scalability issues by not having the softmax attention formalism\nhttps://arxiv.org/abs/1703.07326 - Introduces \"Neighborhood attention\"\nhttps://arxiv.org/abs/1706.06383 - Improved version of \"Neighborhood attention\"\nhttps://arxiv.org/abs/1706.06122 - Attention mechanism in a graph neural net model for multi-agent reinforcement learning"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1509988704966,"tcdate":1509988704966,"number":3,"cdate":1509988704966,"id":"r1KPYM0Cb","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["~Michael_Bronstein1"],"readers":["everyone"],"writers":["~Michael_Bronstein1"],"content":{"title":"particular case of mixture model net?","comment":"The model you propose looks very similar to mixture model networks (MoNet):\n\nhttp://arxiv.org/pdf/1611.0840.pdf (appeared as oral at CVPR 2017)\n\nwhich you did not cite. \n\nMoNet model performed better than GCN and Chebyshev net (both of which can be considered as a particular instance thereof). What is the difference/similarity of your approach compared to MoNet? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1510092427464,"tcdate":1509987972438,"number":1,"cdate":1509987972438,"id":"S12tIMACW","invitation":"ICLR.cc/2018/Conference/-/Paper164/Official_Comment","forum":"rJXMpikCZ","replyto":"B16obCO0W","signatures":["ICLR.cc/2018/Conference/Paper164/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper164/Authors"],"content":{"title":"Requested clarifications","comment":"Thank you very much for your comment - we acknowledge that this detail about our experimental setup was not sufficiently clear in the submitted version and are more than happy to address it appropriately in a subsequent revision.\n\nWe have picked the best hyperparameter configuration considering the validation score on both Cora and PPI, and then reused the Cora architectural hyperparameters on Citeseer. Once the hyperparameters were in place, the early-stopped models were then evaluated on the test set once, and the obtained results are the ones reported in the paper.\n\nWe agree that reporting the averaged model performance would be useful, and we will do this in an updated version of the paper. The results after 10 runs of the same model with different random seeds are (with highlighted standard deviations):\n\nCora: 83.0 +- 0.6 (with a maximum of 83.9%)\nCiteseer: 72.7 +- 0.7 (with a maximum of 74.2%)\nPPI: 0.952 +- 0.006 (with a maximum of 0.966) \n\nThese correspond to state-of-the-art results across all three datasets."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1509980522278,"tcdate":1509980522278,"number":2,"cdate":1509980522278,"id":"ryGdFlCCb","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["~Yedid_Hoshen1"],"readers":["everyone"],"writers":["~Yedid_Hoshen1"],"content":{"title":"Attentional Multi-agent Predictive Modeling","comment":"Interesting work! \n\nI've done some related work, that will be presented at NIPS: https://arxiv.org/abs/1706.06122\nI wonder how the two works compare?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1509642660658,"tcdate":1509642660658,"number":1,"cdate":1509642660658,"id":"B16obCO0W","invitation":"ICLR.cc/2018/Conference/-/Paper164/Public_Comment","forum":"rJXMpikCZ","replyto":"rJXMpikCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Clarification for the experimental setup","comment":"In the main results in the accuracies in Table 2 and F1 scores on Table 3, are those numbers averaged over multiple training instances of the model with random initializations or are they the numbers corresponding to the best performing model? In the former case, how many random instances is it averaged over? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]}},{"tddate":null,"ddate":null,"tmdate":1509739449978,"tcdate":1509043467238,"number":164,"cdate":1509739447320,"id":"rJXMpikCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJXMpikCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Graph Attention Networks","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved state-of-the-art results across three established transductive and inductive graph benchmarks: the Cora and Citeseer citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs are entirely unseen during training).","pdf":"/pdf/a76dcc624f28889b89be89ed3c2bda7f4106acd6.pdf","TL;DR":"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task.","paperhash":"anonymous|graph_attention_networks","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Attention Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper164/Authors"],"keywords":["Deep Learning","Graph Convolutions","Attention","Self-Attention"]},"nonreaders":[],"replyCount":16,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}