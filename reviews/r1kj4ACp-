{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642380749,"tcdate":1512107786962,"number":3,"cdate":1512107786962,"id":"Sy7fJuCxM","invitation":"ICLR.cc/2018/Conference/-/Paper106/Official_Review","forum":"r1kj4ACp-","replyto":"r1kj4ACp-","signatures":["ICLR.cc/2018/Conference/Paper106/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper presented a theoretical result for the generalization of DNN using the maximum entropy principle.","rating":"6: Marginally above acceptance threshold","review":"The presentation of the paper is crisp and clear. The problem formulation is explained clearly and it is well motivated by theorems. It is a theoretical papers and there is no experimental section. This is the only drawback for the paper as the claims is not supported by any experimental section. The author could add some experiments to support the idea presented in the paper.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Deep Learning Generalization by Maximum Entropy","abstract":"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","pdf":"/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf","TL;DR":"We prove that DNN is a recursively approximated solution to the maximum entropy principle.","paperhash":"anonymous|understanding_deep_learning_generalization_by_maximum_entropy","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Learning Generalization by Maximum Entropy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kj4ACp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper106/Authors"],"keywords":["generalization","maximum entropy","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642380788,"tcdate":1512016447290,"number":2,"cdate":1512016447290,"id":"SyDSqb6gz","invitation":"ICLR.cc/2018/Conference/-/Paper106/Official_Review","forum":"r1kj4ACp-","replyto":"r1kj4ACp-","signatures":["ICLR.cc/2018/Conference/Paper106/AnonReviewer3"],"readers":["everyone"],"content":{"title":"extremely hard to follow, needs major revision","rating":"3: Clear rejection","review":"The paper aims to provide a view of deep learning from the perspective of maximum entropy principle.  I found the paper extremely hard to follow and seemingly incorrect in places.  Specifically:\na) In Section 2, the example given to illustrate underfitting and overfitting states that the 5-order polynomial obviously overfits the data.  However, without looking at the test data and ensuring the fact that it indeed was not generated by a 5-order polynomial, I don’t see how such a claim can be made.\nb) In Section 2 the authors state “Imposing extra data hypothesis actually violates the ME principle and degrades the model to non-ME model.” … Statements like this need to be made much clearer, since imposing feature expectation constraints (such as Eq. (3) in Berger et al. 1996) is a perfectly legitimate construct in ME principle.\nc) The opening paragraph of Section 3 is quite unclear; phrases like “how to identify the equivalent feature constraints and simple models” need to be made precise, it is not clear to me what authors mean by this.\nd) I’m not able to really follow Definition 1, perhaps due to unclear notation.  It seems to state that we need to have P(X,Y) = P(X,\\hat{Y}), and if that’s the case not clear what more can be accomplished by maximizing conditional entropy H(\\hat{Y}|X).  Also, there is a spurious w_i in Definition 1.\ne) Definition 2.  Not clear what is meant by notation E_{P(T,Y)}.\nf) Definition 3 uses t_i(x) without defining those, and I think those are different from t_i(x) defined in Definition 2.\n\nI think the paper needs to be substantially revised and clarified before it can be published at ICLR.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Deep Learning Generalization by Maximum Entropy","abstract":"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","pdf":"/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf","TL;DR":"We prove that DNN is a recursively approximated solution to the maximum entropy principle.","paperhash":"anonymous|understanding_deep_learning_generalization_by_maximum_entropy","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Learning Generalization by Maximum Entropy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kj4ACp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper106/Authors"],"keywords":["generalization","maximum entropy","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642380830,"tcdate":1511983949252,"number":1,"cdate":1511983949252,"id":"HkBIjt2xz","invitation":"ICLR.cc/2018/Conference/-/Paper106/Official_Review","forum":"r1kj4ACp-","replyto":"r1kj4ACp-","signatures":["ICLR.cc/2018/Conference/Paper106/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"2: Strong rejection","review":"Summary:\n\nThis paper presents a derivation which links a DNN to recursive application of\nmaximum entropy model fitting. The mathematical notation is unclear, and in\none cases the lemmas are circular (i.e. two lemmas each assume the other is\ncorrect for their proof). Additionally the main theorem requires complete\nindependence, but the second theorem provides pairwise independence, and the\ntwo are not the same.\n\nMajor comments:\n\n- The second condition of the maximum entropy equivalence theorem requires\n  that all T are conditionally independent of Y. This statement is unclear, as\nit could mean pairwise independence, or it could mean jointly independent\n(i.e. for all pairs of non-overlapping subsets A & B of T I(T_A;T_B|Y) = 0).\nThis is the same as saying the mapping X->T is making each dimension of T\northogonal, as otherwise it would introduce correlations. The proof of the\ntheorem assumes that pairwise independence induces joint independence and this\nis not correct.\n\n- Section 4.1 makes an analogy to EM, but gradient descent is not like this\n  process as all the parameters are updated at once, and only optimised by a\nsingle (noisy) step. The optimisation with respect to a single layer is\nconditional on all the other layers remaining fixed, but the gradient\ninformation is stale (as it knows about the previous step of the parameters in\nthe layer above). This means that gradient descent does all 1..L steps in\nparallel, and this is different to the definition given.\n\n- The proofs in Appendix C which are used for the statement I(T_i;T_j) >=\n  I(T_i;T_j|Y) are incomplete, and in generate this statement is not true, so\nrequires proof.\n\n- Lemma 1 appears to assume Lemma 2, and Lemma 2 appears to assume Lemma 1.\n  Either these lemmas are circular or the derivations of both of them are\nunclear.\n\n- In Lemma 3 what is the minimum taken over for the left hand side? Elsewhere\n  the minimum is taken over T, but T does not appear on the left hand side.\nExplicit minimums help the reader to follow the logic, and implicit ones\nshould only be used when it is obvious what the minimum is over.\n\n- In Lemma 5, what does \"T is only related to X\" mean? The proof states that\n  Y -> T -> X forms a Markov chain, but this implies that T is a function of\nY, not X.\n\nMinor comments:\n\n- I assume that the E_{P(X,Y)} notation is the expectation of that probability\n  distribution, but this notation is uncommon, and should be replaced with a\nmore explicit one.\n\n- Markov is usually romanized with a \"k\" not a \"c\".\n\n- The paper is missing numerous prepositions and articles, and contains\n  multiple spelling mistakes & typos.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Deep Learning Generalization by Maximum Entropy","abstract":"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","pdf":"/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf","TL;DR":"We prove that DNN is a recursively approximated solution to the maximum entropy principle.","paperhash":"anonymous|understanding_deep_learning_generalization_by_maximum_entropy","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Learning Generalization by Maximum Entropy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kj4ACp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper106/Authors"],"keywords":["generalization","maximum entropy","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739481195,"tcdate":1508988054823,"number":106,"cdate":1509739478537,"id":"r1kj4ACp-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1kj4ACp-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Understanding Deep Learning Generalization by Maximum Entropy","abstract":"Deep learning achieves remarkable generalization capability with overwhelming number of model parameters. Theoretical understanding of deep learning generalization receives recent attention yet remains not fully explored. This paper attempts to provide an alternative understanding from the perspective of maximum entropy. We first derive two feature conditions that softmax regression strictly apply maximum entropy principle. DNN is then regarded as approximating the feature conditions with multilayer feature learning, and proved to be a recursive solution towards maximum entropy principle. The connection between DNN and maximum entropy well explains why typical designs such as shortcut and regularization improves model generalization, and provides instructions for future model development.","pdf":"/pdf/6eec11d743e000e9a91ad068a56a806b77ccb218.pdf","TL;DR":"We prove that DNN is a recursively approximated solution to the maximum entropy principle.","paperhash":"anonymous|understanding_deep_learning_generalization_by_maximum_entropy","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Deep Learning Generalization by Maximum Entropy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kj4ACp-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper106/Authors"],"keywords":["generalization","maximum entropy","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}