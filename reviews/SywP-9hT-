{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222702534,"tcdate":1511811937500,"number":3,"cdate":1511811937500,"id":"BJYws1cgf","invitation":"ICLR.cc/2018/Conference/-/Paper62/Official_Review","forum":"SywP-9hT-","replyto":"SywP-9hT-","signatures":["ICLR.cc/2018/Conference/Paper62/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Experimental results; differences to Hartono et al., 2015","rating":"4: Ok but not good enough - rejection","review":"The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized. The learning dynamics defined for the network results in specific update equations of the weights W (Eqn. 14), which combine elements of supervised learning and self-organizing maps (SOMs). The weights thus change according to the imposed neighborhood relationship and depending on the class labels.\n\nWhat I like about the approach is the investigation of the interplay between unsupervised and hierarchical supervised learning in a biological context. I agree with the authors that an integrated view of self-organization and learning across layers is presumably required to better understand biological learning. The general methodology also makes sense to me. However, I do have concerns including two major concerns: (A) delimitation of results from earlier work; (B) numerical results (especially Tab. 1).\n\n(A) The paper derives the main update equation of W which combines self-organization and label-sensitive learning - Eqn. 15. This equation is then discussed and the SOM-like updates and the differences to previous pure SOMs are highlighted. The paper also states (Secs. 1 and 2) that the the network studied here is based on Hartono et al, 2015, with the main difference of the sigmoidal ouput layer being replaced by a softmax layer. What is missing is a discussion of the differences regarding the later numerical experiments, and a clear delimitation to Hartono et al., 2015, when Eqn. 15 is discussed. What is the major structural difference to their Eqn. 13 which is discussed along very similar lines as Eqn. 15 of this paper. Also after reading the abstract of this paper, one may think that this is the first paper discussing the SOM / supervised learning combination.\n\n(B) A further difference to Hartono et al, 2015, are comparisons with multi-layer networks, and the presentation and discussion of this comparison is my strongest concern. In the first paragraph of Sec. 3 the competing deep networks are introduced. Then it is stated: \"the number of hidden neurons, as well as the structures for the deep neural networks\nwere empirically tried, and the results of the best settings were registered for comparison\" (1st paragraph Sec. 3). What I do not understand are then the high classification errors reported in Tab. 1. It is known that even basic multi-layer perceptrons (MLPs) result in much lower classification errors, e.g., for MNIST. LeCun et al., 1998, is a classical example with less then 3% error on MNIST with many later examples that improve on these. Also the well-known original DBN paper has MNIST as main example (and main selling point) with close to 1% error. Why are the classification errors for DBN and MLP in the Tab 1 so high? And if they are in reality much lower, then competitiveness of s-rRBF in terms of classification results to these systems is questionable. The table makes me having doubts regarding the competitiveness of S-rRBF. I therefore disagree with the conclusion that this paper has shown that S-rRBFs are \"comparable to the best performer for most of the diverse benchmark applications\" (last paragraph in Conclusion). The feature of providing auxiliary visual information (also conclusion) is much more convincing (but also a feature of Hartono et al, 2015).\n\nMore generally, putting the biological arguments aside, why would a 2D neighborhood relationship be helpful? I see a benefit in interpretation which can help. Also, if there is an intrinsic 2D hidden structure in the data, then imposing a 2D representation can help (as a sort of a prior). But in general there may not be a 2D intrinsic property, or there is a higher dimensional hidden structure - so why not 3D or more? Related to this, why not using an objective that would result in a dynamics similar to a growing neural gas instead of an SOM?\n\n\nMinor:\n\nThe work is first introduced as multi-layer but only the single hidden layer case is actually discussed. I would suggest to either really add multi-hidden-layer results (which is not really doable in a conference revision), or state multi-layer work as outlook.\n\nFig. 5, bad readability of axes labels.\n\nis a hierarchical -> are hierarchical\n\nyields -> yield\n\ntwice \"otherwise\" after Eqn. 7\n\nare can be viewed\n\nthey occurs\n\ncan can readily expanded\n\ntransfer transform\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Organization adds application robustness to deep learners","abstract":"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures. Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance. Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule. Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems. ","pdf":"/pdf/f82ee618a4ce01ab39ffc039385623a0b03590d9.pdf","TL;DR":"integration of self-organization and supervised learning in a hierarchical neural network","paperhash":"anonymous|selforganization_adds_application_robustness_to_deep_learners","_bibtex":"@article{\n  anonymous2018self-organization,\n  title={Self-Organization adds application robustness to deep learners},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SywP-9hT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper62/Authors"],"keywords":["supervised learning","unsupervised learning","self-organization","internal representation","topological structure"]}},{"tddate":null,"ddate":null,"tmdate":1512222702573,"tcdate":1511669052813,"number":2,"cdate":1511669052813,"id":"ryHBphwez","invitation":"ICLR.cc/2018/Conference/-/Paper62/Official_Review","forum":"SywP-9hT-","replyto":"SywP-9hT-","signatures":["ICLR.cc/2018/Conference/Paper62/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of 'Self-organization adds application robustness to deep learners'","rating":"2: Strong rejection","review":"This paper proposes a supervised variant of Kohonen's self-organizing map\n(SOM), i.e., trained by gradient descent with gradient obtained by\nbackprop, using a grid of RBF neurons which respond to the input only if\nthey are in the grid-neighborhood of the 'winner' neuron (closest to the\ninput). This in itself is not even new, but the authors replace a linear\noutput layer with squared error (proposed in another, earlier paper) by a\nsoftmax layer with cross-entropy.  Unsuprisingly, this leads to an improvement.\n\nThe title is misleading. There is nothing deep in this architecture. It is\na shallow architecture with a single RBF-like hidden layer.  There is a\ntiny ounce of novelty in that the authors propose to improve a supervised\nversion of the SOM by using what should have been used in the first place\naccording to modern good practice. And the whole paper seems as if written\ncirca 2010 or 2012.  Another misleading thing is the term self-organizing\nused throughout, which is roughly synonym to learning according to me, and\nnot something uniquely belonging to the SOM family of models, as used by\nthe authors.  As an example of time-travel to the past, the authors talk\nabout RBMs and stacks of auto-encoders as if that was the deep learning\nstate-of-the-art. The authors even call these methods 'recent'! Clearly not\nthe case.  Unfortunately, it's not just talk, they are also the point of\ncomparison in the experiments, i.e., there are no comparison with modern\ndeep learning methods. Even the datasets are outdated (from the 90s?).\nVocabulary is wrong in other places, for example the word semi-supervised\nis wrongly understood and used. Semi-supervised means that one combines\nlabeled and unlabeled data. Where the label 'semi-supervised' is used (page\n4) is actually wrong: yes the labels are used, but of course it is the\n*gradients* which show up in the update, not the labels themselves\ndirectly. It's also not true that there is little research in understanding\nthe formation of internal representations. There is a whole subfields of\npapers trying to interpret the features learned by deep networks, and much\nwork designing learning frameworks and objectives to achieve better\nrepresentations, e.g, to better disentangle the underlying factors.\nIt's also common practice to analyze the representations learned, in\nmany deep learning papers.\n\nThe paper uses much space to show how to compute gradients in the proposed\narchitecture: there is obviously no need for this in a day and age where\ngradients are automatically derived by software.\n\nThe cherry on the sundae are the experimental results. How could the authors\nget 16% on MNIST with an MLP of any kind? It does not seem right at all.\nEven a linear regression would get at least half of that. As there are not\nenough experimental details to judge, it's hard to figure out the problem,\nbut this ppaper is clearly not publishable at any of the quality machine\nlearning venues, for weakness in originality, quality of the writing,\nand poor experiments.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Organization adds application robustness to deep learners","abstract":"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures. Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance. Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule. Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems. ","pdf":"/pdf/f82ee618a4ce01ab39ffc039385623a0b03590d9.pdf","TL;DR":"integration of self-organization and supervised learning in a hierarchical neural network","paperhash":"anonymous|selforganization_adds_application_robustness_to_deep_learners","_bibtex":"@article{\n  anonymous2018self-organization,\n  title={Self-Organization adds application robustness to deep learners},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SywP-9hT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper62/Authors"],"keywords":["supervised learning","unsupervised learning","self-organization","internal representation","topological structure"]}},{"tddate":null,"ddate":null,"tmdate":1512222702612,"tcdate":1511654656787,"number":1,"cdate":1511654656787,"id":"SkK-HYPgM","invitation":"ICLR.cc/2018/Conference/-/Paper62/Official_Review","forum":"SywP-9hT-","replyto":"SywP-9hT-","signatures":["ICLR.cc/2018/Conference/Paper62/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Model description is confusing, and the evaluation is questionable","rating":"2: Strong rejection","review":"This paper proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function. It is trained with supervised learning, by minimising the cross-entropy error between labels and the softmax output.\n\nThe paper's claim of combining unsupervised (self-organising) with supervised training is misleading and confusing. In this model, self-organising is a property of the hidden neurons' activation (eq. 1-3), and the training procedure is entirely supervised. It is misleading to claim any unsupervised or semi-supervised learning based on the *self-organising part* of, for example, eq. 14, which is merely a result of applying chain rule through the hidden neurons' activation.\n\nWhile this model is proposed as an extension of Kohonen's self-organising map (SOM), the paper fails to mention, or compare with, several historically important extension of SOM, which should perhaps at least include the generative topographic mapping (GTM, Bishop et al. 1998), an important probabilistic generalisation of SOM.\n\nFinally, the evaluation of the model in comparison with other models is questionable. For example, while the configuration the paper's baseline models are not given, the baseline accuracy of MNIST classification using MLP is 16.2%. This is much worse than the baseline of 12% in LeCun et al. (1998), using simple linear classifier without any preprocessing. The 7% accuracy from the proposed model is not in the range of modern deep learning models (The state-of-art accuracy is <0.3%). Similar problem also exist in results from other datasets. They are therefore unable to support the paper's claim on robust performance\n\nPros:\nThe question of internal representation is interesting.\nCombining self-organising with classification.\nComparing learned representations from different models.\n\nCons:\nNot clearly written.\nMixing the concept of unsupervised/semi-supervised learning is confusing.\nModel evaluation is questionable.\nDoes not compare existing extensions of SOM.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Organization adds application robustness to deep learners","abstract":"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures. Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance. Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule. Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems. ","pdf":"/pdf/f82ee618a4ce01ab39ffc039385623a0b03590d9.pdf","TL;DR":"integration of self-organization and supervised learning in a hierarchical neural network","paperhash":"anonymous|selforganization_adds_application_robustness_to_deep_learners","_bibtex":"@article{\n  anonymous2018self-organization,\n  title={Self-Organization adds application robustness to deep learners},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SywP-9hT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper62/Authors"],"keywords":["supervised learning","unsupervised learning","self-organization","internal representation","topological structure"]}},{"tddate":null,"ddate":null,"tmdate":1509739507104,"tcdate":1508839775088,"number":62,"cdate":1509739504451,"id":"SywP-9hT-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SywP-9hT-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Self-Organization adds application robustness to deep learners","abstract":"While self-organizing principles have motivated much of early learning models, such principles have rarely been included in deep learning architectures. Indeed, from a supervised learning perspective it seems that topographic constraints are rather decremental to optimal performance. Here we study a network model that incorporates self-organizing maps into a supervised network and show how gradient learning results in a form of a self-organizing learning rule. Moreover, we show that such a model is robust in the sense of its application to a variety of  areas, which is believed to be a hallmark of biological learning systems. ","pdf":"/pdf/f82ee618a4ce01ab39ffc039385623a0b03590d9.pdf","TL;DR":"integration of self-organization and supervised learning in a hierarchical neural network","paperhash":"anonymous|selforganization_adds_application_robustness_to_deep_learners","_bibtex":"@article{\n  anonymous2018self-organization,\n  title={Self-Organization adds application robustness to deep learners},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SywP-9hT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper62/Authors"],"keywords":["supervised learning","unsupervised learning","self-organization","internal representation","topological structure"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}