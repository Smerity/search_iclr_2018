{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222633701,"tcdate":1511769200680,"number":3,"cdate":1511769200680,"id":"HyFOESYxM","invitation":"ICLR.cc/2018/Conference/-/Paper39/Official_Review","forum":"r1AMITFaW","replyto":"r1AMITFaW","signatures":["ICLR.cc/2018/Conference/Paper39/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"This submission first proposes a new variant of LSTM by introducing a set of independent weights for each time step, to learn longer dependency from the input. Then the submission propose a dependent bidirectional structure by using the output as input to the RNN cell to introduce the dependency of the outputs.\n\nWhile LSTM do have problem to learn very long term dependency, the model proposed in this paper is very inefficient, the number of parameters are depend on the the length of sequences. Also, there is no analysis about why adding these additional weights could help the model learn better long-term dependency. In the other word, why this approach is better than attention/ self-attention? How to handle very long sequence and also, how to deal with different length? Just ignore the additional weights?\n\nIn the second part, the author argued a standard seq2seq model is vulnerable to previous erroneous predictions. But I don't understand why the DBRNN can handle it. It essentially just a multitask learning function: L = L_f + L_b + L_fb where error signal backprop to different layer directly which is not new.\n\nThe experimental results are weak. It compare with Seq2Seq model without attention. The other baseline for POS tag is from 1997. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dependent Bidirectional RNN with Super-long Short-term Memory","abstract":"In this work, we first conduct mathematical analysis on the memory decay of three RNN cells; namely, the simple recurrent neural network (SRN), the long short-term memory (LSTM) and the gated recurrent unit (GRU). Based on the analysis, we propose a new design, called the super-long short-term memory (SLSTM), to extend the memory length of a cell.  Next, we present an efficient RNN architecture, called the dependent bidirectional recurrent neural network (DBRNN), for the sequence-in-sequence-out (SISO) problem.  Finally, the superior performance of the DBRNN architecture with the SLSTM cell is demonstrated by experimental results. ","pdf":"/pdf/562dba99b53a2d533b42f40cb3f2c61160457131.pdf","TL;DR":"A highly effective recurrent neural network cell and architectural design for sequence-in-sequence-out problems with super long memory","paperhash":"anonymous|dependent_bidirectional_rnn_with_superlong_shortterm_memory","_bibtex":"@article{\n  anonymous2018dependent,\n  title={Dependent Bidirectional RNN with Super-long Short-term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1AMITFaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper39/Authors"],"keywords":["RNN memory decay","Super-long memory","RNN architecture","LSTM","GRU","BRNN","encoder-decoder","Natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222633743,"tcdate":1511765980110,"number":2,"cdate":1511765980110,"id":"ByNyuNYeM","invitation":"ICLR.cc/2018/Conference/-/Paper39/Official_Review","forum":"r1AMITFaW","replyto":"r1AMITFaW","signatures":["ICLR.cc/2018/Conference/Paper39/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Generally a nice approach, but the paper has several issues, especially, recent SotA should be taken into account","rating":"4: Ok but not good enough - rejection","review":"\nThis paper introduces a different form of Memory cell for RNN which has more capabilities of long-term memorizing. Furthermore, it presents and efficient architecture for sequence-to-sequence mapping.\n\nWhile the claim of the paper sounds very ambitious and good, the paper has several flaws. First of all, the mathematical analysis is a bit problematic. Of course, it is known that Simple Recurrent Networks (SRN) have a vanishing gradient problem. However, the way you proof it is not correct, as you ignore the application of f() for calculating the output (which is routed to the input) and you use an upper bound to show a general behaviour.\nThe analysis of the Memory capabilities of LSTM is a bit simplified, however, it is okay. Note, that various experiments by Schmidhuber's group, as well as Otte & al have shown that LSTM can generalize and memorize to sequences of more than a million time steps, if the learning rates is small enough.\n\nThe extended memory which the authors call SLSTM-I has similar memory capabilities as LSTM. The other one (SLSTM-II) looses the capability of forgetting as it seems. An analysis would be crucial in this paper to show the benefits mathematically. \n\nThe authors should have a look at \"Evolving memory cell structures for sequence learning\" by Justin Bayer, Daan Wierstra, Julian Togelius and J¨urgen Schmidhuber, published in 2009. Note that the SLSTM belongs to the family of networks which could be generated by that paper as well.\n\nAlso \"Neural Architecture Search with Reinforcement Learning\" by Barret Zoph and Quoc V. Le would be interesting.\n\nIn your experiments it would be fair to compare to Cheng et al. 2016\n\nI suggest the authors being more modest with the name of the memory cell as well as with the abstract (especially in the POS experiment, SLSTM is not superior)","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dependent Bidirectional RNN with Super-long Short-term Memory","abstract":"In this work, we first conduct mathematical analysis on the memory decay of three RNN cells; namely, the simple recurrent neural network (SRN), the long short-term memory (LSTM) and the gated recurrent unit (GRU). Based on the analysis, we propose a new design, called the super-long short-term memory (SLSTM), to extend the memory length of a cell.  Next, we present an efficient RNN architecture, called the dependent bidirectional recurrent neural network (DBRNN), for the sequence-in-sequence-out (SISO) problem.  Finally, the superior performance of the DBRNN architecture with the SLSTM cell is demonstrated by experimental results. ","pdf":"/pdf/562dba99b53a2d533b42f40cb3f2c61160457131.pdf","TL;DR":"A highly effective recurrent neural network cell and architectural design for sequence-in-sequence-out problems with super long memory","paperhash":"anonymous|dependent_bidirectional_rnn_with_superlong_shortterm_memory","_bibtex":"@article{\n  anonymous2018dependent,\n  title={Dependent Bidirectional RNN with Super-long Short-term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1AMITFaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper39/Authors"],"keywords":["RNN memory decay","Super-long memory","RNN architecture","LSTM","GRU","BRNN","encoder-decoder","Natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222633781,"tcdate":1511104420455,"number":1,"cdate":1511104420455,"id":"SJ2iJmyeM","invitation":"ICLR.cc/2018/Conference/-/Paper39/Official_Review","forum":"r1AMITFaW","replyto":"r1AMITFaW","signatures":["ICLR.cc/2018/Conference/Paper39/AnonReviewer1"],"readers":["everyone"],"content":{"title":"serious presentation issues","rating":"3: Clear rejection","review":"The paper proposes a new recurrent cell and a new way to make predictions for sequence tagging. It starts with a theoretical analysis of memory capabilities in different RNN cells and goes on with experiments on POS tagging and dependency parsing. There are serious presentation issues in the paper, which make it hard to understand the ideas and claims.\n\nFirst, I was not able to understand the message of the theoretical analysis from Section 2 and could not see how it is different from similar derivations (i.e. using a linearized version of an RNN and eigenvalue decomposition) that can be found in many other papers, including (Bengio et al, 1994) and (Pascanu et al, 2013). Novelty aside, the analysis has presentation issues. SRN is introduced without a nonlinearity from the beginning, although normally it should have one. From the classical upper bound with a power of the largest singular value the paper concludes that “Clearly, the memory will explode if \\lambda_{max} > 1”, which is not true: the memory *may* explode, having an exponentially growing upper bound does not mean that it *will* explode. The notation chosen from LSTM is  different from the standard in deep learning community and was very hard to understand (Y_t is used instead of h_t, and h_t is used instead of c_t). This notation also does not seem consistent with the rest of the paper, for example Equations 28 and 29 suggest that Y_t are discrete outputs and not vectors. \n\nThe novel cell SLSTM-I is meant to be different from LSTM by addition of “input weight vector c_i”, but is not explained where c_i come from. Are they trainable vectors, one for each time step? If yes, then how could such a cell be applied to sequence which are longer than the training ones?\n\nEquations 28, 29, 30 describe a very unusual kind of a Bidirectional Recurrent Network. To the best of my knowledge it is much more common to make one prediction based on future and past information, whereas the paper describes an approach in which first predictions are made separately based on the past and on the future. It is also very common to use several BiRNN layers, whereas the paper only uses one. As for the proposed DBRNN method, unfortunately, I was not able to understand it.\n\nI also have concerns regarding the experiments. Why is seq2seq without attention is used? On such small datasets attention is likely to make a big difference. What’s the point of reporting results of an LSTM without output nonlinearity (Table 5)?\n\nTo sum up, the paper needs a lot work on many fronts, but most importantly, presentation should be improved.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dependent Bidirectional RNN with Super-long Short-term Memory","abstract":"In this work, we first conduct mathematical analysis on the memory decay of three RNN cells; namely, the simple recurrent neural network (SRN), the long short-term memory (LSTM) and the gated recurrent unit (GRU). Based on the analysis, we propose a new design, called the super-long short-term memory (SLSTM), to extend the memory length of a cell.  Next, we present an efficient RNN architecture, called the dependent bidirectional recurrent neural network (DBRNN), for the sequence-in-sequence-out (SISO) problem.  Finally, the superior performance of the DBRNN architecture with the SLSTM cell is demonstrated by experimental results. ","pdf":"/pdf/562dba99b53a2d533b42f40cb3f2c61160457131.pdf","TL;DR":"A highly effective recurrent neural network cell and architectural design for sequence-in-sequence-out problems with super long memory","paperhash":"anonymous|dependent_bidirectional_rnn_with_superlong_shortterm_memory","_bibtex":"@article{\n  anonymous2018dependent,\n  title={Dependent Bidirectional RNN with Super-long Short-term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1AMITFaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper39/Authors"],"keywords":["RNN memory decay","Super-long memory","RNN architecture","LSTM","GRU","BRNN","encoder-decoder","Natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1509739518241,"tcdate":1508656662360,"number":39,"cdate":1509739515569,"id":"r1AMITFaW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1AMITFaW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Dependent Bidirectional RNN with Super-long Short-term Memory","abstract":"In this work, we first conduct mathematical analysis on the memory decay of three RNN cells; namely, the simple recurrent neural network (SRN), the long short-term memory (LSTM) and the gated recurrent unit (GRU). Based on the analysis, we propose a new design, called the super-long short-term memory (SLSTM), to extend the memory length of a cell.  Next, we present an efficient RNN architecture, called the dependent bidirectional recurrent neural network (DBRNN), for the sequence-in-sequence-out (SISO) problem.  Finally, the superior performance of the DBRNN architecture with the SLSTM cell is demonstrated by experimental results. ","pdf":"/pdf/562dba99b53a2d533b42f40cb3f2c61160457131.pdf","TL;DR":"A highly effective recurrent neural network cell and architectural design for sequence-in-sequence-out problems with super long memory","paperhash":"anonymous|dependent_bidirectional_rnn_with_superlong_shortterm_memory","_bibtex":"@article{\n  anonymous2018dependent,\n  title={Dependent Bidirectional RNN with Super-long Short-term Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1AMITFaW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper39/Authors"],"keywords":["RNN memory decay","Super-long memory","RNN architecture","LSTM","GRU","BRNN","encoder-decoder","Natural language processing"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}