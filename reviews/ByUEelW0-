{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222683054,"tcdate":1511854103718,"number":3,"cdate":1511854103718,"id":"rJeml9qlf","invitation":"ICLR.cc/2018/Conference/-/Paper548/Official_Review","forum":"ByUEelW0-","replyto":"ByUEelW0-","signatures":["ICLR.cc/2018/Conference/Paper548/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper is not ready for the publication","rating":"3: Clear rejection","review":"Summary: This paper introduces a model that combines the rotation matrices with the LSTMs. They apply the rotations before the final tanh activation of the LSTM and before applying the output gate. The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations. The paper only provides results on the bAbI task. \n\nQuestions:\nHave you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study)\nHave you tried on other tasks?\nWhy did you just apply the rotations only on d_{t}.\n\nPros:\nUses a simple parametrization of the rotation matrices.\n\nCons:\nNot clear justification and motivations\nThe experiments are really lacking:\nNo ablation study\nThe results are only limited to single toy task.\n\n\nGeneral Comments:\n\nThis paper proposes to use the rotation matrices with LSTMs. However there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output gate. The experiments are seriously lacking, an ablation study should have been made and the results are not good enough. The experiments are only limited to bAbI task which doesnâ€™t tell you much. This paper is not ready for publication, and really feels like it is rushed.\n\nMinor Comment:\nThis paper needs more proper proof-reading. There are some typos in it, e.g.:\n1st page, senstence --> sentence\n4th page, the the ... --> the\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modifying memories in a Recurrent Neural Network Unit","abstract":"Long Short-Term Memory (LSTM) units have the ability to memorise and use long-term dependencies between inputs to generate predictions on time series data. We introduce the concept of modifying the cell state (memory) of LSTMs using rotation matrices parametrised by a new set of trainable weights. This addition shows significant increases of performance on some of the tasks from the bAbI dataset.","pdf":"/pdf/ad6bfb1e9a759a6d440c6c4935599670ba35b8d3.pdf","TL;DR":"Adding a new set of weights to the LSTM that rotate the cell memory improves performance on some bAbI tasks.","paperhash":"anonymous|modifying_memories_in_a_recurrent_neural_network_unit","_bibtex":"@article{\n  anonymous2018modifying,\n  title={Modifying memories in a Recurrent Neural Network Unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByUEelW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper548/Authors"],"keywords":["LSTM","RNN","rotation matrix","long-term memory","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222683093,"tcdate":1511774530953,"number":2,"cdate":1511774530953,"id":"HyoHt8YlG","invitation":"ICLR.cc/2018/Conference/-/Paper548/Official_Review","forum":"ByUEelW0-","replyto":"ByUEelW0-","signatures":["ICLR.cc/2018/Conference/Paper548/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Insufficient motivation and experimental analysis","rating":"4: Ok but not good enough - rejection","review":"The paper proposes an additional transform in the recurrent neural network units. The transform allows for explicit rotations and swaps of the hidden cell dimensions. The idea is illustrated for LSTM units, where the transform is applied after the cell values are computed via the typical LSTM updates.\n\nMy first concern is the motivation. I think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via gates. In the proposed example, it's not clear to me why the gate is expected to be saturated at every time step such that it would require the memory swaps. Alternatively, experimentally showing that the network makes use of swaps in an interpretable way (e.g. at certain sentence positions) could strengthen the motivation.\n\nSecondly, the experimental analysis is not very extensive. The method is only evaluated on the bAbI QA dataset, which is a synthetic dataset. I think a language modeling benchmark and/or a larger scale question answering dataset should be considered.\n\nRegarding the experimental setup, how are the hyper-parameters for the baseline tuned? Have you considered training jointly (across the tasks) as well?\n\nAlso, is the setting the same as in Weston et al (2015)? While for many tasks the numbers reported by Weston et al (2015) and the ones reported here for the LSTM baseline are aligned in the order of magnitude, suggesting that some tasks are easier or more difficult for LSTMs, there are large differences in other cases, for task #5 (here 33.6, Weston 70), for task #16 (here 48, Weston 23), and so on.\n\nFinally, do you have an intuition (w.r.t. to swaps and rotations) regarding the accuracy improvements on tasks #5 and #18?\n\nSome minor issues:\n- The references are somewhat inconsistent in style: some have urls, others do not; some have missing authors, ending with \"et al\".\n- Section 1, second paragraph: senstence\n- Section 3.1, first paragraph: thorugh\n- Section 5: architetures","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modifying memories in a Recurrent Neural Network Unit","abstract":"Long Short-Term Memory (LSTM) units have the ability to memorise and use long-term dependencies between inputs to generate predictions on time series data. We introduce the concept of modifying the cell state (memory) of LSTMs using rotation matrices parametrised by a new set of trainable weights. This addition shows significant increases of performance on some of the tasks from the bAbI dataset.","pdf":"/pdf/ad6bfb1e9a759a6d440c6c4935599670ba35b8d3.pdf","TL;DR":"Adding a new set of weights to the LSTM that rotate the cell memory improves performance on some bAbI tasks.","paperhash":"anonymous|modifying_memories_in_a_recurrent_neural_network_unit","_bibtex":"@article{\n  anonymous2018modifying,\n  title={Modifying memories in a Recurrent Neural Network Unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByUEelW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper548/Authors"],"keywords":["LSTM","RNN","rotation matrix","long-term memory","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222683130,"tcdate":1511754640909,"number":1,"cdate":1511754640909,"id":"SkYqiWteM","invitation":"ICLR.cc/2018/Conference/-/Paper548/Official_Review","forum":"ByUEelW0-","replyto":"ByUEelW0-","signatures":["ICLR.cc/2018/Conference/Paper548/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Insufficient Justification and Comparison","rating":"4: Ok but not good enough - rejection","review":"The paper proposes to add a rotation operation in long short-term memory (LSTM) cells. It performs experiments on bAbI tasks and showed that the results are better than the simple baselines with original LSTM cells. There are a few problems with the paper.\n\nFirstly, the title and abstract discuss \"modifying memories\", but the content is only about a rotation operation. Perhaps the title should be \"Rotation Operation in Long Short-Term Memory\"?\n\nSecondly, the motivation of adding the rotation operation is not properly justified. What does it do that a usual LSTM cell could not learn? Does it reduce the excess representational power compared to the LSTM cell that could result in better models? Or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible before? This is not clear at all after reading the paper. Besides, the idea of using a rotation operation in recurrent networks has been explored before [3].\n\nFinally, the task (bAbI) and baseline models (LSTM from a Keras tutorial) are too weak. There have been recent works that nearly solved the bAbI tasks to perfection (e.g., [1][2][4][5], and many others). The paper presented a solution that is weak compared to these recent results.\n\nIn a summary, the main idea of adding rotation to LSTM cells is not properly justified in the paper, and the results presented are quite weak for publication in ICLR 2018.\n\n[1] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus. End-to-end memory networks, NIPS 2015\n[2] Caiming Xiong, Stephen Merity, Richard Socher. Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016\n[3] Mikael Henaff, Arthur Szlam, Yann LeCun, Recurrent Orthogonal Networks and Long-Memory Tasks, ICML 2016 \n[4] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio, Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes, ICLR 2017\n[5] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun, Tracking the World State with Recurrent Entity Networks, ICLR 2017\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modifying memories in a Recurrent Neural Network Unit","abstract":"Long Short-Term Memory (LSTM) units have the ability to memorise and use long-term dependencies between inputs to generate predictions on time series data. We introduce the concept of modifying the cell state (memory) of LSTMs using rotation matrices parametrised by a new set of trainable weights. This addition shows significant increases of performance on some of the tasks from the bAbI dataset.","pdf":"/pdf/ad6bfb1e9a759a6d440c6c4935599670ba35b8d3.pdf","TL;DR":"Adding a new set of weights to the LSTM that rotate the cell memory improves performance on some bAbI tasks.","paperhash":"anonymous|modifying_memories_in_a_recurrent_neural_network_unit","_bibtex":"@article{\n  anonymous2018modifying,\n  title={Modifying memories in a Recurrent Neural Network Unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByUEelW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper548/Authors"],"keywords":["LSTM","RNN","rotation matrix","long-term memory","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1509727399480,"tcdate":1509727399480,"number":1,"cdate":1509727399480,"id":"r1Jn2Mq0Z","invitation":"ICLR.cc/2018/Conference/-/Paper548/Public_Comment","forum":"ByUEelW0-","replyto":"ByUEelW0-","signatures":["~Jack_William_Rae1"],"readers":["everyone"],"writers":["~Jack_William_Rae1"],"content":{"title":"LSTM baseline","comment":"It is worth noting there is a more recent bAbI LSTM baseline that outperforms the proposed model for almost all of the tasks:\n\nhttps://arxiv.org/abs/1610.09027\n\nTable 2 (in the suppl.). It may be worth comparing to these numbers. In that case the LSTM is jointly trained over all tasks, and is not tuned on a per-task basis. That was with a cell size of 100, trained with RMSProp with a learning rate of 1e-5."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modifying memories in a Recurrent Neural Network Unit","abstract":"Long Short-Term Memory (LSTM) units have the ability to memorise and use long-term dependencies between inputs to generate predictions on time series data. We introduce the concept of modifying the cell state (memory) of LSTMs using rotation matrices parametrised by a new set of trainable weights. This addition shows significant increases of performance on some of the tasks from the bAbI dataset.","pdf":"/pdf/ad6bfb1e9a759a6d440c6c4935599670ba35b8d3.pdf","TL;DR":"Adding a new set of weights to the LSTM that rotate the cell memory improves performance on some bAbI tasks.","paperhash":"anonymous|modifying_memories_in_a_recurrent_neural_network_unit","_bibtex":"@article{\n  anonymous2018modifying,\n  title={Modifying memories in a Recurrent Neural Network Unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByUEelW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper548/Authors"],"keywords":["LSTM","RNN","rotation matrix","long-term memory","natural language processing"]}},{"tddate":null,"ddate":null,"tmdate":1509739242381,"tcdate":1509126190063,"number":548,"cdate":1509739239724,"id":"ByUEelW0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByUEelW0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Modifying memories in a Recurrent Neural Network Unit","abstract":"Long Short-Term Memory (LSTM) units have the ability to memorise and use long-term dependencies between inputs to generate predictions on time series data. We introduce the concept of modifying the cell state (memory) of LSTMs using rotation matrices parametrised by a new set of trainable weights. This addition shows significant increases of performance on some of the tasks from the bAbI dataset.","pdf":"/pdf/ad6bfb1e9a759a6d440c6c4935599670ba35b8d3.pdf","TL;DR":"Adding a new set of weights to the LSTM that rotate the cell memory improves performance on some bAbI tasks.","paperhash":"anonymous|modifying_memories_in_a_recurrent_neural_network_unit","_bibtex":"@article{\n  anonymous2018modifying,\n  title={Modifying memories in a Recurrent Neural Network Unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByUEelW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper548/Authors"],"keywords":["LSTM","RNN","rotation matrix","long-term memory","natural language processing"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}