{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222708704,"tcdate":1511822287860,"number":3,"cdate":1511822287860,"id":"S1dRXMqxG","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Review","forum":"SyAbZb-0Z","replyto":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Initial work on building a framework for finding best performing NN architecture across multiple tasks simultaneously","rating":"4: Ok but not good enough - rejection","review":"In this paper authors are summarizing their work on building a framework for automated neural network (NN) construction across multiple tasks simultaneously. \n\nThey present initial results on the performance of their framework called Multitask Neural Model Search (MNMS) controller. The idea behind building such a framework is motivated by the successes of recently proposed reinforcement based approaches for finding the best NN architecture across the space of all possible architectures. Authors cite the Neural Architecture Search (NAS) framework as an example of such a framework that yields better results compared to NN architectures configured by humans. \n\nOverall I think that the idea is interesting and the work presented in this paper is very promising. Given the depth of the empirical analysis presented the work still feels that itâ€™s in its early stages. In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.\n\nAuthors use two text classification tasks from the NLP domain to showcase the benefits of their proposed architecture. It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs. This is especially the case for the transfer learning task. \n\nIn the NAS overview section, readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler. \n\nAcross the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures. In addition, on the transfer learning evaluation approach they showcase the benefit of using the proposed framework in terms of the initially retrieved architecture and the number of iterations required to obtain the best performing one. \nFor better clarity figures 3 and 5 should be made bigger. \nWhat is LSS in figure 4?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/60d3f0f5b676332d9b390971e9afca9a17cad56e.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222708740,"tcdate":1511748605674,"number":2,"cdate":1511748605674,"id":"HyU-Egtef","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Review","forum":"SyAbZb-0Z","replyto":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The neural architecture design is important and interesting","rating":"7: Good paper, accept","review":"Summary\nThis paper extends Neural Architecture Search (NAS) to the multi-task learning problem. A task conditioned model search controller is learned to handle multiple tasks simultaneously. The experiments are conducted on text data sets to evaluate the proposed method.\n\nPros\n1.\tThe problem of neural architecture design is important and interesting.\n2.\tThe motivation is strong. NAS (Zoph & Le, 2017) needs to train a model for a new task from scratch, which is inefficient. It is reasonable to introduce task embeddings into NAS to obtain a generalization model for multiple tasks.\n\nCons\n1.\tSome important technical details are missing, especially for the details regarding task embeddings.\n2.\tThe experiments are not sufficient.\n\nDetailed Comments\n1.\tThe paper does not provide the method of how to obtain task embeddings. In addition, if task embeddings are obtained by an auxiliary network, is it feasible to update task embeddings by updating the weights of this auxiliary network?\n2.\tThe discussion of off-policy training is questionable. There is no experiment to demonstrate the advantage of off-policy training compared to on-policy training.\n3.\tIn order to demonstrate the effectiveness of the idea of multi-task learning and task conditioning in MNMS, some architecture search methods for single-task should be conducted for comparison. For instance, NAS on SST or the Spanish language identification task should be compared.\n4.\tIn order to demonstrate the efficiency of MNMS, running time results of MNMS and NAS should be reported.\n5.\tIn my opinion, the title is not appropriate. The most important contribution of this paper is to search neural models for multiple tasks simultaneously using task conditioning. Only when this target is achieved, is it possible to transfer a pre-trained controller to new tasks with new task embeddings. Therefore, the title should highlight multitask neural model search rather than transfer learning.\n6.\tIn Figure 5, \"MNAS\" should be \"MNMS\".\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/60d3f0f5b676332d9b390971e9afca9a17cad56e.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222708777,"tcdate":1511609824694,"number":1,"cdate":1511609824694,"id":"HkKJIALgM","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Review","forum":"SyAbZb-0Z","replyto":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice project, a bit slim on the empirical side","rating":"5: Marginally below acceptance threshold","review":"The paper proposes an extension of the Neural Architecture Search approach, in which a single RNN controller is trained with RL to select hyperparameters for child networks that must perform different tasks. The architecture includes the notion of a \"task embedding\", that helps the controller keeping track of similarity between tasks, to facilitate transfer across related tasks.\n\nThe paper is very well written, and based on a simple but interesting idea. It also deals with core issues in current machine learning.\n\nOn the negative side, there is just one experiment, and it is somewhat limited. In the experiment, the proposed model is trained on two very different tasks (English sentiment analysis and Spanish language detection), and then asked to generalize to another English sentiment analysis task and to a Spanish sentiment analysis task. The models converge faster to high accuracy in the proposed transfer learning setup than when trained one a single task with the same architecture search strategy. Moreover, the task embedding for the new English task is closer to that of the training English task, and the same for the training/test Spanish tasks.\n\nMy main concern with the experiment is that the approach is only tested in a setup in which there is a huge difference between two classes of tasks (English vs Spanish), so the model doesn't need to learn very sophisticated task embeddings to group the tasks correctly for transfer. It would be good to see other experiments where there is less of a trivial structure distinguishing tasks, to check if transfer helps.\n\nAlso, I find it surprising that the Corpus Cine sentiment task embedding is not correlated at all with the SST sentiment task. If the controller is really learning something interesting about the nature of the tasks, I would have expected a differential effect, such that IMDB is only correlated with SST, but Corpus Cine is correlated to both the Spanish language identification task and SST. Perhaps, this is worth some discussion.\n\nFinally, it's not clear to me why the multitask architecture was used in the experiment even when no multi-task pre-training was conducted: shouldn't the simple neural architecture search method be used in this case?\n\nMinor points:\n\n\"diffferentiated\": different?\n\n\"outputted actions\": output actions\n\n\"the aim of increase the training stability\": the aim of increasing training stability\n\nInsert references for Polyak averaging and Savitzky-Golay filtering.\n\nFigure 3: specify that the Socher 2013 result is for SST\n\nFigure 4: does LSS stand for SST?\n\nI'm confused by Fig. 6: why aren't the diagonal values 100%?\n\nMNMS is referred to as MNAS in Figure 5.\n\nFor architecture search, the neuroevolution literature should also be cited (https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning).\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/60d3f0f5b676332d9b390971e9afca9a17cad56e.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739181420,"tcdate":1509130502310,"number":649,"cdate":1509739178762,"id":"SyAbZb-0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/60d3f0f5b676332d9b390971e9afca9a17cad56e.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}