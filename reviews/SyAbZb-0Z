{"notes":[{"tddate":null,"ddate":null,"tmdate":1515264028776,"tcdate":1515264028776,"number":7,"cdate":1515264028776,"id":"BkrXu907M","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"B1_TYtaQM","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/AnonReviewer1"],"content":{"title":"thanks for the revision","comment":"Thanks for the revision, that clarifies some important points and puts the results in perspective. While I find the general direction of your work very promising, I stand by my initial point of view that more extensive experiments should be added for a long paper in a major conference."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515194816367,"tcdate":1515194816367,"number":6,"cdate":1515194816367,"id":"B1_TYtaQM","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"rycuQ6nmz","signatures":["ICLR.cc/2018/Conference/Paper649/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/Authors"],"content":{"title":"Revision uploaded","comment":"Apologies - the revision is now uploaded. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515144049971,"tcdate":1515144049971,"number":5,"cdate":1515144049971,"id":"rycuQ6nmz","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"S1J62Ih7f","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/AnonReviewer1"],"content":{"title":"Thanks for reply, missing revision?","comment":"Thanks for your reply. Unfortunately, I do not find it as helpful as it could be, because it refers to a revision of the paper that, as far as I can see, you have not uploaded on the OpenReview site. Consequently, you're pointing to arguments and references I cannot access :(\n\nConcerning the other differences in 4.2, perhaps some ablation would help to see how much they matter?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515117750728,"tcdate":1515117750728,"number":4,"cdate":1515117750728,"id":"S1J62Ih7f","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"HkKJIALgM","signatures":["ICLR.cc/2018/Conference/Paper649/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/Authors"],"content":{"title":"Thank you!","comment":"Again, thank you for the thoughtful and detailed review. In addition to our responses above to the other two reviews, to first clarify the nature of the experiments:\n        \t1. Section 4.2 describes the results of training MNMS models jointly on the SST and Spanish Language Identification tasks.\n        \t2. Section 4.3 uses these multi-task-trained MNMS models as the pre-trained models. The transfer learning results shown are the result of subsequently training the MNMS models initialized to the weights from part 4.2 further on each of the transfer learning tasks (CorpusCine and IMDB). During transfer learning, we initialize a new task embedding vector for the new task that is again trained jointly with the MNMS model. While we transfer learn to a single new task, multi-task pretraining has occurred. Further, after transfer learning, the learned task embedding for the new task can now be directly and meaningfully compared to the existing task embeddings, as in Figure 6.\n \n1. “My main concern with the experiment is that the approach is only tested in a setup in which there is a huge difference between two classes of tasks (English vs Spanish), so the model doesn't need to learn very sophisticated task embeddings to group the tasks correctly for transfer. It would be good to see other experiments where there is less of a trivial structure distinguishing tasks, to check if transfer helps.”\n        \tWe specifically chose the two initial multitask tasks to be different enough that a single set of hyperparameters would not be optimal for both. However, as seen in 4.2, there are other significant differences in the parameters learned for each task beyond the English vs Spanish word embeddings.\n        \tWe have revised the draft to include a further discussion within the conclusion section of the limitations of these experiments and necessary future tasks to demonstrate generalization.\n \n2. “Also, I find it surprising that the Corpus Cine sentiment task embedding is not correlated at all with the SST sentiment task. If the controller is really learning something interesting about the nature of the tasks, I would have expected a differential effect, such that IMDB is only correlated with SST, but Corpus Cine is correlated to both the Spanish language identification task and SST. Perhaps, this is worth some discussion.”\nThis is a good point, and we have updated the discussion to touch on this.\n \n3. “Finally, it's not clear to me why the multitask architecture was used in the experiment even when no multi-task pre-training was conducted: shouldn't the simple neural architecture search method be used in this case?”\n        \tAs clarified above, the transfer learning experiments show the results after multi-task pre-training. Let us know if further clarification can be made.\n \n4. Minor points: thank you for catching these. We have updated the grammatical fixes where we believed appropriate and the figures accordingly, and added a reference to the neuroevolution literature. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515117720021,"tcdate":1515117720021,"number":3,"cdate":1515117720021,"id":"Bkls2InmM","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"HyU-Egtef","signatures":["ICLR.cc/2018/Conference/Paper649/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/Authors"],"content":{"title":"Thank you!","comment":"Thank you for your thoughtful and detailed review! In addition to our response above, to address the other detailed comments within your review:\n1. “The paper does not provide the method of how to obtain task embeddings. In addition, if task embeddings are obtained by an auxiliary network, is it feasible to update task embeddings by updating the weights of this auxiliary network?”\n        \tAs described in 3.2, the task embeddings are randomly initialized vectors that are trained jointly with the controller; these embeddings are therefore learned automatically as part of the training process. During transfer learning to a new task, a new, randomly-initialized vector representation is added to the embedding table for the new task, and the task embedding for the new task is again learned automatically during transfer learning.\n        \tAs with other embedding tables, it is possible to continue updating all of the existing task embeddings along with other network weights during subsequent transfer learning. In this work, the same pre-trained model is separately transfer learned to each of the IMDB and CorpusCine tasks. We therefore do not continue to update the initial pre-training task embeddings here to allow between comparison between the transfer learned tasks.\n \n2. “The discussion of off-policy training is questionable. There is no experiment to demonstrate the advantage of off-policy training compared to on-policy training.”\n \n3. “In order to demonstrate the effectiveness of the idea of multi-task learning and task conditioning in MNMS, some architecture search methods for single-task should be conducted for comparison. For instance, NAS on SST or the Spanish language identification task should be compared.”\n        \tFigure 5 shows the performance of an MNMS model trained on a single task (Corpus Cine and IMDB) as a baseline for comparison with the transfer learned models.\nWe present the task conditioning for simultaneous task training as a stepping stone towards more generalized training for transfer learning to new tasks, rather than as a method for run-time improvements in itself.\n \n4. “In order to demonstrate the efficiency of MNMS, running time results of MNMS and NAS should be reported.”\n        \tMNMS as presented is a direct generalization of NAS, and in the single-task case (as with the single-task, non-pre-trained baselines compared with transfer learning) frameworks are identical. Training with a single, randomly initialized task embedding is equivalent to simply using a standard RNN embedding in the vanilla NAS framework.\n \n5. “In my opinion, the title is not appropriate. The most important contribution of this paper is to search neural models for multiple tasks simultaneously using task conditioning. Only when this target is achieved, is it possible to transfer a pre-trained controller to new tasks with new task embeddings. Therefore, the title should highlight multitask neural model search rather than transfer learning.”\n        \tWhile we do believe that multitask transfer learning is an important contribution, multitask transfer learning is presented as a stepping stone specifically towards enabling transfer learning. We present multitask training and the concept of task embeddings for task conditioning as a method to enable generalization for automated architecture design that can extend to new tasks.\n \n6. “In Figure 5, \"MNAS\" should be \"MNMS\".”\n        \tThis has been updated in the revised draft; thank you for catching this!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515117679015,"tcdate":1515117679015,"number":2,"cdate":1515117679015,"id":"HJDO3Un7z","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"SkeU2L3Qz","signatures":["ICLR.cc/2018/Conference/Paper649/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/Authors"],"content":{"title":"Thank you","comment":"To further address the points made in this review specifically:\n \n1. “Given the depth of the empirical analysis presented the work still feels that it’s in its early stages. In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.”\n        \tAs discussed above, this paper was intended to propose a generalized framework and demonstrate that both multitask training and transfer learning are possible, within these proof-of-concept domains. However, please let us know if there are suggestions for specific further analyses about the current experiments.\n \n2. “It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs. This is especially the case for the transfer learning task.”\n        \tWe have updated the revised draft conclusion to include a more detailed discussion of the limitations of this current study, and to include further discussion of ongoing work and future work to evaluate the framework on additional tasks and task sets, based on this feedback.\n3. “In the NAS overview section, readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler.”\n\tWhile this section was intended as a minimal overview of the original NAS framework (with the understanding that readers could reference the original works for greater detail), we have updated the revised draft to include some additional details, and reduced the size of figure 1.\n       .\n \n4 and 5:  “For better clarity figures 3 and 5 should be made bigger.” and “What is LSS in figure 4?”\n\tThe revised draft corrects the typo (LSS is now SST) as well as a typo in Figure 3.\n        \tWe have updated the revised draft to enlarge figures 3 and 5, and correct the typo (LSS is now SST.) Thank you!  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515117639708,"tcdate":1515117639708,"number":1,"cdate":1515117639708,"id":"SkeU2L3Qz","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Comment","forum":"SyAbZb-0Z","replyto":"S1dRXMqxG","signatures":["ICLR.cc/2018/Conference/Paper649/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper649/Authors"],"content":{"title":"Thank you","comment":"Thank you for the thoughtful and detailed review. We are actively continuing to evaluate the MNMS framework on additional search spaces, non-binary tasks, and tasks outside of the NLP domain. However, we believe that the presented experiments are sufficient to demonstrate two important contributions of the proposed generalized framework, each of which addresses key concerns about previous RL-based automated NN design frameworks such as NAS:\n1. Simultaneous task training is possible.\n        \tThe ability to handle multitask training of any kind addresses key issues regarding\n        \tthe generalizability and feasibility of RL-based automated search frameworks. In\n        \tparticular, learning task representations that allow a single controller model to\n        \tdifferentiate between tasks is necessary for any kind of task generalization using\n        \tthis form of meta-learning framework.\n        \tHowever, especially in RL-based environments, prior work has\n        \tdemonstrated that handling multitask learning is empirically challenging,\n        \teven on relatively simple tasks; as the two papers cited in the related work section\n        \talso discuss, multitask RL training even across two tasks often causes negative\n        \tinterference between the tasks, including cases where gradients from one task\n        \tcompletely dominate the other. Therefore, it is not obvious that a\n        \tNAS-like framework should be able to handle multitask training, even on\nrelatively simple domains, rather than simply collapsing to single, undifferentiated \nparameter choices that are suboptimal for each task. Indeed, as we describe in 3.2, \nmultitask replay is necessary even in this relatively simple domain to ensure adequate \ndifferentiation.\n        \tTherefore, while preliminary, we believe that it is important to show that an RL-\n        \tbased metalearning framework can indeed discover differentiated architectures for\n        \ttwo tasks, which were specifically chosen so that no single, optimal parameter\n        \tsolution existed. Further, the ability to automatically learn vector\n task representations sufficient to encode this differentiation during training, even\nin this relatively simple task domain, offers a necessary step towards further work\nin simultaneous multitask training across more challenging tasks in the future.\n2. Pre-training NAS-like frameworks for future transfer learning to new tasks is possible, and speeds up convergence.\n        \tA primary criticism of RL-based metalearning architectures, such is NAS, is that these methods are extremely time and computationally intensive, rendering them infeasible without computational resources that are not accessible to many researchers. Therefore, the possibility of using pre-trained models for transfer learning to any new task to reduce search time is a necessary step towards making this approach broadly feasible, both for additional research and more challenging tasks.\n        \tHowever, as with multitask training, it is not obvious that this would be possible, even when designing architectures for relatively simple task. Again, especially in RL models, attempting to transfer learn could lead to either 1. premature convergence to suboptimal parameters biased by the pre-training, or 2. no convergence speedup, or even additional convergence time, in which the controller first unlearns its pre-training and then learns the new task. Figure 5 in the results shows that transfer learning in this domain allows the controller to 1. start from a better place in the search space, indicating that NAS-like frameworks can learn knowledge that generalizes to another task, and 2. converge more quickly overall, indicating that this transfer learning can speed up convergence and therefore opening transfer learning as a grounds for further research.\nTherefore, while these empirical results are preliminary, we believe that demonstrating that both of these points are possible are important, nonobvious generalizations on the NAS architecture that offer routes for future study."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642485343,"tcdate":1511822287860,"number":3,"cdate":1511822287860,"id":"S1dRXMqxG","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Review","forum":"SyAbZb-0Z","replyto":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Initial work on building a framework for finding best performing NN architecture across multiple tasks simultaneously","rating":"4: Ok but not good enough - rejection","review":"In this paper authors are summarizing their work on building a framework for automated neural network (NN) construction across multiple tasks simultaneously. \n\nThey present initial results on the performance of their framework called Multitask Neural Model Search (MNMS) controller. The idea behind building such a framework is motivated by the successes of recently proposed reinforcement based approaches for finding the best NN architecture across the space of all possible architectures. Authors cite the Neural Architecture Search (NAS) framework as an example of such a framework that yields better results compared to NN architectures configured by humans. \n\nOverall I think that the idea is interesting and the work presented in this paper is very promising. Given the depth of the empirical analysis presented the work still feels that it’s in its early stages. In its current state and format the major issue with this work is the lack of more in-depth performance analysis which would help the reader draw more solid conclusions about the generalization of the approach.\n\nAuthors use two text classification tasks from the NLP domain to showcase the benefits of their proposed architecture. It would be good if they could expand and analyze how well does their framework generalizes across other non-binary tasks, tasks in other domains and different NNs. This is especially the case for the transfer learning task. \n\nIn the NAS overview section, readers would benefit more if authors spend more time in outlining the RL detail used in the original NAS framework instead of Figure 1 which looks like a space filler. \n\nAcross the two NLP tasks authors show that MNMS models trained simultaneously give better performance than hand tuned architectures. In addition, on the transfer learning evaluation approach they showcase the benefit of using the proposed framework in terms of the initially retrieved architecture and the number of iterations required to obtain the best performing one. \nFor better clarity figures 3 and 5 should be made bigger. \nWhat is LSS in figure 4?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642485379,"tcdate":1511748605674,"number":2,"cdate":1511748605674,"id":"HyU-Egtef","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Review","forum":"SyAbZb-0Z","replyto":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The neural architecture design is important and interesting","rating":"7: Good paper, accept","review":"Summary\nThis paper extends Neural Architecture Search (NAS) to the multi-task learning problem. A task conditioned model search controller is learned to handle multiple tasks simultaneously. The experiments are conducted on text data sets to evaluate the proposed method.\n\nPros\n1.\tThe problem of neural architecture design is important and interesting.\n2.\tThe motivation is strong. NAS (Zoph & Le, 2017) needs to train a model for a new task from scratch, which is inefficient. It is reasonable to introduce task embeddings into NAS to obtain a generalization model for multiple tasks.\n\nCons\n1.\tSome important technical details are missing, especially for the details regarding task embeddings.\n2.\tThe experiments are not sufficient.\n\nDetailed Comments\n1.\tThe paper does not provide the method of how to obtain task embeddings. In addition, if task embeddings are obtained by an auxiliary network, is it feasible to update task embeddings by updating the weights of this auxiliary network?\n2.\tThe discussion of off-policy training is questionable. There is no experiment to demonstrate the advantage of off-policy training compared to on-policy training.\n3.\tIn order to demonstrate the effectiveness of the idea of multi-task learning and task conditioning in MNMS, some architecture search methods for single-task should be conducted for comparison. For instance, NAS on SST or the Spanish language identification task should be compared.\n4.\tIn order to demonstrate the efficiency of MNMS, running time results of MNMS and NAS should be reported.\n5.\tIn my opinion, the title is not appropriate. The most important contribution of this paper is to search neural models for multiple tasks simultaneously using task conditioning. Only when this target is achieved, is it possible to transfer a pre-trained controller to new tasks with new task embeddings. Therefore, the title should highlight multitask neural model search rather than transfer learning.\n6.\tIn Figure 5, \"MNAS\" should be \"MNMS\".\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642485414,"tcdate":1511609824694,"number":1,"cdate":1511609824694,"id":"HkKJIALgM","invitation":"ICLR.cc/2018/Conference/-/Paper649/Official_Review","forum":"SyAbZb-0Z","replyto":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference/Paper649/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice project, a bit slim on the empirical side","rating":"5: Marginally below acceptance threshold","review":"The paper proposes an extension of the Neural Architecture Search approach, in which a single RNN controller is trained with RL to select hyperparameters for child networks that must perform different tasks. The architecture includes the notion of a \"task embedding\", that helps the controller keeping track of similarity between tasks, to facilitate transfer across related tasks.\n\nThe paper is very well written, and based on a simple but interesting idea. It also deals with core issues in current machine learning.\n\nOn the negative side, there is just one experiment, and it is somewhat limited. In the experiment, the proposed model is trained on two very different tasks (English sentiment analysis and Spanish language detection), and then asked to generalize to another English sentiment analysis task and to a Spanish sentiment analysis task. The models converge faster to high accuracy in the proposed transfer learning setup than when trained one a single task with the same architecture search strategy. Moreover, the task embedding for the new English task is closer to that of the training English task, and the same for the training/test Spanish tasks.\n\nMy main concern with the experiment is that the approach is only tested in a setup in which there is a huge difference between two classes of tasks (English vs Spanish), so the model doesn't need to learn very sophisticated task embeddings to group the tasks correctly for transfer. It would be good to see other experiments where there is less of a trivial structure distinguishing tasks, to check if transfer helps.\n\nAlso, I find it surprising that the Corpus Cine sentiment task embedding is not correlated at all with the SST sentiment task. If the controller is really learning something interesting about the nature of the tasks, I would have expected a differential effect, such that IMDB is only correlated with SST, but Corpus Cine is correlated to both the Spanish language identification task and SST. Perhaps, this is worth some discussion.\n\nFinally, it's not clear to me why the multitask architecture was used in the experiment even when no multi-task pre-training was conducted: shouldn't the simple neural architecture search method be used in this case?\n\nMinor points:\n\n\"diffferentiated\": different?\n\n\"outputted actions\": output actions\n\n\"the aim of increase the training stability\": the aim of increasing training stability\n\nInsert references for Polyak averaging and Savitzky-Golay filtering.\n\nFigure 3: specify that the Socher 2013 result is for SST\n\nFigure 4: does LSS stand for SST?\n\nI'm confused by Fig. 6: why aren't the diagonal values 100%?\n\nMNMS is referred to as MNAS in Figure 5.\n\nFor architecture search, the neuroevolution literature should also be cited (https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning).\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]}},{"tddate":null,"ddate":null,"tmdate":1515194661368,"tcdate":1509130502310,"number":649,"cdate":1509739178762,"id":"SyAbZb-0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyAbZb-0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Transfer Learning to Learn with Multitask Neural Model Search","abstract":"Deep learning models require extensive architecture design exploration and hyperparameter optimization to perform well on a given task. The exploration of the model design space is often made by a human expert, and optimized using a combination of grid search and search heuristics over a large space of possible choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach that has been proposed to automate architecture design. NAS has been successfully applied to generate Neural Networks that rival the best human-designed architectures. However, NAS requires sampling, constructing, and training hundreds to thousands of models to achieve well-performing architectures. This procedure needs to be executed from scratch for each new task. The application of NAS to a wide set of tasks currently lacks a way to transfer generalizable knowledge across tasks.\nIn this paper, we present the Multitask Neural Model Search (MNMS) controller. Our goal is to learn a generalizable framework that can condition model construction on successful model searches for previously seen tasks, thus significantly speeding up the search for new tasks. We demonstrate that MNMS can conduct an automated architecture search for multiple tasks simultaneously while still learning well-performing, specialized models for each task. We then show that pre-trained MNMS controllers can transfer learning to new tasks. By leveraging knowledge from previous searches, we find that pre-trained MNMS models start from a better location in the search space and reduce search time on unseen tasks, while still discovering models that outperform published human-designed models.","pdf":"/pdf/f6bc0e4771facae5fab7d132c34c17000c9d09bd.pdf","TL;DR":"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks.","paperhash":"anonymous|transfer_learning_to_learn_with_multitask_neural_model_search","_bibtex":"@article{\n  anonymous2018transfer,\n  title={Transfer Learning to Learn with Multitask Neural Model Search},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyAbZb-0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper649/Authors"],"keywords":["Learning to Learn","Meta learning","Reinforcement learning","Transfer learning"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}