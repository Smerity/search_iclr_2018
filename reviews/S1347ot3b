{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642443958,"tcdate":1511816969548,"number":3,"cdate":1511816969548,"id":"rk-MJ-ceM","invitation":"ICLR.cc/2018/Conference/-/Paper4/Official_Review","forum":"S1347ot3b","replyto":"S1347ot3b","signatures":["ICLR.cc/2018/Conference/Paper4/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Evaluation and analysis of sentence embeddings for automatic summarization is good, but better justification of motivations and significance of results needed.","rating":"3: Clear rejection","review":"This paper examines a number of sentence and document embedding methods for automatic summarization. It pairs a number of recent sentence embedding algorithms (e.g., Paragraph Vectors and Skip-Thought Vectors) with several simple summarization decoding algorithms for sentence selection, and evaluates the resulting output summary on DUC 2004 using ROUGE, based on the general intuition that the selected summary should be similar to the original document in the vector space induced by the embedding algorithm. It further provides a number of analyses of the sentence representations as they relate to summarization, and other aspects of the summarization process including the decoding algorithm.\n\nThe paper was well written and easy to understand. I appreciate the effort to apply these representation techniques in an extrinsic task.\n\nHowever, the signficance of the results may be limited, because the paper does not respond to a long line of work in summarization literature which have addressed many of the same points. In particular, I worry that the paper may in part be reinventing the wheel, in that many of the results are quite incremental with respect to previous observations in the field.\n\nGreedy decoding and non-redundancy: many methods in summarization use greedy decoding algorithms. For example, SumBasic (Nenkova and Vanderwende, 2005), and HierSum (Haghighi and Vanderwende, 2009) are two such papers. This specific topic has been thoroughly expanded on by the work on greedy decoding for submodular objective functions in summarization (Lin and Bilmes, 2011), as well as many papers which focus on how to optimize for both informativeness and non-redundancy (Kulesza and Taskar, 2012). \n\t\nThe idea that the summary should be similar to the entire document is known as centrality. Some papers that exploit or examine that property include (Nenkova and Vanderwende, 2005; Louis and Nenkova, 2009; Cheung and Penn, 2013)\n \nAnother possible reading of the paper is that its novelty lies in the evaluation of sentence embedding models, specifically. However, these methods were not designed for summarization, and I don't see why they should necessarily work well for this task out of the box with simple decoding algorithms without finetuning. Also, the ROUGE results are so far from the SotA that I'm not sure what the value of analyzing this suite of techniques is.\n  \nIn summary, I understand that this paper does not attempt to produce a state-of-the-art summarization system, but I find it hard to understand how it contributes to our understanding of future progress in the summmarization field. If the goal is to use summarization as an extrinsic evaluation of sentence embedding models, there needs to be better justification of this is a good idea when there are so many other issues in content selection that are not due to sentence embedding quality, but which affect summarization results.\n\nReferences:\n\nNenkova and Vanderwende, 2005. The impact of frequency on summarization. Tech report.\nHaghighi and Vanderwende, 2009. Exploring content models for multi-document summarization. NAACL-HLT 2009.\nLin and Bilmes, 2011. A class of submodular functions for document summarization. ACL-HLT 2011.\nKulesza and Taskar, 2012. Learning Determinantal Point Processes.\nLouis and Nenkova, 2009. Automatically evaluating content selection in summarization without human models. EMNLP 2009.\nCheung and Penn, 2013. Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain. ACL 2013.\n\nOther notes:\nThe acknowledgements seem to break double-blind reviewing.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Sentence Vectors Through Automatic Summarization","abstract":"Vector semantics, especially sentence vectors, have recently been used successfully in many areas of natural language processing. However, relatively little work has explored the internal structure and properties of spaces of sentence vectors. In this paper, we will explore the properties of sentence vectors by studying a particular real-world application: Automatic Summarization. In particular, we show that cosine similarity between sentence vectors and document vectors is strongly correlated with sentence importance and that vector semantics can identify and correct gaps between the sentences chosen so far and the document. In addition, we identify specific dimensions which are linked to effective summaries. To our knowledge, this is the first time specific dimensions of sentence embeddings have been connected to sentence properties. We also compare the features of different methods of sentence embeddings. Many of these insights have applications in uses of sentence embeddings far beyond summarization.","pdf":"/pdf/ebf03b1dc5f72c08fc493441abcae04e05abfc58.pdf","TL;DR":"A comparison and detailed analysis of various sentence embedding models through the real-world task of automatic summarization.","paperhash":"anonymous|exploring_sentence_vectors_through_automatic_summarization","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Sentence Vectors Through Automatic Summarization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1347ot3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper4/Authors"],"keywords":["Sentence Vectors","Vector Semantics","Automatic Summarization"]}},{"tddate":null,"ddate":null,"tmdate":1515642443997,"tcdate":1511719891793,"number":2,"cdate":1511719891793,"id":"Sk3CQYOgG","invitation":"ICLR.cc/2018/Conference/-/Paper4/Official_Review","forum":"S1347ot3b","replyto":"S1347ot3b","signatures":["ICLR.cc/2018/Conference/Paper4/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Preliminary work, limited technical contribution","rating":"2: Strong rejection","review":"This paper explored the effectiveness of four existing sentence embedding models on ten different document summarization methods leveraging various works in the literature. Evaluation has been conducted on the DUC-2004 dataset and ROUGE-1 and ROUGE-2 scores are reported. \n\nOverall, the paper significantly suffered from an immature writing style, numerous typos/grammatical mistakes, inconsistent organization of content, and importantly, limited technical contribution. Many recent sentence embedding models are missed such as those from Lin et al. (2017), Gan et al. (2017), Conneau et al. (2017), Jernite et al. (2017) etc. The evaluation and discussion sections were mostly unclear and the results of poorly performing methods were not reported at all making the comparisons and arguments difficult to comprehend. \n\nIn general, the paper seemed to be an ordinary reporting of some preliminary work, which at its current stage would not be much impactful to the research community.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Sentence Vectors Through Automatic Summarization","abstract":"Vector semantics, especially sentence vectors, have recently been used successfully in many areas of natural language processing. However, relatively little work has explored the internal structure and properties of spaces of sentence vectors. In this paper, we will explore the properties of sentence vectors by studying a particular real-world application: Automatic Summarization. In particular, we show that cosine similarity between sentence vectors and document vectors is strongly correlated with sentence importance and that vector semantics can identify and correct gaps between the sentences chosen so far and the document. In addition, we identify specific dimensions which are linked to effective summaries. To our knowledge, this is the first time specific dimensions of sentence embeddings have been connected to sentence properties. We also compare the features of different methods of sentence embeddings. Many of these insights have applications in uses of sentence embeddings far beyond summarization.","pdf":"/pdf/ebf03b1dc5f72c08fc493441abcae04e05abfc58.pdf","TL;DR":"A comparison and detailed analysis of various sentence embedding models through the real-world task of automatic summarization.","paperhash":"anonymous|exploring_sentence_vectors_through_automatic_summarization","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Sentence Vectors Through Automatic Summarization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1347ot3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper4/Authors"],"keywords":["Sentence Vectors","Vector Semantics","Automatic Summarization"]}},{"tddate":null,"ddate":null,"tmdate":1515642444043,"tcdate":1511515789506,"number":1,"cdate":1511515789506,"id":"BkSq8vBxG","invitation":"ICLR.cc/2018/Conference/-/Paper4/Official_Review","forum":"S1347ot3b","replyto":"S1347ot3b","signatures":["ICLR.cc/2018/Conference/Paper4/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Standard extractive summarisation techniques using sentence vectors","rating":"2: Strong rejection","review":"The authors report a number of experiments using off-the-shelf sentence embedding methods for performing extractive summarisation, using a number of simple methods for choosing the extracted sentences. Unfortunately the contribution is too minor, and the work too incremental, to be worthy of a place at a top-tier international conference such as ICLR. The overall presentation is also below the required standard. The work would be better suited for a focused summarisation workshop, where there would be more interest from the participants.\n\nSome of the statements motivating the work are questionable. I don't know if sentence vectors *in particular* have been especially successful in recent NLP (unless we count neural MT with attention as using \"sentence vectors\"). It's also not the case that the sentence reordering and text simplification problems have been solved, as is suggested on p.2. \n\nThe most effective method is a simple greedy technique. I'm not sure I'd describe this as being \"based on fundamental principles of vector semantics\" (p.4).\n\nThe citations often have the authors mentioned twice.\n\nThe reference to \"making or breaking applications\" in the conclusion strikes me as premature to say the least.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Sentence Vectors Through Automatic Summarization","abstract":"Vector semantics, especially sentence vectors, have recently been used successfully in many areas of natural language processing. However, relatively little work has explored the internal structure and properties of spaces of sentence vectors. In this paper, we will explore the properties of sentence vectors by studying a particular real-world application: Automatic Summarization. In particular, we show that cosine similarity between sentence vectors and document vectors is strongly correlated with sentence importance and that vector semantics can identify and correct gaps between the sentences chosen so far and the document. In addition, we identify specific dimensions which are linked to effective summaries. To our knowledge, this is the first time specific dimensions of sentence embeddings have been connected to sentence properties. We also compare the features of different methods of sentence embeddings. Many of these insights have applications in uses of sentence embeddings far beyond summarization.","pdf":"/pdf/ebf03b1dc5f72c08fc493441abcae04e05abfc58.pdf","TL;DR":"A comparison and detailed analysis of various sentence embedding models through the real-world task of automatic summarization.","paperhash":"anonymous|exploring_sentence_vectors_through_automatic_summarization","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Sentence Vectors Through Automatic Summarization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1347ot3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper4/Authors"],"keywords":["Sentence Vectors","Vector Semantics","Automatic Summarization"]}},{"tddate":null,"ddate":null,"tmdate":1509739535555,"tcdate":1507599155776,"number":4,"cdate":1509739532894,"id":"S1347ot3b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1347ot3b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exploring Sentence Vectors Through Automatic Summarization","abstract":"Vector semantics, especially sentence vectors, have recently been used successfully in many areas of natural language processing. However, relatively little work has explored the internal structure and properties of spaces of sentence vectors. In this paper, we will explore the properties of sentence vectors by studying a particular real-world application: Automatic Summarization. In particular, we show that cosine similarity between sentence vectors and document vectors is strongly correlated with sentence importance and that vector semantics can identify and correct gaps between the sentences chosen so far and the document. In addition, we identify specific dimensions which are linked to effective summaries. To our knowledge, this is the first time specific dimensions of sentence embeddings have been connected to sentence properties. We also compare the features of different methods of sentence embeddings. Many of these insights have applications in uses of sentence embeddings far beyond summarization.","pdf":"/pdf/ebf03b1dc5f72c08fc493441abcae04e05abfc58.pdf","TL;DR":"A comparison and detailed analysis of various sentence embedding models through the real-world task of automatic summarization.","paperhash":"anonymous|exploring_sentence_vectors_through_automatic_summarization","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Sentence Vectors Through Automatic Summarization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1347ot3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper4/Authors"],"keywords":["Sentence Vectors","Vector Semantics","Automatic Summarization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}