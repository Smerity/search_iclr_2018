{"notes":[{"tddate":null,"ddate":null,"tmdate":1515613061943,"tcdate":1515613061943,"number":10,"cdate":1515613061943,"id":"S1CtiJ4EM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"HygPguZ-M","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/AnonReviewer2"],"content":{"title":"Re: Answers to questions and our plan to revise the paper","comment":"Thanks for the reply, and for the revisions to the manuscript. It's great that you added more material, in particular more experiments and a theoretical analysis. Unfortunately I'm afraid it's a bit too much for a paper revision, as it would require a re-review, thus I am reluctant to improve my score without re-reading the whole thing carefully (which I lack time for).\n\nI'm still quite concerned by the small number of steps (10M) in experiments. I guess you are limited by your single GPU, which is sad, but I don't think one can draw meaningful conclusions on such small-scale experiments. I'm worried that prioritized experience replay doesn't seem to work much better than Vanilla DQN (no improvement on Median score for instance), while previous work suggests it is an important ingredient (ex: Dueling networks, Rainbow). Assuming this is not an implementation issue, the small number of steps could be the culprit."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515138856666,"tcdate":1515138726563,"number":8,"cdate":1515138726563,"id":"BJ1nRs3Xf","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Revised Contents","comment":"We have uploaded our revised paper. Below is the list of major revisions we made.\n\n1. Added Lin, 1991\n\nOriginal idea of replaying backward is described in the introduction and the related work sections.\nClarified that alg.1 is a special case of Lin's algorithm.\n\n2. MNIST DQN\n\nFigure 3 plotted until 200,000 steps.\nTable 1 reports both mean and median scores at 100,000 steps.\nMore detailed explanations on epsilon scheduling and the time step limit in Appendix C.\n\n3. Arcade Learning Environment\n\nAdded Prioritized ER and Retrace algorithm as baselines.\nFigure 5: changed the set of games from (Atlantis, Breakout, Gopher and Pong) to (Assault, Breakout, Gopher and Video Pinball) and also included results of new baselines.\nAppendix A and Appendix B: results from new baselines added. Appendix A no longer includes standard deviation information due to the margin.\nAppendix C contains specifications of baselines.\n\n4. Supplementary figure of Appendix D\n\nChanged the notations: capital 'A' means the realizations of the sampled episode, lowercase 'a' means the possible action index of the environment.\nDescirbed the update step by step: the first and second iterations.\n\n5. Theoretical Guarantees \n\nAdded a theorem in section 3 that episodic backward update with a diffusion coefficient beta in (0,1) defines a contraction operator and converges to the optimal value in finite and deterministic MDPs.\nStated the proof in Appendix E."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513393300276,"tcdate":1513393300276,"number":6,"cdate":1513393300276,"id":"Hync2bzMz","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["~yacine_mahdid1"],"readers":["everyone"],"writers":["~yacine_mahdid1"],"content":{"title":"Attempt at Reproducibility","comment":"As part of our efforts to reproduce the results of the original paper, we exhaustively researched about DQN, Optimality Tightening and other algorithms in the context of the original paper, and gained an understanding of the proposed EBU algorithm and setup the environment required to implement the algorithm. We attempted to use the code provided at https://github.com/ShibiHe/\nQ-Optimality-Tightening, the outline in the paper and the authors’ assistance to reproduce a subset of their results. \n\nObstacles to reproducibility:\nThe major obstacles we faced when attempting to reproduce the planned subset of the results were:\nDifficulty in translating the conceptual changes pointed out by the authors in their implementation and Q-Optimality Tightening (OT) codebases.\nComputational costs, and inefficiencies in the original OT code which meant that even on a Titan XP, only about 90 steps a second and 30% GPU utilization could be achieved(1). \nOutputs generated from the OT code were the only ones native to the code, therefore core evaluations (assessed and compared in the paper) couldn’t be run.\nLack of original EBU implementation code coupled with a lack of prior exhaustive theoretical knowledge meant that despite the modest number of changes, implementation of EBU was very challenging.\n\nReproduced results: We were able to establish two kinds of baselines, first being the average reward per episode over each of the 40 epochs for 3 games in the Arcade Learning Environment (Breakout, Video Pinball and Pong) and secondly we were able to reproduce and confirm the run time for the OT baseline for all the 3 aforementioned games through extrapolation, noticing signs of decay as epoch progressed for Pong, when running on GTX970, and a few spikes when running Breakout and Video Pinball on Titan XP. We did this using the original hyperparameters of the paper (which were almost the same as the OT code), as specified by the authors. Due to the significant computational cost of each run, we were not able to attempt a wide variety of alternative parameters. We were however, unable to definitively reproduce the author’s original results. The attempt to incorporate EBU was hampered by the size and complexity of the _do_training function and a lack of comments or documentation for the original OT code, which made correlating the pseudo code to the actual code difficult.\n\n\n\n(1). The limitation of GPU occupation saturating at 30% is also mentioned in the Readme of  Q-Optimality Tightening implementation on Github.\n\n\nTyler Kolody, Yacine Mahdid and Rajat Bhateja"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513149883457,"tcdate":1513149807768,"number":7,"cdate":1513149807768,"id":"B1__SU0Zz","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"B11nkS0-M","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Clarification","comment":"I guess your source of confusion is that the same number 30 is used twice.\n\n30 no-op evaluation method does not mean that we test the agent for 30 episodes. But means that each episode starts with at most 30 no-op actions. It is already implemented in the \"_init_episode\" function of \"ale_experiment.py\". And we take the average score of 30 (just by chance, has nothing to do with the \"30\" in 30 no ops evaluation) episodes generated by 30 no-op evaluation method.\n\nSo we modified the \"run_epoch\" function and \"run\" function of  \"ale_experiment.py\".  When the \"testing\" parameter is true, we ran the \"run_episode\" function 30 times and saved the average score. So STEPS_PER_TEST = 125000 is not used. \n\nRefer to page 7 of « Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening», He et al., 2017 :\n\"\"We strictly follow the evaluation procedure in (Mnih et al., 2015) which is often referred to as ‘30\nno-op evaluation.’ During both training and testing, at the start of the episode, the agent always\nperforms a random number of at most 30 no-op actions. During evaluation, our agent plays each\ngame 30 times for up to 5 minutes, and the obtained score is averaged over these 30 runs. An \u000f-\ngreedy policy with \u000f = 0:05 is used. Specifically, for each run, the game episode starts with at most\n30 no-op steps, and ends with ‘death’ or after a maximum of 5 minute game-play, which corresponds\nto 18000 frames.\"\"\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513144578323,"tcdate":1513144231291,"number":5,"cdate":1513144231291,"id":"B11nkS0-M","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"r1lBnW0Zf","signatures":["~Tyler_Kolody1"],"readers":["everyone"],"writers":["~Tyler_Kolody1"],"content":{"title":"Clarification","comment":"I think my confusion stems from the fact that the only test I see is one that is dictated by steps, default=125000, rather than episodes, and explicitly is commented as \"runtime evaluation, not 30 no-op evaluation\" and I'm wondering how to use the 30 op evaluation. I have been unable to track down any way to get scores, but instead only get the standard 3 csv files, based on the training. \n\nThank you"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513131148665,"tcdate":1513131064184,"number":6,"cdate":1513131064184,"id":"r1lBnW0Zf","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"Hyp9C2pbf","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Clarification","comment":"1. The max_steps variable should be 4500 steps. For a fair comparison, we set the same parameters for EBU and other baselines.\n\n2. There are some sources of randomness in the algorithm. \n    1) epsilon greedy exploration\n    2) sampling episode\n    3) number of steps for no-ops\nTo test the robustness of the algorithm, we used 8 different random seeds for the randomness. For each seed, at the end of every epoch we test the agent for 30 episodes with epsilon = 0.05. Since one epoch is 250,000 frames and we train for 1,000,000 frames in total, we have 40 test results for an agent with single random seed. Since there are oscillations in the test score, as mentioned in the paper, we take the best result out of 40 test scores as the result of the agent with that random seed. (following common practice (van Hasselt et al., 2015; Mnih et al., 2015)). Since we have 8 agents with different random seeds, we have 8 such results and we take mean of them to output the raw score.\n\nexample) suppose we have 10 epochs and 2 random seeds.\n\nepoch                     |    1    2    3    4    5    6    7    8    9    10\n\nseed 1 test score  |   10  20  30  40  50  60  40  20  50  50    --> seed 1 result = 60\n\nseed 2 test score  |    5   10  20  30  50  40  30  40  50  40     --> seed 2 result = 50 \n\nWe output mean of the results from all random seeds: (60+50)/2 = 55 as the result\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513122011776,"tcdate":1513111189050,"number":4,"cdate":1513111189050,"id":"Hyp9C2pbf","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"rkbujh2-M","signatures":["~Tyler_Kolody1"],"readers":["everyone"],"writers":["~Tyler_Kolody1"],"content":{"title":"Clarification","comment":"For the final step size, we found the max_steps variable and were wondering if it should be set to 18000 (the number of frames), or if the variable refers to steps and it should be set to 18000/4 frames per step = 4500? Was this change made for the OT baseline, or just the EBU code?\n\nRegarding outputs, I apologize if i've just overlooked something obvious in the code, but how do you get the raw scores for Appendix A? I see the 30 no-ops evaluation, but am not sure where to look/what I'm looking for regarding the output. \n\nUPDATE: The author of the original code mentioned they wrote something separate for those evaluations: was that the case for you as well?\n\nThanks as always"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513045505961,"tcdate":1513044841137,"number":5,"cdate":1513044841137,"id":"rkbujh2-M","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"B1Q8wv2ZG","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Reproducibility of our experiment","comment":"Thank you for your efforts.\n\n1. What we mean by the final time step is not the parameter freeze_interval. But the maximum number of frames that each episode can last. This is a conventional way taken by all other algorithms on the Atari domain since some games may not terminate if the agent takes no significant actions. Refer to \"run_episode\" function of \"ale_experiment.py\". \n\n2. As mentioned in the paper, we trained the agent for 40 epochs (we set 1 epoch = 62,500 steps = 250,000 frames). So that makes a total of 10M frames of training.\n\n3. If you mean the Nature DQN, you may want to use one of the following codes:\n    1) The original Lua code by Deepmind ( https://sites.google.com/a/deepmind.com/dqn/)\n    2) Theano based Deep-Q RL code (https://github.com/spragunr/deep_q_rl)\n\n4. We do not really have any tweaks to accelerate the learning. But the Theano version of the DQN code tends to be faster than the original Lua code. For its simplicity, \"Pong\" takes the least amount of training time out of the 49 games we tried. \n\nBest of luck.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1513023306798,"tcdate":1513023306798,"number":3,"cdate":1513023306798,"id":"B1Q8wv2ZG","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"Bk-FxwW-G","signatures":["~Rajat_Bhateja1"],"readers":["everyone"],"writers":["~Rajat_Bhateja1"],"content":{"title":"Further questions for reproducibility","comment":"Hi, as part of our efforts to reproduce the experiments suggested in your paper, we wanted to ask a few more questions:\n\n1. When you mention the changing the final time step to 18000 frames, did you mean the parameter freeze_interval in q_network.py and did you notice degrading steps per second when you were training it. \n\n2. When running a baseline for nature DQN, how many epochs did you run it for\n\n3. Is there a way to test the DQN without using Optimally Tightening, and finally\n\n4. Did you make any changes or tweaks to accelerate the learning to get the training times you mentioned and if you remember, which game took the least amount of time to train.\n\nOnce again, thanks for your previous response and hoping to hear from you soon.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512335213598,"tcdate":1512335213598,"number":2,"cdate":1512335213598,"id":"HJIuwJGZz","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"Bk-FxwW-G","signatures":["~Tyler_Kolody1"],"readers":["everyone"],"writers":["~Tyler_Kolody1"],"content":{"title":"Thank you","comment":"We really appreciate the quick response and details. \n\nBest of luck"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512305689728,"tcdate":1512305689728,"number":4,"cdate":1512305689728,"id":"B1fm4OW-G","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"H1gBrkcgM","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Our plan to revise the paper","comment":"Thank you for your time and suggestions.\n\nAs you mentioned, we guess there may be some relation between prioritized experience replay and our method. As all the reviewers have mentioned, we will add prioritized experience replay and retrace algorithm as the baseline to compare in the revised version.\n\nAny further suggestions are appreciated."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512305467794,"tcdate":1512305269190,"number":3,"cdate":1512305269190,"id":"ByT_zd-Wz","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"HyxmggJbM","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Our plan to revise the paper","comment":"Thank you for your time and suggestions.\n\nAs you and other reviewers have mentioned, we strongly agree that we lack the comparisons to other related methods. We will try to compare our results and those of prioritized experience replay and retrace algorithm in the revised version. Also we will try to add some theoretical analysis to compare our algorithm to others.\n\nAny further comments and thoughts are appreciated."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512305994107,"tcdate":1512304728475,"number":2,"cdate":1512304728475,"id":"HygPguZ-M","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"SJ3y_pYxM","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Answers to questions and our plan to revise the paper","comment":"Thank you for your detailed feedback and questions.\nI'd like to answer some of questions and share our plan to revise the paper with regards to your feedback.\n\n1. Limited comparison\nWe strongly agree that we need more baseline algorithms to show the effectiveness of our algorithm. As other reviewers have suggested, we will include the performance of prioritized experience replay and retrace algorithm in the revised version.\n\n2.  Idea of replaying experiences in backward\nThank you for the reference, we will mention the relationship between Lin's idea and our methods in the revised version.\n\n3. Poor performance in MNIST DQN\nLearning curve tends to converge so fast for all algorithms when we used simple 2D maze, so it was difficult to compare different algorithms. So we used MNIST images as the state representation to make the learning process of general state transitions harder. We trained the agents for 200,000 steps, and all three algorithms (backward DQN, vanilla DQN, n-step DQN) converge to 1. In the paper, we showed the plots over 100,000 steps to show the effectiveness of our method in the early stages of training. To avoid any confusion, we will show the results until 200,000 steps in the revised version. Note that the vanilla DQN is trained for 50M steps (200M frames) in the Atari domain. Since the MNIST DQN environment is much simple, it is reasonable that the training is done for 0.2M steps. \n\n4. A few more comments on MNIST DQN:\nWe terminated the episode when the agent stays in the maze for more than 1000 time steps.\nWe trained 50 different independent agents each in a different random maze and reported the mean score. But as you suggested, mean may be a bad measure due to outliers. So we will show both mean and median of 50 agents' scores as the result in the revised version.\n\n5. Running time compared to RAINBOW\nRunning time may vary a lot depending on which device and distributed method you use. We used a single GPU to train an agent. As reported in the paper, it took 152 hours to train 490M frames (49 games x 10M frames). RAINBOW takes 10 days to train 200M frames. We will mention that the training time is not the 'mean' training time of 49 games but the 'sum' of training time in the revised version.\n\n6. The last figure\nWe apologize for the confusion. The first column and fourth rows of initialization and recursive updates part should be changed as \"s_1\" -> \"s_2\". The beta is applied only for the positions where the actions were taken in the replay memory, as the update is done from right to left.  a_T = A_2, a_(T-1) = A_1 in the example. We will make this clear in the revised version.\n\n7. Typos and Citations\nWe will correct the typos and citations as your suggestions.\n\nThank you so much for your ideas and suggestions.\nAny further comments are appreciated.\n\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512352295530,"tcdate":1512300665302,"number":1,"cdate":1512300665302,"id":"Bk-FxwW-G","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"H1GqLfb-f","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Reproducibility of our experiment","comment":"We are planning to upload our code after the revision process since we cannot reveal our identity before the final decision. \n\nBut as described in the paper, our code is built upon the codes of the paper (« Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening», He et al., 2017) https://github.com/ShibiHe/Q-Optimality-Tightening\nAll the hyperparameters and network structures are the same as those of above, except that we applied the final time step of 18000 frames (5 mins) for each episode. \n\nThe two major differences between our code and that of Optimality Tightening are the followings.\n1. To implement our backward target generation, we modified the \"_do_training\" function of \"ale_agents.py\".\n2. To sample a random episode, we defined \"random_batch\" function in \"ale_data_set.py\". This function is run only after all steps of previously sampled episode are updated.\n\nThank you.\n"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512281737872,"tcdate":1512281737872,"number":1,"cdate":1512281737872,"id":"H1GqLfb-f","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["~Tyler_Kolody1"],"readers":["everyone"],"writers":["~Tyler_Kolody1"],"content":{"title":"Reproducibility Challenge request","comment":"I'm taking part in the reproducibility challenge put forward by Prof. Joelle Pineau and was wondering if we could have access to your code and any other information that might be pertinent when recreating your experiment. Any information such as hyperparameter values not mentioned in the paper and library versions would be extremely useful. \n\nThank you"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642438044,"tcdate":1512140824202,"number":3,"cdate":1512140824202,"id":"HyxmggJbM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Review","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is interesting, but it lacks the proper comparisons to previously published techniques.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.\n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques.\n\nThe results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA.\n\nIn the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.\n\nThere has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case.\n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642438086,"tcdate":1511810360504,"number":2,"cdate":1511810360504,"id":"H1gBrkcgM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Review","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An RL update for DQN-like agents based on recursive max backups.","rating":"6: Marginally above acceptance threshold","review":"The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update. The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards. This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.). Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max.\n\nThe paper is written in a clear way. The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there. I think this should have been one of the baselines to compare to for that reason.\n\nThe experimental results seem promising in the illustrative MNIST domain. Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642438125,"tcdate":1511802852471,"number":1,"cdate":1511802852471,"id":"SJ3y_pYxM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Review","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A potentially interesting approach, but with weak theoretical and empirical validation","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a « backward » update (i.e. from end to start of episode). The targets’ update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation). This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5.\n\nThe intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL. Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay («  Programming Robots Using Reinforcement Learning and Teaching », Lin, 1991), something that is not mentioned here. That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin’s algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments).\n\nIn the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation. Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time? Or maybe the average is a bad performance measure due to outliers? I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method). Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here). The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm. Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance.\n\nA few additional small remarks and questions:\n- « Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. »: should « unless » be replaced by « if »?\n- In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze?\n- Typo in eq. 3: the - in the max should be a comma\n- There is a good amount of typos and grammar errors, though they do not harm the readability of the paper\n- Citations for « Deep Reinforcement Learning with Double Q-learning » and « Dueling Network Architectures for Deep Reinforcement Learning » could refer to their conference versions\n- « epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner »: please specify the exact formula\n- Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515138488803,"tcdate":1509104382861,"number":356,"cdate":1509739343992,"id":"BJvWjcgAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/dc4dce57d727ed94867eae1f67633983ce333e7f.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]},"nonreaders":[],"replyCount":18,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}