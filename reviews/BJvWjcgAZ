{"notes":[{"tddate":null,"ddate":null,"tmdate":1512335213598,"tcdate":1512335213598,"number":2,"cdate":1512335213598,"id":"HJIuwJGZz","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"Bk-FxwW-G","signatures":["~Tyler_Kolody1"],"readers":["everyone"],"writers":["~Tyler_Kolody1"],"content":{"title":"Thank you","comment":"We really appreciate the quick response and details. \n\nBest of luck"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512305689728,"tcdate":1512305689728,"number":4,"cdate":1512305689728,"id":"B1fm4OW-G","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"H1gBrkcgM","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Our plan to revise the paper","comment":"Thank you for your time and suggestions.\n\nAs you mentioned, we guess there may be some relation between prioritized experience replay and our method. As all the reviewers have mentioned, we will add prioritized experience replay and retrace algorithm as the baseline to compare in the revised version.\n\nAny further suggestions are appreciated."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512305467794,"tcdate":1512305269190,"number":3,"cdate":1512305269190,"id":"ByT_zd-Wz","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"HyxmggJbM","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Our plan to revise the paper","comment":"Thank you for your time and suggestions.\n\nAs you and other reviewers have mentioned, we strongly agree that we lack the comparisons to other related methods. We will try to compare our results and those of prioritized experience replay and retrace algorithm in the revised version. Also we will try to add some theoretical analysis to compare our algorithm to others.\n\nAny further comments and thoughts are appreciated."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512305994107,"tcdate":1512304728475,"number":2,"cdate":1512304728475,"id":"HygPguZ-M","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"SJ3y_pYxM","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Answers to questions and our plan to revise the paper","comment":"Thank you for your detailed feedback and questions.\nI'd like to answer some of questions and share our plan to revise the paper with regards to your feedback.\n\n1. Limited comparison\nWe strongly agree that we need more baseline algorithms to show the effectiveness of our algorithm. As other reviewers have suggested, we will include the performance of prioritized experience replay and retrace algorithm in the revised version.\n\n2.  Idea of replaying experiences in backward\nThank you for the reference, we will mention the relationship between Lin's idea and our methods in the revised version.\n\n3. Poor performance in MNIST DQN\nLearning curve tends to converge so fast for all algorithms when we used simple 2D maze, so it was difficult to compare different algorithms. So we used MNIST images as the state representation to make the learning process of general state transitions harder. We trained the agents for 200,000 steps, and all three algorithms (backward DQN, vanilla DQN, n-step DQN) converge to 1. In the paper, we showed the plots over 100,000 steps to show the effectiveness of our method in the early stages of training. To avoid any confusion, we will show the results until 200,000 steps in the revised version. Note that the vanilla DQN is trained for 50M steps (200M frames) in the Atari domain. Since the MNIST DQN environment is much simple, it is reasonable that the training is done for 0.2M steps. \n\n4. A few more comments on MNIST DQN:\nWe terminated the episode when the agent stays in the maze for more than 1000 time steps.\nWe trained 50 different independent agents each in a different random maze and reported the mean score. But as you suggested, mean may be a bad measure due to outliers. So we will show both mean and median of 50 agents' scores as the result in the revised version.\n\n5. Running time compared to RAINBOW\nRunning time may vary a lot depending on which device and distributed method you use. We used a single GPU to train an agent. As reported in the paper, it took 152 hours to train 490M frames (49 games x 10M frames). RAINBOW takes 10 days to train 200M frames. We will mention that the training time is not the 'mean' training time of 49 games but the 'sum' of training time in the revised version.\n\n6. The last figure\nWe apologize for the confusion. The first column and fourth rows of initialization and recursive updates part should be changed as \"s_1\" -> \"s_2\". The beta is applied only for the positions where the actions were taken in the replay memory, as the update is done from right to left.  a_T = A_2, a_(T-1) = A_1 in the example. We will make this clear in the revised version.\n\n7. Typos and Citations\nWe will correct the typos and citations as your suggestions.\n\nThank you so much for your ideas and suggestions.\nAny further comments are appreciated.\n\n\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512352295530,"tcdate":1512300665302,"number":1,"cdate":1512300665302,"id":"Bk-FxwW-G","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Comment","forum":"BJvWjcgAZ","replyto":"H1GqLfb-f","signatures":["ICLR.cc/2018/Conference/Paper356/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper356/Authors"],"content":{"title":"Reproducibility of our experiment","comment":"We are planning to upload our code after the revision process since we cannot reveal our identity before the final decision. \n\nBut as described in the paper, our code is built upon the codes of the paper (« Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening», He et al., 2017) https://github.com/ShibiHe/Q-Optimality-Tightening\nAll the hyperparameters and network structures are the same as those of above, except that we applied the final time step of 18000 frames (5 mins) for each episode. \n\nThe two major differences between our code and that of Optimality Tightening are the followings.\n1. To implement our backward target generation, we modified the \"_do_training\" function of \"ale_agents.py\".\n2. To sample a random episode, we defined \"random_batch\" function in \"ale_data_set.py\". This function is run only after all steps of previously sampled episode are updated.\n\nThank you.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512281737872,"tcdate":1512281737872,"number":1,"cdate":1512281737872,"id":"H1GqLfb-f","invitation":"ICLR.cc/2018/Conference/-/Paper356/Public_Comment","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["~Tyler_Kolody1"],"readers":["everyone"],"writers":["~Tyler_Kolody1"],"content":{"title":"Reproducibility Challenge request","comment":"I'm taking part in the reproducibility challenge put forward by Prof. Joelle Pineau and was wondering if we could have access to your code and any other information that might be pertinent when recreating your experiment. Any information such as hyperparameter values not mentioned in the paper and library versions would be extremely useful. \n\nThank you"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222624733,"tcdate":1512140824202,"number":3,"cdate":1512140824202,"id":"HyxmggJbM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Review","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is interesting, but it lacks the proper comparisons to previously published techniques.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a new way of sampling data for updates in deep-Q networks. The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.\n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques.\n\nThe results presented by this paper shows improvement over the baseline. But the Atari results is still significantly worse than the current SOTA.\n\nIn the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy. Some (theoretical) analysis would be nice. It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.\n\nThere has been a number of highly relevant papers. Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case.\n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method. Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline. The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222624774,"tcdate":1511810360504,"number":2,"cdate":1511810360504,"id":"H1gBrkcgM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Review","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An RL update for DQN-like agents based on recursive max backups.","rating":"6: Marginally above acceptance threshold","review":"The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update. The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards. This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.). Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max.\n\nThe paper is written in a clear way. The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there. I think this should have been one of the baselines to compare to for that reason.\n\nThe experimental results seem promising in the illustrative MNIST domain. Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222624809,"tcdate":1511802852471,"number":1,"cdate":1511802852471,"id":"SJ3y_pYxM","invitation":"ICLR.cc/2018/Conference/-/Paper356/Official_Review","forum":"BJvWjcgAZ","replyto":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference/Paper356/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A potentially interesting approach, but with weak theoretical and empirical validation","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a « backward » update (i.e. from end to start of episode). The targets’ update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation). This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5.\n\nThe intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL. Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay («  Programming Robots Using Reinforcement Learning and Teaching », Lin, 1991), something that is not mentioned here. That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin’s algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments).\n\nIn the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation. Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time? Or maybe the average is a bad performance measure due to outliers? I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method). Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here). The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm. Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance.\n\nA few additional small remarks and questions:\n- « Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. »: should « unless » be replaced by « if »?\n- In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze?\n- Typo in eq. 3: the - in the max should be a comma\n- There is a good amount of typos and grammar errors, though they do not harm the readability of the paper\n- Citations for « Deep Reinforcement Learning with Double Q-learning » and « Dueling Network Architectures for Deep Reinforcement Learning » could refer to their conference versions\n- « epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner »: please specify the exact formula\n- Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739346811,"tcdate":1509104382861,"number":356,"cdate":1509739343992,"id":"BJvWjcgAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJvWjcgAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update","abstract":"We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.","pdf":"/pdf/e6137ef337e298305c4e3a93e63f991422441aae.pdf","TL;DR":"We propose Episodic Backward Update, a novel deep reinforcement learning algorithm which samples transitions episode by episode and updates values recursively in a backward manner to achieve fast and stable learning.","paperhash":"anonymous|sampleefficient_deep_reinforcement_learning_via_episodic_backward_update","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJvWjcgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper356/Authors"],"keywords":["Deep Learning","Reinforcement Learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}