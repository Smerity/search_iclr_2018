{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222708831,"tcdate":1511833812771,"number":3,"cdate":1511833812771,"id":"Sy6ClHqef","invitation":"ICLR.cc/2018/Conference/-/Paper65/Official_Review","forum":"BkSDMA36Z","replyto":"BkSDMA36Z","signatures":["ICLR.cc/2018/Conference/Paper65/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"() Summary\nIn this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark. The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words. In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a \"context unit\" of size d x K, where d is the embedding size and K the region size. Thus, the model also have a vector representation for pair of word and position in the region. Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the \"context unit\" of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and \"context\" vectors are exchanged). The max-pooling operation is then used to obtain a vector representation of size d. Then a linear classifier is applied on top of the sum of the region embeddings. The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015). They obtain state of the art results on most of the datasets. They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the \"context units\" vector by a scalar. The authors also provide some visualisation of the parameters of their model.\n\n() Discussion\nOverall, I think that the proposed method is sound and well justified. The empirical evaluations, analysis and comparisons to existing methods are well executed. I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art. I suspect that the model is also computationally efficient: can the authors report training time for different datasets? I think that it would make the paper stronger. One of the main limitations of the model, as stated by the authors, is its number of parameters. Could the authors also report these?\n\nWhile the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed. Indeed, the papers contains many typos (I have listed a few), as well as ungrammatical sentences. I also think that a discussion of the \"attention is all you need\" paper by Vaswani et al. is needed, as both articles seem strongly related.\n\nAs a minor comment, I advise the authors to use a different letter for \"word embeddings\" and the \"projected word embeddings\" (equation at the bottom of page 3). It would also make the paper more clear.\n\n() Pros / Cons:\n+ simple yet powerful method for text classification\n+ strong experimental results\n+ ablation study / analysis of influence of parameters\n- writing of the paper\n- missing discussion to the \"attention is all you need paper\", which seems highly relevant\n\n() Typos:\nPage 1\n\"a support vectors machineS\" -> \"a support vector machine\"\n\"performs good\" -> \"performs well\"\n\"the n-grams was widely\" -> \"n-grams were widely\"\n\"to apply large region size\" -> \"to apply to large region size\"\n\"are trained separately\" -> \"do not share parameters\"\n\nPage 2\n\"convolutional neural networks(CNN)\" -> \"convolutional neural networks (CNN)\"\n\"related works\" -> \"related work\"\n\"effective in Wang and Manning\" -> \"effective by Wang and Manning\"\n\"applied on text classification\" -> \"applied to text classification\"\n\"shard(word independent)\" -> \"shard (word independent)\"\n\nPage 3\n\"can be treat\" -> \"can be treated\"\n\"fixed length continues subsequence\" -> \"fixed length contiguous subsequence\"\n\"w_i stands for the\" -> \"w_i standing for the\"\n\"which both the unit\" -> \"where both the unit\"\n\"in vocabulary\" -> \"in the vocabulary\"\n\netc...","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1511780858868,"tcdate":1511777379724,"number":5,"cdate":1511777379724,"id":"B1hPEvKlM","invitation":"ICLR.cc/2018/Conference/-/Paper65/Public_Comment","forum":"BkSDMA36Z","replyto":"H1cW4Kulf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Embeddings in competing methods","comment":"Thank you for asking.  \n\nIn this paper, previous works' experimental results in Table 2 are reported from Joulin et al.(2016). From our knowledge, we believe FastText and the character based methods did not use pre-trained word embeddings, and we are not very sure whether the embeddings are pre-trained  in Discriminative LSTM from now on.  Performances on these eight datasets of word based CNN can be found in Zhang & LeCun (2015)(with and without pre-trained word embeddings), which show the effect of whether the embeddings are pre-trained. \n\nMore details about those experiments under different conditions can be found in the original papers for each method. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222708870,"tcdate":1511730467317,"number":2,"cdate":1511730467317,"id":"r1sXToOgf","invitation":"ICLR.cc/2018/Conference/-/Paper65/Official_Review","forum":"BkSDMA36Z","replyto":"BkSDMA36Z","signatures":["ICLR.cc/2018/Conference/Paper65/AnonReviewer3"],"readers":["everyone"],"content":{"title":"New model for text classification","rating":"6: Marginally above acceptance threshold","review":"The authors present a model for text classification. The parameters of the model are an embedding for each word and a local context unit. The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word). After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task. The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task. The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings. Here the embedding of word i is combined with the elements of the context units of words in the context. To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units). \n\nThe method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy. the choice of baselines is convincing. What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?\n\nThe introduction was fine. Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.\nThe related work section only makes sense *after* there is at least a minimal explanation of what the local context units do. A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear. Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.\n\nThe authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}.  \n\nThe included baselines are extensive and the proposed method outperforms existing methods on most datasets. In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper. Figure 2 and 3 could be next to each other to save space. \nI found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?\n\nQualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level. It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position. Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? \n\nPros:\n + simple model\n + strong quantitative results\n\nCons:\n - notation (i.e. precise definition of r_{i,c})\n - qualitative analysis could be extended\n - writing could be improved  ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1511719938342,"tcdate":1511719938342,"number":2,"cdate":1511719938342,"id":"H1cW4Kulf","invitation":"ICLR.cc/2018/Conference/-/Paper65/Official_Comment","forum":"BkSDMA36Z","replyto":"HJl5GqvlG","signatures":["ICLR.cc/2018/Conference/Paper65/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper65/AnonReviewer3"],"content":{"title":"What about the embeddings in competing methods?","comment":"Thank you so much for the clarifications! What about the embeddings of the competing methods, e.g. Fasttext? Are they pretrained? Are they retrained for the classification task at hand?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1511658119755,"tcdate":1511658119755,"number":4,"cdate":1511658119755,"id":"HJl5GqvlG","invitation":"ICLR.cc/2018/Conference/-/Paper65/Public_Comment","forum":"BkSDMA36Z","replyto":"ryecqWvxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"They are jointly trained as classification model parameters with cross entropy loss","comment":"Thanks for your question.\n\nIn this paper, we are going to produce the task related region embeddings which can be used to improve the performance of classification tasks, so the word embedding matrix E and local context unit matrix U are randomly initialized and jointly trained as classification model parameters, here we use the cross entropy loss as the classification loss.\n\nNoticed that the power of the local context unit on learning task related region embeddings, we are\ninterested to explore its ability to semi-supervised or unsupervised learning in our future work, but in this paper, we only focus on developing a new mechanism to extract the predictive features on small text regions."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222708909,"tcdate":1511634162678,"number":1,"cdate":1511634162678,"id":"ryjxrEwlM","invitation":"ICLR.cc/2018/Conference/-/Paper65/Official_Review","forum":"BkSDMA36Z","replyto":"BkSDMA36Z","signatures":["ICLR.cc/2018/Conference/Paper65/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of \"Bag of region embeddings ...\"","rating":"6: Marginally above acceptance threshold","review":"The authors propose a mechanism for learning task-specific region embeddings for use in text classification. Specifically, this comprises a standard word embedding an accompanying local context embedding. \n\nThe key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size. Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings. The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways. Region embeddings are then composed (summed) and fed through a standard model. \n\nStrong points\n---\n+ The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization. Further, the work is clearly presented.\n\n+ At the very least the model does seem comparable in performance to various recent methods (as per Table 2), however as noted below the gains are marginal and I have some questions on the setup.\n\n+ The authors perform ablation experiments, which are always nice to see. \n\nWeak points\n---\n- I have a critical question for clarification in the experiments. The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach? \n\n- The gains here appear to be consistent, but they seem marginal. The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range). Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see \"A sensitivity analysis of (and practitioners guide to) CNNs...\" Zhang and Wallace, 2015.)\n\n- The related work section seems light. For instance, there is no discussion at all of LSTMs and their application to text classificatio (e.g., Tang et al., EMNLP 2015) -- although it is noted that the authors do compare against D-LSTM,  or char-level CNNs for the same (see Zhang et al., NIPs 2015). Other relevant work not discussed includes Iyyer et al. (ACL 2015). In their respective ways, these papers address some of the same issues the authors consider here. \n\n- The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc. I would have appreciated more intuition behind these approaches. \n\nSmall comments\n---\nThere is a typo in Figure 4 -- \"Howerver\" should be \"However\"","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1511623303642,"tcdate":1511623303642,"number":1,"cdate":1511623303642,"id":"ryecqWvxG","invitation":"ICLR.cc/2018/Conference/-/Paper65/Official_Comment","forum":"BkSDMA36Z","replyto":"BkSDMA36Z","signatures":["ICLR.cc/2018/Conference/Paper65/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper65/AnonReviewer3"],"content":{"title":"Please clarify loss","comment":"How are you training the local context units? Which loss are you optimizing to get the embeddings E and context parameters U?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1511491160192,"tcdate":1511491110109,"number":3,"cdate":1511491110109,"id":"H1AXIZSeG","invitation":"ICLR.cc/2018/Conference/-/Paper65/Public_Comment","forum":"BkSDMA36Z","replyto":"ryjWc4Jgz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Further information of the datasets","comment":"Thank you for your comments and suggestions. We generally agree with your understanding of this paperï¼Œ if there are any problems with the implementation, please let us know.\n\nAbout your questions with datasets:\n\n1)The average document length for the different datasets are following, we will add them in the next revision:\n\n    *Dataset*       *Average Document Length*\n    Yelp P.         156.153767857\n    Yelp F.         157.641587692\n    Yah.A.          111.638106429\n    Sogou           578.654484444\n    DBP             55.3340017857\n    Amz.P.          90.8814119444\n    Amz.F.          92.7758653333\n    AG              43.6560583333\n\nIt is worth to mention that the motivation of our method is not to capture the long distance dependence, but to capture the local features of the text from a new perspective(Although more complex upper layers like RNNs can be applied to capture the long term dependencies in the document, we simply used a bag of region embeddings upper layer).\n\n\n2)The datasets are built from real world and are widely used to evaluate the model performance on text classification tasks(see details in the references of this paper). On some datasets such as AG and Sogou, the Bow and Bag-of-Ngrams can achieve 90%+ accuracy which indeed shows that these datasets are naturally easy to be separated, but we can see that our method achieves the state of the art results against the baselines and along with other deep models(LSTMs, CNNs), on some other more difficult datasets, such as multi-level sentiment analysis(Yelp.F., Amz.F), QA matching(Yah.A), our method yields a significant performance gain compared to the BoW and ngrams baselines (5%-8%), which shows that our method can capture local text features better."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1511111171275,"tcdate":1511111171275,"number":2,"cdate":1511111171275,"id":"ryjWc4Jgz","invitation":"ICLR.cc/2018/Conference/-/Paper65/Public_Comment","forum":"BkSDMA36Z","replyto":"BkSDMA36Z","signatures":["~Shagun_Sodhani1"],"readers":["everyone"],"writers":["~Shagun_Sodhani1"],"content":{"title":"Query about the datasets","comment":"Hey authors\n\nThank you for sharing the implementation of the paper - it goes a long way towards ensuring reproducibility.\n\nMy understanding of the paper is the following: for each word in the vocabulary, along with learning a word vector, learn a context matrix. This context matrix would introduce a soft-attention kind of effect and is expected to be more powerful than the use of just a context vector for capturing the context. Please correct me if there is something wrong in my understanding :)\n\nIn the experiments section, the paper uses 8 different datasets. It would be helpful if the paper also mentions the average document length for the different datasets. That could be a crude proxy to understand how important is it to capture the long term dependencies in the document. Further, the simple baselines of BoW and ngrams give decent performance (around 90% for 5 datasets). Could it be the case that the datasets are not very \"difficult\"? I have not used these datasets and would be glad to know what do the authors feel about the same."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1510743152951,"tcdate":1510743152951,"number":1,"cdate":1510743152951,"id":"SkKu25KJM","invitation":"ICLR.cc/2018/Conference/-/Paper65/Public_Comment","forum":"BkSDMA36Z","replyto":"BkSDMA36Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Code of the proposed models","comment":"In order to facilitate reviewers to reproduce our results, we share the implementation of our method at here(https://github.com/text-representation/local-context-unit).  We will formally open-source our code upon publishing the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1509739505487,"tcdate":1508856412705,"number":65,"cdate":1509739502834,"id":"BkSDMA36Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkSDMA36Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bag of region embeddings via local context unit for text classification","abstract":"Contextual information and word orders are proved valuable for text classification task. To make use of local word order information, n-grams are commonly used features in several models, such as linear models. However, these models commonly suffer the data sparsity problem and are difficult to represent large size region. The discrete or distributed representations of n-grams can be regarded as region embeddings, which are representations of fixed size regions. In this paper, we propose two novel text classification models that learn task specific region embeddings without hand crafted features, hence the drawbacks of n-grams can be overcome. In our model, each word has two attributes, a commonly used word embedding, and an additional local context unit which is used to interact with the word's local context. Both the units and word embeddings are used to generate representations of regions, and are learned as model parameters. Finally, bag of region embeddings of a document is fed to a linear classifier. Experimental results show that our proposed methods achieve state-of-the-art performance on several benchmark datasets. We provide visualizations and analysis illustrating that our proposed local context unit can capture the syntactic and semantic information.","pdf":"/pdf/95e0c4f8d6cf4a4c714db801605bea2b89847735.pdf","paperhash":"anonymous|bag_of_region_embeddings_via_local_context_unit_for_text_classification","_bibtex":"@article{\n  anonymous2018bag,\n  title={Bag of region embeddings via local context unit for text classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkSDMA36Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper65/Authors"],"keywords":["region embedding","local context unit","text classification"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}