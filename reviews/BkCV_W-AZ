{"notes":[{"tddate":null,"ddate":null,"tmdate":1515190421919,"tcdate":1515189595927,"number":4,"cdate":1515189595927,"id":"SkNPS_pXz","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Comment","forum":"BkCV_W-AZ","replyto":"S17ehb1WM","signatures":["ICLR.cc/2018/Conference/Paper694/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper694/Authors"],"content":{"title":"Thanks for your comments","comment":"Thanks for your comments! We have addressed the clarity issues raised by the reviewers and added additional experiments to address reviewer concerns, which we detailed below.\n\n- The presentation of the paper can be improved a bit\n\nWe admit Section 3 can be cleaned up -- in the final we will address this.\n\n- There are some papers that could be connected. Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments. ... One interesting reference: Memory-based control with recurrent neural networks by Heess et al.\n\nThanks! In the final we will elaborate on additional connections with the literature.\n\n- The argument the authors made against recurrent value functions is that recurrent value could be hard to train. An experiment illustrating this effect could be illuminating.\n\nWe added Section 6.3.2 with an experiment to illustrate this effect.\n\nWe compared feedforward and recurrent convolutional policies and value functions learned using A2C on the maze-like ViZDoom MyWayHome scenario. Adding recurrence does seem to have a small positive effect, but it is less than the effect due to e.g. choice of algorithm.\n\n- Can the proposed approach help when we have recurrent value functions? Since recurrence does not guarantee that all information needed is captured.\n\nAs ARM only involves different value function estimators it should be able to handle recurrent value functions. In practice we currently run ARM in batch mode only. An online version of ARM would handle recurrence much more naturally; this work is in progress.\n\n- Potential typos: in the 4th bullet point in section 3.1, should it be \\rho^{\\pi}(h, s')?\n\nThanks, this was a typo, it should be: s' -> h'"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515190290358,"tcdate":1515189477334,"number":3,"cdate":1515189477334,"id":"H1a1BOTQM","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Comment","forum":"BkCV_W-AZ","replyto":"ByyLE_tgG","signatures":["ICLR.cc/2018/Conference/Paper694/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper694/Authors"],"content":{"title":"Thanks for your comments","comment":"Thanks for your comments! We have addressed the clarity issues raised by the reviewers and added additional experiments to address reviewer concerns, which we detailed below.\n\n- Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused.\n\nWe admit Section 3 can be cleaned up -- in the final we will address this.\n\n- The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc.\n\nIn our modified section 3.5 we address this point in more detail.\n\nOne way to think about existing policy gradient methods is that they approximately minimize the KL-divergence between the policy and a target Boltzmann distribution induced either by the Q-function or by the advantage function. (For simplicity we ignore considerations of entropy regularization.) As a result of the KL-divergence/Boltzmann interpretation, the policy gradient update is linearly proportional to the advantage.\n\nBy analogy, if one were to minimize the KL-divergence between the policy and a different kind of target distribution, the resulting gradient updates would also be different. In particular, ARM proposes a distribution based on regret-matching which is proportional to the positively clipped part of the cumulative clipped advantage (i.e. the \"A+\" function). If we were to implement a policy gradient-like version of ARM, then the resulting gradient update would be proportional to the _logarithm_ of the cumulative clipped advantage, and is inherently different than the existing policy gradient update. One consequence is that logarithmic dependence on the advantage means that ARM may be less sensitive to value function overestimation that may result in large positive advantages.\n\n- it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods ... I'd want to understand how/why and whether we should expect this universally\n\nWe added section 3.6 to address this point in more detail.\n\nIn Section 3.6 we make an informal argument based on the regret bounds of CFR/CFR+ vs the convergence rates of other methods. For CFR/CFR+, the regret bound is proportional to the size of the observation space. On the other hand, for the policy gradient method, the regret bound has no direct dependence on the size of the observation space. This suggests that there could be a \"threshold\" level of the observation space size, such that a smaller observation space size (i.e. more partially observable) leads to better relative performance of CFR/CFR+ (and hence ARM), whereas a larger observation space size (i.e. more fully observable) leads to worse relative performance of CFR/CFR+ (and hence ARM).\n\nThis argument suggests that ARM could outperform other methods when there is a high degree of partial observability in a domain (e.g. Minecraft). Conversely, when a domain is nearly fully observable (e.g. Atari) it is possible for ARM to converge slower than other methods -- these match our empirical results on Atari in the Appendix, section 6.3.1.\n\nFinally, there is also a lot of work showing that Q-learning based methods (i.e. DQN) can be much more sample efficient than policy gradient methods (e.g. A3C and TRPO). Estimating something that looks like a Q-function often leads to faster convergence. A deeper reason is that policy gradient methods are better at avoiding Markov assumptions by reducing dependence on value function estimation, but at the cost of sample efficiency."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515190314675,"tcdate":1515189261602,"number":2,"cdate":1515189261602,"id":"rJIfV_TXz","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Comment","forum":"BkCV_W-AZ","replyto":"SkYyvPyWf","signatures":["ICLR.cc/2018/Conference/Paper694/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper694/Authors"],"content":{"title":"Thanks for your comments","comment":"Thanks for your comments! We have addressed the clarity issues raised by the reviewers and added additional experiments to address reviewer concerns, which we detailed below.\n\n- The paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain. To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings  which makes no Markov assumption .\n\nWe have added Section 3.6 with this information.\n\nIn Section 3.6 we make an informal argument based on the regret bounds of CFR/CFR+ vs the convergence rates of other methods. For CFR/CFR+, the regret bound is proportional to the size of the observation space. On the other hand, for the policy gradient method, the regret bound has no direct dependence on the size of the observation space. This suggests that there could be a \"threshold\" level of the observation space size, such that a smaller observation space size (i.e. more partially observable) leads to better relative performance of CFR/CFR+ (and hence ARM), whereas a larger observation space size (i.e. more fully observable) leads to worse relative performance of CFR/CFR+ (and hence ARM).\n\nThis argument suggests that ARM could outperform other methods when there is a high degree of partial observability in a domain (e.g. Minecraft). Conversely, when a domain is nearly fully observable (e.g. Atari) it is possible for ARM to converge slower than other methods -- these match our empirical results on Atari in the Appendix, section 6.3.1.\n\n- adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle.\n\nThanks! This is a good point which we added to our modified Section 3.4.\n\n- this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification)\n\nIn our modified section 3.5 we address this point in more detail.\n\nOne way to think about existing policy gradient methods is that they approximately minimize the KL-divergence between the policy and a target Boltzmann distribution induced either by the Q-function or by the advantage function. (For simplicity we ignore considerations of entropy regularization.) As a result of the KL-divergence/Boltzmann interpretation, the policy gradient update is linearly proportional to the advantage.\n\nBy analogy, if one were to minimize the KL-divergence between the policy and a different kind of target distribution, the resulting gradient updates would also be different. In particular, ARM proposes a distribution based on regret-matching which is proportional to the positively clipped part of the cumulative clipped advantage (i.e. the \"A+\" function). If we were to implement a policy gradient-like version of ARM, then the resulting gradient update would be proportional to the _logarithm_ of the cumulative clipped advantage, and is inherently different than the existing policy gradient update. One consequence is that logarithmic dependence on the advantage means that ARM may be less sensitive to value function overestimation that may result in large positive advantages.\n\n- algorithms like dueling DQN and DDPG are for the best asymptotic results and not for the best convergence rate\n\nRegret in the context of RL can be thought of as \"area over the learning curve (and under the optimal expected return)\". In other words, regret is a measure of sample efficiency. Faster minimization of regret leads to faster convergence rate but generally does not change the overall asymptotic performance that can be attained. Empirically we observe that ARM does achieve higher performance within a finite number of steps on some tasks compared to others (the Minecraft one in particular).\n\nWe are happy to compare with any other methods, but to our knowledge dueling double DQN is a strong baseline for the empirical convergence rate in addition to the asymptotic performance (we didn't compare with DDPG because we only evaluated discrete action space domains).\n\n- More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence\n\nSome general comments about our hyperparameter tuning:\n(a) by fixing the network architecture, we found that the learning rate and other optimizer hyperparams were mostly architecture-dependent (except for TRPO);\n(b) the number of steps to use for n-step returns made the biggest difference and was independent of algorithm."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515116042308,"tcdate":1515116042308,"number":1,"cdate":1515116042308,"id":"r1fM8U3Xz","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Comment","forum":"BkCV_W-AZ","replyto":"Hk29NGimM","signatures":["ICLR.cc/2018/Conference/Paper694/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper694/Authors"],"content":{"title":"Spectral methods and model-based RL","comment":"Thanks for your comment. In our work we only considered model-free deep RL methods, so I'm not super familiar with the SM-UCRL work by Azizzadenesheli et al which involves POMDP model estimation. My initial impression though is that estimation of the POMDP model parameters using spectral methods is an interesting idea for model-based RL in general."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515033747994,"tcdate":1515033747994,"number":1,"cdate":1515033747994,"id":"Hk29NGimM","invitation":"ICLR.cc/2018/Conference/-/Paper694/Public_Comment","forum":"BkCV_W-AZ","replyto":"BkCV_W-AZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Regret of POMDPs","comment":"Hi authors, \n\nThe title of your paper reminds me a theory paper on regret bound of POMDPs. \n\"Reinforcement learning of POMDPs using spectral methods\" which does regret minimization of POMDPS.\nI skimmed your paper a bit, and I think it would good to discuss this paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642493149,"tcdate":1512171278681,"number":3,"cdate":1512171278681,"id":"SkYyvPyWf","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Review","forum":"BkCV_W-AZ","replyto":"BkCV_W-AZ","signatures":["ICLR.cc/2018/Conference/Paper694/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization. The paper claims that the approach can deal with the partial observable domain better than the standard methods. However the results only show that the algorithm converges, in some cases, faster than the previous work.","rating":"5: Marginally below acceptance threshold","review":"Quality and clarity:\n\nThe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization. The paper claims that the approach can deal with the partial observable domain better than the standard methods. However the results only show that the algorithm converges, in some cases, faster than the previous work  reaching asymptotically to a same or worse performance. Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs. \n\nThe paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain. To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no Markov assumption . The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle. This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy. To me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more efficiently.\n\n\nOriginality and Significance:\n\nThe proposed algorithm seems original. However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification). I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence. However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones. Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling. More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence.\n ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642493185,"tcdate":1512147946894,"number":2,"cdate":1512147946894,"id":"S17ehb1WM","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Review","forum":"BkCV_W-AZ","replyto":"BkCV_W-AZ","signatures":["ICLR.cc/2018/Conference/Paper694/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting results on applying counterfactual regret minimization results to Deep RL.","rating":"7: Good paper, accept","review":"This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL. Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better. The results is interesting and novel. This paper should be accepted.\n\nThe presentation of the paper can be improved a bit. Much of the notation introduced in section 3.1 is not used later on. There seems to be a bit of a disconnect before and after section 3.3. The algorithm in deep RL could be explained a bit better.\n\nThere are some papers that could be connected. Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments.\n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals.\n\nThe argument the authors made against recurrent value functions is that recurrent value could be hard to train. An experiment illustrating this effect could be illuminating.\n\nCan the proposed approach help when we have recurrent value functions? Since recurrence does not guarantee that all information needed is captured.\n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al.\n\nPotential typos: in the 4th bullet point in section 3.1, should it be \\rho^{\\pi}(h, s')?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642493224,"tcdate":1511781447187,"number":1,"cdate":1511781447187,"id":"ByyLE_tgG","invitation":"ICLR.cc/2018/Conference/-/Paper694/Official_Review","forum":"BkCV_W-AZ","replyto":"BkCV_W-AZ","signatures":["ICLR.cc/2018/Conference/Paper694/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A regret minimization approach to policy gradient algorithms.","rating":"4: Ok but not good enough - rejection","review":"This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE.\nThe main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions).\nThe claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN.\n\nThere are several things to like about this paper:\n- The authors do a good job of reviewing/referencing several papers in the field of \"regret minimization\" that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives.\n- The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in \"deep reinforcement learning\".\n- The experimental results show that ARM outperforms DQN on a suite of deep RL tasks.\n\nHowever, there are also some negatives:\n- Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused.\n- The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc. At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed.\n- The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability) but we only see results compared to DQN... it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods. Yes A3C and TRPO seem like they perform very poorly compared to ARM... but I'm left wondering how/why?\n\nI feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a paper.\nA lot of the cited literature was also new to me, so it could be that I'm missing something about why this is so interesting.\nHowever, I came away from this paper quite uncertain about the real benefits/differences of ARM versus other similar policy gradient methods... I also didn't feel the experimental evaluations drove a clear message except \"ARM did better than all other methods on these experiments\"... I'd want to understand how/why and whether we should expect this universally.\nThe focus on \"regret minimization perspectives\" didn't really get me too excited...\n\nOverall I would vote against acceptance for this version.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515188400853,"tcdate":1509132342410,"number":694,"cdate":1509739153379,"id":"BkCV_W-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkCV_W-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Regret Minimization for Partially Observable Deep Reinforcement Learning","abstract":"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.","pdf":"/pdf/1981fbc2efcabba97e2e11ee975208873b2b7beb.pdf","TL;DR":"Advantage-based regret minimization is a new deep reinforcement learning algorithm that is particularly effective on partially observable tasks, such as 1st person navigation in Doom and Minecraft.","paperhash":"anonymous|regret_minimization_for_partially_observable_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018regret,\n  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkCV_W-AZ}\n}","keywords":["deep reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper694/Authors"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}