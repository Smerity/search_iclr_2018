{"notes":[{"tddate":null,"ddate":null,"tmdate":1512273371783,"tcdate":1512207417329,"number":1,"cdate":1512207417329,"id":"ByWHEgg-z","invitation":"ICLR.cc/2018/Conference/-/Paper99/Public_Comment","forum":"HJ94fqApW","replyto":"HJ94fqApW","signatures":["~Zhuang_Liu1"],"readers":["everyone"],"writers":["~Zhuang_Liu1"],"content":{"title":"Clarification and Comparison with [1]","comment":"Hi authors, nice paper, well done! In particular, I think the rationale for using Batch Normalization's \\gamma scaling factors as the pruning criterion is explained in a very clear fashion. \n\nHowever, given this \"sparsify BN's \\gamma and prune channels accordingly\" technique is already proposed by the ICCV paper [1], I think it would be great to include an experimental comparison. In my understanding, the essential difference is that you use ISTA to sparsify \\gamma instead of L1 regularization as in [1].\n\nMoreover, I think the \"parallel work\" argument might not be fully valid, as [1] was published and presented before ICLR submission.\n\nNevertheless, I believe the paper is still a good contribution to ICLR if [1] is cited as prior work (rather than parallel work), and a comparison is conducted.\n\n[1] Liu et al. ICCV 2017. Learning Efficient Convolutional Networks through Network Slimming.  https://arxiv.org/abs/1708.06519"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers","abstract":"Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource- limited scenarios. A widely-used practice in relevant work assumes that a smaller- norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.","pdf":"/pdf/c08fc5bcc6cd301f20476f8c589079174c0314ca.pdf","TL;DR":"A CNN model pruning method using ISTA and rescaling trick to enforce sparsity of scaling parameters in batch normalization.","paperhash":"anonymous|rethinking_the_smallernormlessinformative_assumption_in_channel_pruning_of_convolution_layers","_bibtex":"@article{\n  anonymous2018rethinking,\n  title={Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ94fqApW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper99/Authors"],"keywords":["model pruning","batch normalization","convolutional neural network","ISTA"]}},{"tddate":null,"ddate":null,"tmdate":1512222834588,"tcdate":1511839120875,"number":3,"cdate":1511839120875,"id":"B1KcBUqlz","invitation":"ICLR.cc/2018/Conference/-/Paper99/Official_Review","forum":"HJ94fqApW","replyto":"HJ94fqApW","signatures":["ICLR.cc/2018/Conference/Paper99/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A pruning approach based on the batch normalization layer. The algorithm is easy to reproduce and seems to obtain interesting results. Sensibility analysis would be nice","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an interesting  approach to prune a deep model from a computational point of view. The idea is quite simple as pruning using the connection in the batch norm layer. It is interesting to add the memory cost per channel into the optimization process. \n\nThe paper suggests normal pruning does not necessarily preserve the network function. I wonder if this is also applicable to the proposed method and how can this be evidenced. \n\nAs strong points, the paper is easy to follow and does a good review of existing methods. Then, the proposal is simple and easy to reproduce and leads to interesting results. It is clearly written (there are some typos / grammar errors). \n\nAs weak points:\n1) The paper claims the selection of \\alpha is critical but then, this is fixed empirically without proper sensitivity analysis. I would like to see proper discussion here. Why is \\alpha set to 1.0 in the first experiment while set to a different number elsewhere. \n\n2) how is the pruning (as post processing) performed for the base model (the so called model A).\n\nIn section 4, in the algorithmic steps. How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network?\n\n3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers","abstract":"Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource- limited scenarios. A widely-used practice in relevant work assumes that a smaller- norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.","pdf":"/pdf/c08fc5bcc6cd301f20476f8c589079174c0314ca.pdf","TL;DR":"A CNN model pruning method using ISTA and rescaling trick to enforce sparsity of scaling parameters in batch normalization.","paperhash":"anonymous|rethinking_the_smallernormlessinformative_assumption_in_channel_pruning_of_convolution_layers","_bibtex":"@article{\n  anonymous2018rethinking,\n  title={Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ94fqApW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper99/Authors"],"keywords":["model pruning","batch normalization","convolutional neural network","ISTA"]}},{"tddate":null,"ddate":null,"tmdate":1512222834634,"tcdate":1511817149293,"number":2,"cdate":1511817149293,"id":"B1rak-5eG","invitation":"ICLR.cc/2018/Conference/-/Paper99/Official_Review","forum":"HJ94fqApW","replyto":"HJ94fqApW","signatures":["ICLR.cc/2018/Conference/Paper99/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review for Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers","rating":"7: Good paper, accept","review":"This paper is well written and it was easy to follow. The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers. This is achieved by forcing the output of some channels being constant during training. This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter. \n\nThe authors evaluate the performance of the proposed approach on different classification and segmentation tasks. The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet.\n\nSome of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc. Could the authors explain their choices? How sensible is the algorithm to these hyperparameters?\nIt would be nice to see empirically how much of computation the proposed approach takes during training. How much longer does it takes to train the model with the ISTA based constraint?\n\nOverall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers","abstract":"Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource- limited scenarios. A widely-used practice in relevant work assumes that a smaller- norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.","pdf":"/pdf/c08fc5bcc6cd301f20476f8c589079174c0314ca.pdf","TL;DR":"A CNN model pruning method using ISTA and rescaling trick to enforce sparsity of scaling parameters in batch normalization.","paperhash":"anonymous|rethinking_the_smallernormlessinformative_assumption_in_channel_pruning_of_convolution_layers","_bibtex":"@article{\n  anonymous2018rethinking,\n  title={Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ94fqApW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper99/Authors"],"keywords":["model pruning","batch normalization","convolutional neural network","ISTA"]}},{"tddate":null,"ddate":null,"tmdate":1512236438284,"tcdate":1511726049484,"number":1,"cdate":1511726049484,"id":"BJtJ3c_gG","invitation":"ICLR.cc/2018/Conference/-/Paper99/Official_Review","forum":"HJ94fqApW","replyto":"HJ94fqApW","signatures":["ICLR.cc/2018/Conference/Paper99/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Data-dependent channel pruning approach to simplify CNNs. Right questions but missing answers. ","rating":"4: Ok but not good enough - rejection","review":"In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations. The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter \\gamma which is seen as a “gate” to the information flow. Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in \\gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency. In the experiments section, the authors apply their pruning approach on a few representative problems and networks. \n\nThe concept of applying sparsity on \\gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights. However, the ISTA, which is equivalent to L1 penalty on \\gamma is in spirit same as “smaller-norm-less-informative” assumption. Hence, the title seems a bit misleading. \n\nThe quality and clarity of the paper can be improved in some sections. Some specific comments by section:\n\n3. Rethinking Assumptions:\n-\tWhile both issues outlined here are true in general, the specific examples are either artificial or can be resolved fairly easily. For example: L-1 norm penalties only applied on alternate layers is artificial and applying the penalties on all Ws would fix the issue in this case. Also, the scaling issue of W can be resolved by setting the norm of W to 1, as shown in He et. al., 2017. Can the authors provide better examples here?\n-\tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity?\n\n4. Channel Pruning\n-\tThe notation can be improved by defining or replacing “sum_reduced”\n-\tISTA – is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative. Can the authors address the earlier comment about “a theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms”?\n-\tCan the authors address the earlier comment on “how to set thresholds for weights across different layers”, by providing motivation for choice of penalty for each layer? \n-\tCan the authors address the earlier comment on how their approach provides “guarantees for preserving neural net functionality approximately”?\n\n5. Experiments\n-\tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size? (like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection)\n-\tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this. However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify?\n-\tFore/Background: Can the authors add citations for datasets, metrics for this problem?\n\n\nOverall, the channel pruning with sparse \\gammas is an interesting concept and the numerical results seem promising. The authors have started with right motivation and the initial section asks the right questions, however, some of those questions are left unanswered in the subsequent work as detailed above.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers","abstract":"Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource- limited scenarios. A widely-used practice in relevant work assumes that a smaller- norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.","pdf":"/pdf/c08fc5bcc6cd301f20476f8c589079174c0314ca.pdf","TL;DR":"A CNN model pruning method using ISTA and rescaling trick to enforce sparsity of scaling parameters in batch normalization.","paperhash":"anonymous|rethinking_the_smallernormlessinformative_assumption_in_channel_pruning_of_convolution_layers","_bibtex":"@article{\n  anonymous2018rethinking,\n  title={Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ94fqApW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper99/Authors"],"keywords":["model pruning","batch normalization","convolutional neural network","ISTA"]}},{"tddate":null,"ddate":null,"tmdate":1509739485077,"tcdate":1508971057673,"number":99,"cdate":1509739482424,"id":"HJ94fqApW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJ94fqApW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers","abstract":"Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resource- limited scenarios. A widely-used practice in relevant work assumes that a smaller- norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.","pdf":"/pdf/c08fc5bcc6cd301f20476f8c589079174c0314ca.pdf","TL;DR":"A CNN model pruning method using ISTA and rescaling trick to enforce sparsity of scaling parameters in batch normalization.","paperhash":"anonymous|rethinking_the_smallernormlessinformative_assumption_in_channel_pruning_of_convolution_layers","_bibtex":"@article{\n  anonymous2018rethinking,\n  title={Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ94fqApW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper99/Authors"],"keywords":["model pruning","batch normalization","convolutional neural network","ISTA"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}