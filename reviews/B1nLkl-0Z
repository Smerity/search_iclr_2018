{"notes":[{"tddate":null,"ddate":null,"tmdate":1512453945858,"tcdate":1512453945858,"number":3,"cdate":1512453945858,"id":"HkzHPhmWf","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This doesn't seem to actually work","rating":"5: Marginally below acceptance threshold","review":"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.\n\nTo be honest, I didn't really \"get\" this paper.\n* As far I understand, all of the original work policy gradients involved stochastic policies.  Many are/were Gaussian.\n* All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n* As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution.  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value.\n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas.\n\nI thought the little 2-mode MOG was a nice example of the premise of the model.\n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.  Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \"Very well, run your algorithm 10x longer and see where you end up!\"  I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.\n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222682378,"tcdate":1511796182934,"number":2,"cdate":1511796182934,"id":"ByJ1CsYgG","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper introduces smoothed Q-values, a tweak on standard Q-functions. It demonstrates some nice theoretical properties and reasonably successful experiments. The paper is interesting and correct, but unlikely to make a big impact.","rating":"6: Marginally above acceptance threshold","review":"The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter.  It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques.  Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG.\n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract.  The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.  The inclusion of proof summaries in the main text would strengthen this aspect of the paper.\n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.  In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact.\n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222682420,"tcdate":1511753249966,"number":1,"cdate":1511753249966,"id":"S1qQ8ZFlf","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer1"],"readers":["everyone"],"content":{"title":"seems to be a good paper, however, I do not even see an exact algorithm formulation","rating":"6: Marginally above acceptance threshold","review":"I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution. The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \"expected policy gradient\" by Ciosek & Whiteson. All these seems very sound and interesting.\n\nWeakness:\n1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical). Such representation style is highly discouraging and brings about un-necessary readability difficulties. \n\n2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is). So I suggest moving them to the Appendix and make the major focus more narrowed down.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739245678,"tcdate":1509125971867,"number":542,"cdate":1509739243016,"id":"B1nLkl-0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/95b13b92907e002a8dc29283061cc70fd7be65d0.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}