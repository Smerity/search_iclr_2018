{"notes":[{"tddate":null,"ddate":null,"tmdate":1515188383533,"tcdate":1515188383533,"number":3,"cdate":1515188383533,"id":"rywix_TQG","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Comment","forum":"B1nLkl-0Z","replyto":"HkzHPhmWf","signatures":["ICLR.cc/2018/Conference/Paper542/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper542/Authors"],"content":{"title":"Response","comment":"We thank the reviewer for their valuable feedback.\n\nR3: “To be honest, I didn't really \"get\" this paper.”\n\nWe hope that our changes to the paper and the rebuttal make the contributions of the paper clearer.\n\nR3: “As far I understand, all of the original work policy gradients involved stochastic policies.  Many are/were Gaussian.”\n\nIndeed, most of the original work on policy gradients uses stochastic policies. In such a setting, Q-value or value functions are used for variance reduction when estimating the policy gradient; that is, the policy is trained using a form similar to Eq. (4) in the paper.\n\nHowever, more recently, several algorithms have been proposed (e.g., DDPG, SVG), which use Q-values in a different way. They use the gradient of a Q-function approximator to train the policy. This results in a large improvement in sample efficiency over traditional policy gradient methods.  The most widely used of these algorithms, DDPG, is restricted to deterministic policies. Our work extends DDPG to general Gaussian policies, showing that 1) we can directly learn the smoothed Q-values to avoid estimating an additional Monte Carlo sampling step necessary for SVG, 2) the gradient and the Hessian of the smoothed Q-values can be used to update the mean and the covariance parameters of a Gaussian policy. Notably, although SVG uses a stochastic policy, it uses a fixed covariance.\n\nR3: “All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.”\n\nThe smoothed Q-values that we introduce additionally marginalize out the randomness in the first action (a) of a typical Q(s, a) value based on the mean and covariance of the first action. As a result, we avoid an additional Monte Carlo sampling step to draw the first action, as compared to SVG for example.\n\nR3: “As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution. In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value.”\n\nAlthough one could pursue such an approach, it is not equivalent to the direction we pursue in the paper.  Under the above suggestion, where the agent emits an action (\\mu, \\Sigma), the corresponding Q-value function would be a function of both \\mu and \\Sigma. On the other hand, the smoothed Q-value function we consider only takes in \\mu. A key contribution of the paper is showing that even though \\tilde{Q} is not a direct function of \\Sigma, one can still derive an update for \\Sigma based on the Hessian of \\tilde{Q} with respect to mean action.\n\nR3: “I thought the little 2-mode MOG was a nice example of the premise of the model.”\n\nThank you.  We hope our responses contribute to a better understanding of the premise of the approach. We expect that this fundamental smoothing behavior is the source of the improvement over DDPG.\n\nR3: “While I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out. Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.”\n\nTo demonstrate the significance of our experimental results more clearly, we have updated Figure 2 to compare the performance of Smoothie with KL-penalty, DDPG, and TRPO on continuous control benchmarks. Figure 2 makes it clear that our results are statistically significant and Smoothie achieves the state-of-the-art by converging faster and/or achieving better final rewards. On the challenging Hopper and Humanoid tasks, Smoothie achieves double the average reward compared to DDPG without sacrificing sample efficiency. Our previous presentation of the results in two separate figures showing the difference between Smoothie without KL penalty and DDPG, and between Smoothie with and without the KL penalty made the significance of our results less clear.\n\nR3: “I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \"Very well, run your algorithm 10x longer and see where you end up!\"\n\nSample-efficient reinforcement learning (RL) is a key challenge for real world applications of RL, and in this paper we focus on the behavior of the algorithms in the practical data regime. That said, we have included a comparison with TRPO in Figure 2, which shows that in this data regime, TRPO is not competitive. Similar conclusions have been made about TRPO by other papers (e.g. https://arxiv.org/abs/1707.06347).\n\nR3: “The idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?”\n\nReference added.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515188205271,"tcdate":1515188205271,"number":2,"cdate":1515188205271,"id":"BkBxlO6Qz","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Comment","forum":"B1nLkl-0Z","replyto":"ByJ1CsYgG","signatures":["ICLR.cc/2018/Conference/Paper542/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper542/Authors"],"content":{"title":"Response","comment":"We thank the reviewer for their valuable feedback.\n\nR2: “The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.  The inclusion of proof summaries in the main text would strengthen this aspect of the paper.”\n\nThank you for the suggestion. We have updated the text to include proof summaries.\n\nR2: “On the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques. In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact.”\n\nTo demonstrate the significance of our experimental results more clearly, we have updated Figure 2 to compare the performance of Smoothie with KL-penalty, DDPG, and TRPO on continuous control benchmarks. Figure 2 makes it clear that Smoothie achieves the state-of-the-art by converging faster and/or achieving better final rewards. On the challenging Hopper and Humanoid tasks, Smoothie achieves double the average reward compared to DDPG without sacrificing sample efficiency. Our previous presentation of the results in two separate figures showing the difference between Smoothie without KL penalty and DDPG, and between Smoothie with and without the KL penalty made the significance of our results less clear.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515188117483,"tcdate":1515188117483,"number":1,"cdate":1515188117483,"id":"rk6c1_6Qf","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Comment","forum":"B1nLkl-0Z","replyto":"S1qQ8ZFlf","signatures":["ICLR.cc/2018/Conference/Paper542/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper542/Authors"],"content":{"title":"Response","comment":"We thank the reviewer for their valuable feedback.\n\nR1: \"The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper … Such representation style is highly discouraging and brings about un-necessary readability difficulties.\"\n\nWe take the presentation criticism seriously. To improve the exposition, we have updated the paper to include an algorithm box with a pseudo-code description of the implementation.\n\nR1: \"I think the major contribution of the paper is on the algorithmic side instead of theoretical\"\n\nNot entirely.  Note that the derived updates for the mean and covariance parameters of a Gaussian policy in terms of the gradient and Hessian of smoothed Q-values are novel. Also, the relation between the Hessian and the covariance update (Eq. 15) is particularly novel; we are not aware of any similar equations previously used in RL.\n\nR1: \"Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is). So I suggest moving them to the Appendix and make the major focus more narrowed down.”\n\nThank you for the suggestion. We agree that Section 3.3 is a theoretical aside, which may not interest most readers. We have updated the paper to move Section 3.3 to the appendix, leaving more room in the main body for the pseudo-code presentation. On the other hand, we believe Section 3.4 is important as it explains the specific technique which yields the best empirical performance.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642465988,"tcdate":1512453945858,"number":3,"cdate":1512453945858,"id":"HkzHPhmWf","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This doesn't seem to actually work","rating":"5: Marginally below acceptance threshold","review":"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.\n\nTo be honest, I didn't really \"get\" this paper.\n* As far I understand, all of the original work policy gradients involved stochastic policies.  Many are/were Gaussian.\n* All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n* As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution.  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value.\n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas.\n\nI thought the little 2-mode MOG was a nice example of the premise of the model.\n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.  Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \"Very well, run your algorithm 10x longer and see where you end up!\"  I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.\n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642466028,"tcdate":1511796182934,"number":2,"cdate":1511796182934,"id":"ByJ1CsYgG","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper introduces smoothed Q-values, a tweak on standard Q-functions. It demonstrates some nice theoretical properties and reasonably successful experiments. The paper is interesting and correct, but unlikely to make a big impact.","rating":"6: Marginally above acceptance threshold","review":"The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter.  It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques.  Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG.\n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract.  The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.  The inclusion of proof summaries in the main text would strengthen this aspect of the paper.\n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.  In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact.\n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642466065,"tcdate":1511753249966,"number":1,"cdate":1511753249966,"id":"S1qQ8ZFlf","invitation":"ICLR.cc/2018/Conference/-/Paper542/Official_Review","forum":"B1nLkl-0Z","replyto":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference/Paper542/AnonReviewer1"],"readers":["everyone"],"content":{"title":"seems to be a good paper, however, I do not even see an exact algorithm formulation","rating":"6: Marginally above acceptance threshold","review":"I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution. The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \"expected policy gradient\" by Ciosek & Whiteson. All these seems very sound and interesting.\n\nWeakness:\n1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical). Such representation style is highly discouraging and brings about un-necessary readability difficulties. \n\n2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is). So I suggest moving them to the Appendix and make the major focus more narrowed down.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515188009452,"tcdate":1509125971867,"number":542,"cdate":1509739243016,"id":"B1nLkl-0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1nLkl-0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Gaussian Policies from Smoothed Action Value Functions","abstract":"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value used in SARSA. We show that such smoothed Q-values still satisfy a Bellman equation, making them naturally learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned Q-value approximator. The approach is also amenable to proximal optimization techniques by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training allows this approach to achieve strong results on standard continuous control benchmarks.","pdf":"/pdf/7e969aac3368c330f701ab8caa52ae10bd164a94.pdf","TL;DR":"We propose a new Q-value function that enables better learning of Gaussian policies.","paperhash":"anonymous|learning_gaussian_policies_from_smoothed_action_value_functions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Gaussian Policies from Smoothed Action Value Functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nLkl-0Z}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper542/Authors"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}