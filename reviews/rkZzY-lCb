{"notes":[{"tddate":null,"ddate":null,"tmdate":1515186802173,"tcdate":1515186802173,"number":4,"cdate":1515186802173,"id":"H1cuqDpXf","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Comment","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper218/Authors"],"content":{"title":"Summary for major revision ","comment":"Dear Chair,\n\nThe two main criticisms of the paper by the reviewers were (i) lack of references and (ii) that the results of the study were not significant enough to justify the two publications that we were aiming for.  For this reason, we added roughly 3x more citations to the paper to better situate the contributions in the literature.  Additionally, we merged this submission with our other concurrent ICLR manuscript.\n\nWe are hoping that we can get an opportunity to share our results with the ICLR community.  In the unsupervised setting, our work  is the first one to enable leveraging arbitrary feature types (a more general approach than exists in the literature).   In the supervised scenario, we provide evidence that our general method can have better performance than ad-hoc networks that work for a single purpose. \n\nThanks,\nAuthors\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515186668751,"tcdate":1515186668751,"number":3,"cdate":1515186668751,"id":"SJSx5wTmz","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Comment","forum":"rkZzY-lCb","replyto":"HJfRKPFeM","signatures":["ICLR.cc/2018/Conference/Paper218/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper218/Authors"],"content":{"title":"Major revision - significantly improved","comment":"Thank you for your helpful comments.  Because another reviewer suggested merging our two ICLR submissions, we underwent a major revision of the paper and now have two main contributions -- this is, we can calculate embeddings in a supervised setting (labels are available), and in an unsupervised setting (labels are not available). \n\nYou stated two main criticisms to the paper:\n* References. You mentioned that the most striking flaw of the paper is lack of references. We added roughly three times more citations (we increased references from ~12 to ~36). We believe that the paper is now much better situated in the literature.\n* Evaluation. To our knowledge we are the first ones to propose learning unsupervised embeddings for multiple feature types.  The Word2Vec algorithms are  other unsupervised  embedding methods (though, W2V only works with words), and that is why we compare with them. \nBecause of the major revision of the paper, we believe we improved the empirical result section significantly. We added 2 additional datasets (total of 4), and added 4 baselines altogether (CBOW W2V, Matrix Factorization, Collaborative Topic Regression and  DeepCoNN)\n\n\nOther detailed comments:\nWe removed the reference of Levy & Goldberg (but the general point is that factorization machines are a general case of matrix factorization)\nWe rewrote the introduction to make more salient our contributions, and we believe that it is now more clear what the model achieves. We streamlined the notation. Additionally, we clarified the language surrounding Word2Vec. \n\nWe hope that these major revisions address your reservations.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515186414707,"tcdate":1515186384345,"number":2,"cdate":1515186384345,"id":"Bk_CuPamf","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Comment","forum":"rkZzY-lCb","replyto":"r1_2VGLlz","signatures":["ICLR.cc/2018/Conference/Paper218/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper218/Authors"],"content":{"title":"Stronger contribution (better novelty) is in place","comment":"Thank you for the constructive comments. Your main criticism for the paper was that the contribution of our work was not significant enough to justify the two publications that we were aiming for. Following your suggestion, we have combined the two papers and added the relevant parts of the other paper (we only extended our submission with the results that would be relevant to the combined version).\n\nWhile the original paper only addressed unsupervised learning of embeddings,  the revised manuscript also addresses supervised learning of embeddings.  We demonstrate that our general supervised method can have better performance  than recently published single purpose methods (DeepCoNN and Collaborative Topic Regression) on two publicly available datasets, Yelp and CiteULike.  We also explain in more detail how Feat2Vec extends Factorization Machines.   \n\nWe hope that this major revision address your reservations.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515186284063,"tcdate":1515186284063,"number":1,"cdate":1515186284063,"id":"rkNuOP6XG","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Comment","forum":"rkZzY-lCb","replyto":"ByQ1mb0xM","signatures":["ICLR.cc/2018/Conference/Paper218/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper218/Authors"],"content":{"title":"Thank you","comment":"Thank you for your informative comments on our paper. We have added experiments for supervised Feat2Vec, which includes a multi-label prediction task on a public dataset (CiteULike) benchmarked against other state of the art methods. We hope that this experiment at least partially addresses your desire to see Feat2Vec in a K-way classification task.  We would also like to point you to the ranking task done classifying the director of a film based on its task members. The 2.43% Top-1 Precision can be imagined as the performance of the unsupervised F2V embedding algorithm on a K-way classification task (as compared to Word2Vec’s CBOW algorithm). \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642410669,"tcdate":1512080091476,"number":3,"cdate":1512080091476,"id":"ByQ1mb0xM","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Review","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neat representation learning scheme for structured features","rating":"7: Good paper, accept","review":"This paper provides a clean way of learning embeddings for structured features that can be discrete -- indicating presence / absence of a certain quality. Further, these features can be structured i.e. a set of them are of the same 'type'. Unlike, word2vec there is no hard constraint that similar objects must have similar representations and so, the learnt embeddings reflect the likelihood of the observed features. Therefore, this can be used as a multi-label classifier by using two feature types -- the input and the set of categories. This proposed scheme is evaluated on two datasets -- movies and education in a retrieval setting. \n\nI would like to see an evaluation of these features in a classification setting to further demonstrate the utility of these embeddings as compared to directly embedding the discrete features and then performing a K-way classification. For example, I am aware of -- http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642410708,"tcdate":1511778761747,"number":2,"cdate":1511778761747,"id":"HJfRKPFeM","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Review","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not clear what I can learn from this","rating":"2: Strong rejection","review":"SUMMARY.\n\nThe paper presents an extension of word2vec for structured features.\nThe authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features.\nThe learned representation of features is tested on a recommendation-like task.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is not clear and thus I am not sure what I can learn from it.\nFrom what is written on the paper I have trouble to understand the definition of the model the authors propose and also an actual NLP task where the representation induced by the model can be useful.\nFor this reason, I would suggest the authors make clear with a more formal notation, and the use of examples, what the model is supposed to achieve.\n\n----------\n\nDETAILED COMMENTS\nWhen the authors refer to word2vec is not clear if they are referring to skipgram or cbow algorithm, please make it clear.\nBottom of page one: \"a positive example is 'semantic'\", please, use another expression to describe observable examples, 'semantic' does not make sense in this context.\nLevi and Goldberg (2014)  do not say anything about factorization machines, could the authors clarify this point?\nEquation (4), what do i and j stand for? what does \\beta represent? is it the embedding vector? How is this formula related to skipgram or cbow?\nThe introduction of structured deep-in factorization machine should be more clear with examples that give the intuition on the rationale of the model.\nThe experimental section is rather poor, first, the authors only compare themselves with word2ve (cbow), it is not clear what the reader should learn from the results the authors got.\nFinally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642410745,"tcdate":1511560368421,"number":1,"cdate":1511560368421,"id":"r1_2VGLlz","invitation":"ICLR.cc/2018/Conference/-/Paper218/Official_Review","forum":"rkZzY-lCb","replyto":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference/Paper218/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting paper with convincing results, but the approach has limited novelty. Proposed method is based on an approach that is concurrently under review at ICLR18","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThis paper proposes an approach to learn embeddings for structured datasets i.e. datasets which have heterogeneous set of features, as opposed to just words or just pixels. The paper proposes an approach called Feat2vec that relies on Structured Deep-In Factorization machines-- a paper that is concurrently under review at ICLR2018, which I haven't read in depth. The paper compares against a Word2vec baseline that pools all the heterogeneous content learns just one set of embeddings. Results are shown on IMDB movies and a proprietary education platform datasets. In both the tasks, Feat2vec leads to significant reduction in error compared to Word2vec.\n\nComments:\n\nThe paper is well written and addresses an important problem of learning word embeddings when there is inherent structure in the feature space. It is a very practically relevant problem. The novelty of the proposed approach seems limited in light of the related paper that is concurrently under review at ICLR2018, on which this paper heavily relies. Perhaps the authors should consider combining the two papers into one complete paper? The structured deep-in factorization machines allow higher-level interactions in embedding learning which allows the authors to learn embeddings for heterogeneous set of features. The sampling approaches proposed seem pretty straightforward adaptations of existing methods and not novel enough.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515609008273,"tcdate":1509067016720,"number":218,"cdate":1509739421068,"id":"rkZzY-lCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkZzY-lCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Feat2Vec:  Dense Vector Representation for Data with Arbitrary Features","abstract":"Methods that calculate dense vector representations for features in unstructured data—such as words in a document—have proven to be very successful for knowledge representation. We study how to estimate dense representations when multiple feature types exist within a dataset for supervised learning where explicit labels are available, as well as for unsupervised learning where there are no labels. Feat2Vec calculates embeddings for data with multiple feature types enforcing that all different feature types exist in a common space. In the supervised case, we show that our method has advantages over recently proposed methods; such as enabling higher prediction accuracy, and providing a way to avoid the cold-start\nproblem. In the unsupervised case, our experiments suggest that Feat2Vec significantly outperforms existing algorithms that do not leverage the structure of the data. We believe that we are the first to propose a method for learning unsuper vised embeddings that leverage the structure of multiple feature types.","pdf":"/pdf/cfbe2eeef1804fb287b88b26ea994b8c1614c57a.pdf","TL;DR":"Learn dense vector representations of arbitrary types of features in labeled and unlabeled datasets","paperhash":"anonymous|feat2vec_dense_vector_representation_for_data_with_arbitrary_features","_bibtex":"@article{\n  anonymous2018feat2vec:,\n  title={Feat2Vec:  Dense Vector Representation for Data with Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZzY-lCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper218/Authors"],"keywords":["unsupervised learning","supervised learning","knowledge representation","deep learning"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}