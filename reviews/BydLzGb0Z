{"notes":[{"tddate":null,"ddate":null,"tmdate":1515894804163,"tcdate":1515894804163,"number":11,"cdate":1515894804163,"id":"Hy2zdEuNz","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"HyCbe867z","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Re: Review ","comment":"We’d like to thank you again for your review and feedbacks! We have updated our paper with your suggestions (including significantly improved Image Captioning results, comparisons to the Yao et al 2016. model and training time). With more thorough experimentation, we have also shown that Twinnet works for unconditioned generation (as well as conditioned generation). \n\nWould you have any other questions regarding the rebuttal, especially regards to the image captioning experiments and unconditioned generation?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515393740262,"tcdate":1515393740262,"number":5,"cdate":1515393740262,"id":"B1VAfclVG","invitation":"ICLR.cc/2018/Conference/-/Paper790/Public_Comment","forum":"BydLzGb0Z","replyto":"B1_9I7DzG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for providing feedback","comment":"Thank you for addressing our concerns and providing feedback.\n\nWe will check the implementation details, run the experiments again and update the values in the report."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515182452574,"tcdate":1515180037785,"number":9,"cdate":1515180037785,"id":"HyCbe867z","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"HyciX9dxM","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Response to Reviewer 3","comment":"We thank the reviewer for the feedback and comments.\n\nQ: “3) Cons: Image captioning experiment:\nIn the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines. ”\n\nA: We acknowledge this, and we have significant improvements on our image captioning experiments, we have the following improvements, which we also summarized in the comments to all reviewers.\nWe run SAT models with Resnet 152 features. SAT with Twinnet achieved similar performance to (Yao et al, 2016). SAT with Twinnet vs Yao et al, 2016 performances are B1: 73.8 (vs 73.0 Yao), B2: 56.9 (vs 56.5 Yao), B3: 42.0 (vs 42.9 Yao), B4: 30.6 (vs 32.5 Yao), Meteor: 25.2 (vs 25.1 Yao), Cider: 97.3 (vs 98.6 Yao)\nWe have significant improvements on ST and SAT with Resnet 101 features compared to the baseline.\n\nQ: “Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it’s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is. If the authors believe that the 50% case is not necessary, please feel free to explain why.”\n\nA: We have run more thorough examinations, with larger regularization hyperparameter values, for the unconditioned generation. We now have consistent improvements on both conditioned and unconditioned generation tasks. Please see our comment to all reviewers.\n\nQ: “It would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).”\n\nA: We are currently running the forward and backward RNN consecutively, therefore training Twinnet takes around twice the amount of time as training the forward alone. We measured the batch time for running the SAT baseline on Resnet152 feature takes 0.181s/minibatch, and SAT with Twinet is 0.378s/minibatch. Both experiments are run on TitanXP GPUs. We also measured convergence. For the ASR task, the convergence is the same in terms of number of epochs as it can be seen in the learning curve in the paper. For the image captioning, some TwinNet models converge faster, while others have similar convergence rate similar compared to the baseline.\n\n \nQ: “Experiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).”\n\nA: This is a very nice idea! We could see this being used for model-based RL (planning). However, we feel that this tasks deserves to be a separate paper on its own. It would require some amount of investigation to understand how forward and backward would interact in a RL setting. It would indeed be very interesting future work to see how Twinnet could be used in this setting."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515176204902,"tcdate":1515176204902,"number":8,"cdate":1515176204902,"id":"r1SMZBT7z","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"HJzYCPDlf","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Response to Reviewer 1","comment":"We thank the reviewer for your positive feedback and comments! \n\nQ: “On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained?”\n\nA: As the gradient of the regularization term is not backpropagated through the backward network, the backward model can indeed be pre-trained. \n\nQ: In section 2, it is not obvious to me that the regularizer (4) would not be ignored in absence of regularization on the output matrix. I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings. Could you comment why this would not happen?\n\nA: The L2 cost to match the forward and backward states (4) is not backpropagated to the backward model, i.e. the hidden states of the backward h^b are not optimized with respect to the twin cost. Therefore, the backward hidden states may be pushed to a small norm only if it’s beneficial for the reverse language modeling objective.\n\nQ: In this section, why are you not reporting the results from the original Show&Tell paper? How does your implementation compare to the original work?\n\nA: The original ShowTell uses the Inception v3 network for the feature extraction. Therefore the performance is not comparable to our baseline trained with features extracted with ResNet. This result is added to the table now.\n\nQ: dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates. Would your regularizer still be helpful there? At which point would it break?\nA: The experiment on multiple versions of each word is indeed very interesting, thanks for the suggestion. Our new results suggest that the method works also in unconditioned case. Please refer to the comment to all reviewers.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515176131738,"tcdate":1515176131738,"number":7,"cdate":1515176131738,"id":"BJ2TeHpXz","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"B1Fe0Zqxz","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Response to Reviewer 2","comment":"We thank the reviewer for the feedback and comments. We now have improved results across all tasks (image captioning, sequential MNIST and language modelling), we included those in a brief summary in our comment that was addressed to all reviewers. \n\nQ: “I have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them, …”\n\nA: We run the following experiment to test out this hypothesis. We run 2 separate forward generators and force an agreement on those 2 generators. We tested out this model on WSJ dataset for speech recognition, and the model has a validation CER of 9.1% ( vs 9.0% for Baseline) and 8.4% for TwinNet. We conclude that TwinNet works better compared to running 2 forward models and force an agreement between them.\n\nQ: “Also, would using the backward RNN, e.g. for rescoring, bring another advantage? ...”\n\nA: This is an interesting question to think about the backward RNN doing rescoring. Although, we are not totally clear on what ensembles of forward model would be in this case, and we have not received a clarification on this based on our earlier comments.\n\nWe took a best guess of what the reviewer meant in this case. We assume that the reviewer means that we run multiple forward generators and use the backward generator to rescore the results of the forward generator and hence picking the best out of the forward generators. In general, this is an interesting idea, although it is different enough from our current idea that it would deserve credit to be a separate paper on its own.\n\nTwo things to note for the idea of running multiple forward generator and using the backward to rescore is \nInference is at least twice as expensive compared to our current setup, which does not use the backward model during test time.\n\nAs there are multiple choices for rescoring function, it is not trivial to decide which rescoring function to use. In some cases, the rescoring function could also be non-differentiable (for example WER or CER for speech recognition, or Bleu score for image captioning or machine translation).\n\nQ: “Cons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary”\n\nA: We have indeed run the experiment with multiple (2 in our case) forward models and force an agreement between them, and we did not see improvements over the baseline. This supports our hypothesis that the backward RNN is indeed useful and necessary."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515174914949,"tcdate":1515174914949,"number":6,"cdate":1515174914949,"id":"rJob2Ep7z","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"General comment to all reviewers.","comment":"More thorough experimentation allowed us to significantly improve Twinnet results for image captioning, sequential MNIST, and language modelling tasks. We summarize our new results here and update the paper.\n\n- Image Captioning: We have significantly improved results for ShowTell (ST) and ShowAttendTell (SAT) models with TwinNet (Table 2)\n    1. ResNet 101 features + ST + Twinnet \n        Improvement by more than 3 Cider points\n        B1: 71.8 (vs 69.,4),  B2: 54.5 (vs 51.6), B3: 39.4 (vs 36.9), B4: 28.0 (vs 26.3), Meteor: 24.0 (vs 23.4), Cider: 87.7 (vs 84.3)\n    2. ResNet 101 features + SAT + Twinnet\n        Improvement by more than 5 Cider points\n        B1: 72.8 (vs 71.0), B2:  55.7 (vs 53.0),  B3: 41.0(vs 39.0), B4: 29.7 (vs 28.1), Meteor: 25.2 (vs 25.0), Cider: 96.2 (vs 89.2)\n    3. ResNet 152 features + SAT + Twinnet \n        Improvement by 0.7 Cider points\n        B1: 73.8 (vs 73.2), B2:  56.9 (vs 56.3),  B3: 42.0 (vs 41.4), B4: 30.6 (vs 30.1), Meteor: 25.2 (vs 25.3), Cider: 97.3 (vs 96.6)\n-  Sequential MNIST (Table 3 (left))\n    1. LSTM with Dropout  + Twinnet has NLL of 79.12 (vs 79.59 for Baseline)\n    2. LSTM + TwinNet has NLL of 79.35 (vs 79.87 for Baseline)\n-  Wikitext 2 (Table 3 (right))\n    - AWD + LSTM + TwinNet has valid perplexity: 68.0 (vs 68.7) and Test perplexity: 64.9 (vs 65.8)\n-  PennTree Bank (Table 3 (right))\n    - AWD + LSTM + TwinNet has valid perplexity: 61.0 (vs 61.2) and Test perplexity: 58.3 (vs 58.8)\n\nThe improvements in results for sequential MNIST and Wikitext-2 experiments show that TwinNet may also be effective for the case of unconditional generation. In MNIST, the use of a much larger regularization hyperparameter (1.5) was necessary. In PTB, the improvements are minor and more consistent in WikiText-2, which wasn’t included in our experiments. These experiments suggest that our method is applicable in for wider set of tasks comparing to the claim in the earlier version of our paper that TwinNet was better suited for conditional generation tasks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1513727632312,"tcdate":1513727632312,"number":5,"cdate":1513727632312,"id":"B1_9I7DzG","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"BJMXIjbGM","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Few possible issues with reproduced experiments","comment":"We thank you for your attempt in reproducing our paper, we appreciate the effort. In general, the reported results used non stable branches of our code. The full (cleaned) code will be released before the final version of the paper. In the meantime, we put some efforts in making available code that reproduce the numbers that we get with our model.\n1) The sequential MNIST baseline (85 nats) reported in the reproducibility report is significantly worse than our and \n     previously published baselines (several papers report ~80 nats) \n        - van den Oord et al., 2016 report 80.54 nats for the baseline\n        - Lamb et al., 2016 reports 79.52 nats for Professor Forcing\n\t\n\tOur new results on MNIST are:\n\t      Baseline: 79.87\n\t      Twin: 79.35\n\t      Baseline + dropout: 79.59\n\t      Twin + dropout: 79.12\n\tReproducible using the master branch:\n\t      python train_seqmnist_twin.py --twin 0.0 --nlayers 3 --dropout 0.0 --seed 1234\n\t      python train_seqmnist_twin.py --twin 1.5 --nlayers 3 --dropout 0.0 --seed 1234\n\t      python train_seqmnist_twin.py --twin 0.0 --nlayers 3 --dropout 0.3 --seed 1234\n\t      python train_seqmnist_twin.py --twin 1.5 --nlayers 3 --dropout 0.3 --seed 1234\n\n2) The baseline for captioning results with resnet101 features seems to be unreasonably high, please refer to the \n    following papers with published baseline with the same setup. \n        - Rennie et al., 2016\n        - https://openreview.net/forum?id=SJyVzQ-C-\nOur code has not been officially released, we will clean up our code and release it anonymously as soon as possible.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1513405672581,"tcdate":1513367065732,"number":1,"cdate":1513367065732,"id":"BJMXIjbGM","invitation":"ICLR.cc/2018/Conference/-/Paper790/Public_Comment","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Intuitive idea and detailed paper, however, extensive experimentation and testing is required","comment":"\n\n** PAPER SUMMARY **\n\nThe paper describes an approach to facilitate modeling of long-term dependencies in RNNs by implicitly forcing the forward states to hold information about the longer-term future, ie, effectively regularizing RNNs. This is done by using an additional backward RNN and jointly training both the networks. However, only the forward network is used during testing. The results are provided on various tasks like speech recognition, image captioning, language modeling and pixel-by-pixel generation on MNIST.\n\n** TARGET QUESTIONS **\n\n- Verifying results for two tasks - image captioning on MSCOCO dataset and pixel-by-pixel generation for MNIST.\n- Implementation of baseline models to verify the scores mentioned in the paper.\n- Quantitative estimate of the training time taken by both networks.\n- Comparison of the nature of convergence of both networks.\n\n** EXPERIMENTAL METHODOLOGY **\n\nFor implementation purposes, we use the code available on github:\n- Image captioning : https://github.com/nke001/neuraltalk2.pytorch\n- MNIST : https://github.com/nke001/Twinnet\n\n** RESULTS **\n\nAll the models are trained and tested on NVIDIA GeForce GTX 1080 Ti.\n\nImage Captioning\nThe results are reported on Bleu-1 to Bleu-4, Meteor, Rogue-L and CIDEr scores using the code available - https://github.com/tylin/coco-caption. Also, time per iteration (in seconds) for batch size of 64 is given.\n\n- Show & Tell :\n Bleu-1 = 0.725, Bleu-2 = 0.555, Bleu-3 = 0.415, Bleu-4 = 0.310, Meteor = 0.249, Rogue-L = 0.529, CIDEr = 0.953, time/iter = 0.081\n- Show & Tell Twin :\n Bleu-1 = 0.705, Bleu-2 = 0.531, Bleu-3 = 0.390, Bleu-4 = 287, Meteor = 0.239, Rogue-L = 0.517, CIDEr = 0.884, time/iter = 0.261\n- Soft Attention :\n Bleu-1 = 0.730, Bleu-2 = 0.564, Bleu-3 = 0.423, Bleu-4 = 0.315, Meteor = 0.250, Rogue-L = 0.533, CIDEr = 0.976, time/iter = 0.782\n- Soft Attention Twin :\n Bleu-1 = 0.681, Bleu-2 = 0.317, Bleu-3 = 0.113, Bleu-4 = 0.032, Meteor = 0.187, Rogue-L = 0.458, CIDEr = 0.582, time/iter = 1.540 \n\nSequential MNIST\nThe results are reported in terms of negative log likelihood (NLL)  and training time per iteration (in seconds) for both the baseline and twin networks.\n\n- Unconditional Generation :\n  Baseline : NLL = 84.860, time/iter = 0.423\n  Twin net : NLL = 85.520, time/iter = 0.862\n- Conditional Generation :\n  Baseline : NLL = 83.710, time/iter = 0.423\n  Twin net : NLL = 80.870, time/iter = 0.871 \n\n** ANALYSIS AND DISCUSSION **\n\nThe idea is intuitive and well motivated and the paper is explained in great detail. However, based on our results, we believe that there is need for extensive experimentation and testing.\n\n- For image captioning, we observe baseline values around 2-3 % higher whereas the twin net values show little change while using the same values of parameters as those mentioned in the paper. However, we believe that extensive hyperparameter search may lead to better results.\n\n- The observed values for the soft attention twin net are quite less than those reported in the paper. This indicates that the convergence is highly affected, probably due to increased parameters and non-convexity in the optimization surface. This is also evident in plotting train_loss vs iterations, where we observe increased fluctuations.\n\n- For unconditional generation of MNIST, we observe higher values of NLL which is probably because the parameters are not the same as mentioned in the paper. However, the values for baseline and twin net are almost the same in accordance with the paper.\n\n- We also report values for the conditional generation of MNIST. The improved value for the twin net reaffirms the notion that the proposed approach works better for the conditional case. Also, the values for conditional case show improvement over the unconditioned case as expected.\n\nWe would also like to point out few limitations:\n\n- Major downside of the approach is the cost in terms of resources. The twin model requires large memory and takes longer to train (~ 2-4 times) while providing little improvement over the baseline. \n\n- During evaluation we found that the attention twin model gives results like “a woman at table a with cake a”, where it forces the model to look like a sentence from the back side too. This might be the reason for low metric values observed in soft attention twin net model.\n\n- The effect of twin net as a regularizer can be examined against other regularization strategies for comparison purposes. \n\n** CONCLUSION **\n\nThe paper presents a novel approach to regularize RNNs and give results on different datasets indicating wide range of application. However, based on our results, we believe that further experimentation and extensive hyperparameter search is needed. Overall, the paper is detailed, simple to implement and positive empirical results support the described approach.\n\nLink to full report : https://drive.google.com/file/d/1NjAtAHrrY8CdeoykCp2IwxxY8slJOGgH/view?usp=sharing"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1513029493185,"tcdate":1513029493185,"number":4,"cdate":1513029493185,"id":"rk6_yF2-f","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"B1Fe0Zqxz","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Clarification","comment":"We thank you for your review, feedback and suggestions. \n\nWe would like to clarify your question about ensembles. Unlike ensemble methods, our method does not average over multiple predictive models at the evaluation time. Forward and backward networks are “coupled” during training and the backward predictions are discarded during testing. Would you like to see a comparison between ensembles of baselines compared to ensembles of TwinNets?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1513024038673,"tcdate":1513024038673,"number":3,"cdate":1513024038673,"id":"r11N5P2Zf","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Comment","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper790/Authors"],"content":{"title":"Thank reviewers","comment":"We thank all the reviewers for your feedback and suggestions, it was very helpful and allowed us to gain more insights. We plan to run the requested experiments and update the experimental results in the upcoming weeks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1516002826977,"tcdate":1511820784708,"number":3,"cdate":1511820784708,"id":"B1Fe0Zqxz","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Review","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple way to regularize recurrent sequence generators, limited applicability.","rating":"7: Good paper, accept","review":"** post-rebuttal revision **\n\nI thank the authors for running the baseline experiments, especially for running the TwinNet to learn an agreement between two RNNs going forward in time. This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns. I am raising the score.\n\n** original review **\n\n\nThe paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward.\n\nApplied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders. A penalty term is added that forces an agreement between the hidden states of the two decoders. During model evaluation only the forward decoder is used, with the backward operating decoder discarded. The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states.\n\nExperiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network. Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy.\n\nThe proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks.\n\nI have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)? \n\nAlso, would using the backward RNN, e.g. for rescoring, bring another advantage? In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty?\n\nQuality:\nThe proposed approach is well motivated and the experiments show the limits of applicability range of the technique.\n\nClarity:\nThe paper is clearly written.\n\nOriginality:\nThe presented idea seems novel.\n\nSignificance:\nThe method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods. Also, as the authors note the method seems to be limited to conditional sequence generators.\n\nPros and cons:\nPros: the method is simple to implement, the paper lists for what kind of datasets it can be used.\nCons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515642511656,"tcdate":1511723938937,"number":2,"cdate":1511723938937,"id":"HyciX9dxM","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Review","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"\n1) Summary\nThis paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably. The authors propose to train a forward and backward RNN in parallel. The forward RNN predicts forward in time and the backward RNN predicts backwards in time. While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step. In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition.\n\n\n2) Pros:\n+ Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.\n+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected.\n+ Improved performance in speech recognition task.\n+ The idea is clearly explained and well motivated.\n\n\n3) Cons:\nImage captioning experiment:\nIn the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines. This experiment shows improvement over such baselines, however, the performance is still worse compared against baselines such as Lu et al, 2017 and Yao et al, 2016. It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done.\n\n\nUnconditioned generation experiments:\nIn these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help. Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation. The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case. For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given. Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it’s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is. If the authors believe that the 50% case is not necessary, please feel free to explain why.\n\n\nAdditional comments:\nThe method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN).\nIt would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).\n\nExperiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).\n\n\n4) Conclusion\nThe paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse. Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past. The proposed method presents a possible way of better modeling the future, however, some the results do not clearly back up the claim yet. The given score will improve if the authors are able to address the stated issues.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515642511693,"tcdate":1511648889841,"number":1,"cdate":1511648889841,"id":"HJzYCPDlf","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Review","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper reads well, has sufficient reference. The idea is simple and well explained. Positive empirial results support the proposed regularizer.","rating":"8: Top 50% of accepted papers, clear accept","review":"Twin Networks: Using the Future as a Regularizer\n\n** PAPER SUMMARY **\n\nThe authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN. Both RNNs are trained jointly and only the forward model is used at test time. Experiments on conditional generation (speech recognition, image captioning), and unconditional generation (MNIST pixel RNN, language models) show the effectiveness of the regularizer.\n\n** REVIEW SUMMARY **\n\nThe paper reads well, has sufficient reference. The idea is simple and well explained. Positive empirial results support the proposed regularizer.\n\n** DETAILED REVIEW **\n\nOverall, this is a good paper. I have a few suggestions along the text but nothing major.\n\nIn related work, I would cite co-training approaches. In effect, you have two view of a point in time, its past and its future and you force these two views to agree, see  (Blum and Mitchell, 1998) or Xu, Chang, Dacheng Tao, and Chao Xu. \"A survey on multi-view learning.\" arXiv preprint arXiv:1304.5634 (2013). I would also relate your work to distillation/model compression which tries to get one network to behave like another. On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained? \n\nIn section 2, it is not obvious to me that the regularizer (4) would not be ignored in absence of regularization on the output matrix. I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings. Could you comment why this would not happen?\n\nIn Section 4.2, you need to refer to Table 2 in the text. You also need to define the evaluation metrics used. In this section, why are you not reporting the results from the original Show&Tell paper? How does your implementation compare to the original work?\n\nOn unconditional generation, your hypothesis on uncertainty is interesting and could be tested. You could inject uncertainty in the captioning task for instance, e.g. consider that multiple version of each word e.g. dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates. Would your regularizer still be helpful there? At which point would it break?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1515188285101,"tcdate":1509134928036,"number":790,"cdate":1510092364410,"id":"BydLzGb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead. We train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model. The backward network is used only during training, and plays no role during sampling or inference. We hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states). We show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves significant improvement on a COCO caption generation task.","pdf":"/pdf/66160f12508cd45b354ce53e66ce280754078428.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}