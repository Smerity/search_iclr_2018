{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222813473,"tcdate":1511770427690,"number":3,"cdate":1511770427690,"id":"HkVrKHYlG","invitation":"ICLR.cc/2018/Conference/-/Paper915/Official_Review","forum":"H13WofbAb","replyto":"H13WofbAb","signatures":["ICLR.cc/2018/Conference/Paper915/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Weak synchronization with a weak impact","rating":"4: Ok but not good enough - rejection","review":"Paper proposes a weak synchronization approach to synchronous SGD with the goal of improving even with slow parameter servers. This is an improvement on earlier proposals (e.g. Revisiting Synchronous SGD) that allow for slow workers. Empirical results on ResNet50 on CIFAR show promising results for simulations with slow workers and servers, with the proposed approach.\n\nIssues with the paper:\n- Since the paper is focused on empirical results, having results only for ResNet50 on CIFAR is very limiting\n- Empirical results are based on simulations and not real workloads. The choice of simulation constants (% delayed, and delay time) seems somewhat arbitrary as well.\n- For the simulated results, the comparisons seem unfair since the validation error is different. It will be useful to also provide time to a certain accuracy that all of them get to e.g. the validation error of 0.1609 (reached by the 3 important cases).\n\nOverall, the paper proposes an interesting improvement to this area of synchronous training, however it is unable to validate the impact of this proposal.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Distributed Synchronous SGD with Weak Synchronization","abstract":"Distributed training of deep learning is widely conducted with large neural networks and large datasets. Besides asynchronous stochastic gradient descent~(SGD), synchronous SGD is a reasonable alternative with better convergence guarantees. However, synchronous SGD suffers from stragglers. To make things worse, although there are some strategies dealing with slow workers, the issue of slow servers is commonly ignored. In this paper, we propose a new parameter server~(PS) framework dealing with not only slow workers, but also slow servers by weakening the synchronization criterion. The empirical results show good performance when there are stragglers.","pdf":"/pdf/1c1ff517c33f6e17855efaec19617cddbfe2b617.pdf","paperhash":"anonymous|faster_distributed_synchronous_sgd_with_weak_synchronization","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Distributed Synchronous SGD with Weak Synchronization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H13WofbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper915/Authors"],"keywords":["distributed","deep learning","straggler"]}},{"tddate":null,"ddate":null,"tmdate":1512222813515,"tcdate":1511749357859,"number":2,"cdate":1511749357859,"id":"Sy8xDgYxG","invitation":"ICLR.cc/2018/Conference/-/Paper915/Official_Review","forum":"H13WofbAb","replyto":"H13WofbAb","signatures":["ICLR.cc/2018/Conference/Paper915/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Overall, the proposed method is not well-motivated, simple, with no theoretical support, and experimental results are not convincing.","rating":"3: Clear rejection","review":"This paper considers distributed synchronous SGD, and proposes to use \"partial pulling\" to alleviate the problem with slow servers.\n\nThe motivation is that the server may be a straggler. The authors suggested one possibility, namely that the server and some workers are located on the same machine and the workers take most of the computational resource. However, if this is the case, a simple solution would be to move the server to a different node. A more convincing argument for a slow server should be provided.\n\nThough the authors claimed that they used 3 techniques to accelerate synchronous SGD, only partial pulling is proposed by them (the other 2 are borrowed straightforwardly from existing papers). The mechanism of partial pulling is very simple (just let SGD proceed after pulling a partial parameter block instead of the whole block). As mentioned by the authors in section 1, any relaxation in synchrony brings more noise and higher variance to the updates, and also may cause slow convergence or convergence to a poor solution. However, the authors provided no theoretical study on any of these aspects.\n\nExperimental results are not convincing. Only one relatively small dataset (cifar10) is used Moreover, the slow server problem is only simulated by artificially adding delays to the server.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Distributed Synchronous SGD with Weak Synchronization","abstract":"Distributed training of deep learning is widely conducted with large neural networks and large datasets. Besides asynchronous stochastic gradient descent~(SGD), synchronous SGD is a reasonable alternative with better convergence guarantees. However, synchronous SGD suffers from stragglers. To make things worse, although there are some strategies dealing with slow workers, the issue of slow servers is commonly ignored. In this paper, we propose a new parameter server~(PS) framework dealing with not only slow workers, but also slow servers by weakening the synchronization criterion. The empirical results show good performance when there are stragglers.","pdf":"/pdf/1c1ff517c33f6e17855efaec19617cddbfe2b617.pdf","paperhash":"anonymous|faster_distributed_synchronous_sgd_with_weak_synchronization","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Distributed Synchronous SGD with Weak Synchronization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H13WofbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper915/Authors"],"keywords":["distributed","deep learning","straggler"]}},{"tddate":null,"ddate":null,"tmdate":1512222813554,"tcdate":1511740877529,"number":1,"cdate":1511740877529,"id":"H1SAHAdlf","invitation":"ICLR.cc/2018/Conference/-/Paper915/Official_Review","forum":"H13WofbAb","replyto":"H13WofbAb","signatures":["ICLR.cc/2018/Conference/Paper915/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Need to motivate the problem and evaluate thoroughly ","rating":"4: Ok but not good enough - rejection","review":"This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers. Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters. This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method. The method is evaluated with Resnet -50 using synthetic delays.\n\nComments for the author:\n\nThe paper is well-written and easy to follow. The problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large blocks.\n\n1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers. Unfortunately, in your Figure 2, this is not as obvious and not real since it is using simulated delays. More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them. \n\n2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)? The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients. I feel that, with such a design, you may actually end up hurting the performance by sending a large number of small packets in the no failure case. For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem). You need to show the evidence that you do not hurt the failure-free case for a large number of workers.\n\n3) The evaluation is on fairly small workloads (CIFAR-10). Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful. Furthermore, a clear description of your “pull” configuration (such as in Figure 1) i.e. how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%).\n\n4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc). These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline.\n\nOverall, I feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown. To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays). Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Distributed Synchronous SGD with Weak Synchronization","abstract":"Distributed training of deep learning is widely conducted with large neural networks and large datasets. Besides asynchronous stochastic gradient descent~(SGD), synchronous SGD is a reasonable alternative with better convergence guarantees. However, synchronous SGD suffers from stragglers. To make things worse, although there are some strategies dealing with slow workers, the issue of slow servers is commonly ignored. In this paper, we propose a new parameter server~(PS) framework dealing with not only slow workers, but also slow servers by weakening the synchronization criterion. The empirical results show good performance when there are stragglers.","pdf":"/pdf/1c1ff517c33f6e17855efaec19617cddbfe2b617.pdf","paperhash":"anonymous|faster_distributed_synchronous_sgd_with_weak_synchronization","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Distributed Synchronous SGD with Weak Synchronization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H13WofbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper915/Authors"],"keywords":["distributed","deep learning","straggler"]}},{"tddate":null,"ddate":null,"tmdate":1510092385935,"tcdate":1509137186397,"number":915,"cdate":1510092362440,"id":"H13WofbAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H13WofbAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Faster Distributed Synchronous SGD with Weak Synchronization","abstract":"Distributed training of deep learning is widely conducted with large neural networks and large datasets. Besides asynchronous stochastic gradient descent~(SGD), synchronous SGD is a reasonable alternative with better convergence guarantees. However, synchronous SGD suffers from stragglers. To make things worse, although there are some strategies dealing with slow workers, the issue of slow servers is commonly ignored. In this paper, we propose a new parameter server~(PS) framework dealing with not only slow workers, but also slow servers by weakening the synchronization criterion. The empirical results show good performance when there are stragglers.","pdf":"/pdf/1c1ff517c33f6e17855efaec19617cddbfe2b617.pdf","paperhash":"anonymous|faster_distributed_synchronous_sgd_with_weak_synchronization","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Distributed Synchronous SGD with Weak Synchronization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H13WofbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper915/Authors"],"keywords":["distributed","deep learning","straggler"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}