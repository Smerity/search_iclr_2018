{"notes":[{"tddate":null,"ddate":null,"tmdate":1515180948420,"tcdate":1515180948420,"number":3,"cdate":1515180948420,"id":"r1nqmLp7M","invitation":"ICLR.cc/2018/Conference/-/Paper464/Official_Comment","forum":"r1vccClCb","replyto":"Hk4qYw7eG","signatures":["ICLR.cc/2018/Conference/Paper464/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper464/Authors"],"content":{"title":"We really appreciate your valuable review","comment":"Dear Reviewer,\n\nWe really appreciate your valuable review! We have modified our paper based on your feedback by:\n \n1) adding denoising and variational autoencoder (and their neighbor-encoder counterparts) to all experiments, and â€¦\n2) adding a new set of experiment on CIFAR 10 in Section 4.2. In all the experiments, we observed that neighbor-encoder and its variants outperform their autoencoder counterparts when applied in semi-supervised classification (when the number of labeled data available is small) and clustering tasks.\n\nThanks,\nAuthors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1515181051150,"tcdate":1515180905949,"number":2,"cdate":1515180905949,"id":"SJzuXLaXz","invitation":"ICLR.cc/2018/Conference/-/Paper464/Official_Comment","forum":"r1vccClCb","replyto":"HJDy-RKef","signatures":["ICLR.cc/2018/Conference/Paper464/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper464/Authors"],"content":{"title":"Thank you for your helpful review and kind words","comment":"Dear Reviewer,\n\nThank you for your helpful review and kind words! We are glad that you like the idea.\n\nIn the review, you have argued that the neighbor-encoder method is not a completely abstract-unsupervised representation learning method as it requires domain knowledge to define the neighbor relationship. This statement is certainly valid, as we do need some domain knowledge. However, the amount of domain knowledge required by neighbor-encoder is minimal in comparison to what is required by a typical supervised representation learning method: we only need a \"neighbor\" to be defined, the \"non-neighbor\" information is not needed. In other words, we only need to know what is \"similar\" (and this information can be very sparse), but not what is \"not similar\" (the key information needed to divide objects into different classes/clusters). \nFurthermore, note that the domain knowledge provided do not need to be precise. Our MINST example in Section 4.1 simply use Euclidean distance in raw pixel space as the similarity measure to find the neighbors. For the newly added CIFAR10 data set Section 4.2, we use Euclidean distance in a common computer vision feature space as the similarity measure; the feature selected does not have much discriminative power for this data set and only 22% of the object-neighbor pairs are from the same class. Nevertheless, the results (Figure 9 and Table 2) show that all three variants of neighbor-encoder outperform their autoencoder counterparts in both semi-supervised classification (when number of labeled data is small) and clustering tasks.\n\nTo clarify, our claim is not that neighbor-encoder is a purely unsupervised representation learning method. Instead, our claim is that even a tiny amount of domain knowledge can greatly improve unsupervised representation learning, and neighbor-encoder is an effective way to incorporate such domain knowledge into the unsupervised representation learning framework.\n\nFor any comparable norm based neighbor definition, \"curse of dimensionality\" indeed would be a problem. To quantify the severity of such problem, we measured the percentage of object-neighbor pairs being in the same class. For example, in Section 4.4 (originally Section 4.3), about 49% of the object-neighbor pairs in the 40-dimensional feature vector space are in the same class (note that this is relatively high, as the default rate for randomly assigned neighbor is just ~9% for this data set). Another way we envision that can further increase this percentage is to use side information to define a neighbor (as introduced in Section 3.4). For instance, images/document on the same webpage or reviews of the same paper/movie/music could be declared being neighbor of each other. Such side information would much less sensitive to the curse of dimensionality.\n\nThanks,\nAuthors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1515180856842,"tcdate":1515180856842,"number":1,"cdate":1515180856842,"id":"rJ-BQUaXM","invitation":"ICLR.cc/2018/Conference/-/Paper464/Official_Comment","forum":"r1vccClCb","replyto":"S1JBxOqlz","signatures":["ICLR.cc/2018/Conference/Paper464/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper464/Authors"],"content":{"title":"Thank you very much for your valuable review","comment":"Dear Reviewer,\n\nThank you very much for your valuable review. Addressing your concerns has made our paper much stronger. Our responses to the major issues are listed below:\n\nIssue 1: There are some unaddressed theoretical questions. The optimal solution to predict the set of neighbor points in mean-squared metric is to predict the average of those points, which is not well justified as the averaged image can easily fall off the data manifold. This may lead to a more blurry reconstruction when k increases, despite the intra-class targets are tight. It can also in turn harm the latent representation when euclidean neighbors are not actually similar (e.g. images in cifar10/imagenet that are not as simple as 10 digits). This seems to be a defect of the neighbor-encoder method and is not discussed in the paper.\nResponse to Issue 1: Thank you for raising this concern. The issue is addressed by removing the original configuration in question (in which we randomly selected one of the k nearest neighbors as the target to predict) as it does have this \"averaging\" problem. All the experiments are rerun with the most basic neighbor-encoder setting, in which we predict only the nearest neighbor of each object. As the target to predict is fixed, we no longer suffer from the \"averaging neighbors\" problem.\n\nIssue 2: The data sets used in the experiments are relatively small and simple, larger-scale experiments should be conducted. The fluctuations in Figure 9 and 10 suggest the significant variances in the results. Also, more complicated data/images can decrease the actual similarities of euclidean neighbors, thus affecting the results.\nResponse to Issue 2: After we rerun all the experiments described in response to Issue 1, we no longer see significant variance in the results. A new set of experiment on CIFAR 10 is performed and reported in Section 4.2. We also included experiments comparing three variants of neighbor-encoder (vanilla, denoising, variational) with their autoencoder counterparts.\n\nIssue 3: The baselines are weak. Only the most basic auto-encoder is compared, no additional variants or other data augmentation techniques are compared. It is possible other variants improve the basic auto-encoder in similar ways.\nResponse to Issue 3: We added comparison to two more popular variants of autoencoder, the denoising and variational autoencoder, in all of our experiments.\n\nIssue 4: Some results are not very well explained. It seems the performance increases monotonically as the number of neighbors increases (Figure 5, 9, 10). Will this continue or when will the performance decrease? I would expect it to decrease as the far away neighbors will be dissimilar. The authors can either attach the nearest neighbors figures or their statistics, and provide explanations on when and why the performance decrease is expected.\nResponse to Issue 4: We believe that Figure 6 addresses this issue. A new set of experiments is performed by using neighbors that are further away (i.e., changing 1st neighbor to the ith nearest neighbor). The performance decreases as expected when i is larger than 16 because the performance is crippled by lower quality neighbors. Figure 15 shows example neighbor pairs under different proximity settings.\n\nIssue 5: Some notations are confusing and need to be improved. For example, X and Y are actually the same set of images, the separation is a bit confusing; y_i \\in y in last paragraph of page 4 is incorrect, should use something like y_i in N(y).\nResponse to Issue 5: The notation is improved as suggested.\n\nThanks,\nAuthors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1515642452593,"tcdate":1511845942635,"number":3,"cdate":1511845942635,"id":"S1JBxOqlz","invitation":"ICLR.cc/2018/Conference/-/Paper464/Official_Review","forum":"r1vccClCb","replyto":"r1vccClCb","signatures":["ICLR.cc/2018/Conference/Paper464/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review: neighbor-encoder -> neighbor encoder","rating":"4: Ok but not good enough - rejection","review":"This paper presents a variant of auto-encoder that relaxes the decoder targets to be neighbors of a data point. Different from original auto-encoder, where data point x and the decoder output \\hat{x} are forced to be close, the neighbor-encoder encourage the decoder output to be similar to the neighbors of the input data point. By considering the neighbor information, the decoder targets would have smaller intra-class distances, thus larger inter-class distances, which helps to learn better separated latent representation of data in terms of data clusters. The authors conduct experiments on several real but relative small-scale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets. \n\nThe method of neighbor prediction is a simple and small modification of the original auto-encoder, but seems to provide a way to augment the targets such that intra-class distance of decoder targets can be tightened. Improvements in the conducted experiments seem significant compared to the most basic auto-encoder.\n\nMajor issues:\n\nThere are some unaddressed theoretical questions. The optimal solution to predict the set of neighbor points in mean-squared metric is to predict the average of those points, which is not well justified as the averaged image can easily fall off the data manifold. This may lead to a more blurry reconstruction when k increases, despite the intra-class targets are tight. It can also in turn harm the latent representation when euclidean neighbors are not actually similar (e.g. images in cifar10/imagenet that are not as simple as 10 digits). This seems to be a defect of the neighbor-encoder method and is not discussed in the paper.\n\nThe data sets used in the experiments  are relatively small and simple, larger-scale experiments should be conducted. The fluctuations in Figure 9 and 10 suggest the significant variances in the results. Also, more complicated data/images can decrease the actual similarities of euclidean neighbors, thus affecting the results.\n\nThe baselines are weak. Only the most basic auto-encoder is compared, no additional variants or other data augmentation techniques are compared. It is possible other variants improve the basic auto-encoder in similar ways. \n\nSome results are not very well explained. It seems the performance increases monotonically as the number of neighbors increases (Figure 5, 9, 10). Will this continue or when will the performance decrease? I would expect it to decrease as the far away neighbors will be dissimilar. The authors can either attach the nearest neighbors figures or their statistics, and provide explanations on when and why the performance decrease is expected.\n\nSome notations are confusing and need to be improved. For example, X and Y are actually the same set of images, the separation is a bit confusing; y_i \\in y in last paragraph of page 4 is incorrect, should use something like y_i in N(y).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1515642452631,"tcdate":1511805151306,"number":2,"cdate":1511805151306,"id":"HJDy-RKef","invitation":"ICLR.cc/2018/Conference/-/Paper464/Official_Review","forum":"r1vccClCb","replyto":"r1vccClCb","signatures":["ICLR.cc/2018/Conference/Paper464/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice Idea but what about \"Curse of Dimensionality\"?","rating":"6: Marginally above acceptance threshold","review":"A representation learning framework from unsupervised data, based not on auto-encoding (x in, x out), but on neighbor-encoding (x in, N(x) out, where N(.) denotes the neighbor(s) of x) is introduced. \n\nThe underlying idea is interesting, as such, each and every degree of freedom do not synthesize itself similar to the auto-encoder setting, but rather synthesize a neighbor, or k-neighbors. The authors argue that this form of unsupervised learning is more powerful compared to the standard auto-encoder setting, and some preliminary experimental proof is also provided. \n\nHowever, I would argue that this is not a completely abstract - unsupervised representation learning setting since defining what is \"a neighbor\" and what is \"not a neighbor\" requires quite a bit of domain knowledge. As we all know, the euclidian distance, or any other comparable norm, suffers from the \"Curse of Dimensionality\" as the #-of-Dimensions increase. \n\nFor instance, in section 4.3, the 40-dimensional feature vector space is used to define neighbors in. It would be great how the neighborhood topology in that space looks like.\n\nAll in all, I do like the idea as a concept but I am wary about its applicability to real data where defining a good neighborhood metric might be a major challenge of its own. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1515762916005,"tcdate":1511385484285,"number":1,"cdate":1511385484285,"id":"Hk4qYw7eG","invitation":"ICLR.cc/2018/Conference/-/Paper464/Official_Review","forum":"r1vccClCb","replyto":"r1vccClCb","signatures":["ICLR.cc/2018/Conference/Paper464/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Lack of comparison with other autoencoders","rating":"5: Marginally below acceptance threshold","review":"This paper describes a generalization of autoencoders that are trained to reconstruct a close neighbor of its input, instead of merely the input itself. Experiments on 3 datasets show that this yields better representations in terms of post hoc classification with a linear classifier or clustering, compared to a regular autoencoder.\n\nAs the authors recognize, there is a long history of research on variants of autoencoders. Unfortunately this paper compares with none of them. While the authors suggest that, since these variations can be combined with the proposed neighbor reconstruction variant, it's not necessary to compare with these other variations, I disagree. It could very well be that this neighbor trick makes other methods worse for instance. \n\nAt the very least, I would expect a comparison with denoising autoencoders, since they are similar if one thinks of the use of neighbors as a structured form of noise added to the input. It could very well be in fact that simply adding noise to the input is sufficient to force the autoencoder to learn a valuable representation, and that the neighbor reconstruction approach is simply an overly complicated approach of achieving the same results. This is an open question right now that I'd expect this paper to answer.\n\nFinally, I think results would be more impressive and likely to have impact if the authors used datasets that are more commonly used for representation learning, so that a direct performance comparison can be made with previously published results. CIFAR 10 and SVHN would be good alternatives.\n\nOverall, I'm afraid I must recommend that this paper be rejected.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1515180721195,"tcdate":1509120655052,"number":464,"cdate":1509739286900,"id":"r1vccClCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1vccClCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neighbor-encoder","abstract":"We propose a novel unsupervised representation learning framework called neighbor-encoder in which domain knowledge can be trivially incorporated into the learning process without modifying the general encoder-decoder architecture. In contrast to autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors. The proposed neighbor-encoder can be considered as a generalization of autoencoder as the input data can be treated as the nearest neighbor of itself with zero distance. By reformulating the representation learning problem as a neighbor reconstruction problem, domain knowledge can be easily incorporated with appropriate definition of similarity or distance between objects. As such, any existing similarity search algorithms can be easily integrated into our framework. Applications of other algorithms (e.g., association rule mining) in our framework is also possible since the concept of ``neighbor\" is an abstraction which can be appropriately defined differently in different contexts. We have demonstrated the effectiveness of our framework in various domains, including images, time series, music, etc., with various neighbor definitions. Experimental results show that neighbor-encoder outperforms autoencoder in most scenarios we considered.","pdf":"/pdf/37dcfc2fa414e93481ebef2c24570a2c09b5945a.pdf","paperhash":"anonymous|neighborencoder","_bibtex":"@article{\n  anonymous2018neighbor-encoder,\n  title={Neighbor-encoder},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1vccClCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper464/Authors"],"keywords":["unsupervised learning","representation learning","autoencoder"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}