{"notes":[{"tddate":null,"ddate":null,"tmdate":1512810459984,"tcdate":1512810459984,"number":3,"cdate":1512810459984,"id":"S1Nkd7tZG","invitation":"ICLR.cc/2018/Conference/-/Paper922/Official_Comment","forum":"rJfHoM-C-","replyto":"ryht5AYlM","signatures":["ICLR.cc/2018/Conference/Paper922/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper922/Authors"],"content":{"title":"Thanks for your comment","comment":"According to our experiments on miniImageNet and Omniglot, our algorithm overall outperforms Matching Network (NIPS 2016) and Meta Learner (ICLR 2017) on these two benchmarks, even though our algorithm is based on features produced by a simple CNN and performs no refinement on new classes. This indicates that the simplex idea shows the superiority compared with some of new but more complicated algorithms for few-shot learning. \n\nThe algorithm proposed by Kaiser et al (ICLR 2017) indeed outperforms our algorithm on Omniglot. We will make the claim proper in the revision.\nWe followed the experimental protocol presented by Ravi & Larochelle (ICLR 2017). So we did not perform experiments on  GNMT. The two benchmarks we used are the datasets that are most frequently used by other researchers for few-shot learning. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]}},{"tddate":null,"ddate":null,"tmdate":1512808223137,"tcdate":1512808223137,"number":2,"cdate":1512808223137,"id":"BkDm1mFbG","invitation":"ICLR.cc/2018/Conference/-/Paper922/Official_Comment","forum":"rJfHoM-C-","replyto":"r1_gtOolG","signatures":["ICLR.cc/2018/Conference/Paper922/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper922/Authors"],"content":{"title":"Thanks for your comment","comment":"We will cite the papers you listed in the revision. They are excellent works. \n\nBut first, I want to make it clear that we did not use simplex volume as the loss to train networks. As Fig. 4 illustrates, we employed a simple 4-block CNN supervised by softmax. For new classes, we only applied the feature maps produced by the last convolution layer, as shown in Fig. 3. Namely, we did not  use any re-training, fine-tuning,  or other manipulations that refine the CNNs for new classes. Even though, our simplex algorithm overall outperforms Matching Network (NIPS 2016) and Meta Learner (ICLR 2017) on two benchmarks. \n\nFor Prototypical Networks (PN), our simplex idea can also be applicable according to the formulation of this elegant algorithm, e.g. replacing the distance metric in PN with our simplex metric. Our algorithm can also be combined with Meta-learning algorithms such as MAML and TCML, to improve performance, because they are developed from the different level of learning models for few-shot learning. \n\nAbout the typos, the squares are missed in equation (3) and a minus sign in equation (6). These will be revised. Thank you for pointing out these typos.\nBesides,  the local feature representation in Section 3 will be presented in more detail. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]}},{"tddate":null,"ddate":null,"tmdate":1512804758316,"tcdate":1512804758316,"number":1,"cdate":1512804758316,"id":"rJ0c-MK-M","invitation":"ICLR.cc/2018/Conference/-/Paper922/Official_Comment","forum":"rJfHoM-C-","replyto":"H1_Nsp3gz","signatures":["ICLR.cc/2018/Conference/Paper922/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper922/Authors"],"content":{"title":"Thanks for your comment","comment":"Our algorithm works well for few-shot learning case where sizes of matrices for computing determinants are generally up to 5x5.  The computation complexity is actually low. However,  the cubic complexity caused by determinant will significantly slow down our algorithm if there are much more samples in each class. For such case, our algorithm cannot scale well with the number of classes. But for few-shot learning, this is not an issue. \n\nActually, using the bordering method of matrix inversion, we can further expand $P$ with $Q$ in formula (7). Theoretically,  we only need to compute the inverse of $Q$ one time for each class during testing. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]}},{"tddate":null,"ddate":null,"tmdate":1515642531119,"tcdate":1512000304420,"number":3,"cdate":1512000304420,"id":"H1_Nsp3gz","invitation":"ICLR.cc/2018/Conference/-/Paper922/Official_Review","forum":"rJfHoM-C-","replyto":"rJfHoM-C-","signatures":["ICLR.cc/2018/Conference/Paper922/AnonReviewer1"],"readers":["everyone"],"content":{"title":"No Title","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an approach for few-shot classification based on a geometric idea. The basic assumption is that a query instance will be closest to the polytope corresponding to the correct class than to other classes, where they consider polytopes formed by selecting samples from each class as vertices. As a distance metric, authors consider the variation of the volume of each class-polytope when a query instance is added to the corresponding class. Given that there is not a method to calculate the volume of a general convex polytope, they approximate the polytope by the corresponding simplex convex (convex polytope with the condition of n = d). Fortunately, in the case of the simplex, there is close form solution to obtain the volume.\n\nIn general, the paper is well presented and, as far as I know, the proposed idea is novel and sound. Experimentation is correct. Results indicate that the proposed method is able to outperform related state-of-the-art techniques, achieving a reasonable improvement, approx. 1-3% depending of the dataset. \n\nAs a drawback, for each query instance, the method needs to estimate the distance to the simplex of each class, therefore it does not scale well with the number of classes. Authors should comment about this issue, in particular, about the computational complexity of the proposed method. Also, in the cases presented in the paper, the selection of the training instances used to calculate the simplex is straight-forward, however, in a more general case, this could be a relevant problem. It will be good to comment about this issue.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]}},{"tddate":null,"ddate":null,"tmdate":1515875680140,"tcdate":1511913711902,"number":2,"cdate":1511913711902,"id":"r1_gtOolG","invitation":"ICLR.cc/2018/Conference/-/Paper922/Official_Review","forum":"rJfHoM-C-","replyto":"rJfHoM-C-","signatures":["ICLR.cc/2018/Conference/Paper922/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting technique but more complicated and lower performance than recent approaches","rating":"4: Ok but not good enough - rejection","review":"This work proposes a method for few-shot classification that treats a set of embedded points belonging to a class as a simplex. Classification for an unlabeled test point is performed by selecting the class whose augmented simplex has the smallest volume relative to the original class simplex.\n\nStrengths\n- The use of simplices for representing classes in few-shot learning is novel.\n\nWeaknesses\n- A number of recent related few-shot learning approaches are missing from the related work.\n- In light of missing baselines, the proposed method does not perform better than recent few-shot approaches.\n\nI am not an expert on simplices, but the derivations in the paper appear to be correct, with two exceptions: (a) equation 6 appears to be the ratio of C^2(Y U t) / C(Y), (b) equation 6 appears to be missing a minus sign.\n\nThe writing of the paper is relatively clear, however there are several important issues:\n- Background on metric learning is missing.\n- The training loss is not described (i.e. how is the volume ratio from equation 7 converted into a probability distribution over classes?).\n- The local feature representation in Section 3 is unclear and should be explained in more detail.\n- Related work is missing recent few-shot learning approaches, including MAML [1], Prototypical Networks [2], and TCML [3].\n\nThe proposed method is a metric learning approach but it has some additional restrictions relative to other such techniques for few-shot learning such as Matching Networks or Prototypical Networks. One is that the computation of volume ratio involves matrix inversion of P and Q. Another is that the method is not defined when the number of points exceeds the dimensionality of the embedding space. These are not likely to be an issue for few-shot learning, but should be noted as interest in methods that scale gracefully from the few-shot to ordinary classification increases.\n\nRegarding Omniglot results, 20-way 1-shot/5-shot experiments are widely reported in related work but missing from the paper.\n\nWhen the results are viewed in light of missing baselines, such as Prototypical Networks (68.2% on 5-way 5-shot miniImagenet), the proposed method is more complicated and performs significantly worse.\n\nOverall, the proposed approach is interesting but there are significant issues with both background/related work and performance relative to missing baselines.\n\n[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" ICML 2017.\n[2] Snell, Jake, Kevin Swersky, and Richard S. Zemel. \"Prototypical Networks for Few-shot Learning.\" NIPS 2017.\n[3] Mishra, Nikhil, et al. \"Meta-Learning with Temporal Convolutions.\" arXiv preprint arXiv:1707.03141 (2017).\n\nEDIT: I have read the author's response. The background and related work issues are largely fixed in the latest revision of the paper. Thanks also to the authors for clarifying that training proceeds according to the minimization of cross-entropy loss, rather than a loss based on the simplex. In this case, the novelty of the proposed method then lies in the test-time procedure for making a classification decision when a few-shot episode is encountered. Thus the novelty is relatively low in my opinion. From an experimental perspective, I believe that a comparison of the proposed approach to other test-time classification decision rules is warranted to demonstrate that the simplex rule is better than simpler alternatives (for example, fitting a Gaussian distribution to the support examples of each few-shot class and then assigning a test example to  the class with highest posterior probability). My rating remains unchanged.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]}},{"tddate":null,"ddate":null,"tmdate":1515642531203,"tcdate":1511807620209,"number":1,"cdate":1511807620209,"id":"ryht5AYlM","invitation":"ICLR.cc/2018/Conference/-/Paper922/Official_Review","forum":"rJfHoM-C-","replyto":"rJfHoM-C-","signatures":["ICLR.cc/2018/Conference/Paper922/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach but missing baselines and state of the art claims wrong","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a geometric based approach to solving the problem of one-shot and few-shot learning. The basic idea is to use the feature vectors of a particular class to construct a simplex. (I am assuming the dimensions of the vectors are selected so as to exactly construct a simplex? It is not clearly written in the paper). The volume of the simplex is then taken to be a measure of class scatter, and classification happens by assigning the test feature vector to the nearest simplex, where the distances are normalized by the volume of the simplex. \n\nWhile the approach makes sense, I am not convinced that this geometric method plays an important role in increasing the performance on one-shot/few-shot tasks. In particular, one could try simpler approaches like k-NN where the distances to the cluster centers are also normalized by the variance within the clusters. I would suspect that this method is not superior to this simpler baseline. \n\nThe other issue I have with this paper is misleading claims about being state of the art on Omniglot. In particular see Kaiser et al (ICLR 2017), where on 5-way-1-shot an accuracy of 98.4% is reached compared to 94.6% in this paper, and on 5-way-5-shot an accuracy of 99.6% is reached compared to 99.1% in this work. The paper also misses evaluations on various other data sets such as GNMT etc., on which Kaiser et al evaluated their approach.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]}},{"tddate":null,"ddate":null,"tmdate":1514169791819,"tcdate":1509137213676,"number":922,"cdate":1510092362406,"id":"rJfHoM-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJfHoM-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Few-Shot Learning with Simplex","abstract":"Deep learning has made remarkable achievement in many fields. However, learning\nthe parameters of neural networks usually demands a large amount of labeled\ndata. The algorithms of deep learning, therefore, encounter difficulties when applied\nto supervised learning where only little data are available. This specific task\nis called few-shot learning. To address it, we propose a novel algorithm for fewshot\nlearning using discrete geometry, in the sense that the samples in a class are\nmodeled as a reduced simplex. The volume of the simplex is used for the measurement\nof class scatter. During testing, combined with the test sample and the\npoints in the class, a new simplex is formed. Then the similarity between the test\nsample and the class can be quantized with the ratio of volumes of the new simplex\nto the original class simplex. Moreover, we present an approach to constructing\nsimplices using local regions of feature maps yielded by convolutional neural networks.\nExperiments on Omniglot and miniImageNet verify the effectiveness of\nour simplex algorithm on few-shot learning.","pdf":"/pdf/a4dda24d9732db9586f3c057d60d7c0882de68dc.pdf","TL;DR":"A simplex-based geometric method is proposed to cope with few-shot learning problems.","paperhash":"anonymous|fewshot_learning_with_simplex","_bibtex":"@article{\n  anonymous2018few-shot,\n  title={Few-shot learning with simplex},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJfHoM-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper922/Authors"],"keywords":["One-shot learning","few-shot learning","deep learning","simplex"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}