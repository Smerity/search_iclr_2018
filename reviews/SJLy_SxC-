{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222603394,"tcdate":1511808744041,"number":3,"cdate":1511808744041,"id":"Hkxey1cxM","invitation":"ICLR.cc/2018/Conference/-/Paper257/Official_Review","forum":"SJLy_SxC-","replyto":"SJLy_SxC-","signatures":["ICLR.cc/2018/Conference/Paper257/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice idea, the presentation could be improved.","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a nice idea of sparsification of skip connections in DenseNets. The authors decide to use a principle for sparsification that would minimize the distance among layers during the backpropagation. \n\nThe presentation of the paper could be improved. The paper presents an elegant and simple idea in a dense and complex way making the paper difficult to follow. E. g., Fig 1 d is discussed in Appendix and not in the main body of the paper, thus, it could be moved to Appendix section.\n\nTable 1 and 3 presents the results only for LogDenseNet V1, would it be possible to add results for V2 that have different MBD. Also, the budget for the skip connections is defined as log(i) in Table 1 and Table 2 has the budget of log(i/2), would it be possible to add the total number of skip connections to the tables? It would be interesting to compare the total number of skip connections in Jegou et. al. to LogDenseNet V1 in Table 3.\n\nOther issues:\n- Table 3, has an accuracy of nan. What does it mean? Not available or not a number? \n- L is used as the depth, however, in table 1 it appears as short for Log-DenseNetV1. Would it be possible to use another letter here?\n- “…, we make x_i also take the input from x_{i/4}, x_{i/8}, x_{i/16}…”. Shouldn’t x_{1/2} be used too?\n- I’m not sure I understand the reasons behind blurred image in Fig 2 at ½. It is mentioned that “it and its feature are at low resolution”. Could the authors comment on that?\n- Abstract: “… Log-DenseNets are easier than DenseNet to implement and to scale.” It is not clear why would LogDenseNets be easier to implement. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Log-DenseNet: How to Sparsify a DenseNet","abstract":"Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.","pdf":"/pdf/e778934b0e6fff52cb4147a34333b188dcdab2d7.pdf","TL;DR":"We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.","paperhash":"anonymous|logdensenet_how_to_sparsify_a_densenet","_bibtex":"@article{\n  anonymous2018log-densenet:,\n  title={Log-DenseNet: How to Sparsify a DenseNet},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLy_SxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper257/Authors"],"keywords":["DenseNet","sparse shortcut connections","network architecture","scene parsing","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222603439,"tcdate":1511807860239,"number":2,"cdate":1511807860239,"id":"H12ujAYlG","invitation":"ICLR.cc/2018/Conference/-/Paper257/Official_Review","forum":"SJLy_SxC-","replyto":"SJLy_SxC-","signatures":["ICLR.cc/2018/Conference/Paper257/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea to sparsify skip connections in DenseNets, well executed experiments","rating":"6: Marginally above acceptance threshold","review":"This paper introduces a new connectivity pattern for DenseNets, which encourages short distances among layers during backpropagation and gracefully scales to wider and deeper architectures. Experiments are performed to analyze the importance of the skip connections’ place in the context of image classification. Then, results are reported for both image classification and semantic segmentation tasks.\n\nThe clarity of the presentation could be improved. The main contribution of the paper is a network design that places skip connections to minimize the distances between layers, increasing the distance from 1 to 1 + log L when compared to traditional DenseNets. This design principle allows to mitigate the memory required to train DenseNets, which is critical for applications such as semantic segmentation where the input resolution has to be recovered.\n\nExperiments seem well executed; the authors consider several sparse connectivity patterns for DenseNets and provide empirical evidence highlighting the advantages of having a short maximum backpropagation distance (MBD). Moreover, they provide an analysis on the trade-off between the performance of a network and its computational cost.\n\nAlthough literature review is quite extensive, [a] might be relevant to discuss in the Network Compression section.\n[a] https://arxiv.org/pdf/1412.6550.pdf\n\nIt is not clear why Log-DenseNets would be easier to implement than DenseNets, as mentioned in the abstract. Could the authors clarify that?\n\nIn Tables 1-2-3, it would be good to add the results for Log-DenseNet V2. Adding the MBD of each model in the tables would also be beneficial.\n\nIn Table 3, what does “nan” accuracy mean? (DeepLab-LFOV)\n\nFinally, the authors might want to review the citep/cite use in the manuscript. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Log-DenseNet: How to Sparsify a DenseNet","abstract":"Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.","pdf":"/pdf/e778934b0e6fff52cb4147a34333b188dcdab2d7.pdf","TL;DR":"We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.","paperhash":"anonymous|logdensenet_how_to_sparsify_a_densenet","_bibtex":"@article{\n  anonymous2018log-densenet:,\n  title={Log-DenseNet: How to Sparsify a DenseNet},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLy_SxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper257/Authors"],"keywords":["DenseNet","sparse shortcut connections","network architecture","scene parsing","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222603485,"tcdate":1511709301421,"number":1,"cdate":1511709301421,"id":"BJaucU_gM","invitation":"ICLR.cc/2018/Conference/-/Paper257/Official_Review","forum":"SJLy_SxC-","replyto":"SJLy_SxC-","signatures":["ICLR.cc/2018/Conference/Paper257/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper investigates how to impose layer-wise connections in DenseNets most efficiently. The authors propose a connection-pattern, which connects layer i to layer i-2^k, k=0,1,2... The authors also propose maximum backpropgation distance (MBD) for measuring the fluency of gradient flow in the network, and justify the Log-DenseNet's advantage in this framework. Empirically, the author demonstrates the effectiveness of Log-DenseNet by comparing it with two other intuitive connection patterns on CIFAR datasets. Log-DenseNet also improves on FC-DenseNet, where the connection budget is the bottleneck because the feature maps are of high resolutions.\n\n\nStrengths:\n1. Generally, DenseNet is memory-hungry if the connection is dense, and it is worth studying how to sparsify a DenseNet. By showing the improvements on FC-DenseNet, Log-DenseNet demonstrates good potential on tasks which require upsampling of feature maps. \n2. The ablation experiments are well-designed and the visualizations of connectivity pattern are clear.\n\nWeakness:\n1. Adding a comparison with Log-DenseNet and vanilla DenseNet in the Table 2 experiment would make the paper stronger. Also, the NearestHalfAndLog pattern is not used in any latter visual recognition experiments, so I think it's better to just compare LogDenseNet with the two baselines instead. Despite there are CIFAR experiments on Log-DenseNet in latter sections, including results here would be easier to follow.\n2. I would like to see the a comparison with the DenseNet-BC in the segmentation and CIFAR classification tasks, which uses 1x1 conv layers to reduce the number of channels. It should be interesting to study whether it is possible to further sparsify DenseNet-BC, as it has much higher efficiency.\n3. The improvement of efficiency on classifications task is not that significant.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Log-DenseNet: How to Sparsify a DenseNet","abstract":"Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.","pdf":"/pdf/e778934b0e6fff52cb4147a34333b188dcdab2d7.pdf","TL;DR":"We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.","paperhash":"anonymous|logdensenet_how_to_sparsify_a_densenet","_bibtex":"@article{\n  anonymous2018log-densenet:,\n  title={Log-DenseNet: How to Sparsify a DenseNet},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLy_SxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper257/Authors"],"keywords":["DenseNet","sparse shortcut connections","network architecture","scene parsing","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509739400289,"tcdate":1509083102367,"number":257,"cdate":1509739397627,"id":"SJLy_SxC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJLy_SxC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Log-DenseNet: How to Sparsify a DenseNet","abstract":"Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. \nThis work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, Log-DenseNet, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$. Hence, Log-DenseNets are easier than DenseNets to implement and to scale. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on tabula rasa semantic segmentation, and competitive results on visual recognition.","pdf":"/pdf/e778934b0e6fff52cb4147a34333b188dcdab2d7.pdf","TL;DR":"We show shortcut connections should be placed in patterns that minimize between-layer distances during backpropagation, and design networks that achieve log L distances using L log(L) connections.","paperhash":"anonymous|logdensenet_how_to_sparsify_a_densenet","_bibtex":"@article{\n  anonymous2018log-densenet:,\n  title={Log-DenseNet: How to Sparsify a DenseNet},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLy_SxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper257/Authors"],"keywords":["DenseNet","sparse shortcut connections","network architecture","scene parsing","image classification"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}