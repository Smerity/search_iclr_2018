{"notes":[{"tddate":null,"ddate":null,"tmdate":1515773910877,"tcdate":1515773910877,"number":4,"cdate":1515773910877,"id":"r1kyevIEM","invitation":"ICLR.cc/2018/Conference/-/Paper104/Official_Comment","forum":"BySRH6CpW","replyto":"r1OyY_vZG","signatures":["ICLR.cc/2018/Conference/Paper104/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper104/AnonReviewer1"],"content":{"title":"Rebuttal response","comment":"Thank you very much for the response as it clears up some misunderstandings. I have updated the review and the score of the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Discrete Weights Using the Local Reparameterization Trick","abstract":"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","pdf":"/pdf/0f62b8667ae9bd4a6172e682456eebd7b98e48b0.pdf","TL;DR":"Training binary/ternary networks using local reparameterization with the CLT approximation","paperhash":"anonymous|learning_discrete_weights_using_the_local_reparameterization_trick","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Discrete Weights Using the Local Reparameterization Trick},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BySRH6CpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper104/Authors"],"keywords":["deep learning","discrete weight network"]}},{"tddate":null,"ddate":null,"tmdate":1515642379131,"tcdate":1511834168859,"number":3,"cdate":1511834168859,"id":"ryZHzH9gz","invitation":"ICLR.cc/2018/Conference/-/Paper104/Official_Review","forum":"BySRH6CpW","replyto":"BySRH6CpW","signatures":["ICLR.cc/2018/Conference/Paper104/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The Kingma's reparametrization trick for binary and ternary nets","rating":"6: Marginally above acceptance threshold","review":"This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE. Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel.  From the experiments, we can see that the proposed method is effective. \n\nIt seems that there could be more things to show in the experiments part. For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs. Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram. \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand. Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order. Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Discrete Weights Using the Local Reparameterization Trick","abstract":"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","pdf":"/pdf/0f62b8667ae9bd4a6172e682456eebd7b98e48b0.pdf","TL;DR":"Training binary/ternary networks using local reparameterization with the CLT approximation","paperhash":"anonymous|learning_discrete_weights_using_the_local_reparameterization_trick","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Discrete Weights Using the Local Reparameterization Trick},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BySRH6CpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper104/Authors"],"keywords":["deep learning","discrete weight network"]}},{"tddate":null,"ddate":null,"tmdate":1515763426691,"tcdate":1511779725554,"number":2,"cdate":1511779725554,"id":"BJHcawFxM","invitation":"ICLR.cc/2018/Conference/-/Paper104/Official_Review","forum":"BySRH6CpW","replyto":"BySRH6CpW","signatures":["ICLR.cc/2018/Conference/Paper104/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple idea that seem to work but the novelty is limited and some regularization choices might not do what is expected.","rating":"6: Marginally above acceptance threshold","review":"This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization. The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution. As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick. The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting. The method is evaluated on multiple experiments.\n\nThis paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice. My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]. While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward. I would thus suggest that the authors update the paper accordingly. \n\nOther than that, I have some other comments:\n- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve? \n- For the binary setting you mentioned that you had to reduce the entropy thus added a “beta density regulariser”. Did you add R(p) or log R(p) to the objective function? Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid. To force the probability near the endpoints you have to use alpha, beta < 1 which results into a “bowl” shaped Beta distribution. I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization.  \n- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT). \n\n[1] Wang & Manning, Fast Dropout Training.\n\nEdit: After the authors rebuttal I have increased the rating of the paper: \n- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].\n- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p. The lambda term would then serve as an indicator to how much entropy is necessary.\n- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).\n- The authors showed benefits compared to a continuous relaxation baseline.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning Discrete Weights Using the Local Reparameterization Trick","abstract":"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","pdf":"/pdf/0f62b8667ae9bd4a6172e682456eebd7b98e48b0.pdf","TL;DR":"Training binary/ternary networks using local reparameterization with the CLT approximation","paperhash":"anonymous|learning_discrete_weights_using_the_local_reparameterization_trick","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Discrete Weights Using the Local Reparameterization Trick},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BySRH6CpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper104/Authors"],"keywords":["deep learning","discrete weight network"]}},{"tddate":null,"ddate":null,"tmdate":1515642379229,"tcdate":1511536544171,"number":1,"cdate":1511536544171,"id":"SkOjP3Hlf","invitation":"ICLR.cc/2018/Conference/-/Paper104/Official_Review","forum":"BySRH6CpW","replyto":"BySRH6CpW","signatures":["ICLR.cc/2018/Conference/Paper104/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The idea of making use of the local parameterisation trick to learn discrete networks is straight forward but novel and leads to SOTA results.","rating":"7: Good paper, accept","review":"Summary of the paper:\nThe paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights. Results on MNIST, CIFAR-10 and ImageNet are very competitive. \n\nPros:\n- The proposed method leads to state of the art results .\n- The paper is easy to follow and clearly describes the implementation details needed to reach the results. \n\nCons:\n- The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward, but its application for learning discrete networks is to my best knowledge novel and interesting. \n\nIt could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets. \n\n\nMinor comments:\n- Some citations have a strange format: e.g. “in Hubara et al. (2016); Restegari et al. (2016)“ would be better readable as   “by Hubara et al. (2016) and Restegari et al. (2016)“\n-  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and \\mathcal{W} is the joint distribution resulting from independently sampling from  \\mathcal{W}^l_{i,j}. \n- page 6: “on the last full precision network”: should probably be “on the last full precision layer”\n                    “ distributions has” ->  “ distributions have” \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Discrete Weights Using the Local Reparameterization Trick","abstract":"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","pdf":"/pdf/0f62b8667ae9bd4a6172e682456eebd7b98e48b0.pdf","TL;DR":"Training binary/ternary networks using local reparameterization with the CLT approximation","paperhash":"anonymous|learning_discrete_weights_using_the_local_reparameterization_trick","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Discrete Weights Using the Local Reparameterization Trick},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BySRH6CpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper104/Authors"],"keywords":["deep learning","discrete weight network"]}},{"tddate":null,"ddate":null,"tmdate":1514567801737,"tcdate":1508984269523,"number":104,"cdate":1509739479667,"id":"BySRH6CpW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BySRH6CpW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Discrete Weights Using the Local Reparameterization Trick","abstract":"Recent breakthroughs in computer vision make use of large deep neural networks, utilizing the substantial speedup offered by GPUs. For applications running on limited hardware, however, high precision real-time processing can still be a challenge.  One approach to solving this problem is training networks with binary or ternary weights, thus removing the need to calculate multiplications and significantly reducing memory size. In this work, we introduce LR-nets (Local reparameterization networks), a new method for training neural networks with discrete weights using stochastic parameters. We show how a simple modification to the local reparameterization trick, previously used to train Gaussian distributed weights, enables the training of discrete weights. Using the proposed training we test both binary and ternary models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art results on most experiments.","pdf":"/pdf/0f62b8667ae9bd4a6172e682456eebd7b98e48b0.pdf","TL;DR":"Training binary/ternary networks using local reparameterization with the CLT approximation","paperhash":"anonymous|learning_discrete_weights_using_the_local_reparameterization_trick","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Discrete Weights Using the Local Reparameterization Trick},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BySRH6CpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper104/Authors"],"keywords":["deep learning","discrete weight network"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}