{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642414748,"tcdate":1511830050119,"number":3,"cdate":1511830050119,"id":"HJ57MEclz","invitation":"ICLR.cc/2018/Conference/-/Paper239/Official_Review","forum":"rJv8PEgAZ","replyto":"rJv8PEgAZ","signatures":["ICLR.cc/2018/Conference/Paper239/AnonReviewer1"],"readers":["everyone"],"content":{"title":"not yet ready for publication, but may become ready in time with more innovation and experimentation","rating":"4: Ok but not good enough - rejection","review":"This paper describes a method of learning cross-lingual sentence embeddings based on multi-task training with two kinds of losses: one based multilingual skip-gram using both monolingual and parallel text, and one based on sentence embedding similarity (with LSTM averaging as the composition function and a hinge loss with negative sampling as the training objective) using parallel sentence pairs. \n\nPros: \n\nThe paper is reasonably well-written and the approach is clear and well-motivated. Some of the experimental results are encouraging and may lead to a publication down the road. \n\nCons:\n\nThe paper feels a bit preliminary. \n\nWhile the proposed approach may not have been previously published, it does not feel especially novel. It appears to be a straightforward combination of existing techniques. \n\nThis lack of novelty becomes more problematic because the evaluation is not especially thorough, nor are the results particularly impressive. \n\nThe evaluation focuses on the cross-lingual document classification task of Klementiev et al. This task was useful for a few years for benchmarking cross-lingual embedding methods, but it has become seen more as a toy dataset in recent years. The paper only compares to results from 2015 and earlier. Are there any published results on this task from the past two years? The lack of activity is concerning because this is the only task on which results are reported. Furthermore, the papers' methods do not outperform the prior results of Pham et al. (2015).  \n\nThe authors may be able to achieve stronger results than Pham et al by scaling up their approach to more data and by using larger models.  There is abundant parallel and monolingual text for these languages, so the approach could be scaled up greatly to improve the absolute results. \n\nI was also disappointed to see the mixed results for training on more languages, which only helps when English is the source language. Is there a way to explain this trend?\n\nI think the methods make sense and if I were building a product, I would do something similar to what the authors are doing and combine existing techniques into a single framework. But I do not think that the contributions of this submission merit publication in this venue. Perhaps the submission would find a better fit at a multilinguality workshop or representation learning workshop at an NLP conference. \n\n\nMore specific comments and suggestions are below. \n\nMost of the citations should be parenthetical citations (i.e., use \\citep) rather than in-text citations. \n\nSec. 5.1: Please provide a citation for the Adam optimizer. \n\nSec. 5.2: In the description of JMT-Sent-Avg, there is a \"Sent-add\" which I think should instead be \"Sent-Avg\".\n\nSec. 6: \nThe text discusses increasing the dimension to 512, but then Table 1 shows \"dim=500\". Which is it? Sec. 5.1.B. mentions an LSTM dimension of 512, so I'm guessing 512 is correct. But then do the para_doc and BiSkip-UnsupAlign results in that lower part of the table use dimension 500 or 512?\n\nAlso in Sec. 6:\nThe paper includes this sentence: \"There are also significant gains when the document embeddings are obtained from sentence encoders trained in the multi-task setting.\" However, in Table 1, I only see this being the case in the 128-dimensional case, in which 3 of 4 such settings do improve from multi-task learning. In the 500-dimensional case, only 1 of the 4 comparisons shows an improvement from multi-task learning. I wonder why this might be the case that multi-task learning helps in the lower-dimensional case. In any event, I think the text quoted above should be changed. \n\nTypos:\nSec. 1: \"coposition\" --> \"composition\", \"since represent\" --> \"since we represent\" (?)\nSec. 2: \"through\" --> \"thorough\"\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multitask learning of Multilingual Sentence Representations","abstract":"We present a novel multi-task training approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. We construct sentence embeddings by processing word embeddings with an LSTM and by taking an average of the outputs. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a low-resource scenario.","pdf":"/pdf/834bd34a17082ee5f5a3f69511cb3f68b7cd1e42.pdf","TL;DR":"We jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to learn high quality multilingual text embeddings that perform well in the low resource scenario.","paperhash":"anonymous|multitask_learning_of_multilingual_sentence_representations","_bibtex":"@article{\n  anonymous2018multitask,\n  title={Multitask learning of Multilingual Sentence Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJv8PEgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper239/Authors"],"keywords":["multilingual","embedding","representation learning","multi-task learning","low resource"]}},{"tddate":null,"ddate":null,"tmdate":1515642414785,"tcdate":1511823520656,"number":2,"cdate":1511823520656,"id":"rJYiOMcef","invitation":"ICLR.cc/2018/Conference/-/Paper239/Official_Review","forum":"rJv8PEgAZ","replyto":"rJv8PEgAZ","signatures":["ICLR.cc/2018/Conference/Paper239/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Reasonable work, underwhelming experiments","rating":"5: Marginally below acceptance threshold","review":"The draft proposes to jointly learn multilingual embeddings of words and sequences in a multi-task learning setup that critically relies on the availability of monolingual and parallel corpora. The work appears to be largely influenced by Luong et al. (2015) and can be seen as its non-trivial extension.\n\nIt's a well-written text that conveys the contributions straightforwardly, and I'm very happy to say, without boasting. Overall, I enjoyed reading the paper.\n\nI do have two major objections to the current draft, and one minor. The former group of objections unfortunately makes me vote (weak) reject at this point, in hope a future revision will appear elsewhere that corrects the flaws.\n\nHere are my objections.\n\n1) The proposed experiments fall short of verifying the usefulness of the approach. The embeddings are nicely exposed in the cross-lingual document classification task, but not beyond this single task. In my view, this is underwhelming enough for a reject vote, with the abundance of possible cross-lingual processing tasks that leverage either the word embeddings, the sentence representations, or both. For one, there are recent datasets for cross-lingual inference, lexical entailment, lexicon extraction, let alone the low-level cross-lingual processing tasks such as sequence tagging or parsing. I strongly suggest that the experiments be expanded to further support the claims.\n\n2) The low-resource scenario is divorced from the low-resource reality. 100 k sentence pairs per language pairs is out of reach for a very large majority of the world's low-resource languages, as per e.g. Agic et al. (TACL 2016). For some, even the 1 M monolingual sentences might pose a challenge. The experiment in 6.2 thus does not faithfully represent  a low-resource scenario, even if it does tell a story that favors the proposed model. I suggest linking the revisions I propose in point 1 to an updated low-resource experiment that features actual low-resource languages.\n\nThe one minor flaw I note is that the contribution to cross-lingual word embeddings by Gouws et al. (2014) is absent from the otherwise detailed exposition of related work. It is unfortunate because their approach is precisely the one that combines monolingual and sentence-aligned parallel corpora to build bilingual embeddings, thus in many ways similar in spirit to the proposed contribution.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multitask learning of Multilingual Sentence Representations","abstract":"We present a novel multi-task training approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. We construct sentence embeddings by processing word embeddings with an LSTM and by taking an average of the outputs. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a low-resource scenario.","pdf":"/pdf/834bd34a17082ee5f5a3f69511cb3f68b7cd1e42.pdf","TL;DR":"We jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to learn high quality multilingual text embeddings that perform well in the low resource scenario.","paperhash":"anonymous|multitask_learning_of_multilingual_sentence_representations","_bibtex":"@article{\n  anonymous2018multitask,\n  title={Multitask learning of Multilingual Sentence Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJv8PEgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper239/Authors"],"keywords":["multilingual","embedding","representation learning","multi-task learning","low resource"]}},{"tddate":null,"ddate":null,"tmdate":1515642414821,"tcdate":1511812835424,"number":1,"cdate":1511812835424,"id":"Bysyyl5eM","invitation":"ICLR.cc/2018/Conference/-/Paper239/Official_Review","forum":"rJv8PEgAZ","replyto":"rJv8PEgAZ","signatures":["ICLR.cc/2018/Conference/Paper239/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple but interesting idea","rating":"5: Marginally below acceptance threshold","review":"This paper proposed to jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to construct sentence embeddings. They used cross-lingual classification tasks for evaluation. This idea is fairly simple but interesting. Their results on some language pairs showed that the joint training is effective (results on table 1 showed that sent-LSTM worked best with dim=128). The downside of this paper is that their results could not outperform state-of-the-art results. \n\nSome detailed comments:\n-\tThe authors should weaken some of the statements, e.g. ‘since our multilingual skip-gram and cross-lingual sentence similarity models are trained jointly, they can inform each other through the shared word embedding layer and promote the compositionality of learned word embeddings at training time’. Actually, there are no experimental results and evidences in this paper supporting this statement.\n-\tI don’t  see that ‘Amenable to Multi-task modeling’ is a contribution of this paper. The authors should report additional experimental results to prove this statement.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multitask learning of Multilingual Sentence Representations","abstract":"We present a novel multi-task training approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. We construct sentence embeddings by processing word embeddings with an LSTM and by taking an average of the outputs. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a low-resource scenario.","pdf":"/pdf/834bd34a17082ee5f5a3f69511cb3f68b7cd1e42.pdf","TL;DR":"We jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to learn high quality multilingual text embeddings that perform well in the low resource scenario.","paperhash":"anonymous|multitask_learning_of_multilingual_sentence_representations","_bibtex":"@article{\n  anonymous2018multitask,\n  title={Multitask learning of Multilingual Sentence Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJv8PEgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper239/Authors"],"keywords":["multilingual","embedding","representation learning","multi-task learning","low resource"]}},{"tddate":null,"ddate":null,"tmdate":1509739411762,"tcdate":1509078862926,"number":239,"cdate":1509739409114,"id":"rJv8PEgAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJv8PEgAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multitask learning of Multilingual Sentence Representations","abstract":"We present a novel multi-task training approach to learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model. We construct sentence embeddings by processing word embeddings with an LSTM and by taking an average of the outputs. Our architecture can transparently use both monolingual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone. Our model shows competitive performance in a standard cross-lingual document classification task. We also show the effectiveness of our method in a low-resource scenario.","pdf":"/pdf/834bd34a17082ee5f5a3f69511cb3f68b7cd1e42.pdf","TL;DR":"We jointly train a multilingual skip-gram model and a cross-lingual sentence similarity model to learn high quality multilingual text embeddings that perform well in the low resource scenario.","paperhash":"anonymous|multitask_learning_of_multilingual_sentence_representations","_bibtex":"@article{\n  anonymous2018multitask,\n  title={Multitask learning of Multilingual Sentence Representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJv8PEgAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper239/Authors"],"keywords":["multilingual","embedding","representation learning","multi-task learning","low resource"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}