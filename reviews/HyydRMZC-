{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222547689,"tcdate":1512121823614,"number":3,"cdate":1512121823614,"id":"S1uJLjCxz","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Official_Review","forum":"HyydRMZC-","replyto":"HyydRMZC-","signatures":["ICLR.cc/2018/Conference/Paper1060/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Moving the pixels to get adversarial attacks is also possible","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes a new way to create adversarial examples. Instead of changing pixel values they perform spatial transformations. \n\nThe authors obtain a flow field that is optimized to fool a target classifier. A regularization term controlled by a parameter tau is ensuring very small visual difference between the adversarial and the original image. \n\nThe used spatial transformations are differentiable with respect to the flow field (as was already known from previous work on spatial transformations) it is easy to perform gradient descent to optimize the flow that fools classifiers for targeted and untargeted attacks. \n\nThe obtained adversarial examples seem almost imperceivable (at least for ImageNet). \nThis is a new direction of attacks that opens a whole new dimension of things to consider. \n\nIt is hard to evaluate this paper since it opens a new direction but the authors do a good job using numerous datasets, CAM attention visualization and also additional materials with high-res attacks. \n\nThis is a very creative new and important idea in the space of adversarial attacks. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1512222547730,"tcdate":1511827440558,"number":2,"cdate":1511827440558,"id":"ry_xOQ5ef","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Official_Review","forum":"HyydRMZC-","replyto":"HyydRMZC-","signatures":["ICLR.cc/2018/Conference/Paper1060/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting solid paper, could use a few more experiments","rating":"6: Marginally above acceptance threshold","review":"This paper creates adversarial images by imposing a flow field on an image such that the new spatially transformed image fools the classifier. They minimize a total variation loss in addition to the adversarial loss to create perceptually plausible adversarial images, this is claimed to be better than the normal L2 loss functions.\n\nExperiments were done on MNIST, CIFAR-10, and ImageNet, which is very useful to see that the attack works with high dimensional images. However, some numbers on ImageNet would be helpful as the high resolution of it make it potentially different than the low-resolution MNIST and CIFAR.\n\nIt is a bit concerning to see some parts of Fig. 2. Some of Fig. 2 (especially (b)) became so dotted that it no longer seems an adversarial that a human eye cannot detect. And model B in the appendix looks pretty much like a normal model. It might needs some experiments, either human studies, or to test it against an adversarial detector, to ensure that the resulting adversarials are still indeed adversarials to the human eye. Another good thing to run would be to try the 3x3 average pooling restoration mechanism in the following paper:\n\nXin Li, Fuxin Li. Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics . ICCV 2017.\n\nto see whether this new type of adversarial example can still be restored by a 3x3 average pooling the image (I suspect that this is harder to restore by such a simple method than the previous FGSM or OPT-type, but we need some numbers).\n\nI also don't think FGSM and OPT are this bad in Fig. 4. Are the authors sure that if more regularization are used these 2 methods no longer fool the corresponding classifiers?\n\nI like the experiment showing the attention heat maps for different attacks. This experiment shows that the spatial transforming attack (stAdv) changes the attention of the classifier for each target class, and is robust to adversarially trained Inception v3 unlike other attacks like FGSM and CW. \n\nI would likely upgrade to a 7 if those concerns are addressed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1512222547775,"tcdate":1511689733975,"number":1,"cdate":1511689733975,"id":"SJCbAbugf","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Official_Review","forum":"HyydRMZC-","replyto":"HyydRMZC-","signatures":["ICLR.cc/2018/Conference/Paper1060/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An interesting way of creating better adversarial examples.","rating":"7: Good paper, accept","review":"This paper explores a new way of generating adversarial examples by slightly morphing the image to get misclassified by the model. Most other adversarial example generation methods tend to rely on generating high frequency noise patterns based by optimizing the perturbation on an individual pixel level. The new approach relies on gently changing the overall image by computing a flow an spatially transforming the image according to that flow. An important advantage of that approach is that the new attack is harder to protect against than to previous attacks according to the pixel based optimization methods.\n\nThe paper describes a novel model method that might become a new important line of attack. And the paper clearly demonstrates the advantages of this attack on three different data sets.\n\nA minor nitpick: the \"optimization based attack (Opt)\" was first employed in the original \"Intriguing Properties...\" 2013 paper using box-LBFGS as the method of choice predating FGSM.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1511407309261,"tcdate":1511407309261,"number":3,"cdate":1511407309261,"id":"H1S0R27lf","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Official_Comment","forum":"HyydRMZC-","replyto":"ryZVaVr1M","signatures":["ICLR.cc/2018/Conference/Paper1060/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1060/Authors"],"content":{"title":"Reply to \"state-of-the-art attacks and defenses are not compared\"","comment":"The main reason that stAdv is needed or our main goal of the paper is to provide a new way to think about adversarial examples: where an attacker can move pixels by some amount, instead of adding or subtracting pixel values. Investigating this unexplored side of adversarial examples is worthwhile, even if it would involve attacks that are strictly weaker, although that is not the case here. In this paper, we focused on showing the differences between spatially transformed adversarial examples and additive adversarial examples in terms of attack robustness analysis and attention visualization.\n\nHigh attack success rate and visually realistic adversarial examples are goals of lower priority.\n[1a] We thank the comment for their enthusiasm in this new attack and for bringing up the desire for additional success rate comparisons with additive adversarial examples and on more defenses. One result we didn’t include in the paper since it is not the main focus as we mention above is that we can also achieve 100% attack success rate on white-box attacks on adversarially trained models, like other optimization attacks. We can of course add these results in our updated version.\n\nThe Opt attack is C&W’s attack (last sentence in the first paragraph of Intro) and we will clarify this in our updated version. \n\n[1b]  We are definitely interested in further testing our attack method against other state-of-the-art defenses and thank you for the suggestions. One of the two papers mentioned is a concurrent ICLR 2018 submission. The other one is an arxiv paper published in mid-September. It is impossible to directly apply these defenses in our submission. Moreover,  the second reference has shown that Cao’s defense can already be attacked with ensemble optimization method. We are watching for better defense methods, but to the best of our knowledge currently, the most efficient methods are the adversarial training based methods we tested. \n\n[2] As the commenter mentions about the visual quality, yes, we did intend to show that the adversarial examples generated this way are more realistic. We will conduct a user experiment, which the commenter also suggested.  In addition, supporting the claim that the examples are realistic will create useful evidence that that the L_p norm is not a good measurement. The vision community has long since noticed this weakness of the L_p norm, but no better ones are provided. Here we actually show that with relatively high L_p norm, we can still generate perceptually realistic adversarial examples, which raises up an open research direction to propose better distance measurements between adversarial examples and normal instances.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1510514814260,"tcdate":1510514814260,"number":4,"cdate":1510514814260,"id":"HJUFg7L1f","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Public_Comment","forum":"HyydRMZC-","replyto":"ryZVaVr1M","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"region based method https://arxiv.org/abs/1709.05583","comment":"I cannot see that the paper: https://arxiv.org/abs/1709.05583 is a state-of-art defense (or better than adversarial training). Only CW attack is considered in that paper (e.g., no FGSM), experiments are not enough to support this argument.\n\nMeanwhile, this method is more like adding random noises around the each clean image to obtain multiple images for classification (ensemble classification). I cannot see this method can be robust to FGSM with a high value of sigma."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1510456722662,"tcdate":1510456617434,"number":3,"cdate":1510456617434,"id":"ryZVaVr1M","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Public_Comment","forum":"HyydRMZC-","replyto":"HyydRMZC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"state-of-the-art attacks and defenses are not compared.","comment":"I agree with the previous comments that this paper proposes a new method to generate adversarial examples. However, why this new method is needed is not well supported. \n\n1. Is your goal to have a higher attack success rate? \n    \n    1a. The evaluations did not really show the new method has higher attack success rates than the state-of-the-art attacks. In particular, what is exactly Opt attack? the attack proposed by Carlini and Wagner has very high attack success rate (close to 100%) for adverarially trained neural networks. You can refer to these two papers:\n\nhttps://openreview.net/pdf?id=BkpiPMbA-\nhttps://arxiv.org/abs/1709.05583\n\n    1b. The paper did not evaluate their attacks against state-of-the-art defense methods. Adversarial training is not state-of-the-art defense. State-of-the-art defense leverages neighborhood around an instance to predict its label, instead of the single instance alone. It would be interesting to show whether the new attacks are effective against state-of-the-art defense, e.g., https://arxiv.org/abs/1709.05583\n\n\n 2. is your goal to generate more visually realistic adversarial examples? If this is the goal of the paper, I think the authors should provide user studies to evaluate whether their adversarial examples are more visually realistic than existing adversarial examples. Showing several examples is not sufficient. \n"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1510277072737,"tcdate":1510277072737,"number":2,"cdate":1510277072737,"id":"SyFCytM1z","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Official_Comment","forum":"HyydRMZC-","replyto":"SyqTo1-JG","signatures":["ICLR.cc/2018/Conference/Paper1060/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1060/Authors"],"content":{"title":"Reply to \"Tradeoff by changing τ“ ","comment":"you are right, the \\tau here is used to control the tradeoff between visibility-fooling. \nWe apply grid search to tune \\tau, and the corresponding results are shown in the appendix((https://www.dropbox.com/sh/pl7sbecks6ja5g0/AACVdlRg96heBkICOWl1IQm4a?dl=0)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1510173634033,"tcdate":1510173634033,"number":1,"cdate":1510173634033,"id":"SyqTo1-JG","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Public_Comment","forum":"HyydRMZC-","replyto":"Sk3Q5d9AZ","signatures":["~Xun_Huang1"],"readers":["everyone"],"writers":["~Xun_Huang1"],"content":{"title":"Tradeoff by changing τ?","comment":"Is it possible to achieve visibility-fooling tradeoff by changing τ in equation (2)? The weight of  L_{flow} should control the visibility of the attack."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1510092427078,"tcdate":1509751332080,"number":1,"cdate":1509751332080,"id":"Sk3Q5d9AZ","invitation":"ICLR.cc/2018/Conference/-/Paper1060/Official_Comment","forum":"HyydRMZC-","replyto":"S1E7VLNCW","signatures":["ICLR.cc/2018/Conference/Paper1060/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1060/Authors"],"content":{"title":"Reply to \"Visibility-fooling effectiveness tradeoff not so clear\"","comment":"We thank the commenter for the suggestions. We are glad that the commenter finds the idea interesting and thinks it opens a potential direction. We agree that it is hard to evaluate the perceptual similarity of adversarial examples and that L_p may not be the best and is not suitable for our proposed method.\n\nWe emphasize that the perturbations are almost invisible for CIFAR and ImageNet datasets (we cannot tell the difference in the examples in our paper).We believe these results are more important than the results on MNIST, where the differences are visible because these wiggly/sketchy distortions occur more naturally in handwritten digits.\n\nBased on the suggestion, we have gotten the permission from the Chair to add an anonymized link (https://www.dropbox.com/sh/pl7sbecks6ja5g0/AACVdlRg96heBkICOWl1IQm4a?dl=0) here for ten more image examples on MNIST, CIFAR, and ImageNet in high-resolution figures.\nWe will also add more examples in the appendix of our updated version."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1509348379887,"tcdate":1509348379887,"number":1,"cdate":1509348379887,"id":"S1E7VLNCW","invitation":"ICLR.cc/2018/Conference/-/Paper1048/Public_Comment","forum":"HyydRMZC-","replyto":"HyydRMZC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Visibility-fooling effectiveness tradeoff not so clear","comment":"Thanks for the interesting paper -- it opens a new axis for adversarial attacks.\nWhile the idea is interesting, the tradeoff between visibility and fooling effectiveness is not entirely clear.\nThe potential greatest concern that I have about this paper is that the perturbations are way too visible.\n\nFor claiming an adversarial perturbation to be superior to the others, not only fooling rates (or ``attack success rates'') but also perceptual similarity to the original image should be compared. Since there is no standard measure for the latter, people have resorted to Lp distances for additive perturbations. One cannot do the same in this paper because the perturbations are not additive, as the authors have described: \"for stAdv, we cannot use Lp norm to bound the distance as translating a image by one pixel may introduce large Lp penalty.\"\n\nThe paper still compares the attack success rate comparisons against previous attacks in table 2. The authors have used bare-eye observation to set the operating point: \"We instead constrain the spatial transformation flow and show that our adversarial examples have high perceptual quality in Figures 2, 3, and 4.\" But this is hardly convincing, since the perturbations seem to be _always visible_ if one looks closely (fractal patterns along edges), often more visible than previous additive attacks. The claim \"geometric changes are small and locally smooth\" is hard to buy.\n\nAlso the number of visual examples is too stingy for ImageNet (only 2). Given their importance, there should be at least 10 examples per dataset. Also, visualisations in this paper are too low resolution in general and so many image artifacts may be overlooked. \n\nI would suggest the following for making a more convincing case for the paper: \nVisually compare the _minimal_ perturbation needed for a successful attack, on MNIST, CIFAR, and ImageNet, with at least 10 examples in _high resolution_ figures. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]}},{"tddate":null,"ddate":null,"tmdate":1510092381648,"tcdate":1509138027310,"number":1060,"cdate":1510092360353,"id":"HyydRMZC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyydRMZC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Spatially Transformed Adversarial Examples","abstract":"Recent studies show that widely used Deep neural networks (DNNs) are vulnerable to the carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\\mathcal{L}_p$ distance for penalizing perturbations. Different defense methods have also been explored to defend against such adversarial attacks. While the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are more perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted. ","pdf":"/pdf/be9e53c0a815f5a53679d886757da35a237d8e43.pdf","TL;DR":"We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks. ","paperhash":"anonymous|spatially_transformed_adversarial_examples","_bibtex":"@article{\n  anonymous2018spatially,\n  title={Spatially Transformed Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyydRMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1060/Authors"],"keywords":["adversarial examples","spatial transformation"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}