{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222663558,"tcdate":1511816667989,"number":3,"cdate":1511816667989,"id":"H1E1RgqxM","invitation":"ICLR.cc/2018/Conference/-/Paper471/Official_Review","forum":"Bk9zbyZCZ","replyto":"Bk9zbyZCZ","signatures":["ICLR.cc/2018/Conference/Paper471/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"# Summary\nThis paper presents a new external-memory-based neural network (Neural Map) for handling partial observability in reinforcement learning. The proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memory. The results on several memory-related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN/FRMQN. \n\n[Pros]\n- The overall direction toward more flexible/scalable memory is an important research direction in RL.\n- The proposed memory architecture is new. \n- The paper is well-written.\n\n[Cons]\n- The proposed memory architecture is new but a bit limited to 2D/3D navigation tasks.\n- Lack of analysis of the learned memory behavior.\n\n# Novelty and Significance\nThe proposed idea is novel in general. Though [Gupta et al.] proposed an ego-centric neural memory in the RL context, the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory, whereas [Gupta et al.] designed the memory specifically for predicting free space. On the other hand, the proposed method is also specific to navigation tasks in 2D or 3D environment, which is hard to apply to more general memory-related tasks in non-spatial environments. But, it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment.\n\n# Quality\nThe experiment does not show any analysis of the learned memory read/write behavior especially for ego-centric neural map and the 3D environment. It is hard to understand how the agent utilizes the external memory without such an analysis. \n\n# Clarity\nThe paper is overall clear and easy-to-follow except for the following. In the introduction section, the paper claims that \"the expert must set M to a value that is larger than the time horizon of the currently considered task\" when mentioning the limitation of the previous work. In some sense, however, Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":" Neural Map: Structured Memory for Deep Reinforcement Learning","abstract":"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training. ","pdf":"/pdf/b5ff0760825ab5eb47ed177d58713590ace26686.pdf","paperhash":"anonymous|neural_map_structured_memory_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018,\n  title={ Neural Map: Structured Memory for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk9zbyZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper471/Authors"],"keywords":["deep reinforcement learning","deep learning","memory"]}},{"tddate":null,"ddate":null,"tmdate":1512222663601,"tcdate":1511814045847,"number":2,"cdate":1511814045847,"id":"S1Ii7lcxz","invitation":"ICLR.cc/2018/Conference/-/Paper471/Official_Review","forum":"Bk9zbyZCZ","replyto":"Bk9zbyZCZ","signatures":["ICLR.cc/2018/Conference/Paper471/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Important paper about structured memory for navigation","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity. The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory. Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector. The position {x,y} is a binned representation of discrete or continuous coordinates. The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves.\n\nThe experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment. The most important aspect is the capability to build a feature map of previously unseen environments.\n\nThis paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited. It has been improved since, through different sets of experiments and apparently a clearer presentation, but the ideas are the same. I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there). What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research, or are they ignorant, and is the peer review system broken? I quite like the formulation of the NIPS ratings: \"if this paper does not get accepted, I am considering boycotting the conference\".\n\n* The noise model experiment in Appendix D is commendable, but the noise model is somewhat unrealistic (very small variance, zero mean Gaussian) and assumes only drift in x and y, not along the orientation. While this makes sense in grid world environments or rectilinear mazes, it does not correspond to realistic robotic navigation scenarios with wheel skid, missing measurements, etc... Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":" Neural Map: Structured Memory for Deep Reinforcement Learning","abstract":"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training. ","pdf":"/pdf/b5ff0760825ab5eb47ed177d58713590ace26686.pdf","paperhash":"anonymous|neural_map_structured_memory_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018,\n  title={ Neural Map: Structured Memory for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk9zbyZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper471/Authors"],"keywords":["deep reinforcement learning","deep learning","memory"]}},{"tddate":null,"ddate":null,"tmdate":1512222663643,"tcdate":1511751159538,"number":1,"cdate":1511751159538,"id":"ByJWAeFxz","invitation":"ICLR.cc/2018/Conference/-/Paper471/Official_Review","forum":"Bk9zbyZCZ","replyto":"Bk9zbyZCZ","signatures":["ICLR.cc/2018/Conference/Paper471/AnonReviewer1"],"readers":["everyone"],"content":{"title":"OK paper--needs further ablation experiments ","rating":"6: Marginally above acceptance threshold","review":"The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments. The memory consists of a 2D array and includes trainable read/write mechanisms. The RL agent's policy is a function of the context read, read, and next step write vectors (which are functions of the observation). The effectiveness of the proposed architecture is evaluated via reinforcement learning (% of mazes solved). The evaluation included 1000 test mazes--which sets a good precedent for evaluation in this subfield. \n\nMy main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architecture. The downsampling experiment in Section 5.1 seems to indicate the contrary: downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own location. I'm concerned whether the proposed agent is actually employing a navigation strategy, as seems to be suggested, or is simply a good agent architecture for this task (e.g. for optimization reasons). The short experiment in Appendix E seems to try and answer this question, but it's results are anecdotal at best. \n\nIf good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation (world centric) into memory and use something like a value iteration network or shortest path planning to plan routes. My point is that there are classical algorithms to solve navigation even in partially observable 2D grid worlds, why bother with deep RL here? ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":" Neural Map: Structured Memory for Deep Reinforcement Learning","abstract":"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training. ","pdf":"/pdf/b5ff0760825ab5eb47ed177d58713590ace26686.pdf","paperhash":"anonymous|neural_map_structured_memory_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018,\n  title={ Neural Map: Structured Memory for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk9zbyZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper471/Authors"],"keywords":["deep reinforcement learning","deep learning","memory"]}},{"tddate":null,"ddate":null,"tmdate":1509739283938,"tcdate":1509122322523,"number":471,"cdate":1509739281233,"id":"Bk9zbyZCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bk9zbyZCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":" Neural Map: Structured Memory for Deep Reinforcement Learning","abstract":"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training. ","pdf":"/pdf/b5ff0760825ab5eb47ed177d58713590ace26686.pdf","paperhash":"anonymous|neural_map_structured_memory_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018,\n  title={ Neural Map: Structured Memory for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk9zbyZCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper471/Authors"],"keywords":["deep reinforcement learning","deep learning","memory"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}