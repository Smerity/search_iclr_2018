{"notes":[{"tddate":null,"ddate":null,"tmdate":1515022235180,"tcdate":1515021672106,"number":2,"cdate":1515021672106,"id":"SkeOr1imz","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Comment","forum":"B1jscMbAW","replyto":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference/Paper907/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper907/Authors"],"content":{"title":"Rebuttal","comment":"First of all, we thank the three reviewers for their insightful comments on our work.\n\nWe have updated the paper. The main changes are:\n- Changed the abbreviation from DCN to DiCoNet to avoid conflicts.\n- Changed k-means split block from set2set to GNN.\n- Compared k-means to Lloyd's.\n- Added non-synthetic dataset for k-means: Patches of CIFAR-10 images.\n- Added KnapSack problem.\n- Moved TSP to appendix.\n\nAnonReviewer4:\n\nComment1: There should also be a comparison to a k-means solver in the k-means section as an additional baseline.\n\nAns1: We agree with this comment on the k-means experimental section. We have updated the k-means\nsection in the following way:\n      1 - We have changed the split block into a GNN to gain in expressivity (both for the DiCoNet and\n           Baseline). As explained in the text, the graph is created using a Gaussian kernel.\n      2 - We compare its performance with Lloyd's algorithm and Recursive Lloyd's (i.e, solving\n      binary clustering recursively with Lloyd's algorithm). The performance results are shown as a ratio\n      between the model costs after convergence and the algorithms output cost.\n      3 - We have used a non-synthetic dataset. We have taken 3x3x3 patches of images of the CIFAR-10 dataset and applied \n      the clustering models/algorithms for a pre-specified dyadic number of intervals.\n\nComment2: I'm also not sure TSP is an appropriate problem to demonstrate the method's effectiveness. Perhaps another problem that has an explicit divide and conquer strategy could be used instead.\n\nAns2: We have moved the TSP problem to the appendix and introduced the Knapsack problem, which was also\ntackled in (Irwan Bello, Hieu Pham et al. '17). This problem has a clear recursive \nstructure, and we reaffirm this with the DiCoNet performance in the experiments.\n\nComment3: It would also be nice to observe failure cases of the model.\n\nAns3: Actually, DiCoNet performance on TSP is not that good compared to other problems due to the low\nlevel of scale invariance compared to them.\n\nComment4: What is \\rho on page 4? I assume it is some nonlinearity, but this was not specified.\n\nAns4: You are right. \\rho is a pointwise non-lineariy. In particular, \\rho is a sigmoid for the set2set model (split block of the convex hull), and a ReLu for the GNN.\n\nComment 5: On page 5, it says the merge block takes as input two sequences. I thought the merge block was defined on sets? \n\nAns5: The goal of the split block is to find a partition over sets (or structured sets as graphs).\nThe merge block takes into account the order of the previously solved instances. For instance, \nin mergesort (when it merges two already ordered sequences), or the convex hull (where the previously solved instances are sequences of points ordered clockwise or counter-clockwise).\n\nComment6: Author's names should be enclosed in parentheses unless part of the sentence.\n\nAns6: Solved.\n\nComment7: I believe \"then\" should be removed in the sentence \"...scale invariance, then exploiting...\" on page 2.\n\nAns7: You are right, solved.\n\nAnonReviewer2:\n\nComment8: K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. \n\nAns8: Same as Ans1.\n\nComment9: TSP experiments show that \"in distribution\" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). \n\nAns9: We have moved the TSP section to the appendix.\n\nComment10: in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments.\n\nAns10: We have introduced the KnapSack problem to the set of tasks and introduced an extra experiment on k-means with a non-synthetic dataset.\n\nComment11: DCN is already quite commonly used abbreviation for \"Deep Classifier Network\" as well as \"Dynamic Capacity Network\", thus might be a good idea to find different name.\n\nAns11: We have changed the abbreviation to DiCoNet.\n\nComment12: please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example:\nGraph Neural Network Nowak et al. (2017)  should be Graph Neural Network (Nowak et al. (2017))\n\nAns12: Solved."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on several combinatorial and geometric tasks: convex hull,\nclustering, knapsack and euclidean TSP. Thanks to the dynamic programming\nnature of our model, we show significant improvements in terms of generalization\nerror and computational complexity.","pdf":"/pdf/3d31a46271abcb50a42beb845babd9e3bb713d8c.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1515642529683,"tcdate":1512606298563,"number":3,"cdate":1512606298563,"id":"B1Qwc-LWf","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Review","forum":"B1jscMbAW","replyto":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference/Paper907/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Well written, timely idea","rating":"7: Good paper, accept","review":"Summary of paper:\n\nThe paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks.\n\nReview:\n\nThe paper is clearly written. It is sometimes difficult to communicate ideas in this area, so I appreciate the author's effort in choosing good notation. Using an architecture to learn how to split the input, find solutions, then merge these is novel. Previous work in using recursion to solve problems (Cai 2017) used explicit supervision to learn how to split and recurse. The ideas and formalism of the merge and partition operations are valuable contributions. \n\nThe experimental side of the paper is less strong. There are good results on the convex hull problem, which is promising. There should also be a comparison to a k-means solver in the k-means section as an additional baseline. I'm also not sure TSP is an appropriate problem to demonstrate the method's effectiveness. Perhaps another problem that has an explicit divide and conquer strategy could be used instead. It would also be nice to observe failure cases of the model. This could be done by visually showing the partition constructed or seeing how the model learned to merge solutions.\n\nThis is a relatively new area to tackle, so while the experiments section could be strengthened, I think the ideas present in the paper are important and worth publishing.\n\nQuestions:\n\n1. What is \\rho on page 4? I assume it is some nonlinearity, but this was not specified.\n2. On page 5, it says the merge block takes as input two sequences. I thought the merge block was defined on sets? \n\nTypos:\n1. Author's names should be enclosed in parentheses unless part of the sentence.\n2. I believe \"then\" should be removed in the sentence \"...scale invariance, then exploiting...\" on page 2.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on several combinatorial and geometric tasks: convex hull,\nclustering, knapsack and euclidean TSP. Thanks to the dynamic programming\nnature of our model, we show significant improvements in terms of generalization\nerror and computational complexity.","pdf":"/pdf/3d31a46271abcb50a42beb845babd9e3bb713d8c.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1512578073965,"tcdate":1512578073965,"number":1,"cdate":1512578073965,"id":"rkGXn5S-z","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Comment","forum":"B1jscMbAW","replyto":"H1wZwQwef","signatures":["ICLR.cc/2018/Conference/Paper907/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper907/Authors"],"content":{"title":"Answer to reviewer 2","comment":"First of all, we thank the reviewer for the comments.\n\nIndeed, we agree with reviewer 2 that k-means experiments should include real datasets and comparisons with Lloyd.\nWe are currently working on updating the results for the k-means in order to illustrate its real performance compared to Lloyd's algorithm in a benchmarking dataset.\nWe are also planning to do a small update on the TSP to multiple scales.\n\n> Side notes:\nWe will consider updating the name of the paper in order to avoid conflicts with existing architectures.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on several combinatorial and geometric tasks: convex hull,\nclustering, knapsack and euclidean TSP. Thanks to the dynamic programming\nnature of our model, we show significant improvements in terms of generalization\nerror and computational complexity.","pdf":"/pdf/3d31a46271abcb50a42beb845babd9e3bb713d8c.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1515642529720,"tcdate":1511822968850,"number":2,"cdate":1511822968850,"id":"ByZKLz5gz","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Review","forum":"B1jscMbAW","replyto":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference/Paper907/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper","rating":"7: Good paper, accept","review":"This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems. The network has two components: one component learns to split the problem and the other learns to combine solutions to sub-problems. Using this setup, the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach. In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem. In all three cases, the proposed solution outperforms the baselines on larger problem instances. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on several combinatorial and geometric tasks: convex hull,\nclustering, knapsack and euclidean TSP. Thanks to the dynamic programming\nnature of our model, we show significant improvements in terms of generalization\nerror and computational complexity.","pdf":"/pdf/3d31a46271abcb50a42beb845babd9e3bb713d8c.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1515685573864,"tcdate":1511630590577,"number":1,"cdate":1511630590577,"id":"H1wZwQwef","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Review","forum":"B1jscMbAW","replyto":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference/Paper907/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Neural networks enriched with divide and conquer strategy","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics. Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients. The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent). Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP.  However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story.\n\nI will be happy to revisit the rating if the experimental section is enriched.\n\nPros:\n- very easy to follow idea and model\n- simple merge or RL and SL in an end-to-end trainable model\n- improvements over previous solutions\n\nCons:\n- K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. \n- TSP experiments show that \"in distribution\" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). \n- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments\n\nSide notes:\n- DCN is already quite commonly used abbreviation for \"Deep Classifier Network\" as well as \"Dynamic Capacity Network\", thus might be a good idea to find different name.\n- please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example:\nGraph Neural Network Nowak et al. (2017) \nshould be\nGraph Neural Network (Nowak et al. (2017))\n\n# After the update\n\nEvaluation section has been updated threefold:\n- TSP experiments are now in the appendix rather than main part of the paper\n- k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering\n- Knapsack problem has been added\n\nPaper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method. I can see the benefit of trainable approach here, the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that.\n\nI increased rating for the paper, however in order to put the \"clear accept\" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on several combinatorial and geometric tasks: convex hull,\nclustering, knapsack and euclidean TSP. Thanks to the dynamic programming\nnature of our model, we show significant improvements in terms of generalization\nerror and computational complexity.","pdf":"/pdf/3d31a46271abcb50a42beb845babd9e3bb713d8c.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1515056965175,"tcdate":1509137061432,"number":907,"cdate":1510092362555,"id":"B1jscMbAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on several combinatorial and geometric tasks: convex hull,\nclustering, knapsack and euclidean TSP. Thanks to the dynamic programming\nnature of our model, we show significant improvements in terms of generalization\nerror and computational complexity.","pdf":"/pdf/3d31a46271abcb50a42beb845babd9e3bb713d8c.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}