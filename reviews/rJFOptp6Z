{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642510015,"tcdate":1512093189727,"number":3,"cdate":1512093189727,"id":"rJR-8EAgG","invitation":"ICLR.cc/2018/Conference/-/Paper78/Official_Review","forum":"rJFOptp6Z","replyto":"rJFOptp6Z","signatures":["ICLR.cc/2018/Conference/Paper78/AnonReviewer2"],"readers":["everyone"],"content":{"title":"manuscript needs significant revisions","rating":"3: Clear rejection","review":"Summary:\nThe manuscript presents experiments on distilling knowledge from a face classification model to student models for face alignment and verification. By selecting a good initialization strategy and guidelines for selecting appropriate targets for non-classification tasks, the authors achieve improved performance, compared to networks trained from scratch or with different initialization strategies.\n\nReview:\nThe paper seems to be written in a rush. \nI am not sure about the degree of novelty, as pretraining with domain-related data instead of general-purpose ImageNet data has been done before, Liu et al. (2014), for example pretrain a CNN on face classification to be used for emotion recognition. Admitted, knowledge transfer from classification to regression and retrieval tasks is not very common yet, except via pretraining on ImageNet, followed by fine-tuning on the target task.\nMy main concern is with the presentation of the paper. It is very hard to follow! Two reasons are that it has too many grammatical mistakes and that very often a “simple trick” or a “common trick” is mentioned instead of using a descriptive name for the method used.\n\nHere are a few points that might help improving the work:\n1) Many kind of empty phrases are repeated all over the paper, e.g. the reader is teased with mention of a “simple trick” or a “common trick”. I don’t think the phrase “breaking point”, that is repeated a couple of times, is correctly used (see https://www.merriam-webster.com/dictionary/breaking%20point for a defininition).\n2) Section 4.1 does not explain the initialization but just describes motivation and notation.\n3) Clarity of the approach: Using the case of alignment as an example, do you first pretrain both the teacher and student on classification, then finetune the teacher on alignment before the distillation step? \n4) Table 1 mentions Fitnets, but cites Ba & Caruana (2014) instead of Romero et al. (2015)\n5) The “experimental trick” you mention for setting alpha and beta, seems to be just validation, comparing different settings and picking the one yielding the highest improvements. On what partition of the data are you doing this hyperparameter selection?\n6) The details of the architectures are missing, e.g. exactly what changes do you make to the architecture, when you change the task from classification to alignment or verification? What exactly is the “hidden layer” in that architecture?\n7) Minor: Usually there is a space before parentheses (many citations don’t have one)\n\nIn its current form, I cannot recommend the manuscript for acceptance. I get the impression that the experimental work might be of decent quality, but the manuscript fails to convey important details of the method, of the experimental setup and in the interpretation of the results. The overall quality of the write-up has to be significantly improved.\n\nReferences:\nLiu, Mengyi, Ruiping Wang, Shaoxin Li, Shiguang Shan, Zhiwu Huang, and Xilin Chen. \"Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild.\" In Proceedings of the 16th International Conference on Multimodal Interaction, pp. 494-501. ACM, 2014.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification","abstract":"Knowledge distillation is a potential solution for model compression. The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one. Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network. However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered. To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates. In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","pdf":"/pdf/ba8e0b21d1608274c246e61bac781f2c836e9f2a.pdf","TL;DR":"We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification","paperhash":"anonymous|model_distillation_with_knowledge_transfer_from_face_classification_to_alignment_and_verification","_bibtex":"@article{\n  anonymous2018model,\n  title={Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJFOptp6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper78/Authors"],"keywords":["distill","transfer","classification","alignment","verification"]}},{"tddate":null,"ddate":null,"tmdate":1515642510052,"tcdate":1511926266915,"number":2,"cdate":1511926266915,"id":"SkX-5ijlG","invitation":"ICLR.cc/2018/Conference/-/Paper78/Official_Review","forum":"rJFOptp6Z","replyto":"rJFOptp6Z","signatures":["ICLR.cc/2018/Conference/Paper78/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper proposed to transfer the classifier from the model for face classification to the task of alignment and verification. The problem setting is interesting and valuable, however, the contribution is not clearly demonstrated. ","rating":"5: Marginally below acceptance threshold","review":"This paper proposed to transfer the classifier from the model for face classification to the task of alignment and verification. The problem setting is interesting and valuable, however, the contribution is not clearly demonstrated. \n\nSpecifically, it proposed to utilize the teacher model from classification to other tasks, and proposed a unified objective function to model the transferability as shown in Equation (5). The two terms in (5), (7) and (9) are used to transfer the knowledge from the teacher model. It maybe possible to claim that the different terms may play different roles for different tasks. However, there should be some general guidelines for choosing these different terms for regularization, rather than just make the claim purely based on the final results. In table 4 and table 5, the results seem to be not so consistent for using the distillation loss. The author mentioned that it is due to the weak teacher model. However, the teacher model just differs in performance with around 3% in accuracy. How could we define the “good” or “bad” of a teacher model for model distillation/transfer?\n\nBesides, it seems that the improvement comes largely from the trick of initialization as mentioned in Section 3.2. Hence, it is still not clear which parts contribute to the final performance improvements. It could be better if the authors can report the results from each of the components together. \n\n The authors just try the parameter (\\alpha, \\beta) to be (0,0), (1,0), (0,1) and (1,1). I think the range for both values could be any positive real value, and how about the performance for other sets of combinations, like (0.5, 0.5)?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification","abstract":"Knowledge distillation is a potential solution for model compression. The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one. Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network. However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered. To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates. In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","pdf":"/pdf/ba8e0b21d1608274c246e61bac781f2c836e9f2a.pdf","TL;DR":"We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification","paperhash":"anonymous|model_distillation_with_knowledge_transfer_from_face_classification_to_alignment_and_verification","_bibtex":"@article{\n  anonymous2018model,\n  title={Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJFOptp6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper78/Authors"],"keywords":["distill","transfer","classification","alignment","verification"]}},{"tddate":null,"ddate":null,"tmdate":1515642510090,"tcdate":1511730603034,"number":1,"cdate":1511730603034,"id":"B1736j_gz","invitation":"ICLR.cc/2018/Conference/-/Paper78/Official_Review","forum":"rJFOptp6Z","replyto":"rJFOptp6Z","signatures":["ICLR.cc/2018/Conference/Paper78/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Limited scope and contribution","rating":"3: Clear rejection","review":"The paper proposes knowledge distillation on two very specific non-classification tasks. I find the scope of the paper is quite limited and the approach seems hard to generalize to other tasks. There is also very limited technical contribution. I think the paper might be a better fit in conferences on faces such as FG.\n\nPros:\n1. The application of knowledge distillation in face alignment is interesting. \n\nCons:\n1. The writing of the paper can be significantly improved. The technical description is unclear.\n2. The method has two parameters \\alpha and \\beta, and Section 4.2.3. mentions the key is to measure the relevance of tasks. It seems to me defining the relevance between tasks is quite empirical and often confusing. How are they actually selected in the experiments? Sometimes alpha=0, beta=0 works the best which means the added terms are useless?\n3. The paper works on a very limited scope of face alignment. How does the proposed method generalize to other tasks?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification","abstract":"Knowledge distillation is a potential solution for model compression. The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one. Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network. However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered. To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates. In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","pdf":"/pdf/ba8e0b21d1608274c246e61bac781f2c836e9f2a.pdf","TL;DR":"We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification","paperhash":"anonymous|model_distillation_with_knowledge_transfer_from_face_classification_to_alignment_and_verification","_bibtex":"@article{\n  anonymous2018model,\n  title={Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJFOptp6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper78/Authors"],"keywords":["distill","transfer","classification","alignment","verification"]}},{"tddate":null,"ddate":null,"tmdate":1509739498905,"tcdate":1508904305521,"number":78,"cdate":1509739496248,"id":"rJFOptp6Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJFOptp6Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification","abstract":"Knowledge distillation is a potential solution for model compression. The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one. Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network. However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered. To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates. In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.","pdf":"/pdf/ba8e0b21d1608274c246e61bac781f2c836e9f2a.pdf","TL;DR":"We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification","paperhash":"anonymous|model_distillation_with_knowledge_transfer_from_face_classification_to_alignment_and_verification","_bibtex":"@article{\n  anonymous2018model,\n  title={Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJFOptp6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper78/Authors"],"keywords":["distill","transfer","classification","alignment","verification"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}