{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222597622,"tcdate":1511897022297,"number":2,"cdate":1511897022297,"id":"rkIpwEslz","invitation":"ICLR.cc/2018/Conference/-/Paper246/Official_Review","forum":"SyOK1Sg0W","replyto":"SyOK1Sg0W","signatures":["ICLR.cc/2018/Conference/Paper246/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes a method for quantizing neural networks that allows weights to be quantized with different precision depending on their importance","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a method for quantizing neural networks that allows weights to be quantized with different precision depending on their importance, taking into account the loss. If the weights are very relevant, it assigns more bits to them, and in the other extreme it does pruning of the weights.\n\nThis paper addresses a very relevant topic, because in limited resources there is a constrain in memory and computational power, which can be tackled by quantizing the weights of the network. The idea presented is an interesting extension to weight pruning with a close form approximate solution for computing the adaptive quantization of the weights.\n\nThe results presented in the experimental section are promising. The quantization is quite cheap to compute and the results are similar to other state-of-the-art quantization methods. \nFrom the tables and figures, it is difficult to grasp the decrease in accuracy when using the quantized model, compared to the full precision model, and also the relative memory compression. It would be nice to have this reference in the plots of figure 3.  Also, it is difficult to see the benefits in terms of memory/accuracy compromise since not all competing quantization techniques are compared for all the datasets.\nAnother observation is that it seems from figure 2 that a lot of the weights are quantized with around 10 bits, and it is not clear how the compromise accuracy/memory can be turned to less memory, if possible. It would be interesting to know an analogy, for instance, saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits.\n\nOTHER COMMENTS:\n\n-missing references in several points of the paper. For instance, in the second paragraph of the introduction, 1st paragraph of section 2.\n\n- few typos:\n*psi -> \\psi in section 2.3\n*simply -> simplify in proof of lemma 2.2\n*Delta -> \\Delta in last paragraph of section 2.2\n*l2 -> L_2 or l_2 in section 3.1 last paragraph.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Quanization of Neural Networks","abstract":"Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging, due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the change in loss when performing quantization, nor do they take the disparate importance of DNN connections to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each connection weight such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each weight and assign it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rate. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy.\n","pdf":"/pdf/6d77e0e506cba9a2f896f3e54f0dcc253bb864c9.pdf","TL;DR":"An adaptive method for fixed-point quantization of neural networks based on theoretical analysis rather than heuristics. ","paperhash":"anonymous|adaptive_quanization_of_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Quanization of Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyOK1Sg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper246/Authors"],"keywords":["Deep Neural Networks","Model Quantization","Model Compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222597709,"tcdate":1511819596780,"number":1,"cdate":1511819596780,"id":"HkrUKW5eM","invitation":"ICLR.cc/2018/Conference/-/Paper246/Official_Review","forum":"SyOK1Sg0W","replyto":"SyOK1Sg0W","signatures":["ICLR.cc/2018/Conference/Paper246/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting intuitive idea; evaluation needs more clarification","rating":"6: Marginally above acceptance threshold","review":"The authors present an interesting idea to reduce the size of neural networks via adaptive compression, allowing the network to use high precision where it is crucial and low precision in other parts. The problem and the proposed solution is well motivated. However, there are some elements of the manuscript that are hard to follow and need further clarification/information. These need to definitely be addressed before this paper can be accepted.\n\nSpecific comments/questions:\n- Page 1: Towards the bottom, in the 3rd to last line, reference is missing.\n- Page 1: It is a little hard to follow the motivation against existing methods.\n- Page 2: DenseNets and DeepCompression need citations\n- Lemma 2.1 seems interesting - is this original work? This needs to be clarified.\n- Lemma 2.2: Reference to Equation 17 (which has not been presented in the manuscript at this point) seems a little confusing and I am unable to following the reasoning and the subsequent proof which again refers to Equation 17.\n- Alg 2: Should it be $\\Delta$ or $\\Delta_{k+1}$? Because in one if branch, we use $\\Delta$, in the other, we use the subscripted one.\n- Derivation in section 2.3 has some typographical errors.\n- What is $d$ in Equation 20 (with cases)? Without this information, it is unclear how the singular points are handled.\n- Page 6, first paragraph of Section 3: The evaluation is a little confusing - when is the compression being applied during the training process, and how is the training continued post-compression? What does each compression 'pass' constitute of?\n- Figure 1b: what is the 'iteration' on the horizontal axis, is it the number of iterations of Alg3 or Alg2? Hoping it is Alg3 but needs to be clarified in the text.\n- Section 3: What about compression results for CIFAR and SVNH? ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Quanization of Neural Networks","abstract":"Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging, due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the change in loss when performing quantization, nor do they take the disparate importance of DNN connections to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each connection weight such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each weight and assign it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rate. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy.\n","pdf":"/pdf/6d77e0e506cba9a2f896f3e54f0dcc253bb864c9.pdf","TL;DR":"An adaptive method for fixed-point quantization of neural networks based on theoretical analysis rather than heuristics. ","paperhash":"anonymous|adaptive_quanization_of_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Quanization of Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyOK1Sg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper246/Authors"],"keywords":["Deep Neural Networks","Model Quantization","Model Compression"]}},{"tddate":null,"ddate":null,"tmdate":1509739406372,"tcdate":1509080959551,"number":246,"cdate":1509739403710,"id":"SyOK1Sg0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyOK1Sg0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adaptive Quanization of Neural Networks","abstract":"Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging, due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the change in loss when performing quantization, nor do they take the disparate importance of DNN connections to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each connection weight such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each weight and assign it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rate. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy.\n","pdf":"/pdf/6d77e0e506cba9a2f896f3e54f0dcc253bb864c9.pdf","TL;DR":"An adaptive method for fixed-point quantization of neural networks based on theoretical analysis rather than heuristics. ","paperhash":"anonymous|adaptive_quanization_of_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Quanization of Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyOK1Sg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper246/Authors"],"keywords":["Deep Neural Networks","Model Quantization","Model Compression"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}