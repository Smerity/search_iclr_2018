{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222579585,"tcdate":1512044691320,"number":3,"cdate":1512044691320,"id":"Skoq_d6gz","invitation":"ICLR.cc/2018/Conference/-/Paper160/Official_Review","forum":"SJ2ZjsyC-","replyto":"SJ2ZjsyC-","signatures":["ICLR.cc/2018/Conference/Paper160/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice idea but needs stronger evaluation","rating":"4: Ok but not good enough - rejection","review":"This paper presents a semi-supervised extension for applying GANs to regression tasks. The authors propose two architectures: one adds a supervised regression loss to the standard unsupervised GAN discriminator loss. The other replaces the real/fake output of the discriminator with only a real-valued output and then applies a kernel on top of this output to predict if samples are real or fake. The methods are evaluated on a public driving dataset, and are shown to outperform an Improved-GAN which predicts the real-valued labels discretized into 10 classes.\n\nThis is a nice idea, but I am not completely convinced by the experimental results. The proposed method is compared to Improved-GAN where the real-valued labels are discretized into 10 classes. Why 10? How was this chosen? The authors rightfully state that \"[...] this discretization will add some unavoidable quantization error to our training\" (Sec 5.2) and then again in the conclusion \"determining the number of [discretization] classes for each application is non-trivial\", yet nowhere do they explore the effects of this. Surely, this is a very important part of the evaluation? And surely as we improve the discretization-resolution the gap between the two will close? This needs to be evaluated.\n\nAlso, the main motivation for a GAN-based regression model is based on the paucity of labeled training data. However, this is another place where the argument would greatly benefit from some empirical backing. I.e., I would really at least like to see how a discriminative regression model (e.g. a pretrained convnet fine-tuned for regression) compares to the proposed technique when trained (fine-tuned) only on the (smaller) labeled data set, perhaps augmented with standard image augmentation techniques to increase the size.\n\nOverall, I found the paper a little hard to read (especially understanding how Architecture 2 works and moreover what its motivation is) and empirical evaluation a bit lacking. I also found the claims of \"solving\" the regression task using GANs unfounded based on the experimental results presented. \n\nIn conclusion, while the technique looks promising, the novelty seems fairly low and the evaluation can benefit from one or more additional baselines (at the very least showing how varying the discretization resolution of the Improved-GAN affects the results, but preferably one or more discriminative baselines), and also perhaps on one or more additional data sets to showcase the technique's generality.\n\nNits:\n\nSeveral part are quite repetitive and can benefit from a rewrite. Particularly the last paragraphs in the Introduction.\nSection 3:  notation seems inconsistent (p_z(z) vs P_z(z) directly below in Eqn 1)\nThe second architecture needs to be explained a little better, and motivated a little better.\nEqn 5: I think it should be 0 \\geq \\hat{y}, and not 0 \\leq \\hat{y}","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving","abstract":"This research concerns solving the semi-supervised learning problem with generative adversarial networks for regression. In contrast to classification, where only a limited number of distinct classes is given, the regression task is defined as predicting continuous labels for a given dataset. Semi-supervised learning is of vital importance for the applications where a small number of labeled samples is available, or labeling samples is difficult or expensive to collect. A case in point is autonomous driving in which obtaining sufficient labeled samples covering all driving conditions is costly. In this context, we can take advantage of semi-supervised learning techniques with groundbreaking generative models, such as generative adversarial networks. However, almost all proposed GAN-based semi-supervised techniques in the literature are focused on solving the classification problem. Hence, developing a GAN-based semi-supervised method for the regression task is still an open problem. In this work, two different architectures will be proposed to address this problem. In summary, our introduced method is able to predict continuous labels for a training dataset which has only a limited number of labeled samples. Moreover, the application of this technique for solving the end-to-end task in autonomous driving will be presented.  \nWe performed several experiments over a publicly available driving dataset to evaluate our proposed method, and the results are very promising. The results show that our approach generates images with high quality, gives smaller label prediction error and leads to a more stable training compared with the state-of-the-art Improved GAN technique~\\citep{ImprovedGAN2016}.","pdf":"/pdf/90f0e675457dac151517a5ed5c0f15d856366494.pdf","paperhash":"anonymous|semisupervised_regression_with_generative_adversarial_networks_for_end_to_end_learning_in_autonomous_driving","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ2ZjsyC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper160/Authors"],"keywords":["GAN","Regression","SSL","Autonomous Driving"]}},{"tddate":null,"ddate":null,"tmdate":1512222579667,"tcdate":1511820158085,"number":2,"cdate":1511820158085,"id":"By8YsZceM","invitation":"ICLR.cc/2018/Conference/-/Paper160/Official_Review","forum":"SJ2ZjsyC-","replyto":"SJ2ZjsyC-","signatures":["ICLR.cc/2018/Conference/Paper160/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Insufficient experimental evidence and a potentially fatal design flaw with one of the proposed approaches.","rating":"3: Clear rejection","review":"This paper extends semi-supervised GAN approaches, which have been validated for classification, to the task of regression. The paper presents a case study on an end-to-end self-driving dataset where the goal is to predict the steering angle of a driver from a photo of the road.\n\nSince the paper introduces a new dataset, it is important to establish rigorous baselines in order to contextualize experimental results. The baseline of the paper (the Improved GAN architecture) is a good starting point which has previously demonstrated strong semi-supervised results. What is missing, however, is the baseline of a well-tuned purely supervised approach. Since readers are not familiar with the dataset/task it is difficult to interpret results - how important is 4% vs 3% error? Reference points and a variety of results in order to understand effect sizes for the dataset would greatly help in communicating the strength of the approach.\n\nThe paper argues that classification based approaches to regression are sub-optimal due to quantization error. It would be a nice if the authors quantified these statements and explored simple approaches to addressing this. For instance, the authors use only 10 uniformly spaced bins with their Improved-GAN approach. Depending on the distribution of the steering angles (this would be interesting to see) this could result in a significant portion of the dataset falling into only 2 or 3 bins. Does simply increasing the number of bins decrease the error for the Improved GAN baseline? What percentage of error is due to only quantization, when the correct bin is predicted, vs mis-classification, when the wrong bin is predicted. Additionally, alternative approaches to treating regression problems as classification tasks have been quite successful. Improved Semantic Representations From Tree-Structured LSTMS https://arxiv.org/abs/1503.00075, reports a small-modification to the binning approach where the predicted probability distribution over bins is converted back into a continuous score via a weighted average of the bin values outperformed mean-squared error.\n\nThe first architecture closely follows the standard semi-supervised GAN approach of using the last hidden layer of the discriminator as a feature space for a supervised output layer. In this case, the output is a linear regression layer instead of a linear classifier.\n\nThe second architecture uses an explicit kernel function to convert regression outputs of the model into discriminator scores. This corresponds to assigning the valid range of regression targets (0 to 1) as real data samples D(x) = 1 and invalid regression outputs to exponentially decaying values approaching D(x) = 0. The approach does not appear to be well-motivated or well-thought out. It is not clear why an ordering should be imposed such that extreme values of the regression targets correspond with generated samples. If the discriminator is not perfect and thinks a real photo is a generated sample it will be encouraged to assign invalid regression outputs of less than 0 or greater than 1. From the perspective of the motivating task of end to end self-driving this is an extremely unsafe behavior to construct in a model. In effect, the second architecture can only drive when it thinks the photos are real. \n\nFigure 5 shows high variability between experiments as well as strange training dynamics. In particular, the paper calls out Improved-GAN for having unstable training with spikes in performance. Curiously, these spikes only occur on training data and not on test data. Are the authors using a different testing procedure (parameter averaging or a non-IID dataset split) which would explain why train and test performance are very un-correlated? Figure 5b shows architecture 2 having significantly higher training errors than test errors for a significant portion of training.\n\nThe task the paper sets out to explore is a worthy one. However, the experimental results are not sufficiently rigorous to instill confidence in the reader. Additionally, the best reported architecture, Reg-GAN architecture 2, appears to have fundamentally undesirable behavior.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving","abstract":"This research concerns solving the semi-supervised learning problem with generative adversarial networks for regression. In contrast to classification, where only a limited number of distinct classes is given, the regression task is defined as predicting continuous labels for a given dataset. Semi-supervised learning is of vital importance for the applications where a small number of labeled samples is available, or labeling samples is difficult or expensive to collect. A case in point is autonomous driving in which obtaining sufficient labeled samples covering all driving conditions is costly. In this context, we can take advantage of semi-supervised learning techniques with groundbreaking generative models, such as generative adversarial networks. However, almost all proposed GAN-based semi-supervised techniques in the literature are focused on solving the classification problem. Hence, developing a GAN-based semi-supervised method for the regression task is still an open problem. In this work, two different architectures will be proposed to address this problem. In summary, our introduced method is able to predict continuous labels for a training dataset which has only a limited number of labeled samples. Moreover, the application of this technique for solving the end-to-end task in autonomous driving will be presented.  \nWe performed several experiments over a publicly available driving dataset to evaluate our proposed method, and the results are very promising. The results show that our approach generates images with high quality, gives smaller label prediction error and leads to a more stable training compared with the state-of-the-art Improved GAN technique~\\citep{ImprovedGAN2016}.","pdf":"/pdf/90f0e675457dac151517a5ed5c0f15d856366494.pdf","paperhash":"anonymous|semisupervised_regression_with_generative_adversarial_networks_for_end_to_end_learning_in_autonomous_driving","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ2ZjsyC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper160/Authors"],"keywords":["GAN","Regression","SSL","Autonomous Driving"]}},{"tddate":null,"ddate":null,"tmdate":1512222582440,"tcdate":1511764204141,"number":1,"cdate":1511764204141,"id":"r1VeW4KlG","invitation":"ICLR.cc/2018/Conference/-/Paper160/Official_Review","forum":"SJ2ZjsyC-","replyto":"SJ2ZjsyC-","signatures":["ICLR.cc/2018/Conference/Paper160/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Missing related work, comparisons and proper evaluation.","rating":"2: Strong rejection","review":"The paper proposes an extension of the semi-supervised learning method for classification presented as Section 5 in Salimans et al. “Improved techniques for training GANs” to a regression problem.\n\nOverall, the biggest criticisms are a lack of discussion of related work and the evaluation on a single apparently newly collected dataset without any comparison to other methods for semi-supervised regression in the experiments. Also, I am missing clarity in the presentation, throughout.\nI have doubts about the approach in general as well, but I cannot even discuss them without knowing how well it works on a proper benchmark, in comparison to related methods.\n\nThe paper starts out with a series of dubious claims:\n* The authors claim that there is no related work on GANs for regression. There are several notable exceptions. \nE.g. SimGAN, which uses GANs to train a network for eye gaze estimation:\nAshish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, Russ Webb Apple Inc., “Learning from Simulated and Unsupervised Images through Adversarial Training” CVPR 2017 (best paper award)\n\nThe very related work by Bousmalis et al. “Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks”, CVPR 2017 also uses GANs to solve a regression task, namely 3D pose estimation.\n\nGranted, these are conditional GANs that take as additional input images from a source domain related to the target domain.\n\n* “Applying semi-supervised classification techniques to regression comes with the price of converting continuous labels of the dataset to a limited number of classes.”\n\nWhy should quantization be necessary in order to apply semi-supervised techniques. Semi-supervised only means that you have labels for some samples, and a potentially much larger set of samples without labels. It does not imply anything in terms of what the labels are and how they are used. The statement only holds if you want to use a classification method.\n\n* “usually classification techniques require more number of outputs”\nClassification typically has as many outputs as classes, so whether that’s more or less depends on the classification and the regression task. I guess what is meant here is that when you solve the regression task by quantizing the target values and frame it as classification you need more output nodes than with just regression?\n\n\nThe idea of mixing the two tasks of regression and domain prediction into a single scalar by letting the network predict values outside of the valid range of regression values for generated “fake” images seems very strange. The discriminator will always try to pull the output far away from the true target values even when samples from the generator converge very closely to samples from the real input. Additionally different generated samples are penalized differently, depending on how close their target value lies to the edge of the valid range.\n\nEquation 2 seems to be incorrect. The term p_data is used as both the labeled dataset as well a the probability from the predictor. Looking at Salimans et al 2016 it should probably be p_model in the second case.\n\nFigure 1 does not really explain anything if you don’t already know what’s going on, and uses symbols that were not introduced (x_gen, x_lab, x_unlab). If you don’t know it from the text you also don’t understand what the N+1 classes are. In short, it does not add much value.\n\nExperimental Evaluation: \nThe dataset is said to be publicly available. It’s true that you can download it, but it doesn’t seem to be a public dataset in the sense that it was published somewhere along with a description. \nThere is no explanation of the data in the paper whatsoever. How was this data collected? What is the task? There are images from a camera in a driving car and apparently angles of the steering wheel associated to them. What’s the goal? Regressing from a single image how the steering wheel is currently oriented? How should that be possible?\nWhy is the range of values so unbalanced? Is the car going around in a single circle? Also, the values in the data.txt of the dataset are much larger, namely [-159.93, 501.78]. What are these?\n\nWhy is the error given in %? This is a regression task. You are regressing an angle (or are you). Why is the error not an average/median/maximum error angle?\n\nI’ve mentioned above that I am missing comparisons to any other regression method. At least the baseline of just training the regression network supervised on the given data would need to be included.\n\nOther comments:\n\n* The paper needs an overall language check.\n* Was the ^2 of the norm in Equation 3 intentionally left out?\n* Algorithm 1 will likely be an infinite loop, since convergence cannot be guaranteed.\n* Salimans et al. “Improved Techniques for Training GANs” was presented at NIPS 2016. Please use that reference in the bibliography instead of the arxiv one.\n* Why the “x 100” in Equation 6?\n* if you dont give any runtimes, you also don’t need to mention the hardware the experiments were run on.\n* Why is the test error of “Improved GAN” much better than the training error in the beginning?\n* Architecture 2 does not seem converged. Why stop there? The interesting part in terms of convergence (stability of the game) would be towards the \"end\".\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving","abstract":"This research concerns solving the semi-supervised learning problem with generative adversarial networks for regression. In contrast to classification, where only a limited number of distinct classes is given, the regression task is defined as predicting continuous labels for a given dataset. Semi-supervised learning is of vital importance for the applications where a small number of labeled samples is available, or labeling samples is difficult or expensive to collect. A case in point is autonomous driving in which obtaining sufficient labeled samples covering all driving conditions is costly. In this context, we can take advantage of semi-supervised learning techniques with groundbreaking generative models, such as generative adversarial networks. However, almost all proposed GAN-based semi-supervised techniques in the literature are focused on solving the classification problem. Hence, developing a GAN-based semi-supervised method for the regression task is still an open problem. In this work, two different architectures will be proposed to address this problem. In summary, our introduced method is able to predict continuous labels for a training dataset which has only a limited number of labeled samples. Moreover, the application of this technique for solving the end-to-end task in autonomous driving will be presented.  \nWe performed several experiments over a publicly available driving dataset to evaluate our proposed method, and the results are very promising. The results show that our approach generates images with high quality, gives smaller label prediction error and leads to a more stable training compared with the state-of-the-art Improved GAN technique~\\citep{ImprovedGAN2016}.","pdf":"/pdf/90f0e675457dac151517a5ed5c0f15d856366494.pdf","paperhash":"anonymous|semisupervised_regression_with_generative_adversarial_networks_for_end_to_end_learning_in_autonomous_driving","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ2ZjsyC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper160/Authors"],"keywords":["GAN","Regression","SSL","Autonomous Driving"]}},{"tddate":null,"ddate":null,"tmdate":1509739452128,"tcdate":1509042947903,"number":160,"cdate":1509739449467,"id":"SJ2ZjsyC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ2ZjsyC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving","abstract":"This research concerns solving the semi-supervised learning problem with generative adversarial networks for regression. In contrast to classification, where only a limited number of distinct classes is given, the regression task is defined as predicting continuous labels for a given dataset. Semi-supervised learning is of vital importance for the applications where a small number of labeled samples is available, or labeling samples is difficult or expensive to collect. A case in point is autonomous driving in which obtaining sufficient labeled samples covering all driving conditions is costly. In this context, we can take advantage of semi-supervised learning techniques with groundbreaking generative models, such as generative adversarial networks. However, almost all proposed GAN-based semi-supervised techniques in the literature are focused on solving the classification problem. Hence, developing a GAN-based semi-supervised method for the regression task is still an open problem. In this work, two different architectures will be proposed to address this problem. In summary, our introduced method is able to predict continuous labels for a training dataset which has only a limited number of labeled samples. Moreover, the application of this technique for solving the end-to-end task in autonomous driving will be presented.  \nWe performed several experiments over a publicly available driving dataset to evaluate our proposed method, and the results are very promising. The results show that our approach generates images with high quality, gives smaller label prediction error and leads to a more stable training compared with the state-of-the-art Improved GAN technique~\\citep{ImprovedGAN2016}.","pdf":"/pdf/90f0e675457dac151517a5ed5c0f15d856366494.pdf","paperhash":"anonymous|semisupervised_regression_with_generative_adversarial_networks_for_end_to_end_learning_in_autonomous_driving","_bibtex":"@article{\n  anonymous2018semi-supervised,\n  title={Semi-supervised Regression with Generative Adversarial Networks for End to End Learning in Autonomous Driving},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ2ZjsyC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper160/Authors"],"keywords":["GAN","Regression","SSL","Autonomous Driving"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}