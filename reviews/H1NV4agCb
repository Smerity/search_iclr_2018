{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642445587,"tcdate":1511948576988,"number":3,"cdate":1511948576988,"id":"SkFQWZ3ez","invitation":"ICLR.cc/2018/Conference/-/Paper413/Official_Review","forum":"H1NV4agCb","replyto":"H1NV4agCb","signatures":["ICLR.cc/2018/Conference/Paper413/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper derives from the popular RPN method and hypothesizes that pixels at different positions, in top feature maps for tracking, should be treated with various emphasis; and they design four matching (anchors and ground truth boxes) strategies to explore which of them are proper for tracking. This is most important idea proposed in the paper. The model compression part is for speed consideration and the model ensemble idea is a general trick to further improve the performance on VOT 2016.\n\nHowever, I think the paper is not ready for ICLR yet due to several reasons.\n\n- Novelty. The newly proposed tracking loss is still the standard, well-known classification loss with the novel part that \"matching strategy\" is different. I do like the analysis part as to why the first and fourth strategy is chosen in section 3.4. But this seems not to be a big difference from previous work. \n\n- Experiments not enough and not organized well. The paper spares the second half of its novelty to network compression (of which there is barely new; but I do understand the necessity to speed up algorithms in real-time products); however, there is no clear comparison (table or figure) to point out how the acceleration is. Only some words in the paper: \"The proposed network compression xxx four times\", \"our tracker accelerates 60% in speed\", etc. Figure 6 is not depicted in a clear manner. \n\nTable 2 lists the performance comparison across methods; what is the meaning of Information-gain loss? It is fair to compare the model ensemble version of yours to other methods? How about the results of the ensemble version of other methods? There is no ablative study in the experiment also. \n\nThe paper has many presentation drawbacks (syntax errors, format issues, etc.) For example, \"Basing on knowledge distillation theory xxx\" -> Based. I won't list all that I find here. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tracking Loss: Converting Object Detector to Robust Visual Tracker","abstract":"In this paper, we find that by designing a novel loss function entitled, ''tracking loss'', Convolutional Neural Network (CNN) based object detectors can be successfully converted to well-performed visual trackers without any extra computational cost. This property is preferable to visual tracking where annotated video sequences for training are always absent, because rich features learned by detectors from still images could be utilized by dynamic trackers. It also avoids extra machinery such as feature engineering and feature aggregation proposed in previous studies. Tracking loss achieves this property by exploiting the internal structure of feature maps within the detection network and treating different feature points discriminatively. Such structure allows us to simultaneously consider discrimination quality and bounding box accuracy which is found to be crucial to the success. We also propose a network compression method to accelerate tracking speed without performance reduction. That also verifies tracking loss will remain highly effective even if the network is drastically compressed. Furthermore, if we employ a carefully designed tracking loss ensemble, the tracker would be much more robust and accurate. Evaluation results show that our trackers (including the ensemble tracker and two baseline trackers), outperform all state-of-the-art methods on VOT 2016 Challenge in terms of Expected Average Overlap (EAO) and robustness. We will make the code publicly available.","pdf":"/pdf/06ea09534e938f0792175ba1cb770b994107116e.pdf","TL;DR":"We successfully convert a popular detector RPN to a well-performed tracker from the viewpoint of loss function.","paperhash":"anonymous|tracking_loss_converting_object_detector_to_robust_visual_tracker","_bibtex":"@article{\n  anonymous2018tracking,\n  title={Tracking Loss: Converting Object Detector to Robust Visual Tracker},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1NV4agCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper413/Authors"],"keywords":["Object detection","Visual Tracking","Loss function","Region Proposal Network","Network compression"]}},{"tddate":null,"ddate":null,"tmdate":1515642445624,"tcdate":1511753996941,"number":2,"cdate":1511753996941,"id":"rySfFbFgz","invitation":"ICLR.cc/2018/Conference/-/Paper413/Official_Review","forum":"H1NV4agCb","replyto":"H1NV4agCb","signatures":["ICLR.cc/2018/Conference/Paper413/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Mediocre paper without depth or significant contributions.","rating":"3: Clear rejection","review":"In this paper, the authors propose a novel tracking loss to convert the RPN to a tracker. The internal structure of top layer features of RPN is exploited to treat feature points discriminatively. In addition, the proposed compression network speeds up the tracking algorithm. The experimental results on the VOT2016 dataset demonstrate its efficiency in tracking. \n\nThis work is the combination of Faster R-CNN (Ren et al. PAMI 2015) and tracking-by-detection framework. The main contributions proposed in this paper are new tracking loss, network compression and results. \n\nThere are numerous concerns with this work:\n\n1.\tThe new tracking loss shown in equation 2 is similar with the original Faster R-CNN loss shown in equation 1. The only difference is to replace the regression loss with a predefined mask selection loss, which is of little sense that the feature processing can be further fulfilled through one-layer CNN. The empirical operation shown in figure 2 seems arbitrary and lack of theoretical explanation. There is no insight of why doing so. Simply showing the numbers in table 1 does not imply the necessity, which ought to be put in the experiment sections. \n2.\tThe network compression is engineering and lack insight as well. To remove part of the CNN and retrain is a common strategy in the CNN compression methods [a] [b]. There is a lack of discussion with the relationship with prior arts.\n3.\tThe organization is not clear. Section 3.4 should be set in the experiments and Section 3.5 should be set at the beginning of the algorithm. The description of the network compression is not clear enough, especially the training details.  Meanwhile, the presentation is hard to follow. There is no clear expression of how the tracker performs in practice.\n4.\tIn addition, VOT 2016, the method should evaluate on the OTB dataset with the following trackers [c] [d].\n5.\tThe evaluation is not fair. In Sec 6, the authors indicate that MDNet runs at 1FPS while the proposed tracker runs at 1.6FPS. However, MDNet is based on Matlab and the proposed tracker is based on C++ (i.e., Caffe).\n\nReference:\n[a] On Compressing Deep Models by Low Rank and Sparse Decomposition. Yu et al. CVPR 2017.\n[b] Designing Energy-Efficient Convolutional Neural Network Using Energy-Aware Pruning. Yang et al. CVPR 2017.\n[c] ECO: Efficient Convolution Operators for Tracking. Danelljan et al. CVPR 2017.\n[d] Multi-Task Correlation Particle Filter For Robust Object Tracking. Zhang et al. CVPR 2017.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tracking Loss: Converting Object Detector to Robust Visual Tracker","abstract":"In this paper, we find that by designing a novel loss function entitled, ''tracking loss'', Convolutional Neural Network (CNN) based object detectors can be successfully converted to well-performed visual trackers without any extra computational cost. This property is preferable to visual tracking where annotated video sequences for training are always absent, because rich features learned by detectors from still images could be utilized by dynamic trackers. It also avoids extra machinery such as feature engineering and feature aggregation proposed in previous studies. Tracking loss achieves this property by exploiting the internal structure of feature maps within the detection network and treating different feature points discriminatively. Such structure allows us to simultaneously consider discrimination quality and bounding box accuracy which is found to be crucial to the success. We also propose a network compression method to accelerate tracking speed without performance reduction. That also verifies tracking loss will remain highly effective even if the network is drastically compressed. Furthermore, if we employ a carefully designed tracking loss ensemble, the tracker would be much more robust and accurate. Evaluation results show that our trackers (including the ensemble tracker and two baseline trackers), outperform all state-of-the-art methods on VOT 2016 Challenge in terms of Expected Average Overlap (EAO) and robustness. We will make the code publicly available.","pdf":"/pdf/06ea09534e938f0792175ba1cb770b994107116e.pdf","TL;DR":"We successfully convert a popular detector RPN to a well-performed tracker from the viewpoint of loss function.","paperhash":"anonymous|tracking_loss_converting_object_detector_to_robust_visual_tracker","_bibtex":"@article{\n  anonymous2018tracking,\n  title={Tracking Loss: Converting Object Detector to Robust Visual Tracker},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1NV4agCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper413/Authors"],"keywords":["Object detection","Visual Tracking","Loss function","Region Proposal Network","Network compression"]}},{"tddate":null,"ddate":null,"tmdate":1515642445670,"tcdate":1511736819630,"number":1,"cdate":1511736819630,"id":"Bk3gLTuef","invitation":"ICLR.cc/2018/Conference/-/Paper413/Official_Review","forum":"H1NV4agCb","replyto":"H1NV4agCb","signatures":["ICLR.cc/2018/Conference/Paper413/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Technical achievements but limited novelty - perhaps more suited for a computer vision venue. Results not fully transparent.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method to adapt a region proposal network (RPN) to the visual object tracking task. The authors also describe a method to compress the network to improve run-time performance. They also claim that an ensemble of the proposed trackers provides state-of-the-art performance on VOT 2016.\n\nThe main claimed contribution is \"a novel tracking loss which successfully converts a pre-trained object detector RPN to a state-of-the-art visual tracker...\" The idea to adapt an RPN to visual object tracking by generating samples for online learning was previously published in the CVPR 2016 workshops [1]. Some novelty is also claimed in the network compression, but it seems to be a straightforward implementation of knowledge distillation. In this reviewer's estimation, the novelty in this paper is limited to the specific design of the loss function described in Section 3.5.\n\nThe tracking loss is essentially a procedure to limit or gate back-propagation updates to proposal regions that have a high confidence to match the object being tracking. The intuition is that this strategy will enable better online learning and thus tracking performance. A small empirical study was conducted to determine which feature regions from the top layer of the RPN are most effective for this purpose. The authors argue that allowing only high matches could lead to centered but loose bounding boxes, while allowing further matches can improve the bounding box fit but might encourage drift. The loss function is combination of the two, with \\alpha and \\beta weighting the importance (it seems you only need one weight parameter here). No theoretical justification for the approach is given, it seems to be an ad hoc solution to adapt a region proposal architecture to perform online tracking.\n\nThe network compression in Section 4 seems to yield a nice increase in efficiency without any loss in performance. The network ensemble described in Section 5 improves tracking performance over a single network. These are nice technical improvements that push performance, but do not offer much in terms of novelty.\n\nThe proposed tracking network is tested on the VOT 2016 challenge data. The authors claim state-of-the-art performance on this dataset. The source code and raw results of participants of VOT 2016 are all publicly available - but unfortunately the no raw results or source code are provided for this paper either in supplementary material or in anonymous repository (it is not difficult to do this while keeping anonymity). Tracking papers usually provide some video results or, at minimum, still frames, to assist in the evaluation of performance, but none are provided here The Accuracy Rank and Robustness Rank numbers provided in Table 2 seem to be incorrectly computed - these numbers should be integers, see the VOT challenge report [2].\n\nPros:\n+ Strong practical and technical improvements to push performance\n+ State-of-the-art performance on VOT 2016\n\nCons:\n- Difficult to verify results\n- Limited novelty\n- Numerous language problems, making the paper difficult to read and understand in many places.\n\n\n[1] Zhu, G., Porikli, F., & Li, H. (2016). Robust visual tracking with deep convolutional neural network based object proposals on pets. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 26-33).\n[2] VOT challenge report http://data.votchallenge.net/vot2016/presentations/vot_2016_paper.pdf\n\nMinor notes:\n* Clearly explain what an anchor point is\n* Figures are introduced out of order\n* What is the purpose of Figure 1? There seems to be no clear message\n* \"Although ground truths are not provided in tracking...\" - what do you mean by this?\n* Do you fix the aspect ratio of the anchors to 1:1 for tracking, or only to define the confidences in Fig 2?\n* \"RPN is a relative large network to tracking\" - I understand what you mean but it is not written clearly\n* Unclear how \\alpha:\\beta is computed\n* Figure 6 is too small to read.\n* Table 2 size should be increased.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tracking Loss: Converting Object Detector to Robust Visual Tracker","abstract":"In this paper, we find that by designing a novel loss function entitled, ''tracking loss'', Convolutional Neural Network (CNN) based object detectors can be successfully converted to well-performed visual trackers without any extra computational cost. This property is preferable to visual tracking where annotated video sequences for training are always absent, because rich features learned by detectors from still images could be utilized by dynamic trackers. It also avoids extra machinery such as feature engineering and feature aggregation proposed in previous studies. Tracking loss achieves this property by exploiting the internal structure of feature maps within the detection network and treating different feature points discriminatively. Such structure allows us to simultaneously consider discrimination quality and bounding box accuracy which is found to be crucial to the success. We also propose a network compression method to accelerate tracking speed without performance reduction. That also verifies tracking loss will remain highly effective even if the network is drastically compressed. Furthermore, if we employ a carefully designed tracking loss ensemble, the tracker would be much more robust and accurate. Evaluation results show that our trackers (including the ensemble tracker and two baseline trackers), outperform all state-of-the-art methods on VOT 2016 Challenge in terms of Expected Average Overlap (EAO) and robustness. We will make the code publicly available.","pdf":"/pdf/06ea09534e938f0792175ba1cb770b994107116e.pdf","TL;DR":"We successfully convert a popular detector RPN to a well-performed tracker from the viewpoint of loss function.","paperhash":"anonymous|tracking_loss_converting_object_detector_to_robust_visual_tracker","_bibtex":"@article{\n  anonymous2018tracking,\n  title={Tracking Loss: Converting Object Detector to Robust Visual Tracker},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1NV4agCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper413/Authors"],"keywords":["Object detection","Visual Tracking","Loss function","Region Proposal Network","Network compression"]}},{"tddate":null,"ddate":null,"tmdate":1509739318013,"tcdate":1509114924479,"number":413,"cdate":1509739315354,"id":"H1NV4agCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1NV4agCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Tracking Loss: Converting Object Detector to Robust Visual Tracker","abstract":"In this paper, we find that by designing a novel loss function entitled, ''tracking loss'', Convolutional Neural Network (CNN) based object detectors can be successfully converted to well-performed visual trackers without any extra computational cost. This property is preferable to visual tracking where annotated video sequences for training are always absent, because rich features learned by detectors from still images could be utilized by dynamic trackers. It also avoids extra machinery such as feature engineering and feature aggregation proposed in previous studies. Tracking loss achieves this property by exploiting the internal structure of feature maps within the detection network and treating different feature points discriminatively. Such structure allows us to simultaneously consider discrimination quality and bounding box accuracy which is found to be crucial to the success. We also propose a network compression method to accelerate tracking speed without performance reduction. That also verifies tracking loss will remain highly effective even if the network is drastically compressed. Furthermore, if we employ a carefully designed tracking loss ensemble, the tracker would be much more robust and accurate. Evaluation results show that our trackers (including the ensemble tracker and two baseline trackers), outperform all state-of-the-art methods on VOT 2016 Challenge in terms of Expected Average Overlap (EAO) and robustness. We will make the code publicly available.","pdf":"/pdf/06ea09534e938f0792175ba1cb770b994107116e.pdf","TL;DR":"We successfully convert a popular detector RPN to a well-performed tracker from the viewpoint of loss function.","paperhash":"anonymous|tracking_loss_converting_object_detector_to_robust_visual_tracker","_bibtex":"@article{\n  anonymous2018tracking,\n  title={Tracking Loss: Converting Object Detector to Robust Visual Tracker},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1NV4agCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper413/Authors"],"keywords":["Object detection","Visual Tracking","Loss function","Region Proposal Network","Network compression"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}