{"notes":[{"tddate":null,"ddate":null,"tmdate":1512407559284,"tcdate":1512407559284,"number":3,"cdate":1512407559284,"id":"H11zfWQZf","invitation":"ICLR.cc/2018/Conference/-/Paper558/Official_Review","forum":"HyrCWeWCb","replyto":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference/Paper558/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Technique is not clear. Contribution is more like incremental. Experiments are insufficient.","rating":"4: Ok but not good enough - rejection","review":"This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient.\n\nThe major weakness of this paper is the unclear presentation. For example, the algorithm is never fully described, though a handful variants are discussed. How the off-policy version is implemented is missing.\n\nIn experiments, why the off-policy version of TRPO is not compared. Comparing the on-policy results, PCL does not show a significant advantage over TRPO. Moreover, the curves of TRPO is so unstable, which is a bit uncommon. \n\nWhat is the exploration strategy in the experiments? I guess it was softmax probability. However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added.\n\nAnother issue is the discussion of the entropy regularization in the objective function. This regularization, while helping exploration, do changes the original objective. When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance. Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222690725,"tcdate":1511549278763,"number":2,"cdate":1511549278763,"id":"ByDPYkUxG","invitation":"ICLR.cc/2018/Conference/-/Paper558/Official_Review","forum":"HyrCWeWCb","replyto":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference/Paper558/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper","rating":"7: Good paper, accept","review":"Clarity \nThe paper is well-written and clear. \n\nOriginality\nThe paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy. The paper leverages a novel method in determining the coefficient of relative entropy. \n\nSignificance\n- Trust-PCL achieves overall competitive with state-of-the-art external implementations.\n- Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance. \n- Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant. \n- Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful. \n\nPros:\n- The paper is well-written and clear. \n- Competitive with state-of-the-art external implementations\n- Significant empirical advantage over TRPO.\n-  Open source codes.\n\nCons:\n- No ablation studies. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222690767,"tcdate":1511363474362,"number":1,"cdate":1511363474362,"id":"H1ccXfmeG","invitation":"ICLR.cc/2018/Conference/-/Paper558/Official_Review","forum":"HyrCWeWCb","replyto":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference/Paper558/AnonReviewer3"],"readers":["everyone"],"content":{"title":"It might be useful but looks like an incremental work. The technical presentation is not quite clear.","rating":"5: Marginally below acceptance threshold","review":"The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent.  I am not an expert in this area. It is hard to judge the significance of this extension.\n\nThe paper largely follows the work of Nachum et al 2017. The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training. However, the relative entropy term added seems like a marginal modification. Authors claimed that it satisfies the multi-step path consistency but the derivation is missing.\n\nI am a bit confused about the way trust region method is used in the paper. Initially,  problem is written as a constrained optimization problem (12). It is then converted into a penalty form for softmax consistency. Finally, the Lagrange parameter is estimated from the trust region method. In addition, how do you get the Lagrange parameter from epsilon?\n\nThe pseudo code of the algorithm is missing. It would be much clearer if a detailed description of the algorithmic procedure is given.\n\nHow is the performance of Trust-PCL compared to PCL? ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739236820,"tcdate":1509126605520,"number":558,"cdate":1509739234163,"id":"HyrCWeWCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}