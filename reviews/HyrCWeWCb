{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222690725,"tcdate":1511549278763,"number":2,"cdate":1511549278763,"id":"ByDPYkUxG","invitation":"ICLR.cc/2018/Conference/-/Paper558/Official_Review","forum":"HyrCWeWCb","replyto":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference/Paper558/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper","rating":"7: Good paper, accept","review":"Clarity \nThe paper is well-written and clear. \n\nOriginality\nThe paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy. The paper leverages a novel method in determining the coefficient of relative entropy. \n\nSignificance\n- Trust-PCL achieves overall competitive with state-of-the-art external implementations.\n- Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance. \n- Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant. \n- Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful. \n\nPros:\n- The paper is well-written and clear. \n- Competitive with state-of-the-art external implementations\n- Significant empirical advantage over TRPO.\n-  Open source codes.\n\nCons:\n- No ablation studies. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222690767,"tcdate":1511363474362,"number":1,"cdate":1511363474362,"id":"H1ccXfmeG","invitation":"ICLR.cc/2018/Conference/-/Paper558/Official_Review","forum":"HyrCWeWCb","replyto":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference/Paper558/AnonReviewer3"],"readers":["everyone"],"content":{"title":"It might be useful but looks like an incremental work. The technical presentation is not quite clear.","rating":"5: Marginally below acceptance threshold","review":"The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent.  I am not an expert in this area. It is hard to judge the significance of this extension.\n\nThe paper largely follows the work of Nachum et al 2017. The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training. However, the relative entropy term added seems like a marginal modification. Authors claimed that it satisfies the multi-step path consistency but the derivation is missing.\n\nI am a bit confused about the way trust region method is used in the paper. Initially,  problem is written as a constrained optimization problem (12). It is then converted into a penalty form for softmax consistency. Finally, the Lagrange parameter is estimated from the trust region method. In addition, how do you get the Lagrange parameter from epsilon?\n\nThe pseudo code of the algorithm is missing. It would be much clearer if a detailed description of the algorithmic procedure is given.\n\nHow is the performance of Trust-PCL compared to PCL? ","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739236820,"tcdate":1509126605520,"number":558,"cdate":1509739234163,"id":"HyrCWeWCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyrCWeWCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control","abstract":"Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.","pdf":"/pdf/25538d9bacfab724e9abc7030ee812c03e15a246.pdf","TL;DR":"We extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.","paperhash":"anonymous|trustpcl_an_offpolicy_trust_region_method_for_continuous_control","_bibtex":"@article{\n  anonymous2018trust-pcl:,\n  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyrCWeWCb}\n}","keywords":["Reinforcement learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper558/Authors"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}