{"notes":[{"tddate":null,"ddate":null,"tmdate":1512403687211,"tcdate":1512403687211,"number":3,"cdate":1512403687211,"id":"HyJlQlQWf","invitation":"ICLR.cc/2018/Conference/-/Paper480/Official_Review","forum":"rJUBryZ0W","replyto":"rJUBryZ0W","signatures":["ICLR.cc/2018/Conference/Paper480/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting risk bound but empirical evaluation is not convincing","rating":"5: Marginally below acceptance threshold","review":"The paper considers multi-task setting of machine learning. The first contribution of the paper is a novel PAC-Bayesian risk bound. This risk bound serves as an objective function for multi-task machine learning. A second contribution is an algorithm, called LAP, for minimizing a simplified version of this objective function. LAP algorithm uses several training tasks to learn a prior distribution P over hypothesis space. This prior distribution P is then used to find a posterior distribution Q that minimizes the same objective function over the test task. The third contribution is an empirical evaluation of LAP over toy dataset of two clusters and over MNIST.\n\nWhile the paper has the title of \"life-long learning\", the authors admit that all experiments are in multi-task setting, where\nthe training is done over all tasks simultaneously. The novel risk bound and LAP algorithm can definitely be applied to life-long setting, where training tasks are available sequentially. But since there is no empirical evaluation in this setting, I suggest to adjust the title of the paper. \n \nThe novel risk bound of the paper is an extension of the bound from [Pentina & Lampert, ICML 2014]. The extension seems to be quite significant. Unlike the bound of [Pentina & Lampert, ICML 2014], the new bound allows to re-use many different PAC-Bayesian complexity terms that were published previously. \n\nI liked risk bound and optimization sections of the paper. But I was less convinced by the empirical experiments. Since \nthe paper improves the risk bound of [Pentina & Lampert, ICML 2014], I expected to see an empirical comparison of LAP and optimization  algorithm from the latter paper. To make such comparison fair, both optimization algorithms should use the same base algorithm, e.g. ridge regression, as in [Pentina & Lampert, ICML 2014]. Also I suggest to use the datasets from the latter paper.  \n\nThe experiment with multi-task learning over MNIST dataset looks interesting, but it is still a toy experiment. This experiment  will be more convincing with more sophisticated datasets (CIFAR-10, ImageNet) and architectures (e.g. Inception-V4, ResNet). \n\nMinor remarks:\nSection 6, line 4: \"Combing\" -> \"Combining\"\nPage 14, first equation: There should be \"=\" before the second expectation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning by Adjusting Priors","abstract":"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. Under the assumption that future tasks are `related’' to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework. Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks. We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples. In addition to establishing the improved performance available through lifelong learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.","pdf":"/pdf/58c772fd3129a3f2458409a6189467dd7b9ba3a4.pdf","TL;DR":"We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks.","paperhash":"anonymous|lifelong_learning_by_adjusting_priors","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning by Adjusting Priors},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUBryZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper480/Authors"],"keywords":["Lifelong learning","Transfer learning","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222664847,"tcdate":1511818821693,"number":2,"cdate":1511818821693,"id":"H1qBI-5gG","invitation":"ICLR.cc/2018/Conference/-/Paper480/Official_Review","forum":"rJUBryZ0W","replyto":"rJUBryZ0W","signatures":["ICLR.cc/2018/Conference/Paper480/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting algorithm based on a theoretical study, but the main theorem might contain a flaw","rating":"5: Marginally below acceptance threshold","review":"I personally warmly welcome any theoretically grounded methods to perform deep learning. I read the paper with interest, but I have two concerns about the main theoretical result (Theorem 1, lifelong learning PAC-Bayes bound).\n* Firstly, the bound is valid for a [0,1]-valued loss, which does not comply with the losses used in the experiments (Euclidean distance and cross-entropy). This is not a big issue, as I accept that the authors are mainly interested in the learning strategy promoted by the bound. However, this should clearly appear in the theorem statement.\n* Secondly, and more importantly, I doubt that the uaw of the meta-posterior as a distribution over priors for each task is valid. In Proposition 1 (the classical single-task PAC-Bayes bound), the bound is valid with probability 1-delta for one specific choice of prior P, and this choice must be independent of the learning sample S. However, it appears that the bound should be valid uniformly for all P in order to be used in Theorem 1 proof (see Equation 18). From a learning point of view, it seems counterintuitive that the prior used in the KL term to learn from a task relies on the training samples (i.e., the same training samples are used to learn the meta-posterior over priors, and the task specific posterior).  \n\nA note about the experiments:\nI am slightly disappointed that the authors compared their algorithm solely with methods learning from fewer tasks. I would like to see the results obtained by another method using five tasks. A simple idea would be to learn a network independently for each of the five tasks, and consider as a meta-prior an isotropic Gaussian distribution centered on the mean of the five learned weight vectors.\n\nTypos and minor comments:\n- Equation 1: \\ell is never explicitly defined.\n- Equation 4: Please explicitly define m in this context (size of the learning sample drawn from tau).\n- Page 4, before Equation 5: A dot is missing between Q and \"This\".\n- Page 7, line 3: Missing parentheses around equation number 12.\n- Section 5.1.1, line 5: \"The hypothesis class is a the set of...\"\n- Equation 17: Q_1, ... Q_n are irrelevant.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning by Adjusting Priors","abstract":"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. Under the assumption that future tasks are `related’' to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework. Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks. We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples. In addition to establishing the improved performance available through lifelong learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.","pdf":"/pdf/58c772fd3129a3f2458409a6189467dd7b9ba3a4.pdf","TL;DR":"We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks.","paperhash":"anonymous|lifelong_learning_by_adjusting_priors","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning by Adjusting Priors},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUBryZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper480/Authors"],"keywords":["Lifelong learning","Transfer learning","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222664896,"tcdate":1511204273553,"number":1,"cdate":1511204273553,"id":"HyY2Sogef","invitation":"ICLR.cc/2018/Conference/-/Paper480/Official_Review","forum":"rJUBryZ0W","replyto":"rJUBryZ0W","signatures":["ICLR.cc/2018/Conference/Paper480/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well written paper, with very week experiments.","rating":"6: Marginally above acceptance threshold","review":"The author extends existing PAC-Bayes bounds to multi-task learning, to allow the prior to be adapted across different tasks. Inspired by the variational bayes literature, a probabilistic neural network is used to minimize the bound. Results are evaluated on a toy dataset and a synthetically modified version of MNIST. \n\nWhile this paper is well written and addresses an important topic, there are a few points to be discussed:\n\n* Experimental results are really week. The toy experiment only compares the mean of two gaussians. Also, on the synthetic MNIST experiments, no comparison is done with any external algorithms. Neural Statistician, Model-Agnostic Meta-Learning and matching networks all provide decent results on such setup. While it is tolerated to have minimal experiments in a theoretical papers, the theory only extends Pentina & Lampert (2014). Also, similar algorithms can be obtain through variational-bayes evidence lower bound. \n\n* The bound appears to be sub-optimal. A bound where the KL term vanishes by 1/n would probably be tighter. I went in appendix to try to see how the proof could be adapted but it’s definitively not as well written as the rest of the paper. I’m not against putting proofs in appendix but only when it helps clarity. In this case it did not.\n\n* The paper is really about multi-task learning. Lifelong learning implies some continual learning and addressing the catastrophic forgetting issues. I would recommend against overuse of the lifelong learning term.\n\nMinors:\n* Define KLD\n* Section 5.1 : “to toy”\n* Section 5.1.1: “the the”\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning by Adjusting Priors","abstract":"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. Under the assumption that future tasks are `related’' to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework. Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks. We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples. In addition to establishing the improved performance available through lifelong learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.","pdf":"/pdf/58c772fd3129a3f2458409a6189467dd7b9ba3a4.pdf","TL;DR":"We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks.","paperhash":"anonymous|lifelong_learning_by_adjusting_priors","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning by Adjusting Priors},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUBryZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper480/Authors"],"keywords":["Lifelong learning","Transfer learning","PAC-Bayes theory"]}},{"tddate":null,"ddate":null,"tmdate":1509739279411,"tcdate":1509123389657,"number":480,"cdate":1509739276746,"id":"rJUBryZ0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJUBryZ0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Lifelong Learning by Adjusting Priors","abstract":"In representational lifelong learning an agent aims to continually learn to solve novel tasks while updating its representation in light of previous tasks. Under the assumption that future tasks are `related’' to previous tasks, representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We develop a framework for lifelong learning in deep neural networks that is based on generalization bounds, developed within the PAC-Bayes framework. Learning takes place through the construction of a distribution over networks based on the tasks seen so far, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting a history-dependent prior for novel tasks. We develop a gradient-based algorithm implementing these ideas, based on minimizing an objective function motivated by generalization bounds, and demonstrate its effectiveness through numerical examples. In addition to establishing the improved performance available through lifelong learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.","pdf":"/pdf/58c772fd3129a3f2458409a6189467dd7b9ba3a4.pdf","TL;DR":"We develop a lifelong learning approach to transfer learning based on PAC-Bayes theory, whereby priors are adjusted as new tasks are encountered thereby facilitating the learning of novel tasks.","paperhash":"anonymous|lifelong_learning_by_adjusting_priors","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning by Adjusting Priors},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJUBryZ0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper480/Authors"],"keywords":["Lifelong learning","Transfer learning","PAC-Bayes theory"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}