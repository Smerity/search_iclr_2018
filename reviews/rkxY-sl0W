{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222626266,"tcdate":1511809023780,"number":3,"cdate":1511809023780,"id":"Bk_WgJqgM","invitation":"ICLR.cc/2018/Conference/-/Paper365/Official_Review","forum":"rkxY-sl0W","replyto":"rkxY-sl0W","signatures":["ICLR.cc/2018/Conference/Paper365/AnonReviewer2"],"readers":["everyone"],"content":{"title":"New but trivial model, poor experiments","rating":"4: Ok but not good enough - rejection","review":"Authors proposed a neural network based machine translation method between two programming languages. The model is based on both source/target syntax trees and performs an attentional encoder-decoder style network over the tree structure.\n\nThe new things in the paper are the task definition and using the tree-style network in both encoder and decoder. Although each structure of encoder/decoder/attention network is based on the application of some well-known components, unfortunately, the paper pays much space to describe them. On the other hand, the whole model structure looks to be easily generalized to other tree-to-tree tasks and might have some potential to contribute this kind of problems.\n\nIn experimental settings, there are many shortages of the description. First, it is unclear that what the linearization method of the syntax tree is, which could affect the final model accuracy. Second, it is also unclear what the method to generate train/dev/test data is. Are those generated completely randomly? If so, there could be many meaningless (e.g., inexecutable) programs in each dataset. What is the reasonableness of training such kind of data, or are they already avoided from the data? Third, the evaluation metrics \"token/program accuracy\" looks insufficient about measuring the correctness of the program because it has sensitivity about meaningless differences between identifier names and some local coding styles.\n\nAuthors also said that CoffeeScript has a succinct syntax and Javascript has a verbose one without any agreement about what the syntax complexity is. Since any CoffeeScript programs can be compiled into the corresponding Javascript programs, we should assume that CoffeeScript is the only subset of Javascript (without physical difference of syntax), and this translation task may never capture the whole tendency of Javascript. In addition, authors had generated the source CoffeeScript codes, which seems that this task is only one of \"synthetic\" task and no longer capture any real world's programs.\nIf authors were interested in the tendency of real program translation task, they should arrange the experiment by collecting parallel corpora between some unrelated programming languages using resources in the real world.\n\nGlobal attention mechanism looks somewhat not suitable for this task. Probably we can suppress the range of each attention by introducing some prior knowledge about syntax trees (e.g., only paying attention to the descendants in a specific subtree).\n\nSuggestion:\nAfter capturing the motivation of the task, I suspect that the traditional tree-to-tree (also X-to-tree) \"statistical\" machine translation methods still can also work correctly in this task. The traditional methods are basically based on the rule matching, which constructs a target tree by selecting source/target subtree pairs and arranging them according to the actual connections between each subtree in the source tree. This behavior might be suitable to transform syntax trees while keeping their whole structure, and also be able to treat the OOV (e.g., identifier names) problem by a trivial modification. Although it is not necessary, it would like to apply those methods to this task as another baseline if authors are interested in.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tree-to-tree Neural Networks for Program Translation","abstract":"Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing neural network approaches toward tackling this problem. We observe that program translation is a modular procedure, in which one sub-tree in the source is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source to guide the expansion of the decoder. We develop two tasks to evaluate the program translation capability of our tree-to-tree model against two state-of-the-art approaches. We observe that our approach is consistently better than the baselines on all tasks and all metrics with a margin of up to 10%. These results demonstrate that the tree-to-tree model is a promising tool for tackling the program translation problem.","pdf":"/pdf/d1bad83399dc2552c9c8a3b9526dc57b2bdb1d9f.pdf","paperhash":"anonymous|treetotree_neural_networks_for_program_translation","_bibtex":"@article{\n  anonymous2018tree-to-tree,\n  title={Tree-to-tree Neural Networks for Program Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkxY-sl0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper365/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222626305,"tcdate":1511747881439,"number":2,"cdate":1511747881439,"id":"r1-4-eYlf","invitation":"ICLR.cc/2018/Conference/-/Paper365/Official_Review","forum":"rkxY-sl0W","replyto":"rkxY-sl0W","signatures":["ICLR.cc/2018/Conference/Paper365/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Some interesting components, but overstates contribution","rating":"4: Ok but not good enough - rejection","review":"This paper aims to translate source code from one programming language to another using\na neural network architecture that maps trees to trees. The encoder uses an upward pass of\na Tree LSTM to compute embeddings for each subtree of the input, and then the decoder \nconstructs a tree top-down. As nodes are created in the decoder, a hidden state is passed\nfrom parents to children via an LSTM (one for left children, one for right children), and\nan attention mechanism allows nodes in the decoder to attend to subtrees in the encoder.\n\nExperimentally, the model is applied to two synthetic datasets, where programs in the \nsource domain are sampled from a PCFG and then translated to the target domain with a\nhand-coded translator. The model is then trained on these pairs. Results show that the\nnproposed approach outperforms sequence representations or serialized tree representations\nof inputs and outputs.\n\nPros:\n\n- Nice model which seems to perform well.\n\n- Reasonably clear explanation.\n\nA couple questions about the model:\n\n- the encoder uses only bottom-up information to determine embeddings of subtrees. I wonder \nif top-down information would create embeddings with more useful information for the attention\nin the decoder to pick up on.\n\n- I would be interested to know more details about how the hand-coded translator works. Does\nit work in a context-free, bottom-up fashion? That is, recursively translate two children nodes\nand then compute the translation of the parent as a function of the parent node and\ntranslations of the two children? If so, I wonder what is missing from the proposed model\nthat makes it unable to perfectly solve the first task?\n\nCons:\n\n- Only evaluated on synthetic programs, and PCFGs are known to generate unrealistic programs, \nso we can only draw limited conclusions from the results.\n\n- The paper overstates its novelty and doesn't properly deal with related work (see below)\n\nThe paper overstates its novelty and has done a poor job researching related work. \nStatements like \"We are the first to consider employing neural network approaches \ntowards tackling the problem [of translating between programming languages]\" are\nobviously not true (surely many people have *considered* it), and they're particularly\ngrating when the treatment of related  work is poor, as it is in this paper. For example, \nthere are several papers that frame the code migration problem as one of statistical \nmachine translation (see Sec 4.4 of [1] for a review and citations), but this paper \nmakes no reference to them. Further, [2] uses distributed representations for the purpose \nof code migration, which I would call a \"neural network approach,\" so there's not any \nsense that I can see in which this statement is true. The paper further says, \"To the best \nof our knowledge, this is the first tree-to-tree neural network architecture in the \nliterature.\" This is worded better, but it's definitely not the first tree-to-tree \nneural network. See, e.g., [3, 4, 5], one of which is cited, so I'm confused about \nthis claim.\n\nIn total, the model seems clean and somewhat novel, but it has only been tested on \nunrealistic synthetic data, the framing with respect to related work is poor, and the\ncontributions are overstated.\n\n\n[1] https://arxiv.org/abs/1709.06182\n[2]  Trong Duc Nguyen, Anh Tuan Nguyen, and Tien N Nguyen. 2016b. Mapping API elements for code migration with\nvector representations. In Proceedings of the International Conference on Software Engineering (ICSE).\n[3] Socher, Richard, et al. \"Semi-supervised recursive autoencoders for predicting sentiment distributions.\" Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2011.\n[4] https://arxiv.org/abs/1703.01925\n[5] Parisotto, Emilio, et al. \"Neuro-symbolic program synthesis.\" arXiv preprint arXiv:1611.01855 (2016).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tree-to-tree Neural Networks for Program Translation","abstract":"Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing neural network approaches toward tackling this problem. We observe that program translation is a modular procedure, in which one sub-tree in the source is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source to guide the expansion of the decoder. We develop two tasks to evaluate the program translation capability of our tree-to-tree model against two state-of-the-art approaches. We observe that our approach is consistently better than the baselines on all tasks and all metrics with a margin of up to 10%. These results demonstrate that the tree-to-tree model is a promising tool for tackling the program translation problem.","pdf":"/pdf/d1bad83399dc2552c9c8a3b9526dc57b2bdb1d9f.pdf","paperhash":"anonymous|treetotree_neural_networks_for_program_translation","_bibtex":"@article{\n  anonymous2018tree-to-tree,\n  title={Tree-to-tree Neural Networks for Program Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkxY-sl0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper365/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222626345,"tcdate":1511634669638,"number":1,"cdate":1511634669638,"id":"BkLxPNweG","invitation":"ICLR.cc/2018/Conference/-/Paper365/Official_Review","forum":"rkxY-sl0W","replyto":"rkxY-sl0W","signatures":["ICLR.cc/2018/Conference/Paper365/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper presents a tree-to-tree neural network for translating programs\nwritten in one Programming language to another. The model uses soft attention\nmechanism to locate relevant sub-trees in the source program tree when \ndecoding to generate the desired target program tree. The model is evaluated\non two sets of datasets and the tree-to-tree model outperforms seq2tree and\nseq2seq models significantly for the program translation problem.\n\nThis paper is the first to suggest the tree-to-tree network and an interesting\napplication of the network for the program translation problem. The evaluation\nresults demonstrate the benefits of having both tree-based encoder and decoder. \nThe tree encoder, however, is based on the standard Tree-LSTM and the application\nin this case is synthetic as the datasets are generated using a manual rule-based \ntranslation. \n\nQuestions/Comments for authors:\n\nThe current examples are generated using a manually developed rule-based system. \nAs the authors also mention it might be challenging to obtain the aligned examples\nfor training the model in practice. What is the intended use case then of \ntraining the model when the perfect rule-based system is already available?\n\nHow complex are the rules for translating the programs for the two datasets and what\ntype of logic is needed to write such rules? It would be great if the authors can \nprovide the rules used to generate the dataset to better understand the complexity\nof the translation task.\n\nThere are several important details missing regarding the baselines. For the \nseq2seq and seq2tree baseline models, are bidirectional LSTMs used for the encoder?\nWhat type of attention mechanisms are used? Are the hyper-parameters presented in\nTable 1 based on best training performance?\n\nIn section 4.3, it is mentioned that the current models are trained and tested on \nprograms of length 20 and 50. Does the dataset contain programs of length upto \n20/50 or exactly of length 20/50? How is program length defined -- in terms of \ntree nodes or the number of lines in the program?\n\nWhat happens if the models trained with programs upto length 20 are evaluated on \nprograms of larger length say 40? It would be interesting to observe the \ngeneralization capabilities of all the different models.\n\nThere are two benefits of using the tree2tree model: i) use the grammar of the\nlanguage, and ii) use the structure of the tree for locating relevant sub-trees\n(using attention). From the current evaluation results, the empirical benefit\nof using the attention is not clear. How would the accuracies look when using \nthe tree2tree model without attention or when attention vector e_t is set to the\nhidden state h of the expanding node?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tree-to-tree Neural Networks for Program Translation","abstract":"Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing neural network approaches toward tackling this problem. We observe that program translation is a modular procedure, in which one sub-tree in the source is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source to guide the expansion of the decoder. We develop two tasks to evaluate the program translation capability of our tree-to-tree model against two state-of-the-art approaches. We observe that our approach is consistently better than the baselines on all tasks and all metrics with a margin of up to 10%. These results demonstrate that the tree-to-tree model is a promising tool for tackling the program translation problem.","pdf":"/pdf/d1bad83399dc2552c9c8a3b9526dc57b2bdb1d9f.pdf","paperhash":"anonymous|treetotree_neural_networks_for_program_translation","_bibtex":"@article{\n  anonymous2018tree-to-tree,\n  title={Tree-to-tree Neural Networks for Program Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkxY-sl0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper365/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739341716,"tcdate":1509106039829,"number":365,"cdate":1509739339061,"id":"rkxY-sl0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkxY-sl0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Tree-to-tree Neural Networks for Program Translation","abstract":"Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to consider employing neural network approaches toward tackling this problem. We observe that program translation is a modular procedure, in which one sub-tree in the source is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network as an encoder-decoder architecture to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source to guide the expansion of the decoder. We develop two tasks to evaluate the program translation capability of our tree-to-tree model against two state-of-the-art approaches. We observe that our approach is consistently better than the baselines on all tasks and all metrics with a margin of up to 10%. These results demonstrate that the tree-to-tree model is a promising tool for tackling the program translation problem.","pdf":"/pdf/d1bad83399dc2552c9c8a3b9526dc57b2bdb1d9f.pdf","paperhash":"anonymous|treetotree_neural_networks_for_program_translation","_bibtex":"@article{\n  anonymous2018tree-to-tree,\n  title={Tree-to-tree Neural Networks for Program Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkxY-sl0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper365/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}