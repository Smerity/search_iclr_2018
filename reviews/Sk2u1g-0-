{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222682618,"tcdate":1511894321370,"number":3,"cdate":1511894321370,"id":"SyK4pmsgG","invitation":"ICLR.cc/2018/Conference/-/Paper544/Official_Review","forum":"Sk2u1g-0-","replyto":"Sk2u1g-0-","signatures":["ICLR.cc/2018/Conference/Paper544/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is a strong paper presenting a novel approach to few-shot eval-time adaptation in non-stationary environments, supported by multiple analyses in two synthetic domains. ","rating":"9: Top 15% of accepted papers, strong accept","review":"---- Summary ----\nThis paper addresses the problem of learning to operate in non-stationary environments, represented as a Markov chain of distinct tasks. The goal is to meta-learn updates that are optimal with respect to transitions between pairs of tasks, allowing for few-shot execution time adaptation that does not degrade as the environment diverges ever further from the training time task set.\n\nDuring learning, an inner loop iterates iterates over consecutive task pairs. For each pair, (T_i, T_{i+1}) trajectories sampled from T_i are used to construct a local policy that is then used to sample trajectories from T_{i+1}. By calculating the outer-loop policy gradient with respect to expectations of the trajectories sampled from T_i, and the trajectories sampled from T_{i+1} using the locally optimal inner-loop policy, the approach learns updates that are optimal with respect to the Markovian transitions between pairs of consecutive tasks.\n\nThe training time optimization algorithm requires multiple passes through a given sequence of tasks. Since this is not feasible at execution time, the trajectories calculated while solving task T_i are used to calculate updates for task T_{i+1} and these updates are importance weighted w.r.t the sampled trajectories' expectation under the final training-time policy.\n\nThe approach is evaluated on a pair of tasks. In the locomotion task, a six legged agent has to adapt to deal with an increasing inhibition to a pair of its legs. In the new RoboSumo task, agents have to adapt to effectively compete with increasingly competent components, that have been trained for longer periods of time via self-play.\n\nIt is clear that, in the locomotion task, the meta learning strategy maintains performance much more consistently than approaches that adapt through PPO-tracking, or implicitly by maintaining state in the RL^2 approach. This behaviour is less visible in the RoboSumo task (Fig 5.) but it does seem to present. Further experiments show that when the adaptation approaches are forced to fight against each other in 100 round iterated adaptation games, the meta learning strategy is dominant. However, the authors also do point out that this behaviour is highly dependent on the number of episodes allowed in each game, and when the agent can accumulate a large amount of evidence in a given environment the meta learning approach falls behind adaptation through tracking. The bias that allows the agent to learn effectively from few examples precludes it from effectively using many examples.\n\n---- Questions for author ----\nUpdates are performed from \\theta to \\phi_{i+1} rather than from \\phi_i to \\phi_{i+1}. Footnote 2 states that this was due to empirical observations of instability but it also necessitates the importance weight correction during execution time. I would like to know how the authors expect the sample in Eqn 9 to behave in much longer running scenarios, when \\pi_{\\phi} starts to diverge drastically from \\pi_{\\theta} but very few trajectories are available.\n\nThe spider-spider results in Fig. 6 do not support the argument that meta learning is better than PPO tracking in the few-shot regime. Do you have any idea of why this is?\n\n---- Nits ----\nThere is a slight muddiness of notation around the use of \\tau in lines 7 & 9 in  of Algorithm 1. I think it should be edited to line up with the definition given in Eqn. 8. \n\nThe figures in this paper depend excessively and unnecessarily on color. They should be made more printer, and colorblind, friendly.\n\n---- Conclusion ----\nI think this paper would be a very worthy contribution to ICLR. Learning to adapt on the basis of few observations is an important prerequisite for real world agents, and this paper presents a reasonable approach backed up by a suite of informative evaluations. The quality of the writing is high, and the contributions are significant. However, this topic is very much outside of my realm of expertise and I am unfamiliar with the related work, so I am assigning my review a low confidence.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments","abstract":"Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.","pdf":"/pdf/31238e0a6fb7589801d470f1f8ad9ea94c1e472c.pdf","paperhash":"anonymous|continuous_adaptation_via_metalearning_in_nonstationary_and_competitive_environments","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk2u1g-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper544/Authors"],"keywords":["reinforcement learning","nonstationarity","meta-learning","transfer learning","multi-agent"]}},{"tddate":null,"ddate":null,"tmdate":1512222682655,"tcdate":1511844659250,"number":2,"cdate":1511844659250,"id":"BJiNow9gG","invitation":"ICLR.cc/2018/Conference/-/Paper544/Official_Review","forum":"Sk2u1g-0-","replyto":"Sk2u1g-0-","signatures":["ICLR.cc/2018/Conference/Paper544/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review for Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments","rating":"7: Good paper, accept","review":"This paper proposed a gradient-based meta-learning approach for continuous adaptation in nonstationary and adversarial environment. The idea is to treat a nonstationary task as a sequence of stationary tasks and train agents to exploit the dependencies between consecutive tasks such that they can deal with nonstationarities at test time. The proposed method was evaluated based on a nonstationary locomotion and within a competitive multi agent setting. For the later, this paper specifically designed the RomoSumo environment and defined iterated adaptation games to test various aspect of adaptation strategies. The empirical results in both cases demonstrate the efficacy of the proposed meta-learned adaptation rules over the baselines in the few-short regime. The superiority of meta-learners is further justified on a population level.\n\nThe paper addressed a very important problem for general AI and it is well-written. Careful experiment designs, and thorough comparisons make the results conniving. I\n\nFurther comments:\n\n1. In the experiment the trajectory number seems very small, I wonder if directly using importance weight as shown in (9) will cause high variance in the performance?\n\n2. One of the assumption in this work is that trajectories from T_i contain some information about T_{i+1}, I wonder what will happen if the mutually information is very small between them (The extreme case is that two tasks are independent), will current method still perform well?\n\nP7, For the RL^2 policy, the authors mentioned that “…with a given environment (or an opponent), reset the state once the latter changes” How does the agent know when an environment (or opponent) changes? \n\nP10, “This suggests that it meta-learned a particular…” This sentence need to be rewritten.\n\nP10, ELO is undefined\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments","abstract":"Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.","pdf":"/pdf/31238e0a6fb7589801d470f1f8ad9ea94c1e472c.pdf","paperhash":"anonymous|continuous_adaptation_via_metalearning_in_nonstationary_and_competitive_environments","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk2u1g-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper544/Authors"],"keywords":["reinforcement learning","nonstationarity","meta-learning","transfer learning","multi-agent"]}},{"tddate":null,"ddate":null,"tmdate":1512222682701,"tcdate":1511546812776,"number":1,"cdate":1511546812776,"id":"ryBakJUlz","invitation":"ICLR.cc/2018/Conference/-/Paper544/Official_Review","forum":"Sk2u1g-0-","replyto":"Sk2u1g-0-","signatures":["ICLR.cc/2018/Conference/Paper544/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"8: Top 50% of accepted papers, clear accept","review":"This is a dense, rich, and impressive paper on rapid meta-learning. It is already highly polished, so I have mostly minor comments.\n\nRelated work: I think there is a distinction between continual and life-long learning, and I think that your proposed setup is a form of continual learning (see Ring ‘94/‘97). Given the proliferation of terminology for very related setups, I’d encourage you to reuse the old term.\n\nTerminology: I find it confusing which bits are “meta” and which are not, and the paper could gain clarity by making this consistent. In particular, it would be good to explicitly name the “meta-loss” (currently the unnamed triple expectation in (3)). By definition, then, the “meta-gradient” is the gradient of the meta-loss -- and not the one in (2), which is the gradient of the regular loss.\n\nNotation: there’s redundancy/inconsistency in the reward definition: pick either R_T or \\bold{r}, not both, and maybe include R_T in the task tuple definition? It is also confusing that \\mathcal{R} is a loss, not a reward (and is minimized) -- maybe use another symbol?\n\nA question about the importance sampling correction: given that this spans multiple (long) trajectories, don’t the correction weights become really small in practice? Do you have some ballpark numbers?\n\nTypos:\n- “event their learning”\n- “in such setting”\n- “experience to for”\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments","abstract":"Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.","pdf":"/pdf/31238e0a6fb7589801d470f1f8ad9ea94c1e472c.pdf","paperhash":"anonymous|continuous_adaptation_via_metalearning_in_nonstationary_and_competitive_environments","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk2u1g-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper544/Authors"],"keywords":["reinforcement learning","nonstationarity","meta-learning","transfer learning","multi-agent"]}},{"tddate":null,"ddate":null,"tmdate":1509739244575,"tcdate":1509126003606,"number":544,"cdate":1509739241911,"id":"Sk2u1g-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk2u1g-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments","abstract":"Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.","pdf":"/pdf/31238e0a6fb7589801d470f1f8ad9ea94c1e472c.pdf","paperhash":"anonymous|continuous_adaptation_via_metalearning_in_nonstationary_and_competitive_environments","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk2u1g-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper544/Authors"],"keywords":["reinforcement learning","nonstationarity","meta-learning","transfer learning","multi-agent"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}