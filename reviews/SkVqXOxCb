{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222617603,"tcdate":1512170431636,"number":3,"cdate":1512170431636,"id":"HkdTXw1bM","invitation":"ICLR.cc/2018/Conference/-/Paper304/Official_Review","forum":"SkVqXOxCb","replyto":"SkVqXOxCb","signatures":["ICLR.cc/2018/Conference/Paper304/AnonReviewer3"],"readers":["everyone"],"content":{"title":" The paper introduces a new way to address GAN training problems, equating it to potential fields and taking inspiration from electrostatics. ","rating":"7: Good paper, accept","review":"The paper takes an interesting approach to solve the existing problems of GAN training, using Coulomb potential for addressing the learning problem. It is also well written with a clear presentation of the motivation of the problems it is trying to address, the background and proves the optimality of the suggested solution. My understanding and validity of the proof is still an educated guess. I have been through section A.2 , but I'm unfamiliar with the earlier literature on the similar topics so I would not be able to comment on it. \n\nOverall, I think this is a good paper that provides a novel way of looking at and solving problems in GANs. I just had a couple of points in the paper that I would like some clarification on : \n\n* In section 2.2.1 : The notion of the generated a_i not disappearing is something I did not follow. What does it mean for a generated sample to \"not disappear\" ? and this directly extends to the continuity equation in (2). \n\n* In section 1 : in the explanation of the 3rd problem that GANs exhibit i.e.  the generator not being able to generalize the distribution of the input samples, I was hoping if you could give a bit more motivation as to why this happens. I don't think this needs to be included in the paper, but would like to have it for a personal clarification. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1512222617641,"tcdate":1512047579691,"number":2,"cdate":1512047579691,"id":"r1EJEK6lG","invitation":"ICLR.cc/2018/Conference/-/Paper304/Official_Review","forum":"SkVqXOxCb","replyto":"SkVqXOxCb","signatures":["ICLR.cc/2018/Conference/Paper304/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good step forward in improving GANs","rating":"7: Good paper, accept","review":"The authors draw from electrical field dynamics and propose to formulate the GAN learning problem in a way such that generated samples are attracted to training set samples, but repel each other. Optimizing this formulation using gradient descent can be proven to yield only one optimal global Nash equilibrium, which the authors claim allows Coulomb GANs to overcome the \"mode collapse\" issue. Experimental results are reported on image and language modeling tasks, and show that the model can produce very diverse samples, although some samples can consist of somewhat nonsensical interpolations.\n\nThis is a good, well-written paper. It is technically rigorous and empirically convincing. Overall, it presents an interesting approach to overcome the mode collapse problem with GANs. \n\nThe image samples presented -- although of high variability -- are not of very high quality, though, and I somewhat disagree with the claim that \"Coulomb GAN was able to efficiently learn the whole distribution\" (Sec 3.1). At best, it seems to me that the new objective does in fact force the generator to concentrate efforts on learning over the full support of the data distribution, but the lower quality samples and sometimes somewhat bad interpolations seem to suggest to me that it is *not* yet doing so very \"efficiently\". \n\nNonetheless, I think this is an important step forward in improving GANs, and should be accepted for publication.\n\nNote: I did not check all the proofs in the appendix.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1511966163495,"tcdate":1511966163495,"number":3,"cdate":1511966163495,"id":"Hyi0SB3lM","invitation":"ICLR.cc/2018/Conference/-/Paper304/Official_Comment","forum":"SkVqXOxCb","replyto":"ryPND-5gG","signatures":["ICLR.cc/2018/Conference/Paper304/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper304/Authors"],"content":{"title":"Notes about the proof","comment":"Hi! Thanks for your thoughtful comments, we also think Coulomb GANs are an interesting new avenue. For your questions:\n\nad 1:The proof that Eq (16) is a minimum is formally correct: Since the derivative in Eq (18) is != 0 everywhere, the only extreme points we need to check are the boundary. Here, the minimum is at r = 0, as the function increases with r (see Eq 18).\n\nad 2: In order to see the general validity of (21), set epsilon to 0. From (19) follows that the expression is continuously increasing with increasing epsilon. Therefore like with r there is a minimum at the boundary point of \\epsilon=0. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1512222617678,"tcdate":1511825390858,"number":1,"cdate":1511825390858,"id":"BkPeeQ5lf","invitation":"ICLR.cc/2018/Conference/-/Paper304/Official_Review","forum":"SkVqXOxCb","replyto":"SkVqXOxCb","signatures":["ICLR.cc/2018/Conference/Paper304/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Several points should be clarified","rating":"5: Marginally below acceptance threshold","review":"\nIn this paper, the authors interpret the training of GAN by potential field and inspired from which to provide new training procedure for GAN. They claim that under the condition that global optima are achieved for discriminator and generator in each iteration, the Coulomb GAN converges to the global solution. \n\nI think there are several points need to be addressed. \n\n1, I agree that the \"model collapsing\" is due to converging to a local Nash Equilibrium. However, there are more reasons besides the drawback of the loss function, which is emphasized in the paper. Leave the stochastic gradient descent optimization algorithm apart (since most of the neural networks are trained in this way), the parametrization and the richness of discriminator family play a vital role in the model collapsing issue. In fact, even with KL-divergence in which log operation is involved, if one can select reasonable parametrization, e.g., directly handling in function space, the saddle point optimization is convex-concave, which means under the same assumption made in the paper, there is only one global Nash Equilibrium. On the other hand, the richness of the discriminator also important in the training of GAN. I did not get the point about the drawback of III. If indeed as the paper considered in the ideal case, the discriminator is rich enough, III cannot happen. \n\nThe model collapsing is not just because loss function in training GAN. It is caused by the twist of these three issues listed above. Modifying the loss can avoid partially model collapsing, however, it is not appropriate to claim that the proposed algorithm is 'provable'. The assumption in this paper is too restricted, and the discussion is unfair to the existing variants of GAN, e.g., GMMN or Wasserstein GAN, which under some assumptions, there is also only one global Nash Equilibrium. \n\n2, In the training procedure, the discriminator family is important as we discussed. The paper claims that the reason to introduce the extra discriminator is reducing variance. However, such parametrization will introduce bias too. The bias and variance tradeoff should be explicitly discussed here. Ideally, it should contain all the functions formed with Plummer kernel, but not too large (otherwise, it will increase the sample complexity.). Which function family used in the paper is not clear. \n\n\n3, As the authors already realized, the GMMN is one closely related model. It will be more convincing to add the comparison with GMMN. \n\nIn sum, this paper provides an interesting perspective modeling GAN from the potential field, however, there are several issues need to be addressed. I expect to see the reply of the authors regarding the mentioned issues. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1511819055660,"tcdate":1511819055660,"number":4,"cdate":1511819055660,"id":"ryPND-5gG","invitation":"ICLR.cc/2018/Conference/-/Paper304/Public_Comment","forum":"SkVqXOxCb","replyto":"SkVqXOxCb","signatures":["~Emanuele_Sansone1"],"readers":["everyone"],"writers":["~Emanuele_Sansone1"],"content":{"title":"Proof of main theorem 1","comment":"This paper represents the first attempt to formulate GANs as a minimization problem rather than a standard mini-max. \n\nThe overall idea is interesting, but I have some concerns regarding the proof of convergence to the global Nash equilibrium (proof of the main Theorem 1):\n1. The authors study the function \\nabla^2k(a,b) and claim that its minimum corresponds to Eq. 16. The methodology to check the minimality of the function (Eqs. 18-19) is not conventional. One should compute the Hessian matrix and then see if it is positive (semi-)definite.\n2. The last inequality in Eq. 21 is not valid in general. The authors should specify the conditions of validity for that. Note that the bound of \\nabla^2k(a,b) is obtained by setting \\epsilon to zero. One can find non-zero values of  \\epsilon for which the equality does not hold.\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1510317931426,"tcdate":1510317931426,"number":2,"cdate":1510317931426,"id":"SkQ_1mQ1f","invitation":"ICLR.cc/2018/Conference/-/Paper304/Official_Comment","forum":"SkVqXOxCb","replyto":"S1wB0slyf","signatures":["ICLR.cc/2018/Conference/Paper304/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper304/Authors"],"content":{"title":"Clarification","comment":"Hi! Interesting to hear that you're facing a similar argument! First off: Note that in Coulomb GANs, the generator objective does not include a log; but that's just a minor note.\n\nIf I get you right, you're saying that the Generator might get stuck. This is indeed true for generators that haven't got enough capacity to move points freely along the field generated by the potential (e.g. a super small generator, this is why we have Assumption A1 in Theorem 2). But as long as the generator can still move it's points, Theorem 1 guarantees that there are no local NE.\n\nBut I'm not sure I understood you correctly: another interpretation of your question is that d_G / d_Theta = 0 for all possible z_i, but that's only possible if G is a constant function. Is that what you meant?\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1510157887408,"tcdate":1510157887408,"number":3,"cdate":1510157887408,"id":"S1wB0slyf","invitation":"ICLR.cc/2018/Conference/-/Paper304/Public_Comment","forum":"SkVqXOxCb","replyto":"rJDaKf0Rb","signatures":["~Leon_Boellmann1"],"readers":["everyone"],"writers":["~Leon_Boellmann1"],"content":{"title":"Follow up question on local NE","comment":"Dear authors,\nI think about the argument of the local NEs and still have a couple questions. The generator objective function is to minimize f = - \\sum_j log(D(G(z_j))). We take derivative of this w.r.t. the network parameters theta_g, by chain rule we have \ndf/d theta =  - \\sum_j (1/D(G(z_j))) * (d D(x) / dx |x = G(z_j))  *(dG /d theta).\n\nFor traditional GANs, (d D(x) / dx |x = G(z_j))  = 0, which yields the gradients equal to zero. I think Coulomb GAN makes  (d D(x) / dx |x = G(z_j)) non-zero whenever pg is not equal to pd. However, the third term (dG /d theta) could still be zero. We recently came up with a new training algorithm, which exactly faces the same problem. \n\nAccording to my understanding, the proposed GAN considerably reduces the local NEs but not remove all local NEs, because this problem is fundamentally a nonconvex problem. Please let me know if my understanding is correct. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1509988799318,"tcdate":1509988799318,"number":2,"cdate":1509988799318,"id":"rJDaKf0Rb","invitation":"ICLR.cc/2018/Conference/-/Paper304/Public_Comment","forum":"SkVqXOxCb","replyto":"BJI5By0RZ","signatures":["~Leon_Boellmann1"],"readers":["everyone"],"writers":["~Leon_Boellmann1"],"content":{"title":"Thanks a lot! ","comment":"Thanks a lot! I think it is a very good paper. I hope it will be accepted. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1510092427557,"tcdate":1509975437842,"number":1,"cdate":1509975437842,"id":"BJI5By0RZ","invitation":"ICLR.cc/2018/Conference/-/Paper304/Official_Comment","forum":"SkVqXOxCb","replyto":"Sy5L6uoC-","signatures":["ICLR.cc/2018/Conference/Paper304/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper304/Authors"],"content":{"title":"Coulomb GANs have no Local Nash Equlibria","comment":"Hi, thank you for your comment. You are right, as we discussed within the paper, MMD is indeed the closest existing work, and the two differences you pointed out are exactly the two fundamental key novelties of our approach, which allow us to make theoretical guarantees that go over what MMD claim.  To your specific questions:\n\n1. Calculating the true Phi(...) would involve calculating the pairwise interactions between all pairs of training set data points, which is not feasible. Averaging over many \\hat{Phi} (i.e., the potential function spanned by only a single mini-batch), is also very difficult:  Even averaging over many mini-batches is infeasible because of the high variance in approximating the whole field when using only a subset of samples. The key idea is to \"store\" the average field in the discriminator which includes also a smoothing effect and tracks the change of the field. Since the field created by the previous generator is close to the actual field, the discriminator can track the current field. We would expect that MMD approaches would also benefit greatly from adapting this approach. To the best of our knowledge, no MMD paper has ever made this connection and proposed this solution to the problem (from which MMDs do clearly suffer).\n\n2.  Yes, you are right, our kernel is very different from the Gaussian Kernel that is used in MMD. In fact, as we show in Theorem 1,  our kernel can guarantee that we learn the correct distribution (if points are freely movable). Moreover, Hochreiter & Obermayer (2001) show that Gaussian Kernels *do not* guarantee convergence to a unique solution. So our choice of kernel is a very crucial improvement over an MMD approach.\n\n3. This is a tricky point, and we might not have explained this well in the paper: Yes, the original GAN by Goodfellow has a unique Nash Equilibrium when the two distributions match perfectly. This is a true Nash Equlibrium according to the original definition: neither D or G can improve on their own strategy. However, due to the way we train GANs (using gradient based methods), there are also many local Nash Equilibria: situations where neither D or G can improve their own strategy within their local environments. They have to follow their gradients and cannot \"jump\" out of this local environment. We describe an example of this in appendix A1 of the paper (Mode collapse is a special case of such a local optimum). These are not Nash Equilibria in a global sense as assumed in the original definition, as better strategies for both players exist; but those strategies are unreachable with gradient based methods.\n\nTo put it another way: What we ultimately want is to match two distributions. The optimization problem created by Goodfellow's GAN is littered with many local Nash Equilibria (where the distributions don't match but gradients vanish) where optimization will get stuck, even though we know that there is a global NE (where the distributions match) somewhere else. The Coulomb GAN's error surface does not contain such local Nash Equilibria. You cannot construct a situation where a Coulomb GAN's gradients vanish unless you are at the unique global optimum.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1509817681581,"tcdate":1509817681581,"number":1,"cdate":1509817681581,"id":"Sy5L6uoC-","invitation":"ICLR.cc/2018/Conference/-/Paper304/Public_Comment","forum":"SkVqXOxCb","replyto":"SkVqXOxCb","signatures":["~Leon_Boellmann1"],"readers":["everyone"],"writers":["~Leon_Boellmann1"],"content":{"title":"Fundamentally the same with Moment Matching Networks","comment":"This paper is interesting. It relates the GAN learning game to potential field. It seems to me it is fundamentally the same as Moment Matching Networks, in the sense that it aims to minimize the distance between two distributions. In particular, I have the following questions:\n\n1. The discriminator aims to learn the potential function Phi. The optimal D(.) = Phi(.). Since we already know the expression of Phi, why don't we just use Phi for the generator optimization in eq (21)?\n\n2. If the answer to question (1) is affirmative. Then the formulation is exactly the same as \"Generative Moment Matching Networks\", where the generator is updated to minimize some distance between two distributions. The only difference is that in GMM networks, Phi(.) is in the form of kernel, while in this paper Phi(.) is in the form of potential function given by eq. (1). \n\n3. Theorem 2 and the title shows that the proposed GAN converges to the optimal Nash equilibrium. This holds in the condition that the objective functions can reach global minimum. For the original GAN proposed by Ian Goodfellow, if the optimization is with respective to D(.) and pg(.) in the function space, the Nash equilibrium is also unique! \n\nGenerally, one of the main challenges of GAN is that it is not convex-concave in the network parameters. If it is formulated as optimization in terms of the function space D(.) and pg(.), the local optimum is exactly the global one with D(.)=1/2 and pg=pd. The global nash equilibrium of Coulomb GAN is important, but is not a major contributions, considering that all other GANs have a unique Nash equilibrum in D(.) and pg(.). The major contribution of this paper is to provide a new perspective of formulating GAN, which will enable us to borrow the ideas from other fields to design systematic training methods or innovative GAN structures."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]}},{"tddate":null,"ddate":null,"tmdate":1509739375303,"tcdate":1509094283599,"number":304,"cdate":1509739372648,"id":"SkVqXOxCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkVqXOxCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields","abstract":"Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field  of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on LSUN bedrooms, CelebA faces, CIFAR-10 and the Google Billion Word text generation.","pdf":"/pdf/81a09a3d15b8584d3d4fbf42c7ec7b6efcf43388.pdf","TL;DR":"Coulomb GANs can optimally learn a distribution by posing the distribution learning problem as optimizing a potential field","paperhash":"anonymous|coulomb_gans_provably_optimal_nash_equilibria_via_potential_fields","_bibtex":"@article{\n  anonymous2018coulomb,\n  title={Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkVqXOxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper304/Authors"],"keywords":["Deep Learning","Generative Adversarial Network","GAN","Generative Model","Potential Field"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}