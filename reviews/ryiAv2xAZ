{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222634239,"tcdate":1512108863682,"number":3,"cdate":1512108863682,"id":"By_HQdCeG","invitation":"ICLR.cc/2018/Conference/-/Paper393/Official_Review","forum":"ryiAv2xAZ","replyto":"ryiAv2xAZ","signatures":["ICLR.cc/2018/Conference/Paper393/AnonReviewer1"],"readers":["everyone"],"content":{"title":"simple, effective method, some discussion/understanding missing","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new method of detecting in vs. out of distribution samples. Most existing approaches for this deal with detecting out of distributions at *test time* by augmenting input data and or temperature scaling the softmax and applying a simple classification rule based on the output. This paper proposes a different approach (with could be combined with these methods) based on a new training procedure. \n\nThe authors propose to train a generator network in combination with the classifier and an adversarial discriminator. The generator is trained to produce images that (1) fools a standard GAN discriminator and (2) has high entropy (as enforced with the pull-away term from the EBGAN). Classifier is trained to not only maximize classification accuracy on the real training data but also to output a uniform distribution for the generated samples. \n\nThe model is evaluated on CIFAR-10 and SVNH, where several out of distribution datasets are used in each case. Performance gains are clear with respect to the baseline methods.\n\nThis paper is clearly written, proposes a simple model and seems to outperform current methods. One thing missing is a discussion of how this approach is related to semi-supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. \n\n I have some clarifying questions below:\n- Figure 4 is unclear: does \"Confidence loss with original GAN\" refer to the method where the classifier is pretrained and then \"Joint confidence loss\" is with joint training? What does \"Confidence loss (KL on SVHN/CIFAR-10)\" refer to?\n\n- Why does the join training improve the ability of the model to generalize to out-of-distribution datasets not seen during training?\n\n- Why is the pull away term necessary and how does the model perform without it? Most GAN models are able to stably train without such explicit terms such as the pull away or batch discrimination. Is the proposed model unstable without the pull-away term? \n\n- How does this compare with a method whereby instead of pushing the fake sample's softmax distribution to be uniform, the model is simply a trained to classify them as an additional \"out of distribution\" class? This exact approach has been used to do semi supervised learning with GANS [1][2]. More generally, could the authors comment on how this approach is related to these semi-supervised approaches? \n\n- Did you try combining the classifier and discriminator into one model as in [1][2]?\n\n[1] Semi-Supervised Learning with Generative Adversarial Networks (https://arxiv.org/abs/1606.01583)\n[2] Good Semi-supervised Learning that Requires a Bad GAN (https://arxiv.org/abs/1705.09783)","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples","abstract":"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.","pdf":"/pdf/c8f106bb78ff7263110a10c2687ba54d3fca736a.pdf","paperhash":"anonymous|training_confidencecalibrated_classifiers_for_detecting_outofdistribution_samples","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryiAv2xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper393/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222634281,"tcdate":1511819751148,"number":2,"cdate":1511819751148,"id":"B1klq-5lG","invitation":"ICLR.cc/2018/Conference/-/Paper393/Official_Review","forum":"ryiAv2xAZ","replyto":"ryiAv2xAZ","signatures":["ICLR.cc/2018/Conference/Paper393/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting idea for robust classification","rating":"7: Good paper, accept","review":"The manuscript proposes a generative approach to detect which samples are within vs. out of the sample space of the training distribution. This distribution is used to adjust the classifier so it makes confident predictions within sample, and less confident predictions out of sample, where presumably it is prone to mistakes. Evaluation on several datasets suggests that accounting for the within-sample distribution in this way can often actually improve evaluation performance, and can help the model detect outliers.\n\nThe manuscript is reasonably well written overall, though some of the writing could be improved e.g. a clearer description of the cost function in section 2. However, equation 4 and algorithm 1 were very helpful in clarifying the cost function. The manuscript also does a good job giving pointers to related prior work. The problem of interest is timely and important, and the provided solution seems reasonable and is well evaluated.\n\nLooking at the cost function and the intuition, the difference in figure 1 seems to be primarily due to the relative number of samples used during optimization -- and not to anything inherent about the distribution as is claimed. In particular, if a proportional number of samples is generated for the 50x50 case, I would expect the plots to be similar. I suggest the authors modify the claim of figure 1 accordingly.\n\nAlong those lines, it would be interesting if instead of the uniform distribution, a model that explicitly models within vs. out of sample might perform better? Though this is partially canceled out by the other terms in the optimization.\n\nFinally, the authors claim that the PT is approximately equal to entropy. The cited reference (Zhao et. al. 2017) does not justify the claim. I suggest the authors remove this claim or correctly justify it.\n\nQuestions:\n - Could the authors comment on cases where such a strong within-sample assumption may adversely affect performance?\n - Could the authors comment on how the modifications affect prediction score calibration?\n - Could the authors comment on whether they think the proposed approach may be more resilient to adversarial attacks?\n\nMinor issues:\n - Figure 1 is unclear using dots. Perhaps the authors can try plotting a smoothed decision boundary to clarify the idea?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples","abstract":"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.","pdf":"/pdf/c8f106bb78ff7263110a10c2687ba54d3fca736a.pdf","paperhash":"anonymous|training_confidencecalibrated_classifiers_for_detecting_outofdistribution_samples","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryiAv2xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper393/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222634326,"tcdate":1511818946645,"number":1,"cdate":1511818946645,"id":"B1ja8-9lf","invitation":"ICLR.cc/2018/Conference/-/Paper393/Official_Review","forum":"ryiAv2xAZ","replyto":"ryiAv2xAZ","signatures":["ICLR.cc/2018/Conference/Paper393/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea, but not yet convinced","rating":"5: Marginally below acceptance threshold","review":"This paper presents a novel approach to calibrate classifiers for out of distribution samples. In additional to the original cross entropy loss, the “confidence loss”  was proposed to guarantee the out of distribution points have low confidence in the classifier. As out of distribution samples are hard to obtain, authors also propose to use GAN generating “boundary” samples as out of distribution samples. \n\nThe problem setting is new and objective (1) is interesting and reasonable. However, I am not very convinced that objective (3) will generate boundary samples. Suppose that theta is set appropriately so that p_theta (y|x) gives a uniform distribution over labels for out of distribution samples. Because of the construction of U(y), which uniformly assign labels to generated out of distribution samples, the conditional probability p_g (y|x) should always be uniform so p_g (y|x) divided by p_theta (y|x) is almost always 1. The KL divergence in (a) of (3) should always be approximately 0 no matter what samples are generated. \n\nI also have a few other concerns: \n1. There seems to be a related work: \n[1] Perello-Nieto et al., Background Check: A general technique to build more reliable and versatile classifiers, ICDM 2016, \nWhere authors constructed a classifier, which output K+1 labels and the K+1-th label is the “background noise” label for this classification problem. Is the method in [1] applicable to this paper’s setting?  Moreover, [1] did not seem to generate any out of distribution samples. \n\n2. I am not so sure that how the actual out of distribution detection was done (did I miss something here?). Authors repeatedly mentioned “maximum prediction values”, but it was not defined throughout the paper. \nAlgorithm 1. is called “minimization for detection and generating out of distribution (samples)”, but this is only gradient descent, right? I do not see a detection procedure. Given the title also contains “detecting”, I feel authors should write explicitly how the detection is done in the main body. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples","abstract":"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.","pdf":"/pdf/c8f106bb78ff7263110a10c2687ba54d3fca736a.pdf","paperhash":"anonymous|training_confidencecalibrated_classifiers_for_detecting_outofdistribution_samples","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryiAv2xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper393/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739327305,"tcdate":1509111762954,"number":393,"cdate":1509739324644,"id":"ryiAv2xAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryiAv2xAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples","abstract":"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.","pdf":"/pdf/c8f106bb78ff7263110a10c2687ba54d3fca736a.pdf","paperhash":"anonymous|training_confidencecalibrated_classifiers_for_detecting_outofdistribution_samples","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryiAv2xAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper393/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}