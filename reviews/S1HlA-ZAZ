{"notes":[{"tddate":null,"ddate":null,"tmdate":1512277740512,"tcdate":1512277740512,"number":3,"cdate":1512277740512,"id":"r14ew-W-G","invitation":"ICLR.cc/2018/Conference/-/Paper738/Official_Review","forum":"S1HlA-ZAZ","replyto":"S1HlA-ZAZ","signatures":["ICLR.cc/2018/Conference/Paper738/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Generalization of the distributed memory","rating":"7: Good paper, accept","review":"This paper generalizes the sparse distributed memory model of Kanerva to the Kanerva Machine by formulating a variational generative model of episodes with memory as the prior. \n\nPlease discuss the difference from other papers that implement memory as a generative model, i.e. (Bornschein, Mnih, Zoran, Rezende 2017)\n\nA probabilistic interpretation of Kanerva’s model was given before (Anderson, 1989 http://ieeexplore.ieee.org/document/118597/ ) and (Abbott, Hamrick, Griffiths, 2013). Please discuss.\n\nI found the relation to Kanerva’s original model interesting and well explained. The original model was motivated by human long term memory and neuroscience. It would be nice if the authors can provide what neuroscience implications their work has, and comment on its biological plausibility.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Kanerva Machine: A Generative Distributed Memory","abstract":"We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva’s sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.","pdf":"/pdf/98b9042b0e1316d0eb941563e78db9c5202dedf6.pdf","TL;DR":"A generative memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory.","paperhash":"anonymous|the_kanerva_machine_a_generative_distributed_memory","_bibtex":"@article{\n  anonymous2018the,\n  title={The Kanerva Machine: A Generative Distributed Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1HlA-ZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper738/Authors"],"keywords":["memory","generative model","inference","neural network","hierarchical model"]}},{"tddate":null,"ddate":null,"tmdate":1512222738141,"tcdate":1511862155161,"number":2,"cdate":1511862155161,"id":"HJ7qJh9eM","invitation":"ICLR.cc/2018/Conference/-/Paper738/Official_Review","forum":"S1HlA-ZAZ","replyto":"S1HlA-ZAZ","signatures":["ICLR.cc/2018/Conference/Paper738/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Kanerva Machine Review","rating":"7: Good paper, accept","review":"The paper presents the Kanerva Machine, extending an interesting older conceptual memory model to modern usage. The review of Kanerva’s sparse distributed memory in the appendix was appreciated. While the analyses and bounds of the original work were only proven when restricted to uniform and binary data, the extensions proposed bring it to modern domain of non-uniform and floating point data.\n\nThe iterative reading mechanism which provides denoising and reconstruction when within tolerable error bounds, whilst no longer analytically provable, is well shown experimentally.\nThe experiments and results on Omniglot and CIFAR provide an interesting insight to the model's behaviour with the comparisons to VAE and DNC also seem well constructed.\n\nThe discussions regarding efficiency and potential optimizations of writing inference model were also interesting and indeed the low rank approximation of U seems an interesting future direction.\n\nOverall I found the paper well written and reintroduced + reframed a relatively underutilized but well theoretically founded model for modern use.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Kanerva Machine: A Generative Distributed Memory","abstract":"We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva’s sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.","pdf":"/pdf/98b9042b0e1316d0eb941563e78db9c5202dedf6.pdf","TL;DR":"A generative memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory.","paperhash":"anonymous|the_kanerva_machine_a_generative_distributed_memory","_bibtex":"@article{\n  anonymous2018the,\n  title={The Kanerva Machine: A Generative Distributed Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1HlA-ZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper738/Authors"],"keywords":["memory","generative model","inference","neural network","hierarchical model"]}},{"tddate":null,"ddate":null,"tmdate":1512222738182,"tcdate":1511818504432,"number":1,"cdate":1511818504432,"id":"rJlzr-5lM","invitation":"ICLR.cc/2018/Conference/-/Paper738/Official_Review","forum":"S1HlA-ZAZ","replyto":"S1HlA-ZAZ","signatures":["ICLR.cc/2018/Conference/Paper738/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Proposes a deep generative model where the prior distribution consists of a probabilistic memory. Neat idea, little discussion and comparison to related work.","rating":"6: Marginally above acceptance threshold","review":"The generative model comprises a real-valued matrix M (with a multivariate normal prior) that serves\nas the memory for an episode (an unordered set of datapoints). For each datapoint a marginally independent\nlatent variable y_t is used to index into M and realize a conditional density\nof another latent variable z. z_t is used to generate the data.\n\nThe proposal of learning with a probabilistic memory is interesting and the framework proposed is elegant and cleanly explained. The model is evaluated on the following tasks:\n* Qualitative results on denoising and one-shot generation using the Omniglot dataset.\n* Qualitative results on sampling from the model using the CIFAR dataset.\n* Likelihood estimation on the Omniglot dataset\n\nQuestions and concerns: \n\nThe model appears novel and is interesting, the experiments, however, are lacking in that they\ndo not compare against other any recently proposed memory augmented deep generative models [Bornschein et al] and [Li et. al] (https://arxiv.org/pdf/1602.07416.pdf). At the very minimum, the paper should include a discussion and a comparison with the latter. Doing so will help better understand what is gained from using retaining a probabilistic form of memory versus a determinstic memory indexed with attention as in [Li et. al].\n\nHow does the model perform as a function of varying T (size of episodes) during training? It would be interesting to see how well the model performs in the limiting case of T=1.\n\nWhat is the task being solved in Section 4.4 by the DNC and the Kanerva machine? Please state this in the main paper.\n\nTraining and Evaluation: There is a mismatch in the training and evaluation procedure the implications of which I don't\nfully understand yet. The text states that the model was trained where each observation in an episode comprised randomly sampled datapoints. This corresponds to a generative process where (1) a memory is randomly drawn, (2) each observation in the episode is an independent draws from the memory conditioned decoder. During training,\npoints in an episode are randomly selected. At test time, (if I understand correctly, please correct me if I haven't), the model is evaluated by having multiple copies of the same test point within an episode. Is that correct? If so, doesn't that correspond to evaluating the model under a different generative assumption? Why is this OK?\n\nLikelihood evaluation: Could you expand on how the ELBO of 68.3 is computed under the model for a single test image in the Omniglot dataset? The text says that the likelihood of each data-point was divided by T (the length of the episode considered). This seems at odds with models, such as DRAW, evaluate the likelihood -- once at the end of the generative drawing process. What is the per-pixel likelihood obtained on the CIFAR dataset and what is the likelihood on a model where T=1 (for omniglot/cifar)?\n\nUsing Labels: Following up on the previous point, what happens if labelled information from Omniglot or CIFAR is used to define points within an episode during the training procedure? Does this help or hurt performance?\n\nFor the denoising comparison, how do the results compare to those obtained if you simulate a Markov Chain (sample latent state conditioned on noisy image, sample latent state, sample denoised observation, repeat using denoised observation) using a VAE?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Kanerva Machine: A Generative Distributed Memory","abstract":"We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva’s sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.","pdf":"/pdf/98b9042b0e1316d0eb941563e78db9c5202dedf6.pdf","TL;DR":"A generative memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory.","paperhash":"anonymous|the_kanerva_machine_a_generative_distributed_memory","_bibtex":"@article{\n  anonymous2018the,\n  title={The Kanerva Machine: A Generative Distributed Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1HlA-ZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper738/Authors"],"keywords":["memory","generative model","inference","neural network","hierarchical model"]}},{"tddate":null,"ddate":null,"tmdate":1509739131446,"tcdate":1509133804761,"number":738,"cdate":1509739128783,"id":"S1HlA-ZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1HlA-ZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Kanerva Machine: A Generative Distributed Memory","abstract":"We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva’s sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.","pdf":"/pdf/98b9042b0e1316d0eb941563e78db9c5202dedf6.pdf","TL;DR":"A generative memory model that combines slow-learning neural networks and a fast-adapting linear Gaussian model as memory.","paperhash":"anonymous|the_kanerva_machine_a_generative_distributed_memory","_bibtex":"@article{\n  anonymous2018the,\n  title={The Kanerva Machine: A Generative Distributed Memory},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1HlA-ZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper738/Authors"],"keywords":["memory","generative model","inference","neural network","hierarchical model"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}