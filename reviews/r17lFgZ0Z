{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222700772,"tcdate":1511859212182,"number":3,"cdate":1511859212182,"id":"rkEMEscxf","invitation":"ICLR.cc/2018/Conference/-/Paper608/Official_Review","forum":"r17lFgZ0Z","replyto":"r17lFgZ0Z","signatures":["ICLR.cc/2018/Conference/Paper608/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Useful but incremental contribution to NLG/dialog metrics research","rating":"5: Marginally below acceptance threshold","review":"The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. They find weak but statistically significant correlations for a subset of the evaluated metrics, an improvement over the situation that has been observed in open-domain dialog generation.\nOther than the necessarily condensed model section (describing a model explained at greater length in a different work) the paper is quite clear and well-written throughout, and the authors' explication of metrics like BLEU and greedy matching is straightforward and readable. But the novel work in the paper is limited to the human evaluations collected and the correlation studies run, and the authors' efforts to analyze and extend these results fall short of what I'd like to see in a conference paper.\nSome other points:\n1. Where does the paper's framework for response generation (i.e., dialog act vectors and delexicalized/lexicalized slot-value pairs) fit into the landscape of task-oriented dialog agent research? Is it the dominant or state-of-the-art approach?\n2. The sentence \"This model is a variant of the “ld-sc-LSTM” model proposed by Sharma et al. (2017) which is based on an encoder-decoder framework\" is ambiguous; what is apparently meant is that Sharma et al. (2017) introduced the hld-scLSTM, not simply the ld-scLSTM.\n3. What happens to the correlation coefficients when exact reference matches (a significant component of the highly-rated upper right clusters) are removed?\n4. The paper's conclusion naturally suggests the question of whether these results extend to more difficult dialog generation datasets. Can the authors explain why the datasets used here were chosen over e.g. El Asri et al. (2017) and Novikova et al. (2016)?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation","abstract":"Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.","pdf":"/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf","paperhash":"anonymous|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation","_bibtex":"@article{\n  anonymous2018relevance,\n  title={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17lFgZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper608/Authors"],"keywords":["task-oriented dialog","goal-oriented dialog","nlg evaluation","natural language generation","automated metrics for nlg"]}},{"tddate":null,"ddate":null,"tmdate":1512222700814,"tcdate":1511823512671,"number":2,"cdate":1511823512671,"id":"SJWidMceG","invitation":"ICLR.cc/2018/Conference/-/Paper608/Official_Review","forum":"r17lFgZ0Z","replyto":"r17lFgZ0Z","signatures":["ICLR.cc/2018/Conference/Paper608/AnonReviewer1"],"readers":["everyone"],"content":{"title":"borderline","rating":"5: Marginally below acceptance threshold","review":"1) This paper conducts an empirical study of different unsupervised metrics' correlations in task-oriented dialogue generation. This paper can be considered as an extension of Liu, et al, 2016 while the later one did an empirical study in non-task-oriented dialogue generation.  \n\n2)My questions are as follows:\ni) The author should give the more detailed definition of what is non-task-oriented and task-oriented dialogue system. The third paragraph in the introduction should include one use case about non-task-oriented dialogue system, such as chatbots.\nii) I do not think DSTC2 is good dataset here in the experiments. Maybe the dataset is too simple with limited options or the training/testing are very similar to each other, even the random could achieve very good performance in table 1 and 2. For example, the random solution is only 0.005 (out of 1) worse then d-scLSTM, and it also has a close performance compared with other metrics. Even the random could achieve 0.8 (out of 1) in BLEU, this is a very high performance.\niii) About the scatter plot Figure 3, the authors should include more points with a bad metric score (similar to Figure 1 in Liu 2016). \niv) About the correlations in figure b, especially for BLEU and METEOR, I do not think they have good correlations with human's judgments. \nv) BLEU usually correlates with human better when 4 or more references are provided. I suggest the authors include some dataset with 4 or more references instead of just 2 references.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation","abstract":"Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.","pdf":"/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf","paperhash":"anonymous|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation","_bibtex":"@article{\n  anonymous2018relevance,\n  title={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17lFgZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper608/Authors"],"keywords":["task-oriented dialog","goal-oriented dialog","nlg evaluation","natural language generation","automated metrics for nlg"]}},{"tddate":null,"ddate":null,"tmdate":1512222700857,"tcdate":1511171318125,"number":1,"cdate":1511171318125,"id":"Hy0xrQegf","invitation":"ICLR.cc/2018/Conference/-/Paper608/Official_Review","forum":"r17lFgZ0Z","replyto":"r17lFgZ0Z","signatures":["ICLR.cc/2018/Conference/Paper608/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Useful conclusion; not enough novel contribution to warranty publication","rating":"4: Ok but not good enough - rejection","review":"This paper's main thesis is that automatic metrics like BLEU, ROUGE, or METEOR is suitable for task-oriented natural language generation (NLG). In particular, the paper presents a counterargument to \"How NOT To Evaluate Your Dialogue System...\" where Wei et al argue that automatic metrics are not correlated or only weakly correlated with human eval on dialogue generation. The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval.\n\nOverall, this paper presents a useful conclusion: use METEOR for evaluating task oriented NLG. However, there isn't enough novel contribution in this paper to warrant a publication. Many of the details unnecessary: 1) various LSTM model descriptions are unhelpful given the base LSTM model does just as well on the presented tasks 2) Many embedding based eval methods are proposed but no conclusions are drawn from any of these techniques.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation","abstract":"Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.","pdf":"/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf","paperhash":"anonymous|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation","_bibtex":"@article{\n  anonymous2018relevance,\n  title={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17lFgZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper608/Authors"],"keywords":["task-oriented dialog","goal-oriented dialog","nlg evaluation","natural language generation","automated metrics for nlg"]}},{"tddate":null,"ddate":null,"tmdate":1509739204371,"tcdate":1509128426755,"number":608,"cdate":1509739201713,"id":"r17lFgZ0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r17lFgZ0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation","abstract":"Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.","pdf":"/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf","paperhash":"anonymous|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation","_bibtex":"@article{\n  anonymous2018relevance,\n  title={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r17lFgZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper608/Authors"],"keywords":["task-oriented dialog","goal-oriented dialog","nlg evaluation","natural language generation","automated metrics for nlg"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}