{"notes":[{"tddate":null,"ddate":null,"tmdate":1515170538492,"tcdate":1515170538492,"number":5,"cdate":1515170538492,"id":"rJGlj7pmf","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Comment","forum":"HyyP33gAZ","replyto":"r1F_FQBZG","signatures":["ICLR.cc/2018/Conference/Paper403/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper403/Authors"],"content":{"title":"Experiments on Class-Splitting","comment":"We have conducted additional experiments on combining AM-GAN with the class-splitting technique proposed in [1] which is orthogonal to our work and has shown to improve quality of generated samples. Unfortunately, we found that it fails to further improve Inception Score in our setting. This might be due to the fact that the quality of split-classes is not good enough, which largely depends on the features it learns and the clustering algorithm it uses. It requires further investigations to make split-classes more effective and we would leave it as future work.\n\n[1] Guillermo, L. Grinblat, Lucas, C. Uzal, and Pablo, M. Granitto. Class-splitting generative adversarial\nnetworks. arXiv preprint arXiv:1709.07359, 2017.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1512549134064,"tcdate":1512548970580,"number":4,"cdate":1512548970580,"id":"HJ7_9mrWG","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Comment","forum":"HyyP33gAZ","replyto":"HyUOlCNlf","signatures":["ICLR.cc/2018/Conference/Paper403/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper403/Authors"],"content":{"title":"Response","comment":"We sincerely thank you for your constructive feedback. We have revised the paper and fixed the confusing statements. More descriptions about the tables and figures have been added in their captions.\n\n1. Why not characterize the discriminator as a 2K-way classifier (K real vs K fake)?\n\nThis is an interesting idea and we have thought about this originally. However, we did not feel strongly that considering K fake logits would help in our case: \n\na) Introducing specific real class logits in the discriminator makes it possible to assign a specific target class for each generated sample, which provides a clearer guidance to the generator. However, a fake class will not be used as the target for the generated sample. In this sense, how and whether we can benefit from using K specific fake class logits are still unknown.\n\nb) Introducing more fake classes does influence the gradient that the generator receives from the discriminator. When optimizing a generated sample, only the target class logit is encouraged while all the others are otherwise discouraged. Thus, replacing a single fake class with K fake classes changes the discouraged recipient from the overall fake class to the K specific fake classes. It requires further investigations to figure out whether this will help or not. We leave it as our future work.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1512549148460,"tcdate":1512548909919,"number":3,"cdate":1512548909919,"id":"SkIE9XHbG","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Comment","forum":"HyyP33gAZ","replyto":"SJ5g2WcgM","signatures":["ICLR.cc/2018/Conference/Paper403/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper403/Authors"],"content":{"title":"Response","comment":"We sincerely thank you for your comprehensive comments on our paper.\n\n1. What does dynamic/predefined labeling in Table 1 means? How does it apply to LabelGAN? \n\nModels with dynamic labeling and predefined labeling settings require different network structures (G and D’s capacities). The “dynamic” and “predefined” in Tables 1 and 3 represent two experimental settings which differ in network structures. Under the “dynamic” setting, if a model requires specific target class (AC-GAN, AM-GAN), we apply dynamic labeling; under the “predefined” setting, if a model requires specific target class, we apply predefined labeling; for models that do not need target class (GAN, LabelGAN), neither of them are applied and the two (“dynamic” and “predefined”) only differ in network structures. In this way, we compare various models with almost identical network structures. We have revised the caption of Table 1. It should be clear now.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1512549100055,"tcdate":1512548721492,"number":2,"cdate":1512548721492,"id":"r1F_FQBZG","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Comment","forum":"HyyP33gAZ","replyto":"HJZyvxT1G","signatures":["ICLR.cc/2018/Conference/Paper403/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper403/Authors"],"content":{"title":"Response","comment":"We sincerely thank you for your constructive advice on our paper. We have substantially revised our paper according to your comments.\n\n1. Unfocused Presentation:\n\na) Superfluous Preliminary. We shorten the preliminary section and only keep necessary equations that are referred in later sections. \n\nb) Section 6.1 on Inception Score. We have discarded the inessential part of the discussions in Section 6.1 and only kept the definition of Inception Score.\n\nc) Section 5.1 on Dynamic Labeling. Dynamic labeling brings important improvements to AM-GAN, and is applicable to other models that require target class for each generated sample, such as AC-GAN. It is an alternative to predefined labeling, and affects the models largely according to our experiments. As such, we consider it necessary to keep the discussions about dynamic labeling. We have made the point clearer in the revised version.\n\n2. Quantitative Evaluation:\n\na) No Error Bar in Table 1. We have added error bars in Table 1, where AM-GAN still consistently outperforms variants of AC-GAN and LabelGAN by a large margin in terms of both Inception Score and AM Score.\n\nb) Table 2 Shows Small Gains. As shown in Table 2, AM-GAN achieves 8.91±0.11 Inception Score on CIFAR-10, which significantly outperforms all the baseline methods, including Improved GAN (8.09±0.07), AC-GAN (8.25±0.07), WGAN-GP + AC (8.42±0.10) and SGAN (8.59±0.11). When compared to Splitting GAN (8.87±0.09), an orthogonal work to AM-GAN, which enhances the class label information via class splitting, the improvement seems to be less significant. However, since it is orthogonal to AM-GAN, we can combine them to further improve the results. As it takes some time to conduct additional experiments, we would add such results in the later version.\n\n3. MS-SSIM: \n\nSorry for missing the descriptions on MS-SSIM. We actually borrow the usage of MS-SSIM from AC-GAN (Odena et al., 2016) which measures the MS-SSIM scores between a set of randomly sampled pairs of images within a given class and uses the mean of the MS-SSIM scores, where a high mean MS-SSIM indicates intra-class mode collapse or low sample diversity in the class. In our paper, we report the maximum of the mean MS-SSIM over the 10 classes in CIFAR-10, with which we judge whether there exists obvious intra-class mode collapse. We have added corresponding descriptions and citations to MS-SSIM in the new version (the first paragraph in Section 7 and the caption of Table 2).\n\n4. Inception Score as a Diversity Measurement:\n\nA direct answer is that Inception Score, i.e. exp(H(E_x[C(x)])-E_x[H(C(x))]), has two terms and, more importantly, when the generator collapsed to a single point, the first term H(E_x[C(x)]) (the entropy of “the mean classification distribution of all samples”) and the second term E_x[H(C(x))] (the mean classification entropy score for each sample) in Inception Score would be actually equal. Though the second term can be high, the overall Inception Score, in this case, would always be the minimal value 1.0 (exp^0).\n\nIt is worth noting that the first and second terms are highly correlated. As in Section 6.3, we provide an alternative explanation by understanding Inception Score in the KL divergence formulation, i.e. exp(E_x[KL(C(x),E_x[C(x)])]), which involves a single term and can be interpreted as it requires each sample’s distribution to be highly different from the overall distribution of the generator. In this view, it measures the sample diversity. We further demonstrate that Inception Score can capture sample diversity well with synthetic experiments:  assuming the generator perfectly generates a subset of the training data, with the subset growing to cover the entire dataset, Inception Score is monotonically increasing. Please also refer to Sections 6.2 and 6.3 in our paper for more details.\n\nThe comments have been very useful for us to improve our paper, and we have updated our paper according to your valuable review comments. Please check it."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1515642444670,"tcdate":1511820274890,"number":3,"cdate":1511820274890,"id":"SJ5g2WcgM","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Review","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference/Paper403/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Thorough investigation and extension of class-aware GAN approaches","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper is a thorough investigation of various “class aware” GAN architectures. It purposes a variety of modifications on existing approaches and additionally provides extensive analysis of the commonly used Inception Score evaluation metric.\n\nThe paper starts by introducing and analyzing two previous class aware GANs - a variant of the Improved GAN architecture used for semi-supervised results (named Label GAN in this work) and AC-GAN, which augments the standard discriminator with an auxiliary classifier to classify both real and generated samples as specific classes. \n\nThe paper then discusses the differences between these two approaches and analyzes the loss functions and their corresponding gradients. Label GAN’s loss encourages the generator to assign all probability mass cumulatively across the k-different label classes while the discriminator tries to assign all probability mass to the k+1th output corresponding to a “generated” class. The paper views the generators loss as a form of implicit class target loss.\n\nThis analysis motivates the paper’s proposed extension, called Activation Maximization. It corresponds to a variant of Label GAN where the generator is encouraged to maximize the probability of a specific class for every sample instead of just the cumulative probability assigned to label classes. The proposed approach performs strongly according to inception score on CIFAR-10 and includes additional experiments on Tiny Imagenet to further increase confidence in the results.\n\nA discussion throughout the paper involves dealing with the issue of mode collapse - a problem plaguing standard GAN variants. In particular the paper discusses how variants of class conditioning effect this problem. The paper presents a useful experimental finding - dynamic labeling, where targets are assigned based on whatever the discriminator thinks is the most likely label, helps prevent mode collapse compared to the predefined assignment approach used in AC-GAN / standard class conditioning.\n\nI am unclear how exactly predefined vs dynamic labeling is applied in the case of the Label GAN results in Table 1. The definition of dynamic labeling is specific to the generator as I interpreted it. But Label GAN includes no class specific loss for the generator. I assume it refers to the form of generator - whether it is class conditional or not - even though it would have no explicit loss for the class conditional version. It would be nice if the authors could clarify the details of this setup.\n\nThe paper additionally performs a thorough investigation of the inception score and proposes a new metric the AM score. Through analysis of the behavior of the inception score has been lacking so this is an important contribution as well.\n\nAs a reader, I found this paper to be thorough, honest, and thoughtful. It is a strong contribution to the “class aware” GAN literature.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1511764916456,"tcdate":1511764916456,"number":3,"cdate":1511764916456,"id":"rknnXVKxz","invitation":"ICLR.cc/2018/Conference/-/Paper403/Public_Comment","forum":"HyyP33gAZ","replyto":"B1ZnkAYkz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Discussion on Fréchet Inception Distance (FID)","comment":"Thanks for your reference to Fréchet Inception Distance (FID). \n\nFID measures the distance between two distributions using their means and variances after a fixed mapping, e.g. Inception Network, which works well in practice as illustrated in [1]. As another evaluation metric for generative models, we would add discussions in the revision.\n\nA concern on FID is that the mean and variance of the distribution are not sufficient to represent the whole distribution. That is, for any given distribution, we can always design another distribution which is totally different from the given distribution but has the same mean and variance. We are not sure whether this would cause a problem in practice.\n\nAlso, FID is actually orthogonal to Inception Score and AM Score. FID directly measures the distance between generated distribution and real-data distribution, while Inception Score mainly measures the sample diversity and AM Score mainly measures the sample quality.\n\nAs for the failure of Inception Score on CelebA illustrated in [1], according to our analysis, Inception Score works as a diversity measurement and we might need a more suitable classifier (maybe a classifier trained on a face dataset) to make it work on CelebA. FID seems to have the benefit of being not sensitive to the choice of the mapping function, though it also remains uncertain whether the Inception Network is always the best choice as the mapping function for variant models."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1515642444706,"tcdate":1511477358052,"number":2,"cdate":1511477358052,"id":"HyUOlCNlf","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Review","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference/Paper403/AnonReviewer2"],"readers":["everyone"],"content":{"title":"good paper with thorough experiments!","rating":"7: Good paper, accept","review":"+ Pros:\n- The paper properly compares and discusses the connection between AM-GAN and class conditional GANs in the literature (AC-GAN, LabelGAN)\n- The experiments are thorough\n- Relation to activation maximization in neural visualization is also properly mentioned\n- The authors publish code and honestly share that they could not reproduce AC-GAN's results and thus using to its best variant AC-GAN* that they come up with. I find this an important practice worth encouraging!\n- The analysis of Inception score is sound.\n+ Cons:\n- A few presentation/clarity issues as below\n- This paper leaves me wonder why AM-GAN rather than simply characterizing D as a 2K-way classifier (1K real vs 1K fake).\n\n+ Clarity: \nThe paper is generally well-written. However, there are a few places that can be improved:\n- In 2.2, the authors mentioned \"In fact, the above formulation is a modified version of the original AC-GAN..\", which puts readers confusion whether they were previously just discussed AC-GAN or AC-GAN* (because the previous paragraph says \"AC-GAN are defined as..\".\n- Fig. 2: it's not clear what the authors trying to say if looking at only figures and caption. I'd suggest describe more in the caption and follow the concept figure in Odena et al. 2016.\n- A few typos here and there e.g. \"[a]n diversity measurement\"\n\n+ Originality: AM-GAN is an incremental work by applying AM to GAN. However, I have no problems with this.\n+ Significance: \n- Authors show that in quantitative measures, AM-GAN is better than existing GANs on CIFAR-10 / TinyImageNet. Although I don't find much a real difference by visually comparing of samples of AM-GAN to AC-GAN*.\n\nOverall, this is a good paper with thorough experiments supporting their findings regarding AM-GAN and Inception score!","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1515642444741,"tcdate":1510962904891,"number":1,"cdate":1510962904891,"id":"HJZyvxT1G","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Review","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference/Paper403/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of Activation Maximization","rating":"5: Marginally below acceptance threshold","review":"\nI thank the authors for the thoughtful responses and updated manuscript. Although the manuscript is improved, I still feel it is unfocused and may be substantially improved, thus my review score remains unchanged.\n\n===============\n\nThe authors describe a new version of a generative adversarial network (GAN) for generating images that is heavily related to class-conditional GAN's. The authors highlight several additional results on evaluation metrics and demonstrate some favorable analyses using their new proposed GAN.\n\nMajor comments:\n1) Unfocused presentation. The paper presents a superfluous and extended background section that needs to be cut down substantially. The authors should aim for a concise presentation of their work in 8 pages. Additionally, the authors present several results (e.g. Section 5.1 on dynamic labeling, Section 6.1 on Inception score) that do not appear to improve the results of the paper, but merely provide commentary. The authors should either defend why these sections are useful or central to the arguments in the paper; otherwise, remove them.\n\n2) Quantitative evaluation highlight small gains. The gains in Table 1 seem to be quite small and additionally there are no error bars so it is hard to assess what is statistically meaningful. Table 2 highlights some error bars but again the gains some quite small. Given that the AM-GAN seems like a small change from an AC-GAN model, I am not convinced there is much gained using this model.\n\n3) MS-SSIM. The authors' discussion of MS-SSIM is fairly confusing. MS-SSIM is a measure of image similarity between a pair of images. However, the authors quote an MS-SSIM for various GAN models in Table 3. What does this number mean?  I suspect the authors are calculating some cumululative statistics across many images, but I was not able to find a description, nor understand what these statistics mean.\n\n4) 'Inception score as a diversity measurement.' This argument is not clear to me. Inception scores can be quite high for an individual image indicating that the image 'looks' like a given class in a discriminative model.  If a generative model always generates a single, good image of a 'dog', then the classification score would be quite high but the generative model would be very poor because the images are not diverse. Hence, I do not see how the inception score captures this property.\n\nIf the authors can address all of these points in a substantive manner, I would consider raising my rating.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1510756264583,"tcdate":1510756264583,"number":2,"cdate":1510756264583,"id":"B1ZnkAYkz","invitation":"ICLR.cc/2018/Conference/-/Paper403/Public_Comment","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Fréchet Inception Distance (FID) to evaluate GANs","comment":"[1] proposed the Fréchet Inception Distance (FID) to evaluate GANs which is the Fréchet distance aka Wasserstein-2 distance between the real world and generated samples statistics.  The statistics consists of the first two moments such that the sample quality (first moment match) and variation (second moment match) are covered.\n\nAs highlighted here in Section 6.2, for datasets not covering all ImageNet classes e.g. celebA, CIFAR-10 etc, the entropy of E_x~G[C(x)] is going down not up as soon as a trained GAN starts producing correctly samples falling only in some of the ImageNet classes.  [1] also showed inconistent behaviour of the Inception Score in their experiments (see Appendix A1). Especially interesting here is experiment 6 where a dataset (celebA) is increasingly mixed with ImageNet samples. The Inception Score shows a contradictory behaviour, while the FID captures this contamination, and other disturbance variants, very well. \n\nThe authors should discuss their proposed AM Score compared to the FID, also under consideration that the FID does not\nneed an accordingly pretrained classifier.\n\n[1] https://arxiv.org/abs/1706.08500"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1510092427289,"tcdate":1509862874846,"number":1,"cdate":1509862874846,"id":"Sy7107h0b","invitation":"ICLR.cc/2018/Conference/-/Paper403/Official_Comment","forum":"HyyP33gAZ","replyto":"SJC9zbqCZ","signatures":["ICLR.cc/2018/Conference/Paper403/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper403/Authors"],"content":{"title":"Thanks for your correction.","comment":"Yeah, it is indeed a mistake. We will correct it in the revision. Thanks. ^_^"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1509720725697,"tcdate":1509720725697,"number":1,"cdate":1509720725697,"id":"SJC9zbqCZ","invitation":"ICLR.cc/2018/Conference/-/Paper403/Public_Comment","forum":"HyyP33gAZ","replyto":"HyyP33gAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Minor comment","comment":"In Table 2, the citation for SGAN should be Huang et al. instead."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]}},{"tddate":null,"ddate":null,"tmdate":1515172779490,"tcdate":1509112919152,"number":403,"cdate":1509739319139,"id":"HyyP33gAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyyP33gAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Activation Maximization Generative Adversarial Nets","abstract":"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.","pdf":"/pdf/21ddb1e55631264e5ca473adc203292a1c552060.pdf","TL;DR":"Understand how class labels help GAN training. Propose a new evaluation metric for generative models. ","paperhash":"anonymous|activation_maximization_generative_adversarial_nets","_bibtex":"@article{\n  anonymous2018activation,\n  title={Activation Maximization Generative Adversarial Nets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyyP33gAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper403/Authors"],"keywords":["Generative Adversarial Nets","GANs","Evaluation Metrics","Generative Model","Deep Learning","Adversarial Learning","Inception Score","AM Score"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}