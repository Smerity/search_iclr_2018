{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222768889,"tcdate":1511902037429,"number":3,"cdate":1511902037429,"id":"rJp8iHslM","invitation":"ICLR.cc/2018/Conference/-/Paper799/Official_Review","forum":"SJLlmG-AZ","replyto":"SJLlmG-AZ","signatures":["ICLR.cc/2018/Conference/Paper799/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The proposed unsupervised losses appear weak","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method that given a sequence of frames, estimates a corresponding motion embedding to be the hidden state of an RNN (over convolutional features) at the last frame of the sequence. The parameters of the motion embedding are trained to preserve properties of associativity  and invertibility of motion, where the frame sequences have been recomposed (from video frames) in various way to create pairs of frame sequences with those -automatically obtained- properties. This means, the motion embedding is essentially trained without any human annotations.\nExperimentally, the paper shows that in synthetic moving MNIST frame sequences motion embedding discovers different patterns of motion, while it ignores image appearance (i.e., the digit label). The paper also shows that linear regressor trained in KITTI on top of the unsupervised motion embedding to estimate camera motion performs better than chance. \nQ to the authors: what labelled data were used to train the linear regressor in the KITTI experiment? \nEmpirically, it appears that supervision by preserving group transformations may not  be immensely valuable for learning motion representations. \n\n\n\nPros\n1)The neural architecture for motion embedding computation appears reasonable\n2)The paper tackles an interesting problem\n\nCons\n1)For a big part of the introduction the paper refers to the problem of `` ````\"learning motion” or `''understanding motion”  without being specific what it means by that. \n2)The empirical results are not convincing of the strength of imposing group transformations for self-supervised learning of motion embeddings.\n3)The KITTI experiment is not well explained as it is not clear how this regressor was trained to predict egomotion out of the motion embedding.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding image motion with group representations ","abstract":"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","pdf":"/pdf/afdfc32c464b376abd178c8fbe29f5d7858437a3.pdf","TL;DR":"We propose of method of using group properties to learn a representation of motion without labels and demonstrate the use of this method for representing 2D and 3D motion.","paperhash":"anonymous|understanding_image_motion_with_group_representations","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding image motion with group representations },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLlmG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper799/Authors"],"keywords":["vision","motion","recurrent neural networks","self-supervised learning","unsupervised learning","group theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222768932,"tcdate":1511766777782,"number":2,"cdate":1511766777782,"id":"SkG-iVtlz","invitation":"ICLR.cc/2018/Conference/-/Paper799/Official_Review","forum":"SJLlmG-AZ","replyto":"SJLlmG-AZ","signatures":["ICLR.cc/2018/Conference/Paper799/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting idea, but more detailed experimental validation is needed. ","rating":"5: Marginally below acceptance threshold","review":"Paper proposes an approach for learning video motion features in an unsupervised manner. A number of constraints are used to optimize the neural network that consists of CNN + RNN (LSTM). Constraints stem from group structure of sequences and include associativity and inevitability. For example, forward-backward motions should cancel each other out and motions should be additive. Optimized network is illustrated to produce features that can be used to regress odometry. \n\nOverall the approach is interesting from the conceptual point of view, however, experimental validation is very preliminary. This makes it difficult to asses the significance and viability of the approach. In particular, the lack of direct comparison, makes it difficult to asses whether the proposed group constraints are competitive with brightness constancy (or similar) constraints used to learn motion in an unsupervised manner in other papers. \n\nIt is true that proposed model may be able to learn less local motion information, but it is not clear if this is what happens in practice. In order to put the findings in perspective authors should compare to unsupervised optical flow approach (e.g., unsupervised optical flow produced by one of the proposed CNN networks and used to predict odometer on KITTI for fair comparison). Without a comparison of this form the paper is incomplete and the findings are difficult to put in the context of state-of-the-art.\n\nAlso, saying that learned features can predict odometry “better than chance” (Section 4.2 and Table 2) seems like a pretty low bar for a generic feature representation. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding image motion with group representations ","abstract":"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","pdf":"/pdf/afdfc32c464b376abd178c8fbe29f5d7858437a3.pdf","TL;DR":"We propose of method of using group properties to learn a representation of motion without labels and demonstrate the use of this method for representing 2D and 3D motion.","paperhash":"anonymous|understanding_image_motion_with_group_representations","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding image motion with group representations },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLlmG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper799/Authors"],"keywords":["vision","motion","recurrent neural networks","self-supervised learning","unsupervised learning","group theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222768969,"tcdate":1511439486439,"number":1,"cdate":1511439486439,"id":"S18F244xz","invitation":"ICLR.cc/2018/Conference/-/Paper799/Official_Review","forum":"SJLlmG-AZ","replyto":"SJLlmG-AZ","signatures":["ICLR.cc/2018/Conference/Paper799/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting approach to estimate motion","rating":"7: Good paper, accept","review":"The authors propose to learn the rigid motion group (translation and rotation) from a latent representation of image sequences without the need for explicit labels.\nWithin their data driven approach they pose minimal assumptions on the model, requiring the group properties (associativity, invertibility, identity) to be fulfilled.\nTheir model comprises CNN elements to generate a latent representation in motion space and LSTM elements to compose these representations through time.\nThey experimentally demonstrate their method on sequences of MINST digits and the KITTI dataset.\n\nPros:\n- interesting concept of combining algebraic structure with a data driven method\n- clear idea development and well written\n- transparent model with enough information for re-implementation\n- honest pointers to scenarios where the method might not work well\n\nCons:\n- the method is only intrinsically evaluated (Tables 2 and 3), but not compared with results from other motion estimation methods","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding image motion with group representations ","abstract":"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","pdf":"/pdf/afdfc32c464b376abd178c8fbe29f5d7858437a3.pdf","TL;DR":"We propose of method of using group properties to learn a representation of motion without labels and demonstrate the use of this method for representing 2D and 3D motion.","paperhash":"anonymous|understanding_image_motion_with_group_representations","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding image motion with group representations },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLlmG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper799/Authors"],"keywords":["vision","motion","recurrent neural networks","self-supervised learning","unsupervised learning","group theory"]}},{"tddate":null,"ddate":null,"tmdate":1509739095236,"tcdate":1509135086095,"number":799,"cdate":1509739092577,"id":"SJLlmG-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJLlmG-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Understanding image motion with group representations ","abstract":"Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain.","pdf":"/pdf/afdfc32c464b376abd178c8fbe29f5d7858437a3.pdf","TL;DR":"We propose of method of using group properties to learn a representation of motion without labels and demonstrate the use of this method for representing 2D and 3D motion.","paperhash":"anonymous|understanding_image_motion_with_group_representations","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding image motion with group representations },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJLlmG-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper799/Authors"],"keywords":["vision","motion","recurrent neural networks","self-supervised learning","unsupervised learning","group theory"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}