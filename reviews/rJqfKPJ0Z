{"notes":[{"tddate":null,"ddate":null,"tmdate":1514421413081,"tcdate":1514421413081,"number":5,"cdate":1514421413081,"id":"Sy6inn-7z","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Comment","forum":"rJqfKPJ0Z","replyto":"rysS4qvzz","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper133/AnonReviewer3"],"content":{"title":"Practical idea, but incremental","comment":"Since I could not modify the previous review, I show the evaluation results in the comment. \n`Evaluation:\n5: Marginally below acceptance threshold\n3: The reviewer is fairly confident that the evaluation is correct\n\nThis paper introduces a novel method to generate adversarial examples so that the resulting examples lie inside the specified region by construction without clipping. \n\nStrength:\n-Experimental results show that the proposed adversarial examples successfully fool multiple recent defense methods, such as feature squeezing, ensemble defenses, and JPEG encoding.\n-Tested with ImageNet \n\nWeakness:\n-The results are not compared with known attacks experimentally\n\nI think additional experiments are needed to demonstrate that the proposed adversarial example is superior to other adversarial examples. Since no experimental comparison is not conducted, I could not how much the proposed adversarial examples works robustly across multiple defense methods compared to other adversarial examples.\n\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1513716408885,"tcdate":1513716408885,"number":3,"cdate":1513716408885,"id":"ryWp9xwGf","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Comment","forum":"rJqfKPJ0Z","replyto":"B1fZIQcxM","signatures":["ICLR.cc/2018/Conference/Paper133/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper133/Authors"],"content":{"title":"Our response to AnonReviewer3","comment":"We honestly do not understand your evaluation since sentence “Evolutionary algorithms are also used by authors in [15] to find adversarial examples ...” is not a self citation. Believe us that we are not Nguyen et al and we are not related  to them at all. You will see it clearly when the names well be unveiled. \nSo, please take the time to reconsider your evaluation and give us a fair review as our paper is the result of several months of work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1513716281104,"tcdate":1513716281104,"number":2,"cdate":1513716281104,"id":"SkWHqxDzf","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Comment","forum":"rJqfKPJ0Z","replyto":"ryU7ZMsgf","signatures":["ICLR.cc/2018/Conference/Paper133/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper133/Authors"],"content":{"title":"Our response to AnonReviewer2","comment":"Thanks a lot for taking the time to read the paper and provide us with you review.\nWe do not claim in the paper that CIA attacks are the most robust ones as we did not indeed give any comparison to other methods. We however show that they are an answer to some issues met in literature. First, avoid the clipping that degrades the quality of attacks. We give a comparison to C&W. (Figure 1).  Second, we show that CIA attacks are effective against recent published defenses : ensembling (by the way, at least three papers submitted to ICLR2018 claim this defense to be effective), smoothing and JPG encoding. After the paper submission, we continued our experiments and made comparison to C&W and FGSM. They show a non negligible improvement in attacks success using CIA approach. If adding the results would change your review to an acceptance, we would like to do it.\nAbout the grey-box attack, you are definitely right . However, the purpose of this section 3.3 was to show that ensembling can be considered as a defense as it limits the attacks but not totally effective. Using another classifier would likely show that the transferability is even more limited. This would only reinforce our claim about the lack of transferability which was already tackled in the previous sections.\nFinally, the English of the paper can be improved. We will do it in the revised version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1513716111757,"tcdate":1513716111757,"number":1,"cdate":1513716111757,"id":"Byu9txvMM","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Comment","forum":"rJqfKPJ0Z","replyto":"BkrIo4ixG","signatures":["ICLR.cc/2018/Conference/Paper133/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper133/Authors"],"content":{"title":"Our response to AnonReviewer1","comment":"First, thank for taking the time to read the paper and provide this review.\nThe approach is not really incremental as we do not go from an easy case then harden it at each new experiment. In the Tabl2 we show the non transferabity of attacks when making targeted attacks which is against what is often claimed in literature. Table 3 gives the results of attacking several classifiers at once.  This shows that ensembling is not always effective as often claimed (by the way, at least three papers submitted to ICLR2018 claim it!). Table 4 provides results of attacking another defense, i.e.  spatial smoothing. Table 5 shows that smoothed attacks are not necessarily successful if defense does not use smoothing. Once again, this reveals that the idea behind smoothing as an efficient defense is not true. Table 6 gives the results of attacking a defense with and without smoothing at the same time. Table 7 presents a combination of ensembling and smoothing defense.  We could have given this last table directly at the beginning but we think honestly that this would make the paper more difficult to read. \nThe paper is obviously not written in a perfect English and this can be improved. We will do it in the revised version. But overall we think that we bring some interesting results to the community:\n- Avoid clipping to perform more robust attacks\n- Perform effective attacks against strong defenses like ensembling and smoothing.\n- Make partial crafting without affecting the whole content of input data (images in Figure 3)\n- Finally, CIA attacks can be applied beyond images.\nWe hope this answer will give you satisfaction and change your review to an acceptance."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1515642397073,"tcdate":1511897933126,"number":3,"cdate":1511897933126,"id":"BkrIo4ixG","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Review","forum":"rJqfKPJ0Z","replyto":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Incremental but interesting results for adversarial examples","rating":"5: Marginally below acceptance threshold","review":"In this paper the authors present a new method for generating adversarial examples by constraining the perturbations to fall in a bounded region.  Further, experimentally, they demonstrate that learning the perturbations to balance errors against multiple classifiers can overcome many common defenses used against adversarial examples.\n\nPros:\n- Simple, easy to apply technique\n- Positive results in a wide variety of settings.\n\nCons:\n- Writing is a bit awkward at points.\n- Approach seems fairly incremental.\n\nOverall, the results are interesting but the technique seems relatively incremental.\n\nDetails:\n\n\"To find the center of domain definition...\" paragraph should probably go after the cases are described.  Confusing as to what is being referred to where it currently is written.\n\nTable 1: please use periods not commas (as in Table 2), e.g. 96.1 not 96,1\n\ninexistent --> non-existent\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1515642397114,"tcdate":1511887133620,"number":2,"cdate":1511887133620,"id":"ryU7ZMsgf","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Review","forum":"rJqfKPJ0Z","replyto":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting reparametrization, but too little experimental support","rating":"4: Ok but not good enough - rejection","review":"This paper presents a reparametrization of the perturbation applied to features in adversarial examples based attacks. It tests this attack variation on against Inception-family classifiers on ImageNet. It shows some experimental robustness to JPEG encoding defense.\n\nSpecifically about the method: Instead of perturbating a feature x_i by delta_i, as in other attacks, with delta_i in range [-Delta_i, Delta_i], they propose to perturbate x_i^*, which is recentered in the domain of x_i through a heuristic ((x_i ± Delta_i + domain boundary that would be clipped)/2), and have a similar heuristic for computing a Delta_i^*. Instead of perturbating x_i^* directly by delta_i, they compute the perturbed x by x_i^* + Delta_i^* * g(r_i), so they follow the gradient of loss to misclassify w.r.t. r (instead of delta). \n\n+/-:\n+ The presentation of the method is clear.\n+ ImageNet is a good dataset to benchmark on.\n- (!) The (ensemble) white-box attack is effective but the results are not compared to anything else, e.g. it could be compared to (vanilla) FGSM nor C&W.\n- The other attack demonstrated is actually a grey-box attack, as 4 out of the 5 classifiers are known, they are attacking the 5th, but in particular all the 5 classifiers are Inception-family models.\n- The experimental section is a bit sloppy at times (e.g. enumerating more than what is actually done, starting at 3.1.1.).\n- The results on their JPEG approximation scheme seem too explorative (early in their development) to be properly compared.\n\nI think that the paper need some more work, in particular to make more convincing experiments that the benefit lies in CIA (baselines comparison), and that it really is robust across these defenses shown in the paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1515642397152,"tcdate":1511826937781,"number":1,"cdate":1511826937781,"id":"B1fZIQcxM","invitation":"ICLR.cc/2018/Conference/-/Paper133/Official_Review","forum":"rJqfKPJ0Z","replyto":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference/Paper133/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Clipping Free Attacks Against Neural Networks ","rating":"3: Clear rejection","review":"The paper is not anonymized. In page 2, the first line, the authors revealed [15] is a self-citation and [15] is not anonumized in the reference list.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]}},{"tddate":null,"ddate":null,"tmdate":1509739467401,"tcdate":1509026065718,"number":133,"cdate":1509739464751,"id":"rJqfKPJ0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJqfKPJ0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Clipping Free Attacks Against Neural Networks","abstract":"During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out this\nfact and different approaches have been proposed to generate attacks while adding\na limited perturbation to the original data. The most robust known method so far\nis the so called C&W attack [1]. Nonetheless, a countermeasure known as fea-\nture squeezing coupled with ensemble defense showed that most of these attacks\ncan be destroyed [6]. In this paper, we present a new method we call Centered\nInitial Attack (CIA) whose advantage is twofold : first, it insures by construc-\ntion the maximum perturbation to be smaller than a threshold fixed beforehand,\nwithout the clipping process that degrades the quality of attacks. Second, it is\nrobust against recently introduced defenses such as feature squeezing, JPEG en-\ncoding and even against a voting ensemble of defenses. While its application is\nnot limited to images, we illustrate this using five of the current best classifiers\non ImageNet dataset among which two are adversarialy retrained on purpose to\nbe robust against attacks. With a fixed maximum perturbation of only 1.5% on\nany pixel, around 80% of attacks (targeted) fool the voting ensemble defense and\nnearly 100% when the perturbation is only 6%. While this shows how it is difficult\nto defend against CIA attacks, the last section of the paper gives some guidelines\nto limit their impact.","pdf":"/pdf/95f8192a83712b77f8dfc4837821960a191e96f1.pdf","TL;DR":" In this paper, a new method we call Centered Initial Attack (CIA) is provided. It insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process.","paperhash":"anonymous|clipping_free_attacks_against_neural_networks","_bibtex":"@article{\n  anonymous2018clipping,\n  title={Clipping Free Attacks Against Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJqfKPJ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper133/Authors"],"keywords":["Adversarial examples","Neural Networks","Clipping"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}