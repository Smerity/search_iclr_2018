{"notes":[{"tddate":null,"ddate":null,"tmdate":1513991851032,"tcdate":1513991851032,"number":4,"cdate":1513991851032,"id":"rJ7n07jGz","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Comment","forum":"SkZ-BnyCW","replyto":"SkZ-BnyCW","signatures":["ICLR.cc/2018/Conference/Paper170/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper170/Authors"],"content":{"title":"Response and Clarification","comment":"We thank reviewers for their valuable feedback. There are several points we would like to clarify.\n\nMotivation: The motivation of the work is to study the effect of a learnable prior for generative models with latent variables. The learnable prior can potentially take forms of any graphical model, while here we focus on using RBM due to its simplicity. This strength over vanilla VAEs has been shown on CIFAR10 and ImageNet64 experiments. In addition, these models can still be quantitatively evaluated in terms of density estimation (as we demonstrate in our work), which is a benefit compared top GAN-like models since they are relatively hard to evaluate quantitatively.\n\nQuantitative performance: Our model performs well on MNIST. The ResNet model uses a similar network as IAF-VAE, which was previously shown to achieve state-of-the-art results using deep convnets (cited in Table 2 and mentioned in the main text). IAF-VAE was about 2 nats better than vanilla VAE using the same ResNet architecture. Thus, the fact that our model performs slightly better than IAF-VAE shows that the learned RBM prior has some merits. The current state-of-the-art models use PixelCNN/PixelRNN-based decoders, which by themselves are very strong density estimation models. PixelCNN/PixelRNN-based decoders can potentially be integrated into our framework as well. \n\nRegarding CIFAR10: Compressing real valued images into binary is a hard problem. In fact, if we increase the dimension of the latent space from 1024 to 2048 and use a 2048-4096 RBM, the performance of our model can be substantially improved, achieving Test NLL of 4.54 bits/dim, the same as that of IWAE. \n\nWe focus on comparing our results with IWAE as it represents a strong baseline that uses discrete latent variables. We also compared with models that use unconditional Bernoulli prior, but these models performed much worse, compared to both, IWAE and our model, in terms of both density estimation and generated samples. \n\n\nClarifications for Reviewer 2\n\nThank you for your detailed feedback. \n\n1. Regarding inference network mapping into [0,1] and not into {0,1}: \n\nAs mentioned at the end of the pretraining section, the “soft-binarization” is removed after pertaining and a sigmoid layer is added to produce valid probabilities. During joint training, we only use samples {0, 1}.\n\n2. Regarding tightness of the lower-bound:\n\nLower bound in Eq.2 is tighter than a single sample bound in Eq.1 (as was originally derived in IWAE paper). We will change the text to make this point clear.\n\n3. Regarding \"multivariate Bernoulli modeled by an RBM\": \n\nThanks for pointing this out. Yes the use of multivariate Bernoulli is not rigorous here. We will change this in revision.\n\n4. Improving notation:\n\nThanks for catching typos. The x_data refers to data points from training set while x_sample is samples from model distribution. M refers to the model distribution. We will check other notations as well.\n\n5. Regarding parallel tempering/centred RBMs:\n\nThanks for pointing this out. Here we mean that this method has produced good RBM results in practice on MNIST in terms of density estimation, and has been widely used in practice (in addition to Persistent CD) We will correct this. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1511990566324,"tcdate":1511990566324,"number":3,"cdate":1511990566324,"id":"BJ0mHjnlf","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Comment","forum":"SkZ-BnyCW","replyto":"SkaVZ-9xf","signatures":["ICLR.cc/2018/Conference/Paper170/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper170/Authors"],"content":{"title":"Clarifications and comments","comment":"Thank you for your comments \n\n>>> Just to clarify: You pretrain the encoder/decoder pair and pretrain a RBM on their latent representation? And then during joint training (section 3.2) you block direct gradient flow (disable the soft-binarization) and use VIMCO?\n\nYes. The direct gradient flow is blocked during joint training with VIMCO and Contrastive RWS.\n\n\n>>> I’m not convinced I follow the second part of the argument in 3.0: Training SBNs or DBNs with e.g. Gumbel-softmax based methods would allow gradients to flow in a very similar fashion. Doesn’t the “gradient flow” depend on the training method / gradient estimator (in your case soft-binarization) rather than this models structure?\n\nThe difference between our model and SBNs or DBNs is better illustrated in Figure 1. In DBN and SBN, every layer is stochastic and defines its own generative distribution p(z_i+1|z_i). There are N stochastic layers and thus N intermediate generative distributions. Correspondingly, there are N approximate inference distributions (as denoted by multiple upward arrows). The i-th inference layer is trained to approximate the posterior distribution of i-th generative layer. The learning signal for each layer depends locally on the input output states, but not on the gradient information that is propagated from deeper layers. In our model, the decoder and encoder are deterministic and continuous. They define one pair of p(x|z) and q(z|x) and there are no intermediate stochastic layers. Thus multiple layers in encoder/decoder are trained with gradients that flow freely within encoder/decoder.\n\nIf we train DBN or SBN with Gumbel-softmax based methods, the stochastic nodes in each layer are replaced with continuous approximations. In that case the gradient flow between layers is possible but not as freely as in our model because still in DBN or SBN, both the encoder and decoder would still be represented as stochastic layers. Since soft-binarization is only used during pretraining, our model does not contain any continuous relaxation during joint training (our encoder and decoder are continuous and deterministic), which is a major difference from continuous relaxation based methods.\n\n\n>>> From this perspective your model is directly comparable to Discrete VAEs, isn’t it? Where Discrete VAEs introduced a different reparam. based training method.\n\nBoth Discrete VAEs and our model utilize deep continuous encoder/decoder for stronger representation power. Discrete VAEs have a layer of continuous latent variables between bipartite Boltzmann machine prior and encoder/decoder. They project posterior and prior into a continuous space to make the \"autoencoder\" part of the loss be fully differentiable. \n\nIn our method, the \"autoencoder\" is fully differentiable during pretraining. In joint training, encoder and decoder only transmit stochastic states and thus block the gradient flow between them. Another difference in architecture is that Discrete VAEs use bipartite BM with both parts connected to the rest of the model while our models use 2 layer BM (RBM) with first layer connected to the latent space z and the second being fully hidden. In terms of density estimation, our model outperforms Discrete VAE on MNIST(79.58 vs. 81.01, as reported by [1]). We will clarify this point and add additional experimental results comparing our model to Discrete VAEs.\n\n\n>>> Have you tried alternative pretraining methods? E.g. using Gumbel-softmax based instead eqn. (3), or VIMCO trained factorial top layer SBN? Do we have any idea why joint training might be so hard?\n\nWe tried using soft-binarization with Gaussian noise but that introduces strong artifacts in generated images. Pretraining with Gumbel-softmax based methods is an interesting idea. In their original paper [2][3], the methods are tested on models with one or two layers. Thus it is hard to conclude immediately whether that will work as effectively on deep networks. But that will be worth exploring. Pretraining with SBN is more tricky. Note that it still does not solve the problem that deep generative models are very hard to train from scratch with REINFORCE style algorithms and discrete latent variables. Pretraining with factorial prior also adds stronger constrain on the approximate posterior compared with the RBM prior.\n\n\n>>> The title seems very broad - large parts of the paper propose and evaluate a pretraining procedure for a specific two layer DBM architecture.\n\nThank you for pointing this out, we will make the title be more focused. \n\n\n>>> I’m curious: What are typical log Z estimate for your models in table 1?\n\nBetween 265 and 270.\n\n\n[1] Discrete Variational Autoencoders\n[2] Categorical Reparameterization with Gumbel-Softmax\n[3] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642403309,"tcdate":1511821141828,"number":3,"cdate":1511821141828,"id":"rkp8JGcef","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Review","forum":"SkZ-BnyCW","replyto":"SkZ-BnyCW","signatures":["ICLR.cc/2018/Conference/Paper170/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting work","rating":"4: Ok but not good enough - rejection","review":"Interesting work, but I’m not convinced by the arguments nor by the experiments. Similar models have been trained before; it’s not clear that the proposed pretraining procedure is a practical step forwards. And quite some decisions seem ad-hoc and not principled. \n\nNevertheless, interesting work for everyone interested in RBMs as priors for “binary VAEs”. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1511817525454,"tcdate":1511817525454,"number":2,"cdate":1511817525454,"id":"SkaVZ-9xf","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Comment","forum":"SkZ-BnyCW","replyto":"SkZ-BnyCW","signatures":["ICLR.cc/2018/Conference/Paper170/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper170/AnonReviewer1"],"content":{"title":"Interesting work","comment":"Interesting work!\n\nJust to clarify: You pretrain the encoder/decoder pair and pretrain a RBM on their latent representation? And then during joint training (section 3.2) you block direct gradient flow (disable the soft-binarization) and use VIMCO? \n\nI’m not convinced I follow the second part of the argument in 3.0: Training SBNs or DBNs with e.g. Gumbel-softmax based methods would allow gradients to flow in a very similar fashion. Doesn’t the “gradient flow” depend on the training method / gradient estimator (in your case soft-binarization) rather than this models structure? \n\nFrom this perspective your model is directly comparable to Discrete VAEs, isn’t it? Where Discrete VAEs introduced a different reparam. based training method. \n\nHave you tried alternative pretraining methods? E.g. using Gumbel-softmax based instead eqn. (3), or VIMCO trained factorial top later SBN? Do we have any idea why joint training might be so hard?\n\nThe title seems very broad - large parts of the paper propose and evaluate a pretraining procedure for a specific two layer DBM architecture. \n\nI’m curious: What are typical log Z estimate for your models in table 1? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642403352,"tcdate":1511385581211,"number":2,"cdate":1511385581211,"id":"SkHg5PQxf","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Review","forum":"SkZ-BnyCW","replyto":"SkZ-BnyCW","signatures":["ICLR.cc/2018/Conference/Paper170/AnonReviewer3"],"readers":["everyone"],"content":{"title":"OK, but fairly incremental and results underwhelming","rating":"5: Marginally below acceptance threshold","review":"While I acknowledge that training generative models with binary latent variables is hard, I'm not sure this paper really makes valuable progress in this direction. The only results that seem promising are those on binarized MNIST, for the non-convolutional architecture, and this setting isn't particularly exciting. All other experiments seem to suggest that the proposed model/algorithm is behind the state of the art. Moreover, the proposed approach is fairly incremental, compared to existing work on RWS, VIMCO, etc.\n\nSo while this work seem to have been seriously and thoughtfully executed, I think it falls short of the ICLR acceptance bar.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642403391,"tcdate":1511028054604,"number":1,"cdate":1511028054604,"id":"r1yDSeCJM","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Review","forum":"SkZ-BnyCW","replyto":"SkZ-BnyCW","signatures":["ICLR.cc/2018/Conference/Paper170/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes to augment a variational auto encoder (VAE) with an binary restricted Boltzmann machine (RBM) in the role of the prior of the generative model. Clarity could be improved in some aspects and the advantages of the proposed model compared to existing ones did not become totally clear to me.","rating":"4: Ok but not good enough - rejection","review":"Summary of the paper:\nThe paper proposes to augment a variational auto encoder (VAE) with an binary restricted Boltzmann machine (RBM) in the role of the prior of the generative model. To yield a good initialisation of the parameters of the RBM and the inference network a special pertaining procedure is introduced. The model produces competitive Likelihood results on MNIST and was further tested on CIFAR 10. \n\nClarity and quality: \n\n1. From the description of the pertaining procedure and the appendix B I got the impression that the inference network maps into [0,1] and not into {0,1}.  Does it mean, you are not really considering binary latent variables (making the RBM model the values in [0,1] by its probability p(z|h))? \n\n2. on page 2:\nRWS....\"derive a tighter lower bound\": Where does the \"tighter\" refer to? \n\n3. \"multivariate Bernoulli modeled by an RBM\": Note,  while in a multivariate Bernoulli the binary variables would be independent from each others, this is usually not the case for the visible variables of RBMs (only in the conditional distribution given the state of the hidden variables).\n\n4. The notation could be improved, e.g.:\n-x_data and x_sample are not explained\n- M is not defined in equation 5. \n\n5. \"this training method has been previously used to produce the best results on MNIST\" Note, that parallel tempering often leads to better results when training RBMs (see http://proceedings.mlr.press/v9/desjardins10a/desjardins10a.pdf) . Furthermore, centred RBMs are also get better results than vanilla RBMs (see: http://jmlr.org/papers/v17/14-237.html).\n\nOriginality and significance:\nAs already mentioned in a comment on open-review the current version of the paper misses to mention one very related work: \"discrete variational auto encoders\". Also \"bidirectional Helmholtz machines\" could be mentioned as generative model with discrete latent variables.  The results for both should also be reported in Table 1 (discrete VAEs: 81,01, BiHMs: 84,3). \n\nFrom the motivation the advantages of the model did not become very clear to me. Main advantage seems to be the good likelihood result on MNIST (but likelihood does not improve compared to IWAE on CIFAR 10 for example). However, using an RBM as prior has the disadvantage that sampling from the generative model requires running a Markov chain now while having a solely directed generative model allows for fast sampling. \n\nExperiments show good likelihood results on MNIST. Best results are obtained when using a ResNet decoder. I wondered how much a standard VAE is improved by using such a powerful decoder. Reporting this, would allow to understand, how much is gained from using a RBM for learning the prior. \n\nMinor comments:\npage 1:\n\"debut of variational auto encoder (VAE) and reparametrization trick\" -> debut of variational auto encoders (VAE) and the reparametrization trick\",\npage 2:\n\"with respect to the parameter of  p(x,z)\" -> \"with respect to the parameters of  p(x,z)\"\n\"parameters in p\" -> \"parameters of p\" \n\"is multivariate Bernoulli\" ->  \"is a multivariate Bernoulli\"\n\"we compute them\" -> \"we compute it\" \npage 3:\n\"help find a good\" ->  \"help to find a good\"\npage 7:\n\"possible apply\" -> \"possible to apply\"","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510365671579,"tcdate":1510365671579,"number":1,"cdate":1510365671579,"id":"H1gx50Xyf","invitation":"ICLR.cc/2018/Conference/-/Paper170/Official_Comment","forum":"SkZ-BnyCW","replyto":"BJdtEE8AW","signatures":["ICLR.cc/2018/Conference/Paper170/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper170/Authors"],"content":{"title":"Comparison with Discrete VAE","comment":"Thank you for pointing out this paper! We will include the discussion on Discrete VAE in future revisions.\n\nAlthough both Discrete VAE and our model have discrete latent variables, the detailed architectures are different. In Discrete VAE, the RBM (or bipartite Boltzmann machine) is fully hidden(z) and connects to the encoder-decoder through a set of continuous smoothing variables. Our model uses a latent(z)-hidden(h) RBM. The visible layer of RBM directly connects to encoder-decoder and they only exchange discrete states.\n\nThese two papers also have different focuses. In Discrete VAE, they introduce a method to project posterior and prior into a continuous space so that the discrete variables can be integrated out. This can be seen as “reparameterizing” discrete into continuous to make the autoencoder term fully differentiable. In our paper, we try to answer whether it is possible to train a DBN-inspired model without any reparameterization but with proper learning procedure. This results in a conceptually straightforward model that actually outperforms the Discrete VAE on MNIST (79.58 VS 81.01). We also study how well our method scales to real images, which is not mentioned in many previous works using discrete latent variables."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509471360346,"tcdate":1509471360346,"number":1,"cdate":1509471360346,"id":"BJdtEE8AW","invitation":"ICLR.cc/2018/Conference/-/Paper170/Public_Comment","forum":"SkZ-BnyCW","replyto":"SkZ-BnyCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"A more informative title would be helpful","comment":"What were your reasons for choosing such a general title? It would be understandable if this paper were the first work in this area or if it provided some sort of unifying view of prior of work on such models (DBNs, DBMs, SBNs etc.), but it is not the case.\n\nIt would also be good to discuss how the proposed model is related to Discrete VAEs, which also combine an RBM with a directed mapping."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739447269,"tcdate":1509045497127,"number":170,"cdate":1509739444612,"id":"SkZ-BnyCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkZ-BnyCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Deep Generative Models With Discrete Latent Variables","abstract":"There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.","pdf":"/pdf/4e50884f3b8f84d8ba787040962d51c95fb4d588.pdf","paperhash":"anonymous|learning_deep_generative_models_with_discrete_latent_variables","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Generative Models With Discrete Latent Variables},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkZ-BnyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper170/Authors"],"keywords":["deep generative models","deep learning"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}