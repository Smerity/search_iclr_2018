{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642529819,"tcdate":1511757944632,"number":3,"cdate":1511757944632,"id":"Hy-t_ztgG","invitation":"ICLR.cc/2018/Conference/-/Paper909/Official_Review","forum":"HkinqfbAb","replyto":"HkinqfbAb","signatures":["ICLR.cc/2018/Conference/Paper909/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple and effective compression method, but needs refinement and large-scale experiments","rating":"6: Marginally above acceptance threshold","review":"As the authors mentioned, weight-sharing and pruning are not new to neural network compression. The proposed method resembles a lot with the deep compression work (Han et. al. 2016), with the distinction of clustering across different layers and a Lasso regularizer to encourage sparsity of the weights. Even though the change seems minimal, the authors has demonstrated the effectiveness on the benchmark.\n\nBut the description of the optimization strategy in Section 3 needs some refinement. In the soft-tying stage, why only the regularizer (1) is considered, not the sparsity one? In the hard-tying stage, would the clustering change in each iteration? If not, this has reduced to the constrained problem as in the Hashed Compression work (Chen et. al. 2015) where the regularizer (1) has no effect since the clustering is fixed and all the weights in the same cluster are equal. Even though it is claimed that the proposed method does not require a pre-trained model to initialize, the soft-tying stage seems to take the responsibility to \"pre-train\" the model.\n\nThe experiment section is a weak point. It is much less convincing with no comparison result of compression on large neural networks and large datasets. The only compression result on large neural network (VGG-11) comes with no baseline comparisons. But it already tells something: 1) what is the classification result for reference network without compression? 2) the compression ratio has significantly reduced comparing with those for MNIST. It is hard to say if the compression performance could generalize to large networks.\n\nAlso, it would be good to have an ablation test on different parts of the objective function and the two optimization stages to show the importance of each part, especially the removal of the soft-tying stage and the L1 regularizer versus a simple pruning technique after each iteration. This maybe a minor issue, but would be interesting to know: what would the compression performance be if the classification accuracy maintains the same level as that of the deep compression. As discussed in the paper, it is a trade-off between accuracy and compression. The network could be compressed to very small size but with significant accuracy loss.\n\nSome minor issues:\n- In Section 1, the authors discussed a bunch of pitfalls of existing compression techniques, such as large number of parameters, local minimum issues and layer-wise approaches. It would be clearer if the authors could explicitly and succinctly discuss which pitfalls are resolved and how by the proposed method towards the end of the Introduction section. \n- In Section 4.2, the authors discussed the insensitivity of the proposed method to switching frequency. But there is no quantitative results shown to support the claims.\n- What is the threshold for pruning zero weight used in Table 2?\n- There are many references and comparisons missing: Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations in NIPS 17 for instance. This paper also considers quantization for compression which is related to this work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Automatic Parameter Tying in Neural Networks","abstract":"Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance. However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both. In this paper, we propose a novel algorithm that jointly learns and compresses a neural network. The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty. We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints. Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches. ","pdf":"/pdf/2bea0252e90c15d8e3ed04a50e02f12ce984b9ae.pdf","TL;DR":"A k-means prior combined with L1 regularization yields state-of-the-art compression results.","paperhash":"anonymous|automatic_parameter_tying_in_neural_networks","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Parameter Tying in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkinqfbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper909/Authors"],"keywords":["neural network","quantization","compression"]}},{"tddate":null,"ddate":null,"tmdate":1515642529858,"tcdate":1511732167425,"number":2,"cdate":1511732167425,"id":"Byk0Q2_xz","invitation":"ICLR.cc/2018/Conference/-/Paper909/Official_Review","forum":"HkinqfbAb","replyto":"HkinqfbAb","signatures":["ICLR.cc/2018/Conference/Paper909/AnonReviewer2"],"readers":["everyone"],"content":{"title":"comments on K-means and L1 regularization","rating":"6: Marginally above acceptance threshold","review":"This is yet another paper on parameter tying and compression of DNNs/CNNs.  The key idea here is a soft parameter tying under the K-means regularization on top of which an L1 regularization is further imposed for promoting sparsity. This strategy seems to help the hard tying in a later stage while keeping decent performance.  The idea is sort of interesting and the reported experimental results appear to be supportive.  However, I have following concerns/comments. \n\n1.  The roles played by K-means and L1 regularization are a little confusing from the paper.  In Eq.3, it appears that the L1 regularization is always used in optimization. However, in Eq.4,  the L1 norm is not included.  So the question is,  in the soft-tying step, is L1 regularization always used?  Or a more general question,  how important is it to regularize the cross-entropy with both K-means and L1? \n\n2. A follow-up question on K-means and L1.   If no L1 regularization, does the K-means soft-tying followed by a hard-tying work as well as using the L1 regularization throughout? \n\n3. It would be helpful to say a few words on the storage of the model parameters. \n\n4. It would be helpful to show if the proposed technique work well on sequential models like LSTMs.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Automatic Parameter Tying in Neural Networks","abstract":"Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance. However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both. In this paper, we propose a novel algorithm that jointly learns and compresses a neural network. The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty. We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints. Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches. ","pdf":"/pdf/2bea0252e90c15d8e3ed04a50e02f12ce984b9ae.pdf","TL;DR":"A k-means prior combined with L1 regularization yields state-of-the-art compression results.","paperhash":"anonymous|automatic_parameter_tying_in_neural_networks","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Parameter Tying in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkinqfbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper909/Authors"],"keywords":["neural network","quantization","compression"]}},{"tddate":null,"ddate":null,"tmdate":1515695814995,"tcdate":1511709318701,"number":1,"cdate":1511709318701,"id":"Bky9cL_eG","invitation":"ICLR.cc/2018/Conference/-/Paper909/Official_Review","forum":"HkinqfbAb","replyto":"HkinqfbAb","signatures":["ICLR.cc/2018/Conference/Paper909/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Automatic Parameter Tying in Neural Networks","rating":"6: Marginally above acceptance threshold","review":"Approach is interesting however my main reservation is with the data set used for experiments and making general (!) conclusions. MNIST, CIFAR-10 are too simple tasks perhaps suitable for debugging but not for a comprehensive validation of quantization/compression techniques. Looking at the results, I see a horrific degradation of 25-43% relative to DC baseline despite being told about only a minimal loss in accuracy. A number of general statements is made based on MNIST data, such as on page 3 when comparing GMM and k-means priors, on page 7 and 8 when claiming that parameter tying and sparsity do not act strongly to improve generalization. In addition, by making a list of all hyper parameters you tuned I am not confident that your claim that this approach requires less tuning. \n\nAdditional comments:\n\n(a) you did not mention student-teacher training\n(b) reference to previously not introduced K-means prior at the end of section 1\n(c) what is that special version of 1-D K-means?\n(d) Beginning of section 4.1 is hard to follow as you are referring to some experiments not shown in the paper.\n(e) Where is 8th cluster hiding in Figure 1b?\n(f) Any comparison to a classic compression technique would be beneficial.\n(g) You are referring to a sparsity at the end of page 8 without formally defining it. \n(h) Can you label each subfigure in Figure 3 so I do not need to refer to the caption? Can you discuss this diagram in the main text, otherwise what is the point of dumping it in the appendix?\n(i) I do not understand Figure 4 without explanation. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Automatic Parameter Tying in Neural Networks","abstract":"Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance. However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both. In this paper, we propose a novel algorithm that jointly learns and compresses a neural network. The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty. We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints. Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches. ","pdf":"/pdf/2bea0252e90c15d8e3ed04a50e02f12ce984b9ae.pdf","TL;DR":"A k-means prior combined with L1 regularization yields state-of-the-art compression results.","paperhash":"anonymous|automatic_parameter_tying_in_neural_networks","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Parameter Tying in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkinqfbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper909/Authors"],"keywords":["neural network","quantization","compression"]}},{"tddate":null,"ddate":null,"tmdate":1515183240481,"tcdate":1509137074612,"number":909,"cdate":1509739033994,"id":"HkinqfbAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkinqfbAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Automatic Parameter Tying in Neural Networks","abstract":"Recently, there has been growing interest in methods that perform neural network compression, namely techniques that attempt to substantially reduce the size of a neural network without significant reduction in performance. However, most existing methods are post-processing approaches in that they take a learned neural network as input and output a compressed network by either forcing several parameters to take the same value (parameter tying via quantization) or pruning irrelevant edges (pruning) or both. In this paper, we propose a novel algorithm that jointly learns and compresses a neural network. The key idea in our approach is to change the optimization criteria by adding $k$ independent Gaussian priors over the parameters and a sparsity penalty. We show that our approach is easy to implement using existing neural network libraries, generalizes L1 and L2 regularization and elegantly enforces parameter tying as well as pruning constraints. Experimentally, we demonstrate that our new algorithm yields state-of-the-art compression on several standard benchmarks with minimal loss in accuracy while requiring little to no hyperparameter tuning as compared with related, competing approaches. ","pdf":"/pdf/2bea0252e90c15d8e3ed04a50e02f12ce984b9ae.pdf","TL;DR":"A k-means prior combined with L1 regularization yields state-of-the-art compression results.","paperhash":"anonymous|automatic_parameter_tying_in_neural_networks","_bibtex":"@article{\n  anonymous2018automatic,\n  title={Automatic Parameter Tying in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkinqfbAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper909/Authors"],"keywords":["neural network","quantization","compression"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}