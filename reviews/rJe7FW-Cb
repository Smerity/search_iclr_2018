{"notes":[{"tddate":null,"ddate":null,"tmdate":1516651446660,"tcdate":1516651446660,"number":14,"cdate":1516651446660,"id":"SJypm6QBf","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"H1VpcYQSz","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: Relevant paper with spatial regularization and attention","comment":"Thank you for the interest in our paper. We have included this explanation in the “Related Work” section, as a specialized solution for multilabel classification, where instead of learning universal modules, a ResNet is modified to improve its multilabel classification by enhancing the predictions with the learned most relevant regions.\n\nDifferently from this ICLR, in [1], instead of designing a general mechanism like the one proposed in our submission, the authors design an specialized attention mechanism for multilabel classification and test it on MSCOCO, NUS-WIDE, and WIDER. Namely, they use the features in “res4b22 relu” in order to extract attention scores for each label through three convolutional layers. To avoid attending to labels not present for the input being processed, these attention maps are multiplied by “confidence maps”, which are learned to be 1 if the label is present, and 0 if not. The attentional predictions are average with the network predictions. Differently, we want to incorporate fine detail at different levels of abstraction to the final prediction, thus, we propose a general “Attention Module”, that can be applied at many levels to any network, to enhance the final prediction weighted by the relevance of each prediction (for instance, details in the texture might help do distinguish between two birds which are similar at abstract level).\n\nChanges will be visible in the final version of the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1516636859723,"tcdate":1516636859723,"number":1,"cdate":1516636859723,"id":"H1VpcYQSz","invitation":"ICLR.cc/2018/Conference/-/Paper701/Public_Comment","forum":"rJe7FW-Cb","replyto":"rJe7FW-Cb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relevant paper with spatial regularization and attention ","comment":"Just for completeness, a relevant paper that learns spatial regularizations using an attention mechanism on a final ResNet representation is [1]. \n\n[1] Zhu, F., Li, H., Ouyang, W., Yu, N., & Wang, X. Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification. in CVPR 2017"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515767794560,"tcdate":1515767794560,"number":12,"cdate":1515767794560,"id":"ByogdHI4M","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"BJZ7a4ING","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: Thanks but not enough...","comment":"Thank you for the thorough review. We think the comments help to keep a high standards on this conference, and our paper has greately improved the quality thanks to them.\n\n>> Unfortunately, I do not think this rebuttal addresses my main complaint. I understand that the benchmarks include systems that use attentional mechanisms. My main issue is that the paper is about attention but different attentional mechanisms are never compared on a level play-field (i.e., using the same architectures, optimizers, etc etc). There is no way from the benchmarks to properly assess how much of the improvement is actually driven by the proposed attentional mechanism as opposed to anything else. \n\nWe understand the concern, this is exactly why we worked hard during the review period to find the time to include a comparison between STNs and the proposed attention mechanism under the same exact settings (same base architecture, learning algorithm, hyperparameters, training steps, etc.) showing that ours generalizes much better. Moreover, through all the manuscript, we emphasize that our approach is simpler and faster than other competing approaches.\n\n>> This is all the more problematic given how small the improvements are.\n\nIn the second point of the responses to AnonReviewer3 (3/3) (https://openreview.net/forum?id=rJe7FW-Cb&noteId=BJZ7a4ING&noteId=HkabPkOQM), we explain that the improvement is not so modest given the current context. In our case, improvement is comparable to that found in STN, for example.\n\n>> There is no way from the benchmarks to properly assess how much of the improvement is actually driven by the proposed attentional mechanism as opposed to anything else. \n\nWe think it is clear that plugging the proposed mechanism into a state-of-the-art CNN results in an improvement. In fact, in every table we show how the augmented models are always better.\n\n>> I would also add that with all that said the proposed mechanism remains relatively incremental with respect to related work (work properly cited) and that it seems to be better suited for a more specialized conference. \n\nWe still think our work helps indeed to build better representations, and it could be of inspiration for future work in any other field."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515765016712,"tcdate":1515765016712,"number":11,"cdate":1515765016712,"id":"BJZ7a4ING","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"H1H_vJ_QM","signatures":["ICLR.cc/2018/Conference/Paper701/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/AnonReviewer3"],"content":{"title":"Thanks but not enough...","comment":"Unfortunately, I do not think this rebuttal addresses my main complaint. I understand that the benchmarks include systems that use attentional mechanisms. My main issue is that the paper is about attention but different attentional mechanisms are never compared on a level play-field (i.e., using the same architectures, optimizers, etc etc). There is no way from the benchmarks to properly assess how much of the improvement is actually driven by the proposed attentional mechanism as opposed to anything else. This is all the more problematic given how small the improvements are. I would also add that with all that said the proposed mechanism remains relatively incremental with respect to related work (work properly cited) and that it seems to be better suited for a more specialized conference. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514825581118,"tcdate":1514825581118,"number":9,"cdate":1514825581118,"id":"H1H_vJ_QM","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"rkzOQxcgM","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: AnonReviewer3","comment":"We thank the reviewer for the feedback,\n\n>> First, some claimed are made about how the proposed approach \"enhances most of the desirable properties from previous approaches” (see pp 1-2) but these claims are never backed up. \n\nWith this sentence we tried to convey that the proposed model accumulates the best of the following properties in the literature: (i) it works in a single pass because it uses a single feed-forward CNN, differently from recurrent and two-step models, (ii) it is trained with SGD instead of RL, thus it presents faster convergence and it does not require sampling, (iii) it can be used to augment any architecture, as we show for WRNs, and (iv) it is simple to implement (instead of creating a whole new network architecture, we just add the attention heads and the attention outputs to an already existing one, eq 9). In order to better back up these properties, we have added a table in the introduction (Table 2) comparing the different architectures in the literature with ours, showing that ours accumulates the best of them.\n\n>> More generally since the paper focuses on attention, other attentional approaches should be used as benchmarks beyond the WRN baseline. \n\nPlease note that we include other attentional approaches for fine-grained recognition in all tables: \n  * Table 3 -> FAM [A]; \n  * Table 4 -> RA-CNN [B], STN [C], B-CNN [D], PD [E], FCAN [F]; \n  * Table 5 -> DVAN [G], FCAN [F], B-CNN [C], RA-CNN [B]; \n  * Table 6 -> DVAN [G], FCAN [F], RA-CNN [B]. \n\nMoreover, most of these approaches propose singular architectures that have been especially engineered for solving their respective recognition tasks, while the purpose of our approach is to demonstrate that our proposed mechanism works on general purpose architectures.\n\n[A] Rodríguez, P., Cucurull, G., Gonfaus, J. M., Roca, F. X., & Gonzalez, J. (2017). Age and gender recognition in the wild with deep attention. Pattern Recognition, 72, 563-571.\n[B] Fu, J., Zheng, H., & Mei, T. (2017, July). Look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition. In Conf. on Computer Vision and Pattern Recognition.\n[C] Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. In Advances in Neural Information Processing Systems (pp. 2017-2025).\n[D] Lin, T. Y., RoyChowdhury, A., & Maji, S. (2015). Bilinear cnn models for fine-grained visual recognition. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1449-1457).\n[E] Zhang, N., Donahue, J., Girshick, R., & Darrell, T. (2014, September). Part-based R-CNNs for fine-grained category detection. In European conference on computer vision (pp. 834-849). Springer, Cham.\n[F] Liu, X., Xia, T., Wang, J., & Lin, Y. (2016). Fully convolutional attention localization networks: Efficient attention localization for fine-grained recognition. arXiv preprint arXiv:1603.06765.\n[G] Zhao, B., Wu, X., Feng, J., Peng, Q., & Yan, S. (2016). Diversified visual attention networks for fine-grained object classification. arXiv preprint arXiv:1606.08572."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514825531948,"tcdate":1514825531948,"number":8,"cdate":1514825531948,"id":"HkNSDkdQG","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"rkzOQxcgM","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: AnonReviewer3","comment":">> If the authors want to claim that the proposed approach is \"more robust to deformation and clutter” then they should design an experiment that shows that this is the case. \n\nIn the new introduced experiments on Cluttered Translated MNIST (Section 4.1 in the new version of the paper), we confirm that indeed the proposed method is more robust than the baseline.\n\n>> Beyond, the approach seems a little ad hoc. No real rationale is provided for the different mechanisms including the gating etc and certainly no experimental validation is provided to demonstrate the need for these mechanisms. \n\nIn section 3, the rationale for the different mechanisms is mentioned in each of the subsections. For instance, gates regulate the relative importance of the predictions of each attention head. This is important when AW is high and the current input has a few informative regions. In this case, just one attention head would be enough and thus, heads focusing in other regions can be dampened by the gates. This explanation is now added in section 3.4 and the conclusion.\nIn addition, we have included experiments on cluttered MNIST showing that gates are critical to obtaining good performances with high AW (see Figure 4d in the new version of the paper).\n\n>> More generally, it is not clear from reading the paper specifically what computational limitation of the CNN is being solved by the proposed attentional mechanism. \n\nThe proposed paper addresses the same problem as other attentional methods in the literature [A,B,C, ...], i.e. it enhances the model to find the most informative parts of the image and to discard irrelevant information. This is especially relevant for fine-grained recognition, where some details are more informative than other salient features of the image.\n\n[A] Ba, J., Mnih, V., & Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. ICLR2015.\n[B] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., ... & Bengio, Y. (2015, June). Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning (pp. 2048-2057).\n[C] Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. In Advances in Neural Information Processing Systems (pp. 2017-2025).\n\n>> Some of the masks shown in Fig 3 seem rather suspicious and prompt this referee to think that the networks are seriously overfitting to the data.\n\nThe train loss vs validation loss difference does not suggest that the proposed model suffers from greater overfitting than the original architecture. Moreover, inspired by this comment, we have designed a test on cluttered MNIST showing that the attention augmented model generalizes better on the test set when increasing the number of distractors (unseen during training), see Figure 4e in the new version of the paper. We hypothesize that attention prevents the model from memorizing uninformative parts of the image, which could be associated with noise. Section 4.1 and the conclusion now reflect this new finding. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514825477465,"tcdate":1514825477465,"number":7,"cdate":1514825477465,"id":"HkabPkOQM","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"rkzOQxcgM","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: AnonReviewer3","comment":">> For instance, why would attending to a right ear help in gender recognition? \n\nIn the Adience dataset, most women wear earrings, so the network might have learned to look at ears whenever possible.\n\n>> The proposed extension adds several hyperparameters (for instance the number K of attention heads). Apologies if I missed it but I am not clear how this was optimized for the experiments reported. In general, the paper could be clearer. For instance, it is not clear from either the text or Fig 2 how H goes from XxYxK for the attention head o XxYxN for the output head.\n\nIn Figure 2a, Z (of size XxYxN) is convolved with K XxYx1 masks. When these are multiplied by Z again, we obtain K XxYxN feature maps (broadcasting). Figure 2b depicts the output process for 1 of the K XxYxN feature maps. We have updated Figure2 to include all this information.\n\n>> As a final point, I would say that while some of the criticisms could be addressed in a revision, the improvements seem relatively modest. \n\nGiven that we use a strong baseline such a WRN, we do not think that the improvements are modest. Please note that other relevant papers such as Spatial Transformer Networks [A] only reported an improvement of 0.8% on CUB200-2011 with respect to their own baseline (see table 3 in their work), and residual attention networks report 0.05% improvement on Cifar100 [B]. Most importantly, the improvement is consistently obtained across datasets, and in 3 datatsets we outperform current state of the art.\n\n[A] Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. In Advances in Neural Information Processing Systems (pp. 2017-2025).\n[B] Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., ... & Tang, X. (2017). Residual Attention Network for Image Classification. CVPR2017.\n\n>> Given that the focus of the paper is already limited to fine-grained recognition, it seems that the paper would be better suited for a computer vision conference.\n\nThis work helps to build better feature representations applied to Computer Vision, which it is clearly inside the scope of this conference, from the website (http://www.iclr.cc/): “The performance of machine learning methods is heavily dependent on the choice of data representation (or features) on which they are applied…. Applications in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field...”\n\n>>Minor point:  \"we incorporate the advantages of visual and biological attention mechanisms” not sure this statement makes much sense. Seems like visual and biological are distinct attributes but visual attention can be biological (or not, I guess) and it is not clear how biological the proposed approach is. Certainly no attempt is made by the authors to connect to biology.\n\nThe way the proposed mechanism relates to biological attention is similar to the relationship between artificial and real neural networks, this is similarly done in [A]. Thus we have corrected the statement for “we incorporate the advantages inspired by visual and biological attention mechanisms, as stated in [A]”\n\n[A] Ba, J., Mnih, V., & Kavukcuoglu, K. (2014). Multiple object recognition with visual attention. ICLR2015.\n\n>> \"top-down feed-forward attention mechanism” -> it should be just feed-forward attention. Not clear what \"top-down feed-forward” attention could be…\n\nIn the literature, bottom-up attention is referred to the process of finding the most relevant regions of the image at the feature level, i.e. regions that are salient from their surroundings, while top-down attention refers to a high level process which finds the most relevant part of an input taking into account global information [A] (in a CNN top-down usually means to choose the regions to attend at the output instead of directly doing it at the feature level, which is the case for [B,C].)\n\n[A] Connor, Charles E., Howard E. Egeth, and Steven Yantis. \"Visual attention: bottom-up versus top-down.\" Current Biology14.19 (2004): R850-R852.\n[B] Oliva, A., Torralba, A., Castelhano, M. S., & Henderson, J. M. (2003, September). Top-down control of visual attention in object detection. In Image processing, 2003. icip 2003. proceedings. 2003 international conference on (Vol. 1, pp. I-253). IEEE.\n[C] Rodríguez, P., Cucurull, G., Gonfaus, J. M., Roca, F. X., & Gonzalez, J. (2017). Age and gender recognition in the wild with deep attention. Pattern Recognition, 72, 563-571.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514824228988,"tcdate":1514824228988,"number":5,"cdate":1514824228988,"id":"SyTmfkumG","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"Sky96rolf","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: AnonReviewer2","comment":"Thanks for the feedback,\n\n>> 1) Both attention depth and attention width are small. \n\nAlthough higher AD and AW do result in an increment of accuracy, we considered that 2 was enough to demonstrate that the proposed mechanism enhances the baseline models at negligible computational cost. In order to address this concern, we have included experiments on deformable mnist where it can be seen that the performance increases with higher AW, and AD (Figure 4b and 4c in the new version of the paper). \n\n>> The choice of which layer to add this module is unclear to me.  \n\nPlease note that the same placing problem is present in most of the well-known CNN layers such as Dropout, Local-contrast normalization, Spatial Transformers, etc. \nHowever, as we answer to R1’s first question, we have established a systematic methodology which consists in adding the attention mechanism after each subsampling layer of the WRN in order to obtain features of different levels at the smallest possible computational cost. This information is now included at the end of section 3.3, when Table 2 is introduced, and in the second paragraph of section 4. \n\n>> 2) No analysis on using the extra regularization loss actually helps.\n\nThe analysis has been included in section 4.1, where experiments with Cluttered Translated MNIST show that regularization adds an extra performance increment.\n\n>> 3) My main concern is the improvement gain is very small. In Table3, the gain of using the gate module is only 0.1%. It argues that this attention module can be added to any layer but experiments show only 1 layer and 1 attention map already achieve most of the improvement. \n\nWe hope that the new experiments on Cluttered Translated MNIST in section 4.1 help to clarify this point. Also, as it can be seen in Figure 4d, gates are crucial when AD and AW grow. \n\n>> From Table 4 to Table 7, WRNA compared to WRN only improve ~1% on average.  \n\nPlease note that 1% is a remarkable amount given that, for instance, other relevant papers such as Spatial Transformer Networks only reported an improvement of 0.8% on CUB200-2011 with respect to their own baseline (see table 3 in their work). Moreover, in the case of residual attention networks [B], the reported improvement on Cifar100 is 0.05%.\n\n[A] Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. In Advances in Neural Information Processing Systems (pp. 2017-2025).\n[B] Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., ... & Tang, X. (2017). Residual Attention Network for Image Classification. CVPR2017.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514823931826,"tcdate":1514823931826,"number":4,"cdate":1514823931826,"id":"r1V-bJOXG","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"ry2OdYCeM","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Re: AnonReviewer1","comment":"Thank you for your comments,\n\n>> However, the presentation of the method is bit harder to follow. It is not clear to me if the attention modules are applied over all  pooling layers.\n\nAny layer of the network can be augmented with the attention mechanism. We chose to use the augmentation after each pooling layer in order to reduce even further the computational cost. We have clarified this point at the end of section 3.3, when Table 2 is introduced, and in the second paragraph of section 4.\n\n>> How they are combined? \n\nAs it can be seen in Fig 1, 2a, and 2b, a 1x1 convolution is applied to the output of the layer we want to augment, producing an attentional heatmap. This heatmap is then element-wise multiplied with a copy of the layer output, and the result is used to predict the class probabilities and a confidence score. This process is applied to an arbitrary number N of layers, producing N class probability vectors, and N confidence scores. Then all the class predictions are weighted by the confidence scores (softmax normalized so that they add-up to 1) and averaged (using Eq 9). This is the final combined prediction of the network. This overall explanation is now placed in the “Overview” section before section 3.1.\n\n>> Why use cross -correlation as the regulariser? Why not much stronger constraint such as orthogonality over elements of M in equation 1? \n\nPlease note that the 2-norm operation requires to square all the elements of the matrix, thus the minimum norm is achieved when the inner product of all the different pairs of masks is 0 (orthogonal). Thus, orthogonality is constrained by regularizing the 2-norm of a matrix. This is now clarified after Eq 3.\n\n>> What is the impact of this regularisation?\n\nIn order to address questions R1.1, etc.. we have added experiments on deformable mnist, showing the importance of each module. In figure 4d it can be seen that the regularized model performs better than the unregularized counterpart.\n\n>> Why use soft-max in equation 1? One may use a Sigmoid as well? Is it better to use soft-max?\n\nWe use softmax because it constrains the network to choose only one region in the image, thus forcing it to learn which is the most discriminative region. Using sigmoids attains the risk of just learning to predict 1s for every region, or all zeros. Note that multiple regions can still be identified by using multiple attention heads. This explanation has been included in section 3.1.\n\n>> Equation 9 is not entirely clear to me. Undefined notations.\n\n“output” is the predicted vector of class probabilities, “g_net” is the confidence score for the original output of the network “output_net” (without attention). This information has been appended after equation 9.\n\n>> In Table 2, why stop from AD= 2 and AW=2?  What is the performance of AD=1, AW=1 with G? Why not perform this experiment over all 5 datasets? \n\nWe had to constrain the number of experiments to a limited amount of time and resources, which makes it difficult to brute-force all hyperparameter combinations with all datasets. We hope that this question is now clarified with the experiments on deformable-mnist (Section 4.1 in the new version of the paper).\n\n>> Is this performances, dataset specific?\n\nNo, generally increasing AD and AW results in better performance in all datasets.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514823590107,"tcdate":1514823590107,"number":3,"cdate":1514823590107,"id":"B10s11dmz","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Comment","forum":"rJe7FW-Cb","replyto":"rJe7FW-Cb","signatures":["ICLR.cc/2018/Conference/Paper701/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper701/Authors"],"content":{"title":"Paper v2.0","comment":"We thank all the reviewers for their highly valuable feedback. We have addressed all comments one by one, and we have accordingly updated the manuscript. Changes appear in blue.\nList of changes:\n  * A new table (Table 2) in the introduction has been added to clarify the advantages of our proposal with respect to the literature.\n  * An overview section has been added to section 3 (Section 3.1) to summarize and clarify how the different submodules fit together.\n  * Undefined notations have been clarified in equation 9.\n  * Ablation experiments on cluttered translated MNIST have been introduced in section 4.1.\n  * Textual clarifications addressing comments from the reviewers.\n\nThanks to these improvements resulting from the review process, the manuscript has substantially clarified the contribution of our work, has improved the technical quality and has also enhanced the experimental quality. We trust these improvements make it even more appealing for publication at this conference.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515642494329,"tcdate":1512114292359,"number":3,"cdate":1512114292359,"id":"ry2OdYCeM","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Review","forum":"rJe7FW-Cb","replyto":"rJe7FW-Cb","signatures":["ICLR.cc/2018/Conference/Paper701/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review for A Painless Attention Mechanism for Convolutional Neural Networks ","rating":"6: Marginally above acceptance threshold","review":"Paper presents an interesting attention mechanism for fine-grained image classification. Introduction states that the method is simple and easy to understand. However, the presentation of the method is bit harder to follow. It is not clear to me if the attention modules are applied over all  pooling layers. How they are combined? \n\nWhy use cross -correlation as the regulariser? Why not much stronger constraint such as orthogonality over elements of M in equation 1? What is the impact of this regularisation?\n\nWhy use soft-max in equation 1? One may use a Sigmoid as well? Is it better to use soft-max?\n\nEquation 9 is not entirely clear to me. Undefined notations.\n\nIn Table 2, why stop from AD= 2 and AW=2?  What is the performance of AD=1, AW=1 with G? Why not perform this experiment over all 5 datasets? Is this performances, dataset specific?\n\nThe method is compared against 5 datasets. Obtained results are quite good.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515642494375,"tcdate":1511902599415,"number":2,"cdate":1511902599415,"id":"Sky96rolf","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Review","forum":"rJe7FW-Cb","replyto":"rJe7FW-Cb","signatures":["ICLR.cc/2018/Conference/Paper701/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Improvement gain is small","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a feed-forward attention mechanism for fine-grained image classification. It is modular and can be added to any convolutional layer, the attention model uses CNN feature activations to find the most informative parts then combine with the original feature map for the final prediction. Experiments show that wide residual net together with this new attention mechanism achieve slightly better performance on several fine-grained image classification tasks.\n\nStrength of this work:\n1) It is end-to-end trainable and doesn't require multiple stages, prediction can be done in single feedforward pass.\n2) Easy to train and doesn't increase the model size a lot.\n\nWeakness:\n1) Both attention depth and attention width are small. The choice of which layer to add this module is unclear to me. \n2) No analysis on using the extra regularization loss actually helps.\n3) My main concern is the improvement gain is very small. In Table3, the gain of using the gate module is only 0.1%. It argues that this attention module can be added to any layer but experiments show only 1 layer and 1 attention map already achieve most of the improvement. From Table 4 to Table 7, WRNA compared to WRN only improve ~1% on average.  \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515642494420,"tcdate":1511813993821,"number":1,"cdate":1511813993821,"id":"rkzOQxcgM","invitation":"ICLR.cc/2018/Conference/-/Paper701/Official_Review","forum":"rJe7FW-Cb","replyto":"rJe7FW-Cb","signatures":["ICLR.cc/2018/Conference/Paper701/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A limited evaluation and a limited contribution","rating":"5: Marginally below acceptance threshold","review":"The manuscript describes a novel attentional mechanism applied to fine-grained recognition. \n\nOn the positive side, the approach seems to consistently improve the recognition accuracy of the baseline  (a wide residual net). The approach is also consistently tested on the main fine-grained recognition datasets (the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100).\n\nOn the negative side, the paper could be better written and motivated.\n\nFirst, some claimed are made about how the proposed approach \"enhances most of the desirable properties from previous approaches” (see pp 1-2) but these claims are never backed up. More generally since the paper focuses on attention, other attentional approaches should be used as benchmarks beyond the WRN baseline. If the authors want to claim that the proposed approach is \"more robust to deformation and clutter” then they should design an experiment that shows that this is the case. \n\nBeyond, the approach seems a little ad hoc. No real rationale is provided for the different mechanisms including the gating etc and certainly no experimental validation is provided to demonstrate the need for these mechanisms. More generally, it is not clear from reading the paper specifically what computational limitation of the CNN is being solved by the proposed attentional mechanism. \n\nSome of the masks shown in Fig 3 seem rather suspicious and prompt this referee to think that the networks are seriously overfitting to the data. For instance, why would attending to a right ear help in gender recognition? \n\nThe proposed extension adds several hyperparameters (for instance the number K of attention heads). Apologies if I missed it but I am not clear how this was optimized for the experiments reported. In general, the paper could be clearer. For instance, it is not clear from either the text or Fig 2 how H goes from XxYxK for the attention head o XxYxN for the output head.\n\nAs a final point, I would say that while some of the criticisms could be addressed in a revision, the improvements seem relatively modest. Given that the focus of the paper is already limited to fine-grained recognition, it seems that the paper would be better suited for a computer vision conference.\n\n\nMinor point: \n\n\"we incorporate the advantages of visual and biological attention mechanisms” not sure this statement makes much sense. Seems like visual and biological are distinct attributes but visual attention can be biological (or not, I guess) and it is not clear how biological the proposed approach is. Certainly no attempt is made by the authors to connect to biology.\n\n\"top-down feed-forward attention mechanism” -> it should be just feed-forward attention. Not clear what \"top-down feed-forward” attention could be...","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]}},{"tddate":null,"ddate":null,"tmdate":1514823202042,"tcdate":1509132568152,"number":701,"cdate":1509739149550,"id":"rJe7FW-Cb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJe7FW-Cb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Painless Attention Mechanism for Convolutional Neural Networks","abstract":"We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. The proposed mechanism reuses CNN feature activations to find the most informative parts of the image at different depths with the help of gating mechanisms and without part annotations. Thus, it can be used to augment any layer of a CNN to extract low- and high-level local information to be more discriminative. \n\nDifferently, from other approaches, the mechanism we propose just needs a single pass through the input and it can be trained end-to-end through SGD. As a consequence, the proposed mechanism is modular, architecture-independent, easy to implement, and faster than iterative approaches.\n\nExperiments show that, when augmented with our approach, Wide Residual Networks systematically achieve superior performance on each of five different fine-grained recognition datasets: the Adience age and gender recognition benchmark, Caltech-UCSD Birds-200-2011, Stanford Dogs, Stanford Cars, and UEC Food-100, obtaining competitive and state-of-the-art scores.","pdf":"/pdf/f567a7427678ea1e282a25a03b4d7c78faa2a3bd.pdf","TL;DR":"We enhance CNNs with a novel attention mechanism for fine-grained recognition. Superior performance is obtained on 5 datasets.","paperhash":"anonymous|a_painless_attention_mechanism_for_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A Painless Attention Mechanism for Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJe7FW-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper701/Authors"],"keywords":["computer vision","deep learning","convolutional neural networks","attention"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}