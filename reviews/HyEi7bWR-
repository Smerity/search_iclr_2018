{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222713143,"tcdate":1511908793946,"number":3,"cdate":1511908793946,"id":"HyGpBPslM","invitation":"ICLR.cc/2018/Conference/-/Paper664/Official_Review","forum":"HyEi7bWR-","replyto":"HyEi7bWR-","signatures":["ICLR.cc/2018/Conference/Paper664/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Novel parametrization of RNNs allows representing orthogonal weight matrices relatively easily","rating":"5: Marginally below acceptance threshold","review":"The paper is clearly written, with a good coverage of previous relevant literature. \nThe contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced.\nTherefore, the paper must show that this new method performs better in some way compared with previous methods. They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT.\n\nPros:\n1. New, relatively simple method for learning orthogonal weight matrices for RNN\n\n2. Clearly written\n\n3. Quite good results on several relevant tasks.\n\nCons:\n1. Technical novelty is somewhat limited\n\n2. Experiments do not evaluate run time, memory use, computational complexity, or stability. Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Orthogonal Recurrent Neural Networks with Scaled Cayley Transform","abstract":"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).  We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform. Such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.","pdf":"/pdf/5012d24079fd0628df63891eb8af9d3cd005d119.pdf","TL;DR":"A novel approach to maintain orthogonal recurrent weight matrices in a RNN.","paperhash":"anonymous|orthogonal_recurrent_neural_networks_with_scaled_cayley_transform","_bibtex":"@article{\n  anonymous2018orthogonal,\n  title={Orthogonal Recurrent Neural Networks with Scaled Cayley Transform},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyEi7bWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper664/Authors"],"keywords":["recurrent neural networks","vanishing gradients","exploding gradients","orthogonal","unitary","long term dependencies","uRNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222713183,"tcdate":1511822587037,"number":2,"cdate":1511822587037,"id":"rkQbrzqxM","invitation":"ICLR.cc/2018/Conference/-/Paper664/Official_Review","forum":"HyEi7bWR-","replyto":"HyEi7bWR-","signatures":["ICLR.cc/2018/Conference/Paper664/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An alternative parametrization of Unitary RNNs","rating":"6: Marginally above acceptance threshold","review":"This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks.\n\nI think the paper is well-written.  Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method.\n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments?\n\n2- How sensitive is the results to the number of -1 in the diagonal matrix?\n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs. In MNIST experiment, for example, better numbers are reported for larger LSTMs. I think matching the number of hidden units could be helpful. Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000. I appreciate if authors can provide more results in these settings.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Orthogonal Recurrent Neural Networks with Scaled Cayley Transform","abstract":"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).  We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform. Such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.","pdf":"/pdf/5012d24079fd0628df63891eb8af9d3cd005d119.pdf","TL;DR":"A novel approach to maintain orthogonal recurrent weight matrices in a RNN.","paperhash":"anonymous|orthogonal_recurrent_neural_networks_with_scaled_cayley_transform","_bibtex":"@article{\n  anonymous2018orthogonal,\n  title={Orthogonal Recurrent Neural Networks with Scaled Cayley Transform},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyEi7bWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper664/Authors"],"keywords":["recurrent neural networks","vanishing gradients","exploding gradients","orthogonal","unitary","long term dependencies","uRNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222713222,"tcdate":1511818717939,"number":1,"cdate":1511818717939,"id":"SkLk8W9lM","invitation":"ICLR.cc/2018/Conference/-/Paper664/Official_Review","forum":"HyEi7bWR-","replyto":"HyEi7bWR-","signatures":["ICLR.cc/2018/Conference/Paper664/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review of \"Orthogonal Recurrent Neural Networks with Scaled Cayley Transform\"","rating":"7: Good paper, accept","review":"This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques\n\nComments:\n-- It’s not clear to me how D is determined for each test. Given the definition in Theorem 3.1 it seems like you would have to have some knowledge of how many eigenvalues in W you expect to be close to -1. \n-- For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant.\n-- The ylabel in Figure 1 is “Test Loss” which I didn’t see defined. Is this test loss the cross entropy? If so, I think it would be more effective to label the plot with that.\n-- The plots in figure 1 and 2 have different colors to represent the same set of techniques. I would suggest keeping a  consistent color scheme\n-- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified.\n-- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified.\n-- How is test set accuracy defined in section 5.3? Classifying digits? Recreating digits? \n-- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%. However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters.\n-- Overall it’s unclear to me how to completely determine the benefit of this technique over the others because, for each of the tests, different techniques may have superior performance. For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy. scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3. Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset. I don’t understand the significance of each test and why the relative performance of the techniques vary from one to the other.\n-- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability. You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Orthogonal Recurrent Neural Networks with Scaled Cayley Transform","abstract":"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).  We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform. Such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.","pdf":"/pdf/5012d24079fd0628df63891eb8af9d3cd005d119.pdf","TL;DR":"A novel approach to maintain orthogonal recurrent weight matrices in a RNN.","paperhash":"anonymous|orthogonal_recurrent_neural_networks_with_scaled_cayley_transform","_bibtex":"@article{\n  anonymous2018orthogonal,\n  title={Orthogonal Recurrent Neural Networks with Scaled Cayley Transform},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyEi7bWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper664/Authors"],"keywords":["recurrent neural networks","vanishing gradients","exploding gradients","orthogonal","unitary","long term dependencies","uRNN"]}},{"tddate":null,"ddate":null,"tmdate":1509739173124,"tcdate":1509131163550,"number":664,"cdate":1509739170461,"id":"HyEi7bWR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyEi7bWR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Orthogonal Recurrent Neural Networks with Scaled Cayley Transform","abstract":"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).  We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform. Such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.","pdf":"/pdf/5012d24079fd0628df63891eb8af9d3cd005d119.pdf","TL;DR":"A novel approach to maintain orthogonal recurrent weight matrices in a RNN.","paperhash":"anonymous|orthogonal_recurrent_neural_networks_with_scaled_cayley_transform","_bibtex":"@article{\n  anonymous2018orthogonal,\n  title={Orthogonal Recurrent Neural Networks with Scaled Cayley Transform},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyEi7bWR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper664/Authors"],"keywords":["recurrent neural networks","vanishing gradients","exploding gradients","orthogonal","unitary","long term dependencies","uRNN"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}