{"notes":[{"tddate":null,"ddate":null,"tmdate":1512325530484,"tcdate":1512325530484,"number":3,"cdate":1512325530484,"id":"B1fsb6bWz","invitation":"ICLR.cc/2018/Conference/-/Paper13/Official_Review","forum":"rkA1f3NpZ","replyto":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference/Paper13/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple but effective idea, light in the presentation and experimentations ","rating":"4: Ok but not good enough - rejection","review":"In this manuscript, the authors empirically investigated the robustness of some different deep neural networks ensembles to two types of attacks, namely FGSM and BIM, on two popular datasets, MNIST and CIFAR10. The authors concluded that the ensembles are more accurate on both clean and adversaries samples than a single deep neural network. Therefore, the ensembles are more robust in terms of the ability to correctly classify the adversary attacks.\n\nAs the authors stated, an attack that is designed to fool one network does not necessarily fool the other networks in the same way. This is likely why ensembles appear more robust than single deep learners. However, robustness of ensembles to the white-box attacks that are generated from the ensemble is still low for FGS. Generally speaking, although FGS attacks generated from one network can fool less the whole ensembles, generating FGS adversaries from a given ensemble is still able to effectively fool it. Therefore, if the attacker has access to the ensemble or even know the classification system based on that ensemble, then the ensemble-based system is still vulnerable to the attacks generated specifically from it. Simple ensemble methods are not likely to confer significant robustness gains against adversaries.\n\nIn contrast to FGS results, surprisingly BIM-Grad1 is able to fool more the ensemble than BIM-Grad2. Therefore, it seems that if the attacker makes BIM adversaries from only a single classifier, then she can simply and yet effectively mislead the whole ensemble. In comparison to BIM-Grad2, BIM-Grad1 results show that BIM attacks from one network (BIM-Grad1) can more successfully fool the other different networks in the ensembles in a similar way! BIM-Grad2 is not that much able to fool the ensemble-based system even this attack generated from the ensemble (white-box attacks). In order to confirm the robustness of the ensembles to BIM attacks, the authors can do more experiments by generating BIM-Grad2 attacks with higher number of iterations.\n\nIndeed, the low number of iterations might cause the lower rate of success for generating adversaries by BIM-Grad2. In fact, BIM adversaries from the ensembles might require more number of iterations to effectively fool the majority of the members in the ensembles. Therefore, increasing the number of iterations can increase the successful rate of generating BIM-Average Grad2 adversaries. Note that in this case, it is recommended to compare the amount of distortion (perturbation) with different number of iterations in order to indicate the effectiveness of the ensembles to white-box BIM attacks.\n\nDespite to averaging the output probabilities to compute the ensemble final prediction, the authors generated the adversaries from the ensemble by computing the sum of the gradients of the classifiers loss. A proper approach would have been to average of these gradients. The fact the sum is not divided by the number of members (i.e., sum of gradients instead of average of gradients) is increasing the step size of the adversarial method proportionally to the ensemble size, raising questions on the validity of the comparison with the single-model adversarial generation.\n\nOverall, I found the paper as having several methodological flaws in the experimental part, and rather light in terms of novel ideas. As noticed in the introduction, the idea of using ensemble for enhancing robustness as already been proposed. Making a paper only to restate it, is too light for acceptation. Moreover, experimental setup using a lot of space for comparing results on standard datasets (i.e., MNIST and CIFAR10), even with long presentation of these datasets. Several issues are raised in the current experiments and require adjustments. Experiments should also be more elaborated to make the case stronger, following at least some of indications provided. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222566145,"tcdate":1511706646927,"number":2,"cdate":1511706646927,"id":"r1k7g8Oxz","invitation":"ICLR.cc/2018/Conference/-/Paper13/Official_Review","forum":"rkA1f3NpZ","replyto":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference/Paper13/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A solid and effective idea, but a limited analysis. ","rating":"5: Marginally below acceptance threshold","review":"Summary: This paper proposes to use ensembling as an adversarial defense mechanism. The defense is evaluated on MNIST and CIFAR10 ans shows reasonable performance against FGSM and BIM.\n\nClarity: The paper is clearly written and easy to follow. \n\nOriginality: Building an ensemble of models is a well-studied strategy that was shown long ago to improve generalization. As far as I know, this paper is however the first to empirically study the robustness of ensembles against adversarial examples. \n\nQuality: While this paper contributes to show that ensembling works reasonably well against adversarial examples, I find the contribution limited in general.\n- The method is not compared against other adversarial defenses. \n- The results illustrate that adding Gaussian noise on the training data clearly outperforms the other considered ensembling strategies. However, the authors do not go beyond this observation and do not appear to try to understand why it is the case. \n- Similarly, the Bagging strategy is shown to perform reasonably well (although it appears as a weaker strategy than Gaussian noise) but no further analysis is carried out. For instance, it is known that the reduction of variance is maximal in an ensemble when its constituents are maximally decorrelated. It would be worth studying more systematically if this correlation (or 'diversity') has an effect on the robustness against adversarial examples. \n- I didn't understand the motivation behind considering two distinct gradient estimators. Why deriving the exact gradient of an ensemble is more complicated?\n\nPros: \n- Simple and effective strategy.\n- Clearly written paper. \nCons:\n- Not compared against other defenses.\n- Limited analysis of the results. \n- Ensembling neural networks is very costly in terms of training. This should be considered.\n\nOverall, this paper presents an interesting and promising direction of research. However, I find the current analysis (empirically and/or theoretically) to be too limited to constitutes a solid enough piece of work. For this reason, I do not recommend this paper for acceptance. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222566185,"tcdate":1511532659706,"number":1,"cdate":1511532659706,"id":"By2d_sBef","invitation":"ICLR.cc/2018/Conference/-/Paper13/Official_Review","forum":"rkA1f3NpZ","replyto":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference/Paper13/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple technique missing comparison with related ones.","rating":"6: Marginally above acceptance threshold","review":"This paper describes the use of ensemble methods to improve the robustness of neural networks to adversarial examples. Adversarial examples are images that have been slightly modified (e.g. by adding some small perturbation) so that the neural network will predict a wrong class label.\n\nEnsemble methods have been used by the machine learning community since long time ago to provide more robust and accurate predictions.\n\nIn this paper the authors explore their use to increase the robustness of neural networks to adversarial examples.\n\nDifferent ensembles of 10 neural networks are considered. These include techniques such as bagging or injecting noise in the \ntraining data. \n\nThe results obtained show that ensemble methods can sometimes significantly improve the robustness against adversarial examples. However,\nthe performance of the ensemble is also highly deteriorated by these examples, although not as much as the one of a single neural network.\n\nThe paper is clearly written.\n\nI think that this is an interesting paper for the deep learning community showing the benefits of ensemble methods against adversarial\nexamples. My main concern with this paper is the lack of comparison with alternate techniques to increase the robustness against adversarial examples. The authors should have compared with the methods described in:\n\n(Goodfellow et al., 2014; Papernot et al., 2016c), \n(Papernot et al., 2016d) \n(Gu & Rigazio, 2014)\n\nFurthermore, the ensemble approach has the main disadvantage of increasing the prediction time by a lot. For example, with 10 elements in the ensemble, predictions are 10 times more expensive.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]}},{"tddate":null,"ddate":null,"tmdate":1509739530284,"tcdate":1508323814314,"number":13,"cdate":1509739527629,"id":"rkA1f3NpZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}