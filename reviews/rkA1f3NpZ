{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222566145,"tcdate":1511706646927,"number":2,"cdate":1511706646927,"id":"r1k7g8Oxz","invitation":"ICLR.cc/2018/Conference/-/Paper13/Official_Review","forum":"rkA1f3NpZ","replyto":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference/Paper13/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A solid and effective idea, but a limited analysis. ","rating":"5: Marginally below acceptance threshold","review":"Summary: This paper proposes to use ensembling as an adversarial defense mechanism. The defense is evaluated on MNIST and CIFAR10 ans shows reasonable performance against FGSM and BIM.\n\nClarity: The paper is clearly written and easy to follow. \n\nOriginality: Building an ensemble of models is a well-studied strategy that was shown long ago to improve generalization. As far as I know, this paper is however the first to empirically study the robustness of ensembles against adversarial examples. \n\nQuality: While this paper contributes to show that ensembling works reasonably well against adversarial examples, I find the contribution limited in general.\n- The method is not compared against other adversarial defenses. \n- The results illustrate that adding Gaussian noise on the training data clearly outperforms the other considered ensembling strategies. However, the authors do not go beyond this observation and do not appear to try to understand why it is the case. \n- Similarly, the Bagging strategy is shown to perform reasonably well (although it appears as a weaker strategy than Gaussian noise) but no further analysis is carried out. For instance, it is known that the reduction of variance is maximal in an ensemble when its constituents are maximally decorrelated. It would be worth studying more systematically if this correlation (or 'diversity') has an effect on the robustness against adversarial examples. \n- I didn't understand the motivation behind considering two distinct gradient estimators. Why deriving the exact gradient of an ensemble is more complicated?\n\nPros: \n- Simple and effective strategy.\n- Clearly written paper. \nCons:\n- Not compared against other defenses.\n- Limited analysis of the results. \n- Ensembling neural networks is very costly in terms of training. This should be considered.\n\nOverall, this paper presents an interesting and promising direction of research. However, I find the current analysis (empirically and/or theoretically) to be too limited to constitutes a solid enough piece of work. For this reason, I do not recommend this paper for acceptance. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222566185,"tcdate":1511532659706,"number":1,"cdate":1511532659706,"id":"By2d_sBef","invitation":"ICLR.cc/2018/Conference/-/Paper13/Official_Review","forum":"rkA1f3NpZ","replyto":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference/Paper13/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple technique missing comparison with related ones.","rating":"6: Marginally above acceptance threshold","review":"This paper describes the use of ensemble methods to improve the robustness of neural networks to adversarial examples. Adversarial examples are images that have been slightly modified (e.g. by adding some small perturbation) so that the neural network will predict a wrong class label.\n\nEnsemble methods have been used by the machine learning community since long time ago to provide more robust and accurate predictions.\n\nIn this paper the authors explore their use to increase the robustness of neural networks to adversarial examples.\n\nDifferent ensembles of 10 neural networks are considered. These include techniques such as bagging or injecting noise in the \ntraining data. \n\nThe results obtained show that ensemble methods can sometimes significantly improve the robustness against adversarial examples. However,\nthe performance of the ensemble is also highly deteriorated by these examples, although not as much as the one of a single neural network.\n\nThe paper is clearly written.\n\nI think that this is an interesting paper for the deep learning community showing the benefits of ensemble methods against adversarial\nexamples. My main concern with this paper is the lack of comparison with alternate techniques to increase the robustness against adversarial examples. The authors should have compared with the methods described in:\n\n(Goodfellow et al., 2014; Papernot et al., 2016c), \n(Papernot et al., 2016d) \n(Gu & Rigazio, 2014)\n\nFurthermore, the ensemble approach has the main disadvantage of increasing the prediction time by a lot. For example, with 10 elements in the ensemble, predictions are 10 times more expensive.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]}},{"tddate":null,"ddate":null,"tmdate":1509739530284,"tcdate":1508323814314,"number":13,"cdate":1509739527629,"id":"rkA1f3NpZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkA1f3NpZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks","abstract":"Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to  make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.","pdf":"/pdf/30e2ad5a4185db0eea358cbf62256ced5f5f3edf.pdf","TL;DR":"Using ensemble methods as a defense to adversarial perturbations against deep neural networks.","paperhash":"anonymous|ensemble_methods_as_a_defense_to_adversarial_perturbations_against_deep_neural_networks","_bibtex":"@article{\n  anonymous2018ensemble,\n  title={Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkA1f3NpZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper13/Authors"],"keywords":["Ensemble Method","Adversarial Perturbations","Deep Neural Networks","Defense","Attack"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}