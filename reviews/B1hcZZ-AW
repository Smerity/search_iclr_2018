{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222709340,"tcdate":1512092591915,"number":2,"cdate":1512092591915,"id":"rJO3m40ef","invitation":"ICLR.cc/2018/Conference/-/Paper653/Official_Review","forum":"B1hcZZ-AW","replyto":"B1hcZZ-AW","signatures":["ICLR.cc/2018/Conference/Paper653/AnonReviewer3"],"readers":["everyone"],"content":{"title":"very good paper","rating":"9: Top 15% of accepted papers, strong accept","review":"Summary:\nThe manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model. The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model. After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding. This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection. The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy. As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models.\n\nReview:\nThe manuscript describes the proposed algorithm in great detail and the description is easy to follow. The experimental analysis of the approach is very convincing and confirms the author’s claims. \nUsing the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation. I am looking forward to seeing work on the research goals outlined in the Future Directions section.\n\nA few questions/comments:\n1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to? Are these multiple rollouts of the policies? If so, shouldn’t the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)? I think I might have missed something here.\n2) Minor: some of the citations are a bit awkward, e.g. on page 7: “algorithm from Williams Williams (1992). I would use the \\citet command from natbib for such citations and \\citep for parenthesized citations, e.g. “... incorporate dark knowledge (Hinton et al., 2015)” or “The MNIST (LeCun et al., 1998) dataset...” \n3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations.\n4) Appendix: Section 8 states “Below are the results”, but the figure landed on the next page. I would either try to force the figures to be output at that position (not in or after Section 9) or write \"Figures X-Y show the results\". Also in Section 11, Figure 13 should be referenced with the \\ref command\n5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)?\n6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments?\n7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution?\n\nOverall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning","abstract":"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.","pdf":"/pdf/9dd5216601d9d515e2777623a6e5e129230bae69.pdf","TL;DR":"A novel reinforcement learning based approach to compress deep neural networks with knowledge distillation","paperhash":"anonymous|n2n_learning_network_to_network_compression_via_policy_gradient_reinforcement_learning","_bibtex":"@article{\n  anonymous2018n2n,\n  title={N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hcZZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper653/Authors"],"keywords":["Deep learning","Neural networks","Model compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222709383,"tcdate":1511755791705,"number":1,"cdate":1511755791705,"id":"S1dzeGtxz","invitation":"ICLR.cc/2018/Conference/-/Paper653/Official_Review","forum":"B1hcZZ-AW","replyto":"B1hcZZ-AW","signatures":["ICLR.cc/2018/Conference/Paper653/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process.\n\nThe draft is well-written, and the method is clearly explained. However, I have the following concerns for this draft:\n\n1. The technical contribution is not enough. First, the use of reinforcement learning is quite straightforward. Second, the proposed method seems not significantly different from the architecture search method in [1][2] – their major difference seems to be the use of “remove” instead of “add” when manipulating the parameters. It is unclear whether this difference is substantial, and whether the proposed method is better than the architecture search method.\n\n2. I also have concern with the time efficiency of the proposed method. Reinforcement learning involves multiple rounds of knowledge distillation, and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations. Therefore, the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets (such as ImageNet). It would be necessary for the authors to make direct discussions on this issue, in order to convince others that their proposed method has practical value.\n\n[1] Zoph, Barret, and Quoc V. Le. \"Neural architecture search with reinforcement learning.\" ICLR (2017).\n[2] Baker, Bowen, et al. \"Designing Neural Network Architectures using Reinforcement Learning.\" ICLR (2017).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning","abstract":"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.","pdf":"/pdf/9dd5216601d9d515e2777623a6e5e129230bae69.pdf","TL;DR":"A novel reinforcement learning based approach to compress deep neural networks with knowledge distillation","paperhash":"anonymous|n2n_learning_network_to_network_compression_via_policy_gradient_reinforcement_learning","_bibtex":"@article{\n  anonymous2018n2n,\n  title={N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hcZZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper653/Authors"],"keywords":["Deep learning","Neural networks","Model compression"]}},{"tddate":null,"ddate":null,"tmdate":1509739179186,"tcdate":1509130644518,"number":653,"cdate":1509739176521,"id":"B1hcZZ-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1hcZZ-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning","abstract":"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger 'teacher' network as input and outputs a compressed 'student' network derived from the 'teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large 'teacher' model. In the second stage, another  recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input 'teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller 'teacher' networks can be used to rapidly speed up training on larger 'teacher' networks.","pdf":"/pdf/9dd5216601d9d515e2777623a6e5e129230bae69.pdf","TL;DR":"A novel reinforcement learning based approach to compress deep neural networks with knowledge distillation","paperhash":"anonymous|n2n_learning_network_to_network_compression_via_policy_gradient_reinforcement_learning","_bibtex":"@article{\n  anonymous2018n2n,\n  title={N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1hcZZ-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper653/Authors"],"keywords":["Deep learning","Neural networks","Model compression"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}