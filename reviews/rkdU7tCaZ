{"notes":[{"tddate":null,"ddate":null,"tmdate":1514769534886,"tcdate":1514769534886,"number":4,"cdate":1514769534886,"id":"SywY2Zvmz","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Comment","forum":"rkdU7tCaZ","replyto":"B1Z3O3HeG","signatures":["ICLR.cc/2018/Conference/Paper96/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper96/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thanks for your review.\n\n“Looking at the proposed update rule for Dynamic Evaluation though, the Global Prior seems to be an implementation of the Fast Weights idea. It would be great to explore that connection, or at least learn about how much the Global Prior helps.”\n\nThis reviewer makes an insightful comment about the relationship between dynamic evaluation and fast weights. Dynamic evaluation in general does relate to fast weights (and could even be considered a type of fast weights, although it differs from traditional fast weights in the update mechanism), and the global prior we use is similar to the decay sometimes used in fast weights. We added a paragraph about this in the related work section, and updated the paper to mention the relationship with the decay rule of fast weights when we introduce the global prior. \n\nWe did provide some experiments exploring how much the global prior helps in the submitted version of the paper in table 1; a simple L2 global prior helps slightly, and scaling decay rates by RMS gradient values helps a bit more. \n\n“The sparse update idea feels very much an afterthought and so do the experiments with Spanish.”\n\nWe included the sparse update idea to address the high memory cost of dynamic evaluation when mini-batching. We included the Spanish experiments to show how dynamic evaluation handles domain adaptation, and as an illustration of the features that dynamic evaluation can learn to model on the fly. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1514769129498,"tcdate":1514769129498,"number":3,"cdate":1514769129498,"id":"HJ-esWv7G","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Comment","forum":"rkdU7tCaZ","replyto":"Sk-EjzwxG","signatures":["ICLR.cc/2018/Conference/Paper96/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper96/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Thanks for your review. \n\nThis reviewer requested that we add a word-level experiment on a larger dataset, and we are pleased to say that we were able to include experiments on word-level text8 in section 7.2 of the updated paper. In summary, we achieved test perplexities of static eval: 87.5, neural cache: 75.1, dynamic eval: 70.3 .These results show that dynamic evaluation still provides a large improvement to word-level language modelling on a larger dataset. \n\nAnother point about the word-level experiments is that WikiText-103 and WikiText-2 use the same test set. Our result of 44.3 on the WikiText test set (using WikiText-2 for training) outperforms the static model on WikiText-103 from the neural cache paper (Grave et al. 2017) , which achieves a perplexity of 48.7.  Our result also approaches the performance of LSTM+neural cache on WikiText-103 from (Grave et al. 2017), which achieved a perplexity of 40.8. So despite using 50 times less data, our results on WikiText-2 are competitive with previous approaches trained on WikiText-103.\n\nAs for the memory requirements of the method, we did present a result for character-level language modelling with our sparse dynamic evaluation that used 0.5% of the number of adaptation parameters of regular dynamic evaluation.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1514768484682,"tcdate":1514768484682,"number":2,"cdate":1514768484682,"id":"HyTDubw7z","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Comment","forum":"rkdU7tCaZ","replyto":"BkGlZbceG","signatures":["ICLR.cc/2018/Conference/Paper96/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper96/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thank you for your review. \n\nThis reviewer makes unreasonable claims that the improvements in the paper are “marginal”, and that language modelling is not a sufficient benchmark. As pointed out by AnonReviewer2 in the comments of this review, the results in this paper are quite strong, and language modelling is very sensible for evaluating new techniques. This reviewer also claims that the paper is not well-written, but provides very little evidence to support this.  Overall, this reviewer has no valid scientific criticisms of the paper.\n \n\"the paper also shows (marginal) improvement from a conventional method (neural cache RNNLM)\"\n\nOur results on WikiText-2, where we demonstrate a 7.7 perplexity point improvement over the previous state-of-the-art, would be considered far more than a \"marginal\" improvement by almost any standard. For instance, we report much larger perplexity gains on WikiText-2 as compared with contemporary ICLR submissions such as [1,2,3], which all use similar baselines to our experiments. Our improvements to character-level language modelling were also far more than \"marginal\".\n\n\"Some variables are not explicitly introduced when they are appeared including i, n, g, and l\"\n\n g and l are not variables, they are subscripts used to denote \"global\" and \"local\".  If the reviewer thought g and l were variables, we can understand why the reviewer may have been confused by our explanations of our method. However, we did explicitly introduce all variables that use g and l subscripts, so this really should have been clear to the reviewer.\n\nThe reviewer also mentions a few minor variables that are not “explicitly introduced”, however every one of these variables is defined implicitly in sequence/set notation. \n\n\" n is already used in Section 2 as a number of sequences.\"\n\nTo avoid confusion, we replaced n in section 2 with M, since we also use M in section 3 as the number of sequences in a slightly different context.\n\n“Abstract: it is difficult to guess the characteristics of the proposed method only with a term “dynamic evaluation”. It’s better to explain it in more detail in the abstract.”\n\n Our use of the term dynamic evaluation was described in the second sentence of the abstract. We elected not to describe the specific engineering details of our dynamic evaluation method because this is beyond the scope of the abstract.\n\n\"It’s better to provide relative performance (comparison) of the numbers (perplexity and bits/char) from conventional methods.\" \n\nWe elected not to provide relative performance comparisons, because the \"deltas\" to perplexity and bits/character are almost meaningless without knowledge of how strong the baseline numbers are. Providing the static evaluation numbers alongside the dynamic evaluation numbers would also be unreasonable as it is too many results for an abstract. Providing the overall results with dynamic evaluation is the most concise way to demonstrate the effectiveness of our method, especially since all of these results improve the state-of-the-art.\n\n\"Why does the paper only provide examples for SGD and RMSprop? Can we apply it to other optimization methods including Adam and Adadelta?\"\n\nThe goal of these experiments is to demonstrate the utility of the proposed modifications of dynamic evaluation as compared to past approaches (which used SGD). There are an infinite number of dynamic evaluation approaches, we don't claim anywhere that ours is the best possible-- just that all of our suggested modifications improve on past approaches.\n\nAs for the two approaches suggested by the reviewer, ADAM or ADAM derived methods could be reasonable to use for dynamic evaluation, but were found not to work as well in preliminary experiments. Adadelta likely would not be sensible for dynamic evaluation, because the learning rates of Adadelta decrease over training.  If dynamic evaluation were applied with Adadelta, the rate of adaptation to recent history would decrease later in the test set, which would likely hurt performance. \n\n\"equation (9): is this new matrix introduced for every layer? Need some explanations.\"\n\nThe mLSTM we applied this to only used 1 recurrent layer, so this distinction was arbitrary in the context of our experiments. In a multilayer-RNN, the new matrix could be introduced for every layer or just 1 layer. We've added a sentence in the paper clarifying this point. \n\n\"It’s better to provide the citation of Chainer. \" \n\nwe added this to the current version.\n\n“AWD-LSTM: The paper should provide the full name of AWD-LSTM when it is first appeared.”\n\nWe now provide the full name of AWD-LSTM at first appearance.\n\n[1] Memory-based Parameter Adaptation. ICLR 2018 submission. https://openreview.net/pdf?id=SkFqf0lAZ\n\n[2] Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. ICLR 2018 submission. https://openreview.net/pdf?id=HkwZSG-CZ \n\n[3] Fraternal Dropout. ICLR 2018 submission . https://openreview.net/pdf?id=SJyVzQ-C-\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1512473201677,"tcdate":1512473201677,"number":1,"cdate":1512473201677,"id":"S19dzZ4bM","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Comment","forum":"rkdU7tCaZ","replyto":"BkGlZbceG","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper96/AnonReviewer2"],"content":{"title":"Re: perplexity vs MT and ASR","comment":"Vanilla neural machine translation can be viewed as conditional language modelling (most often trained for perplexity) with additional evaluation noise in the form of BLEU. Until we find a better loss, language modelling is a very good option to explore new models, optimization and evaluation techniques.\n\nThe paper has issues that can be fixed up but it also has great results. It is far from a clear rejection."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1515642535600,"tcdate":1511817449990,"number":3,"cdate":1511817449990,"id":"BkGlZbceG","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Review","forum":"rkdU7tCaZ","replyto":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"dynamic evaluation of neural sequence models\"","rating":"3: Clear rejection","review":"This paper proposes a dynamic evaluation of recurrent neural network language models by updating model parameters with certain segment lengths.\n\nPros.\n- Simple adaptation scheme seems to work, and the paper also shows (marginal) improvement from a conventional method (neural cache RNNLM) \nCons.\n- The paper is not well written due to undefined variables/indexes, confused explanations, not clear explanations of the proposed method in abstract and introduction (see the comments below)\n- Although the perplexity is an important measure, it’s better to show the effectiveness of the proposed method with more practical tasks including machine translation and speech recognition. \n\nComments:\n- Abstract: it is difficult to guess the characteristics of the proposed method only with a term “dynamic evaluation”. It’s better to explain it in more detail in the abstract.\n- Abstract: It’s better to provide relative performance (comparison) of the numbers (perplexity and bits/char) from conventional methods.\n- Section 2: Some variables are not explicitly introduced when they are appeared including i, n, g, and l\n- Section 3: same comment with the above for M. Also n is already used in Section 2 as a number of sequences.\n- Section 5. Why does the paper only provide examples for SGD and RMSprop? Can we apply it to other optimization methods including Adam and Adadelta?\n- Section 6, equation (9): is this new matrix introduced for every layer? Need some explanations.\n- Section 7.1: It’s better to provide the citation of Chainer.\n- Section 7.1 “AWD-LSTM”: The paper should provide the full name of AWD-LSTM when it is first appeared.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1515800083346,"tcdate":1511627561510,"number":2,"cdate":1511627561510,"id":"Sk-EjzwxG","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Review","forum":"rkdU7tCaZ","replyto":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The proposed improvement to the dynamic evaluation method yields some performance gains on the considered benchmarks, but the authors are missing an evaluation on a larger data set.","rating":"7: Good paper, accept","review":"The authors provide an improved implementation of the idea of dynamic evaluation, where the update of the parameters used in the last time step proposed in (Mikolov et al. 2010) is replaced with a back-propagation through the last few time steps, and uses  RMSprop rather than vanilla SGD. The method is applied to word level and character level language modeling where it yields some gains in perplexity. The algorithm also appears able to perform domain adaptation, in a setting where a character-level language model trained mostly on English manages to quickly adapt to a Spanish test set. \n\nWhile the general idea is not novel, the implementation choices matter, and the authors provide one which appears to work well with recently proposed models. The character level experiments on the multiplicative LSTM make the most convincing point, providing a significant improvement over already good results on medium size data sets. Figure 2 also makes a strong case for the method's suitability for applications where domain adaptation is important.\n\nThe paper's weakest part is the word level language modeling section. Given the small size of the data sets considered, the results provided are of limited use, especially since the development set is used to fit the RMSprop hyper-parameters. How sensitive are the final results to this choice? Comparing dynamic evaluation to neural cache models is a good idea, given how both depend en medium-term history: (Grave et al. 2017) provide results on the larger text8 and wiki103, it would be useful to see results for dynamic evaluation at least on the former.\n\nAn indication of the actual additional evaluation time for word-level, char-level and sparse char-level dynamic evaluation would also be welcome.\n\nPros:\n- Good new implementation of an existing idea\n- Significant perplexity gains on character level language modeling\n- Good at domain adaptation\n\nCons:\n- Memory requirements of the method\n- Word-level language modeling experiments need to be run on larger data sets\n\n(Edit: the authors did respond satisfactorily to the original concern about the size of the word-level data set)","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1515642535674,"tcdate":1511536809339,"number":1,"cdate":1511536809339,"id":"B1Z3O3HeG","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Review","forum":"rkdU7tCaZ","replyto":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Dynamic Evaluation or is it Fast Weights at test time?","rating":"7: Good paper, accept","review":"This paper takes AWD-LSTM, a recent, state of the art language model that was equipped with a Neural Cache, swaps the cache out for Dynamic Evaluation and improves the perplexities.\n\nDynamic Evaluation was the baseline that was most obviously missing from the original Neural Cache paper (Grave, 2016) and from the AWD-LSTM paper. In this sense, this work fills in a gap.\n\nLooking at the proposed update rule for Dynamic Evaluation though, the Global Prior seems to be an implementation of the Fast Weights idea. It would be great to explore that connection, or at least learn about how much the Global Prior helps.\n\nThe sparse update idea feels very much an afterthought and so do the experiments with Spanish.\n\nAll in all, this paper could be improved a lot but it is hard to argue with the strong results ...\n\nUpdate:  I'm happy with how the authors have addressed these and other comments in revision 2 of the paper and I've bumped the rating from 6 to 7.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1514765740795,"tcdate":1508967247781,"number":96,"cdate":1509739484072,"id":"rkdU7tCaZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/d7fbc5ec8944466faaf84922c0891472b15af107.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}