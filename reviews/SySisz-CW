{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642533937,"tcdate":1511816884904,"number":3,"cdate":1511816884904,"id":"BJT2Cecgz","invitation":"ICLR.cc/2018/Conference/-/Paper943/Official_Review","forum":"SySisz-CW","replyto":"SySisz-CW","signatures":["ICLR.cc/2018/Conference/Paper943/AnonReviewer3"],"readers":["everyone"],"content":{"title":"As far as I could understand, interesting and potentially useful","rating":"7: Good paper, accept","review":"The paper presents an application of a measure of dependence between the input power spectrum and the frequency response of a filter (Spectral Density Ratio from [Shajarisales et al 2015]) to cascades of two filters in successive layers of deep convolutional networks. The authors apply their newly defined measure to DCGANs and plain VAEs with ReLUs, and show that dependency between successive layers may lead to bad performance. \n\nThe paper proposed a possibly interesting approach, but I found it quite hard to follow, especially Section 4, which I thought was quite unstructured. Also Section 3 could be improved and simplified. It would be also good to add some more related work. I’m not an expert, but I assume there must be some similar idea in CNNs. \n\nFrom my limited point of view, this seems like a sound, novel and potentially useful application of a interesting idea. If the writing was improved, I think the paper may have even more impact.\n\nSmaller details: some spacing issues, some extra punctuation (pg 5 “. . Hence”), a typo (pg. 7 “training of the VAE did not lead to values as satisfactory AS what we obtained with the GAN”)\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the difference between building and extracting patterns: a causal analysis of deep generative models.","abstract":"Generative models are important tools to capture and investigate the properties of complex empirical data. Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data. Does learning the parameters of both architectures obey the same rules? We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference. In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.\n","pdf":"/pdf/414548aca01670f57637d1d495b984e927b771cd.pdf","TL;DR":"We use causal inference to characterise the architecture of generative models","paperhash":"anonymous|on_the_difference_between_building_and_extracting_patterns_a_causal_analysis_of_deep_generative_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the difference between building and extracting patterns: a causal analysis of deep generative models.},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SySisz-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper943/Authors"],"keywords":["GAN","VAE","causality"]}},{"tddate":null,"ddate":null,"tmdate":1515642533974,"tcdate":1511598038412,"number":2,"cdate":1511598038412,"id":"rkA0vi8gz","invitation":"ICLR.cc/2018/Conference/-/Paper943/Official_Review","forum":"SySisz-CW","replyto":"SySisz-CW","signatures":["ICLR.cc/2018/Conference/Paper943/AnonReviewer1"],"readers":["everyone"],"content":{"title":"an interesting analysis of deep generative models using causality","rating":"7: Good paper, accept","review":"This work exploits the causality principle to quantify how the weights of successive layers adapt to each other.  Some interesting results are obtained, such as \"enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures\" . Generally, the result is interesting and the presentation is easy to follow. However, the proposed approach and the experiments are not convincible enough.  For example,  it is hard to obtain the conclusion \"more independence lead to better performance\" from the experimental results. Maybe more justifications are needed.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the difference between building and extracting patterns: a causal analysis of deep generative models.","abstract":"Generative models are important tools to capture and investigate the properties of complex empirical data. Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data. Does learning the parameters of both architectures obey the same rules? We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference. In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.\n","pdf":"/pdf/414548aca01670f57637d1d495b984e927b771cd.pdf","TL;DR":"We use causal inference to characterise the architecture of generative models","paperhash":"anonymous|on_the_difference_between_building_and_extracting_patterns_a_causal_analysis_of_deep_generative_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the difference between building and extracting patterns: a causal analysis of deep generative models.},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SySisz-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper943/Authors"],"keywords":["GAN","VAE","causality"]}},{"tddate":null,"ddate":null,"tmdate":1515642534012,"tcdate":1510771461828,"number":1,"cdate":1510771461828,"id":"H1CZob5Jz","invitation":"ICLR.cc/2018/Conference/-/Paper943/Official_Review","forum":"SySisz-CW","replyto":"SySisz-CW","signatures":["ICLR.cc/2018/Conference/Paper943/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting metric, but no actionable steps or improvement results","rating":"2: Strong rejection","review":"This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN. The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences. By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies). This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework. The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework. They also claim that this analysis can be used to improve the performance of the models.\n\nPros:\n-- SDR characterisation of the convolutional filters is interesting\n-- The authors show that filters with different characteristics are responsible for different aspects of image modelling\n\nCons:\n-- The authors do not actually demonstrate how their analysis can be used to improve VAEs or GANs\n-- Their proposed SDR analysis does not actually find much difference between the generator and the discriminator of the GAN \n-- The clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form). Grammatical and spelling mistake are frequent. More background information could be helpful in section 2.2. All figures (but in particular Figure 3) need more informative captions\n-- The authors talk a lot about disentangling in the introduction, but this does not seem to be followed up in the rest of the text. Furthermore, they are missing a reference to beta-VAE (Higgins et al, 2017) when discussing VAE-based approaches to disentangled factor learning\n\n\nIn summary, the paper is not ready for publication in its current form. The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the difference between building and extracting patterns: a causal analysis of deep generative models.","abstract":"Generative models are important tools to capture and investigate the properties of complex empirical data. Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data. Does learning the parameters of both architectures obey the same rules? We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference. In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.\n","pdf":"/pdf/414548aca01670f57637d1d495b984e927b771cd.pdf","TL;DR":"We use causal inference to characterise the architecture of generative models","paperhash":"anonymous|on_the_difference_between_building_and_extracting_patterns_a_causal_analysis_of_deep_generative_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the difference between building and extracting patterns: a causal analysis of deep generative models.},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SySisz-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper943/Authors"],"keywords":["GAN","VAE","causality"]}},{"tddate":null,"ddate":null,"tmdate":1515322493725,"tcdate":1509137322246,"number":943,"cdate":1510092362034,"id":"SySisz-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SySisz-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the difference between building and extracting patterns: a causal analysis of deep generative models.","abstract":"Generative models are important tools to capture and investigate the properties of complex empirical data. Recent developments such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) use two very similar, but \\textit{reverse}, deep convolutional architectures, one to generate and one to extract information from data. Does learning the parameters of both architectures obey the same rules? We exploit the causality principle of independence of mechanisms to quantify how the weights of successive layers adapt to each other. Using the recently introduced Spectral Independence Criterion, we quantify the dependencies between the kernels of successive convolutional layers and show that those are more independent for the generative process than for information extraction, in line with results from the field of causal inference. In addition, our experiments on generation of human faces suggest that more independence between successive layers of generators results in improved performance of these architectures.\n","pdf":"/pdf/414548aca01670f57637d1d495b984e927b771cd.pdf","TL;DR":"We use causal inference to characterise the architecture of generative models","paperhash":"anonymous|on_the_difference_between_building_and_extracting_patterns_a_causal_analysis_of_deep_generative_models","_bibtex":"@article{\n  anonymous2018on,\n  title={On the difference between building and extracting patterns: a causal analysis of deep generative models.},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SySisz-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper943/Authors"],"keywords":["GAN","VAE","causality"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}