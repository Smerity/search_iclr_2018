{"notes":[{"tddate":null,"ddate":null,"tmdate":1515049878164,"tcdate":1515049878164,"number":8,"cdate":1515049878164,"id":"Sy09XLsmf","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"B19pUEjlz","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Response to ICLR 2018 Conference Paper318 AnonReviewer2","comment":"We thank the reviewer for the thorough analysis and insightful comments. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515134531742,"tcdate":1515049498547,"number":7,"cdate":1515049498547,"id":"rkmQzUoQG","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"Sk-OB6wlM","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Response to ICLR 2018 Conference Paper318 AnonReviewer3","comment":"We thank the reviewer for the thorough analysis and insightful comments. Although it is well known that multi-task learning can lead to performance improvement on similar tasks, there is not too many research on FashionMNIST and NotMNIST datasets. And we also visualize the results obtained with/without multi-task learning to analyze our experiment result. Hopeful it contributes to the research community."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515131942219,"tcdate":1515048510132,"number":6,"cdate":1515048510132,"id":"rkUS0Hj7f","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"rJPeuDOxM","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Response to ICLR 2018 Conference Paper318 AnonReviewer1","comment":"Thank you for the very pertinent and exhaustive comments about our work.  We humbly accept the shortcomings you said about our paper and have already revised the shortcomings in our paper.\n\nAbout the unclear points, our answers as below:\nIn order to compare the result between multi-task learning and single task, we must keep the architecture of the network unchanging.\nThe reason we implement \"zero-padding\" is when the multi-task learning process is executed, we combine the training data of MNIST-like datasets together. In order to train a 20(or 30) ways classifier, we have to extend target labels by zero-padding.\nThe “two-stage learning rate decay scheme” we implemented is just a way to decrease learning rate. We found it is helpful to improve recognition accuracies."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1513510568187,"tcdate":1513510568187,"number":5,"cdate":1513510568187,"id":"rJgnUCmGM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"Hyo0zyzGz","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Reviews after reproducing the results","comment":"Thanks for your comments:\nI am interested in your fine tune of the hyper-parameters. How did you achieve 99.88% accuracy? Could you please tell me your hyper-parameters settings and network architecture?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1513398731605,"tcdate":1513398731605,"number":8,"cdate":1513398731605,"id":"rJVCZ7zzz","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reproducibility Challenge","comment":"REPLICATED MODEL\n\nThe approach presented in this paper involves the application of the multi-task learning methodology to a supervised learning task in order to determine the effectiveness of using multi-task learning versus using only single-task learning. The data-sets used included three MNIST-like data-sets: MNIST, Not-MNIST, and Fashion-MNIST.\n\nThe multi-task learning paradigm involved pre-training the network with different combinations of the three data-sets. These combinations included three single-tasks, three bi-tasks, and one tri-tasks. The single-tasks refer to simply using each of the three data-sets individually. The three bi-tasks include the three possible combinations of two data-sets: MNIST+Not-MNIST, MNIST+Fashion-MNIST, and Not-MNIST+Fashion-MNIST. The tri-task referred to using all three data-sets for pre-training.\n\nThe weights from the pre-trained networks are then used to initialize a training network for each data-set individually. Note that for the bi-task learning, the image recognition task is not performed on the data-set that was not involved in the pre-training process. For example, if the pre-training was done on a pooled Not-MNIST and Fashion-MNIST data-set, then the image recognition is not performed on the MNIST data-set as the MNIST was not involved in the pre-training process. In addition, for the single-task learning, there was no pre-training procedure.\n\nThe architecture of the neural network was the all convolutional neural network. The whole architecture was constructed using Tensorflow in the replication.\n\nFor the multi-task learning models, the neural network is first pre-trained for 50 epochs in the multi-task context (bi-task or tri-task). With the parameters transferred over, the neural network is then re-trained for another 50 epochs in the single-task context before conducting the final image-recognition, for a total on 100 epochs on the multi-task learning models. For the single-task learning, the neural network is solely trained for 50 epochs on the individual data-set before performing the image-recognition task.\n\nFor each instance of the 50 epochs, a two-stage learning rate decay scheme is employed. For the first stage of 25 epochs in the instance, the learning rate was initialized to 10^{-3} and in the second stage of 25 epochs, the learning rate initialization was reduced to 10^{-5}. \n\nREPLICATION RESULTS\n\nThe results of the replication are as follows: On the MNIST data-set, the single-task achieved 99.59%, MNIST+Fashion-MNIST achieved 99.61%, MNIST+Not-MNIST achieved 99.58%, and MNIST+Fashion-MNIST+Not-MNIST achieved 99.64%. On Not-MNIST, single-task achieved 97.02%, Fashion-MNIST+Not-MNIST achieved 96.95%, MNIST+Not-MNIST achieved 97.08%, and MNIST+Fashion-MNIST+Not-MNIST achieved 96.91%. On Fashion-MNIST, single-task achieved 93.92%, Fashion-MNIST+Not-MNIST achieved 93.81%, MNIST+Fashion-MNIST achieved 93.56%, and MNIST+Fashion-MNIST+Not-MNIST achieved 93.57%.\n\nThe original study indicated a consistent increase in accuracy from single-task to tri-task learning, with a general pattern of increase in accuracy going from single-task to multi-task models. The results we obtained did not match this pattern in general. 6 of the 9 multi-task models had a lower accuracy than the corresponding single-task models. In addition, the single-task model achieved the highest accuracy for the Fashion-MNIST data-set.\n\nThe discrepancy in the results is based on very small absolute differences. Since each model was trained only once, the difference in results may have been simply due to chance. Keeping this in mind, further analysis using more training experiments and subsequent statistical significance analysis would be helpful for ascertaining the accuracy of multi-task learning versus single-task learning for MNIST and MNIST-like data-sets."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1513396331930,"tcdate":1513396331930,"number":7,"cdate":1513396331930,"id":"By4d_zGff","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reproducibility Challenge: Multi-task Learning on MNIST Image Datasets","comment":"This paper seeks to apply multi task learning to MNIST-like image classification tasks. The authors intend to show that classifiers whose weights have been initialized from a multi task classifier trained on several different datasets will outperform single task classifiers trained on a single dataset pertaining to the single task.\n\nThe paper outline several methods of applying multi task learning to neural networks at the input layer, hidden layers, and final output layer. The experiment applies multi-task learning to recognizing digits, fashion items, and letters from the MNIST, FashionMNIST, and NotMNIST datasets respectively. It uses the entire FashionMNIST and MNIST datasets (60,000 training set and 10,000 test set images each) and a subset of 70,000 NotMNIST images  to train their learners. First a multi-task classifier is trained on different combinations of datasets (MNIST + FashionMNIST, FashionMNIST + Not MNIST, etc.)  for 25 epochs with a learning rate initialized to 1e-3, then another 25 epochs with an initial learning rate of 1e-5. The weights of the trained multi task learners are then used as the initial weights for single task learners that are then trained for a specific task in the same procedure with a different dataset (i.e. an MNIST single task learner trains further on the MNIST dataset). \n\nAll hyperparameters are clearly outlined, including learning rate, optimization function used, and number of epochs ran for training. The authors mention a decay rate for the learning rate but did not specify it. Furthermore they released the code used to run their experiments, and in their code they make extensive use of batch normalization and dropout in their A-CNNs, though the paper makes no mention of it. This made it more difficult to obtain their exact results, because without the code it would have been impossible to recreate their exact training procedure. We did not include batch normalization and dropout in our architecture even after this discovery, to better gauge how well the paper's results can be reproduced based strictly on what is written in it.\n\nThis team was able to closely reproduce the author’s work with a combination of C++, Python, and bash scripts, thanks to the detailed architecture and hyper parameters outlined in the paper. A C++ code preprocessed the datasets, obtained from their official websites, and saved them in a custom format as .ocv files. Python scripts built the learners using Keras Tensorflow and fit the models to the appropriate datasets using the same hyperparameters specified by the original paper. A bash script called the Pythons scripts and passed in different arguments for which datasets to train on and what to set the initial learning rate to. \n\nThe results reported by the paper seem to favor the author’s conclusion that multi-task learners are more powerful than their single  task counterparts; on the task of classifying digits from the MNIST dataset, for example, a single task learner whose weights were initialized randomly got an accuracy of 99.56\\%, whereas a single task learner initialized from multi task learner trained on the MNIST and Fashion MNIST datasets got an accuracy of 99.71\\%. This team obtained some results that agreed with the author's conclusion, but others that contradicted it. For the same set of comparative results, for example, this team obtained accuracies of 99.36\\% and 99.21\\% respectively, where the single task learner out performed its multi task counterpart. \n\nOverall the results obtained by the authors were easy to reproduce as the authors provided ample detail of their method and architectures used. This team obtained some results that correlated with the author's conclusions, and some that did not. These discrepancies could be attributed to methods used by the others that went unmentioned in the paper, though with results between classifiers differentiating by less than half a percentile in some cases (both ours and the authors) it is difficult to discern a significant trend from regular fluctuation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1513382708597,"tcdate":1513382610776,"number":6,"cdate":1513382610776,"id":"Hyo0zyzGz","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["~Amar_Kumar1"],"readers":["everyone"],"writers":["~Amar_Kumar1"],"content":{"title":"Reviews after reproducing the results","comment":"This paper examined the improvement that multi-task learning could bring through training a fixed-structure\nall-convolutional neuron network using three image text datasets. Multi-task learning was achieved by first training\na network in a pre-defined way to get the weights between layers and then using this weights to initialize the\nnetworks for single-task learning.\n1) Implementation: We tried to reproduce the accuracy results in the paper for single-task learning and multi-task\nlearning on MNIST, NotMNIST and FashionMNIST datasets. A same CNN structure was used throughout. The\nauthors provided the code which was simple, elegantly written and well formatted. Thus we were able to reproduce\nthe results very easily. For single-task learning, 50 epochs were used to train the model with a single dataset using\n4:1 training and validation data split.\nFew key differences were observed between the actual implemented model and the model presented in the\npaper. Moreover, some important details regarding the model architecture were missing. For example, there was\nno mention of batch normalization in the manuscript, although it was used in the code. Batch normalization was\nused after each convolution layer. It was difficult to reproduce the results without such details. But as soon as the\ncode was given the entire structure was very clear. However, hyperparameters and structure of the model could\nhave been explained in more detail. Inclusion of some statistics comparing accuracy of single and multi-tasking\nmodels would have been more useful.\n2) Reproduced Results: Denoting MNIST, NotMNIST and FashionMNIST by M, N and F respectively, the\naccuracies that we obtained on MNIST using M, M+N, M+F and M+N+F are 99.71%, 99.67%, 99.67% and\n99.63% respectively; the results on NotMNIST for N, M+N, N+F and M+N+F are 97.55%, 97.53%, 97.43%,\n97.54% and the results on FashionMNIST for F, F+N, M+F and M+N+F are 94.81%, 94.92%, 94.90% and 95.07\nrespectively.We obtained slightly different results as compared to manuscript. The reason behind this could be\ndifferent version of tensorflow (we used 1.4.0). It was very helpful of the authors to mention the exact version of\nthe libraries used which supported our ease of reproducing it.\nWhen reproducing the results for single-task learning, we tried to tune the hyper-parameters and achieved a better\naccuracy of 99.88% for single-task learning than in the paper. Hence, we believe that some more hyper-parameter\noptimization and then implementing a multi-task learning would be helpful. Overall, this paper brings out a very\ninteresting insight which can be of great use if the same thing could be extended to other different domains"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1513310014351,"tcdate":1513310014351,"number":4,"cdate":1513310014351,"id":"ByLrDaeGf","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"HJdSRRnWz","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Details of the t-SNE plot","comment":"The code has been upload to Github:https://github.com/st70712/image-multi-task-learning-\nYou can find the detail in \"TSNE_CNN.ipynb\" file."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"ddate":null,"tddate":1513313835174,"tmdate":1513313888848,"tcdate":1513053759791,"number":5,"cdate":1513053759791,"id":"HJdSRRnWz","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Details of the t-SNE plot","comment":"Can you guys please comment on how did you generate the t-SNE plot or share the code for it.\nThanks"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512377005647,"tcdate":1512377005647,"number":3,"cdate":1512377005647,"id":"HJLn9FM-z","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"S1za-QyZf","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Design choices","comment":"Dear reader:\nThe CNN model which we reference from 'Striving for Simplicity: The All Convolutional Net'. \nYou can find more detail in their paper.The “two-stage learning rate decay scheme” we implemented is just a way to decrease learning rate.We found it is helpful to improve recognition accuracies.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512153530340,"tcdate":1512153530340,"number":4,"cdate":1512153530340,"id":"S1za-QyZf","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"BJ-kwu9eM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Design choices ","comment":"Hi I am wondering why you chose the specific CNN structure (e.g. hyper-parameters) and why you chose this changing learning rate. Could you please justify your choices?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512014964220,"tcdate":1512014964220,"number":2,"cdate":1512014964220,"id":"BJnd4ZTlf","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"SyveRxjlM","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Code for the Paper","comment":"github link:https://github.com/st70712/image-multi-task-learning-"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515642431075,"tcdate":1511896769633,"number":3,"cdate":1511896769633,"id":"B19pUEjlz","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Review","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference/Paper318/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper applies multi-task learning to MNIST type image datasets and gets reasonably interesting and expected results. However, there is not too much novelty in methodology or formalism in the work.","rating":"5: Marginally below acceptance threshold","review":"The paper applies multi-task learning to MNIST (M), FashionNIST (F), and NotMNIST (N) datasets. That is, the authors first train a neural network (with a specific architecture; in this case, it is an all-convolutional network) on a combination of the datasets (M+F; F+N; N+M; M+F+N) and then use the learned weights (in all but the output layer) to initialize the weights for task-specific training on each of the datasets. The authors observe that for each of the combinations, the above approach does better than training on a dataset individually. Further, in all but one case, initializing weights based on training on M+F+N gives the best performance. The improvements are not striking but are noticeable. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1511882223312,"tcdate":1511882223312,"number":3,"cdate":1511882223312,"id":"SyveRxjlM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"BJ-kwu9eM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Code for the Paper","comment":"Hello, we are also intending to reproduce the results of your paper. Could you please post the github link here as a comment once you manage to upload the code? \n\nWould you also be able to post the exact 60,000 train and 10,000 test data points you used from the Not MNIST data set, so that we could use the exact same datasets  to reproduce your results? \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1511847641258,"tcdate":1511847641258,"number":1,"cdate":1511847641258,"id":"BJ-kwu9eM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"SJV6JGqgM","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Dear Readers:","comment":"Thanks for your comments:\n1.Yes, I use cross-entropy as the loss function in my experiment.\n2.The hyper-parameters of single-task models are as same as multi-task models. Each network is trained with 50 epochs. A two-stage learning rate decay scheme is implemented. The initial learning rate is 0.001 for the first stage of 25 epochs, and 0.00001 for the second stage of 25 epochs. The size of a mini-batch is set to 100.\n3.Learning rate decay function is: LearningRate = LearningRate * 1/(1 + decay * epoch). \nThe decay argument is set to LearningRate /25\n4.I will upload my code to github as soon as possible.\n\n\n\nHope the response above clarified your questions."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1511821309450,"tcdate":1511821243888,"number":2,"cdate":1511821243888,"id":"SJV6JGqgM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Implementation details","comment":"This is not really a comment. As we intend to reproduce the results in your paper, we would like to know more about the implementation details.\n\nCould you please tell us:\n1. if you used cross-entropy error or any other criteria as the loss function, \n2. how many epochs did you use to train the pre-trained single-task models and what the learning rate and the mini-batch size were, and\n3. what function was used to specify the decay of the learning rate for training the multi-task models? \n\nBesides, would it be possible for you to share your code with us?\n\nThanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515642431113,"tcdate":1511712750705,"number":2,"cdate":1511712750705,"id":"rJPeuDOxM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Review","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference/Paper318/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The manuscript applies multi-task learning to the three MNIST-like datasets by simply pre-training the parameters of the networks using samples of datasets. The presented idea is quite simple and there is no technique contribution.","rating":"4: Ok but not good enough - rejection","review":"The manuscript mainly utilizing the data from all three MNIST-like datasets to pre-train the parameters of joint classification networks, and the pre-trained parameters are utilized to initialize the disjoint classification networks (of the three datasets).\n\nThe presented idea is quite simple and the authors only re-affirm that multi-task learning can lead to performance improvement by simultaneously leverage the information of multiple tasks. There is no technique contribution.\n\nPros:\n1.\tThe main idea is clearly presented.\n2.\tIt is interesting to visualize the results obtained with/without multi-task learning in Figure 6.\n\nCons:\n1.\tThe contribution is quite limited since the authors only apply multi-task learning to the three MNIST-like datasets and there is no technique contribution.\n2.\tThere is no difference between the architecture of the single-task learning network and multi-task learning network.\n3.\tMany unclear points, e.g., there is no description for “zero-padding” and why it can enhance target label. What is the “two-stage learning rate decay scheme” and why it is implemented? It is also unclear what can we observed from Figure 4.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515642431152,"tcdate":1511671144674,"number":1,"cdate":1511671144674,"id":"Sk-OB6wlM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Review","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference/Paper318/AnonReviewer3"],"readers":["everyone"],"content":{"title":"a paper with limited novelty","rating":"5: Marginally below acceptance threshold","review":"This paper presents a multi-task neural network for classification on MNIST-like datasets.\n\nThe main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited.\n\nThe experiments do not bring too much insights.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515043997366,"tcdate":1509096190959,"number":318,"cdate":1509739364239,"id":"S1PWi_lC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/9b31c998332ed0e65b5d8beebeefb1cc3a68997a.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]},"nonreaders":[],"replyCount":18,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}