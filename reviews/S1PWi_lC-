{"notes":[{"tddate":null,"ddate":null,"tmdate":1512377005647,"tcdate":1512377005647,"number":3,"cdate":1512377005647,"id":"HJLn9FM-z","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"S1za-QyZf","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Design choices","comment":"Dear reader:\nThe CNN model which we reference from 'Striving for Simplicity: The All Convolutional Net'. \nYou can find more detail in their paper.The “two-stage learning rate decay scheme” we implemented is just a way to decrease learning rate.We found it is helpful to improve recognition accuracies.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512153530340,"tcdate":1512153530340,"number":4,"cdate":1512153530340,"id":"S1za-QyZf","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"BJ-kwu9eM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Design choices ","comment":"Hi I am wondering why you chose the specific CNN structure (e.g. hyper-parameters) and why you chose this changing learning rate. Could you please justify your choices?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512014964220,"tcdate":1512014964220,"number":2,"cdate":1512014964220,"id":"BJnd4ZTlf","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"SyveRxjlM","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Code for the Paper","comment":"github link:https://github.com/st70712/image-multi-task-learning-"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222619562,"tcdate":1511896769633,"number":3,"cdate":1511896769633,"id":"B19pUEjlz","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Review","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference/Paper318/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper applies multi-task learning to MNIST type image datasets and gets reasonably interesting and expected results. However, there is not too much novelty in methodology or formalism in the work.","rating":"5: Marginally below acceptance threshold","review":"The paper applies multi-task learning to MNIST (M), FashionNIST (F), and NotMNIST (N) datasets. That is, the authors first train a neural network (with a specific architecture; in this case, it is an all-convolutional network) on a combination of the datasets (M+F; F+N; N+M; M+F+N) and then use the learned weights (in all but the output layer) to initialize the weights for task-specific training on each of the datasets. The authors observe that for each of the combinations, the above approach does better than training on a dataset individually. Further, in all but one case, initializing weights based on training on M+F+N gives the best performance. The improvements are not striking but are noticeable. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1511882223312,"tcdate":1511882223312,"number":3,"cdate":1511882223312,"id":"SyveRxjlM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"BJ-kwu9eM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Code for the Paper","comment":"Hello, we are also intending to reproduce the results of your paper. Could you please post the github link here as a comment once you manage to upload the code? \n\nWould you also be able to post the exact 60,000 train and 10,000 test data points you used from the Not MNIST data set, so that we could use the exact same datasets  to reproduce your results? \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1511847641258,"tcdate":1511847641258,"number":1,"cdate":1511847641258,"id":"BJ-kwu9eM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Comment","forum":"S1PWi_lC-","replyto":"SJV6JGqgM","signatures":["ICLR.cc/2018/Conference/Paper318/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper318/Authors"],"content":{"title":"Dear Readers:","comment":"Thanks for your comments:\n1.Yes, I use cross-entropy as the loss function in my experiment.\n2.The hyper-parameters of single-task models are as same as multi-task models. Each network is trained with 50 epochs. A two-stage learning rate decay scheme is implemented. The initial learning rate is 0.001 for the first stage of 25 epochs, and 0.00001 for the second stage of 25 epochs. The size of a mini-batch is set to 100.\n3.Learning rate decay function is: LearningRate = LearningRate * 1/(1 + decay * epoch). \nThe decay argument is set to LearningRate /25\n4.I will upload my code to github as soon as possible.\n\n\n\nHope the response above clarified your questions."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1511821309450,"tcdate":1511821243888,"number":2,"cdate":1511821243888,"id":"SJV6JGqgM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Public_Comment","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Implementation details","comment":"This is not really a comment. As we intend to reproduce the results in your paper, we would like to know more about the implementation details.\n\nCould you please tell us:\n1. if you used cross-entropy error or any other criteria as the loss function, \n2. how many epochs did you use to train the pre-trained single-task models and what the learning rate and the mini-batch size were, and\n3. what function was used to specify the decay of the learning rate for training the multi-task models? \n\nBesides, would it be possible for you to share your code with us?\n\nThanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222619603,"tcdate":1511712750705,"number":2,"cdate":1511712750705,"id":"rJPeuDOxM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Review","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference/Paper318/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The manuscript applies multi-task learning to the three MNIST-like datasets by simply pre-training the parameters of the networks using samples of datasets. The presented idea is quite simple and there is no technique contribution.","rating":"3: Clear rejection","review":"The manuscript mainly utilizing the data from all three MNIST-like datasets to pre-train the parameters of joint classification networks, and the pre-trained parameters are utilized to initialize the disjoint classification networks (of the three datasets).\n\nThe presented idea is quite simple and the authors only re-affirm that multi-task learning can lead to performance improvement by simultaneously leverage the information of multiple tasks. There is no technique contribution.\n\nPros:\n1.\tThe main idea is clearly presented.\n2.\tIt is interesting to visualize the results obtained with/without multi-task learning in Figure 6.\n\nCons:\n1.\tThe contribution is quite limited since the authors only apply multi-task learning to the three MNIST-like datasets and there is no technique contribution.\n2.\tThere is no difference between the architecture of the single-task learning network and multi-task learning network.\n3.\tMany unclear points, e.g., there is no description for “zero-padding” and why it can enhance target label. What is the “two-stage learning rate decay scheme” and why it is implemented? It is also unclear what can we observed from Figure 4.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222619641,"tcdate":1511671144674,"number":1,"cdate":1511671144674,"id":"Sk-OB6wlM","invitation":"ICLR.cc/2018/Conference/-/Paper318/Official_Review","forum":"S1PWi_lC-","replyto":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference/Paper318/AnonReviewer3"],"readers":["everyone"],"content":{"title":"a paper with limited novelty","rating":"5: Marginally below acceptance threshold","review":"This paper presents a multi-task neural network for classification on MNIST-like datasets.\n\nThe main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited.\n\nThe experiments do not bring too much insights.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]}},{"tddate":null,"ddate":null,"tmdate":1509739366901,"tcdate":1509096190959,"number":318,"cdate":1509739364239,"id":"S1PWi_lC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1PWi_lC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multi-task Learning on MNIST Image Datasets","abstract":"We apply multi-task learning to image classification tasks on MNIST-like datasets. MNIST dataset has been referred to as the {\\em drosophila} of machine learning and has been the testbed of many learning theories. The NotMNIST dataset and the FashionMNIST dataset have been created with the MNIST dataset as reference. In this work, we exploit these MNIST-like datasets for multi-task learning. The datasets are pooled together for learning the parameters of joint classification networks. Then the learned parameters are used as the initial parameters to retrain disjoint classification networks. The baseline recognition model are all-convolution neural networks. Without multi-task learning, the recognition accuracies for MNIST, NotMNIST and FashionMNIST are 99.56\\%, 97.22\\% and 94.32\\% respectively. With multi-task learning to pre-train the networks, the recognition accuracies are respectively 99.70\\%, 97.46\\% and 95.25\\%. The results re-affirm that multi-task learning framework, even with data with different genres, does lead to significant improvement.\n","pdf":"/pdf/388f07e17d3de2d9a8cdff49f3b022967beada4c.pdf","TL;DR":"multi-task learning works ","paperhash":"anonymous|multitask_learning_on_mnist_image_datasets","_bibtex":"@article{\n  anonymous2018multi-task,\n  title={Multi-task Learning on MNIST Image Datasets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1PWi_lC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper318/Authors"],"keywords":["multi-task learning","MNIST","image recognition"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}