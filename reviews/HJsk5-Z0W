{"notes":[{"tddate":null,"ddate":null,"tmdate":1515179426266,"tcdate":1515179293355,"number":1,"cdate":1515179293355,"id":"SJrX6Hpmz","invitation":"ICLR.cc/2018/Conference/-/Paper704/Official_Comment","forum":"HJsk5-Z0W","replyto":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference/Paper704/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper704/Authors"],"content":{"title":"Withdrawing paper","comment":"Dear reviewers,\n\nThank you for your very insightful comments. We are withdrawing this paper and using some of these results to supplement a different paper.  \n\nWe are not clicking on \"withdrawing\" paper yet, as this submission would be de-anonymized immediately.\n\nThanks,\nAuthor"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]}},{"tddate":null,"ddate":null,"tmdate":1515642494771,"tcdate":1512661669250,"number":3,"cdate":1512661669250,"id":"Sk6iGkvbG","invitation":"ICLR.cc/2018/Conference/-/Paper704/Official_Review","forum":"HJsk5-Z0W","replyto":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference/Paper704/AnonReviewer1"],"readers":["everyone"],"content":{"title":"context and contribution of this work is not very clear to me","rating":"4: Ok but not good enough - rejection","review":"This paper presents a method for matrix factorization using DNNs. The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7). From the methodology point of view, such extensions are relatively straightforward. As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically. The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features.\n\nWhile the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer. In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem). I think this paper does not connect very well with that literature. When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework. I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods, or if the success of this paper is mostly due to clever processing of text features using DNNs.\n\nSome detailed comments:\n1. eqn 4 does not indicate any rank-r factors. \n2. some statements do not seem straightforward/justified to me:  \n    -- the paper uses the word \"inference\" several times without definition\n    -- \"if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \". Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples?\n    -- \"Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\". \n3. the experimental setup seems quite unusual to me: \"since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\". This seems very problematic if most of the entries are not observed. Why cannot you use the typical evaluation procedure for collaborative filtering, where you hide some known entries during model training, and evaluate on these entries during test? ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]}},{"tddate":null,"ddate":null,"tmdate":1515642494810,"tcdate":1512010742347,"number":2,"cdate":1512010742347,"id":"Bk0lEg6eG","invitation":"ICLR.cc/2018/Conference/-/Paper704/Official_Review","forum":"HJsk5-Z0W","replyto":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference/Paper704/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Novel model for Collaborative filtering. Seems reasonable overall. Empirical study would be more convincing by including classic recsys datasets˚.","rating":"4: Ok but not good enough - rejection","review":"The authors introduce a novel novel for collaborative filtering. The proposed model combines some of the strengths of factorization machines and of polynomial regression. Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected).\n\nThe paper is well written overall and relatively easy to understand. The study seems fairly thorough (both vanilla and cold-start experiments are reported).\n\nOverall the paper feels a little bit incomplete . This is particularly apparent in the empirical study. Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results. Here are some suggestions about how to achieve that: \n\n1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets.\n\n2) Report results on well-known CF datasets. Movielens comes to mind.\n\n3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)\n\n4) Explore the models and shed some lights on where the gains are coming from.\n\n\nMinor: \n\n- How do you deal with unobserved preferences in the implicit case?\n\n- I found the idea of Figure 1 very good but in its current form I didn't find it particularly insightful (these \"clouds\" are hard to interpret).\n\n- It may also be worth adding this reference when discussing neural factorization:\nhttp://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]}},{"tddate":null,"ddate":null,"tmdate":1515642494851,"tcdate":1511654365290,"number":1,"cdate":1511654365290,"id":"rkSyVFDeG","invitation":"ICLR.cc/2018/Conference/-/Paper704/Official_Review","forum":"HJsk5-Z0W","replyto":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference/Paper704/AnonReviewer3"],"readers":["everyone"],"content":{"title":"proposes a model that is equivalent to known work","rating":"3: Clear rejection","review":"This paper proposes to improve time complexity of factorization machine. Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong. Specifically, the dot product can be computed as (which is linear to feature size)\n\n(\\sum x_i \\beta_i)^T (\\sum x_i \\beta_i) - \\sum_i x_i^2 beta_i^T beta_i\n\nThe projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one. When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]}},{"tddate":null,"ddate":null,"tmdate":1509739150550,"tcdate":1509132770905,"number":704,"cdate":1509739147889,"id":"HJsk5-Z0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJsk5-Z0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Structured Deep Factorization Machine: Towards General-Purpose Architectures","abstract":"In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size—this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.","pdf":"/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf","TL;DR":"Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.","paperhash":"anonymous|structured_deep_factorization_machine_towards_generalpurpose_architectures","_bibtex":"@article{\n  anonymous2018structured,\n  title={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJsk5-Z0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper704/Authors"],"keywords":["factorization","general-purpose methods"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}