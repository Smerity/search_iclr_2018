{"notes":[{"tddate":null,"ddate":null,"tmdate":1515179316987,"tcdate":1515179316987,"number":3,"cdate":1515179316987,"id":"SypN6BT7M","invitation":"ICLR.cc/2018/Conference/-/Paper184/Official_Comment","forum":"rJ3fy0k0Z","replyto":"S1_na_OlG","signatures":["ICLR.cc/2018/Conference/Paper184/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper184/Authors"],"content":{"title":"Thank you for positive evaluations.","comment":"Thank you for your constructive comments and positive evaluations on our paper. We will clarify the role of SSF in the camera-ready version.\n\n> My interpretation is that the main original contribution of the paper (besides changing a stochastic policy for a deterministic one) is to integrate an automatic estimate of the density of the expert (probability of a state to be visited by the expert policy)\n\nThank you for clearly understanding the role of SSF.\n\n> Indeed, the deterministic policy is certainly helpful but it is tested in a deterministic continuous control task. So I'm not sure about how it generalizes to other tasks.\n\nThe expert's policy used in the experimetns is a stochastic one. Hence, the proposed method works not only on a deterministic continuous control tasks but also a stochastic one. We expect that it generalizes well to other tasks.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515179172495,"tcdate":1515179172495,"number":2,"cdate":1515179172495,"id":"SknsnHTQG","invitation":"ICLR.cc/2018/Conference/-/Paper184/Official_Comment","forum":"rJ3fy0k0Z","replyto":"B1nuCculG","signatures":["ICLR.cc/2018/Conference/Paper184/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper184/Authors"],"content":{"title":"Responses","comment":"Thank you for your constructive comments on our paper. We will fix typos and clarify the role of SSF in the camera-ready version.\n\n> The authors also briefly discuss the problem of the little overlap between the teacher's covered state space and the learner's. A state screening function (SSF) method is proposed to drive the learner to remain in areas of the state space that have been covered by the teacher.\n\nThe main purpose of introducing a SSF is not what you mentioned. Since we use the Jacobian of reward function to derive PG as opposed to prior IL works, the Jacobian is supposed to have information about how to get close to the expert's behavior for the learner. However, in the IRL objective (4), which is general in (max-margin) IRL literature, the reward function could know how the expert acts just only on the states appearing in the demonstration. In other words, the Jacobian could have information about how to get close to the expert's behavior just only on states appearing in the demonstration. What we claimed in Sec.3.2 is that the Jacobian for states which does not appear in the demonstration is just garbage for the learner since it does not give any information about how to get close to the expert. The main purpose of introducing the SSF is to sweep the garbage as much as possible.\n\n> However, the reviewer finds the main contribution rather incremental in its nature. Replacing a stochastic policy with a deterministic one does not change much the original GAIL algorithm, since the adoption of stochastic policies is often used just to have differentiable parameterized policies, and if the action space is continuous, then there is not much need for it (except for exploration, which is done here through re-initializations anyway)\n\nFigure.1 shows worse performance of Ours \\setminus SSF which just replace a stochastic policy with a deterministic one. If Ours \\setminus SSF worked well, we agree with your opinion that the main contribution is just incremental. However, introducing the SSF besides replacing a stochastic policy with a deterministic one is required to imitate the expert's behavior. Hence, we don't agree that the proposed method is just incremental.  \n\n> My guess is that if someone would use the GAIL algorithm for real problems (e.g, robotic task), they would reduce the stochasticity of the behavior policy, which would make it virtually similar in term of data efficiency to the proposed method.\n\nBecause the GAIL algorithm is an on-policy algorithm, it essentially requires much interactions for an update and never uses behavior policy. Hence, it would not make it virtually similar in term of data efficiency to the proposed method which is off-policy algorithm.\n\n> Cons:\n> - Incremental improvement over GAIL\n\nAs mentioned above, we think that the proposed method is not just incremental improvement over GAIL. \n\n> - Experiments only on simulated toy problems \n\nWe wonder why you thought the Mujoco tasks are just \"toy\" problems. Even though those tasks are not real-world problems, they have not been solved until GAIL has been proposed. In addition, the variants of GAIL (Baram et al., 2017; Wang et al., 2017; Hausman et al.) also evaluated their performance using those tasks. Hence, we think that those tasks are enough difficult to solve and can be used as a well-suited benchmark to evaluate whether the proposed method is applicable to the real-world problems in comparison with other IL algorithms.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515178969191,"tcdate":1515178969191,"number":1,"cdate":1515178969191,"id":"S1WJnrpmz","invitation":"ICLR.cc/2018/Conference/-/Paper184/Official_Comment","forum":"rJ3fy0k0Z","replyto":"S1tVQ5Kef","signatures":["ICLR.cc/2018/Conference/Paper184/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper184/Authors"],"content":{"title":"Responses","comment":"Thank you for your constructive comments on our paper. We will fix typos and Figure.1. in the camera-ready version. \n\n> The justification for filtering is pretty weak. \n\nSince Figure.1 shows worse performance of Ours \\setminus SSF which does not filter states appearing in the demonstration, we think that the justification is enough.\n\n> What is the statistical basis for doing so?\n\nIntroducing a SSF is a kind of heuristic method, but it works as mentioned above.\n\n> Is it a form of a standard variance reduction approach? Is it a novel variance reduction approach? If so, is it more generally applicable?\n\nIntroducing the SSF itself is not a variance reduction approach. We would say that direct use of the Joacobian of (single-step) reward function rather than that of Q-function to derive the PG (8) might reduce the variance because the range of outputs are bounded.\nSince we use the Jacobian of reward function to derive PG as opposed to prior IL works, the Jacobian is supposed to have information about how to get close to the expert's behavior for the learner. However, in the IRL objective (4), which is general in (max-margin) IRL literature, the reward function could know how the expert acts just only on the states appearing in the demonstration. In other words, the Jacobian could have the information about how to get close to the expert's behavior just only on states appearing in the demonstration. What we claimed in Sec.3.2 is that the Jacobian for states which does not appear in the demonstration is just garbage for the learner since it does not give any information about how to get close to the expert. The main purpose of introducing the SSF is to sweep the garbage as much as possible. The prior IL works have never mentioned about the garbage."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642405555,"tcdate":1511789361151,"number":3,"cdate":1511789361151,"id":"S1tVQ5Kef","invitation":"ICLR.cc/2018/Conference/-/Paper184/Official_Review","forum":"rJ3fy0k0Z","replyto":"rJ3fy0k0Z","signatures":["ICLR.cc/2018/Conference/Paper184/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Combines IRL, adversarial training, and ideas from deterministic policy gradients. Paper is hard to read. MuJoCo results are good.","rating":"5: Marginally below acceptance threshold","review":"The paper lists 5 previous very recent papers that combine IRL, adversarial learning, and stochastic policies. The goal of this paper is to do the same thing but with deterministic policies as a way of decreasing the sample complexity. The approach is related to that used in the deterministic policy gradient work. Imitation learning results on the standard control problems appear very encouraging.\n\nDetailed comments:\n\n\"s with environment\" -> \"s with the environment\"?\n\n\"that IL algorithm\" -> \"that IL algorithms\".\n\n\"e to the real-world environments\" -> \"e to real-world environments\".\n\n\" two folds\" -> \" two fold\".\n\n\"adopting deterministic policy\" -> \"adopting a deterministic policy\".\n\n\"those appeared on the expert’s demonstrations\" -> \"those appearing in the expert’s demonstrations\".\n\n\"t tens of times less interactions\" -> \"t tens of times fewer interactions\".\n\nOk, I can't flag all of the examples of disfluency. The examples above come from just the abstract. The text of the paper seems even less well edited. I'd highly recommend getting some help proof reading the work.\n\n\"Thus, the noisy policy updates could frequently be performed in IL and make the learner’s policy poor. From this observation, we assume that preventing the noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations benefits to the imitation.\": The justification for filtering is pretty weak. What is the statistical basis for doing so? Is it a form of a standard variance reduction approach? Is it a novel variance reduction approach? If so, is it more generally applicable?\n\nUnfortunately, the text in Figure 1 is too small. The smallest font size you should use is that of a footnote in the text. As such, it is very difficult to assess the results.\n\nAs best I can tell, the empirical results seem impressive and interesting.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642405594,"tcdate":1511726708162,"number":2,"cdate":1511726708162,"id":"B1nuCculG","invitation":"ICLR.cc/2018/Conference/-/Paper184/Official_Review","forum":"rJ3fy0k0Z","replyto":"rJ3fy0k0Z","signatures":["ICLR.cc/2018/Conference/Paper184/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper proposes an extension of the generative adversarial imitation learning (GAIL) algorithm by replacing the stochastic policy of the learner with a deterministic one. Simulation results with MuJoCo physics simulator show that this simple trick reduces the amount of needed data by an order of magnitude.","rating":"5: Marginally below acceptance threshold","review":"This paper considers the problem of model-free imitation learning. The problem is formulated in the framework of generative adversarial imitation learning (GAIL), wherein we alternate between optimizing reward parameters and learner policy's parameters. The reward parameters are optimized so that the margin between the cost of the learner's policy and the expert's policy is maximized. The learner's policy is optimized (using any model-free RL method) so that the same cost margin is minimized. Previous formulation of GAIL uses a stochastic behavior policy and the RIENFORCE-like algorithms. The authors of this paper propose to use a deterministic policy instead, and apply the deterministic policy gradient DPG (Silver et al., 2014) for optimizing the behavior policy. \nThe authors also briefly discuss the problem of the little overlap between the teacher's covered state space and the learner's. A state screening function (SSF) method is proposed to drive the learner to remain in areas of the state space that have been covered by the teacher. Although, a more detailed discussion and a clearer explanation is needed to clarify what SSF is actually doing, based on the provided formulation.\nExcept from a few typos here and there, the paper is overall well-written. The proposed idea seems new. However, the reviewer finds the main contribution rather incremental in its nature. Replacing a stochastic policy with a deterministic one does not change much the original GAIL algorithm, since the adoption of stochastic policies is often used just to have differentiable parameterized policies, and if the action space is continuous, then there is not much need for it (except for exploration, which is done here through re-initializations anyway). My guess is that if someone would use the GAIL algorithm for real problems (e.g, robotic task), they would significantly reduce the stochasticity of the behavior policy, which would make it virtually similar in term of data efficiency to the proposed method.\nPros:\n- A new GAIL formulation for saving on interaction data. \nCons:\n- Incremental improvement over GAIL\n- Experiments only on simulated toy problems \n- No theoretical guarantees for the state screening function (SSF) method","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642405633,"tcdate":1511718320481,"number":1,"cdate":1511718320481,"id":"S1_na_OlG","invitation":"ICLR.cc/2018/Conference/-/Paper184/Official_Review","forum":"rJ3fy0k0Z","replyto":"rJ3fy0k0Z","signatures":["ICLR.cc/2018/Conference/Paper184/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Hard to read","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to extend the determinist policy gradient algorithm to learn from demonstrations. The method is combined with a type of density estimation of the expert to avoid noisy policy updates. It is tested on Mujoco tasks with expert demonstrations generated with a pre-trained network. \n\nI found the paper a bit hard to read. My interpretation is that the main original contribution of the paper (besides changing a stochastic policy for a deterministic one) is to integrate an automatic estimate of the density of the expert (probability of a state to be visited by the expert policy) so that the policy is not updated by gradient coming from transitions that are unlikely to be generated by the expert policy. \n\nI do think that this part is interesting and I would have liked this trick to be used with other imitation methods. Indeed, the deterministic policy is certainly helpful but it is tested in a deterministic continuous control task. So I'm not sure about how it generalizes to other tasks. Also, the expert demonstration are generated by the pre-trained network so the distribution of the expert is indeed the distribution of the optimal policy. So I'm not sure the experiments tell a lot. But if the density estimation could be combined with other methods and tested on other tasks, I think this could be a good paper.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739439716,"tcdate":1509052180164,"number":184,"cdate":1509739437057,"id":"rJ3fy0k0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJ3fy0k0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deterministic Policy Imitation Gradient Algorithm","abstract":"The goal of imitation learning (IL) is to enable a learner to imitate an expert’s behavior given the expert’s demonstrations. Recently, generative adversarial imitation learning (GAIL) has successfully achieved it even on complex continuous control tasks. However, GAIL requires a huge number of interactions with environment during training. We believe that IL algorithm could be more applicable to the real-world environments if the number of interactions could be reduced. To this end, we propose a model free, off-policy IL algorithm for continuous control. The keys of our algorithm are two folds: 1) adopting deterministic policy that allows us to derive a novel type of policy gradient which we call deterministic policy imitation gradient (DPIG), 2) introducing a function which we call state screening function (SSF) to avoid noisy policy updates with states that are not typical of those appeared on the expert’s demonstrations. Experimental results show that our algorithm can achieve the goal of IL with at least tens of times less interactions than GAIL on a variety of continuous control tasks.","pdf":"/pdf/3346975b01f567da84bfff70e662a63207c1a38f.pdf","TL;DR":"We propose a model free imitation learning algorithm that is able to reduce number of interactions with environment in comparison with state-of-the-art imitation learning algorithm namely GAIL.","paperhash":"anonymous|deterministic_policy_imitation_gradient_algorithm","_bibtex":"@article{\n  anonymous2018deterministic,\n  title={Deterministic Policy Imitation Gradient Algorithm},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ3fy0k0Z}\n}","keywords":["Imitation Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper184/Authors"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}