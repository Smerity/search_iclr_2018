{"notes":[{"tddate":null,"ddate":null,"tmdate":1514645799080,"tcdate":1514644805480,"number":5,"cdate":1514644805480,"id":"HyaBB7SmM","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Comment","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper840/Authors"],"content":{"title":"Authors' General Response to Reviewers","comment":"We thank all reviewers for their valuable comments and suggestions. We here address the common concerns/suggestions and summarize the modifications in the latest revision.\n\nWe first emphasize our contributions. In our work, we propose a model that exploits structure priors for continuous reinforcement learning. We show competitive performance on standard tasks, and focus on showing the model’s ability to perform transfer learning to different agents. To the best of our knowledge, we are the first to address transfer learning between different agents for continuous control tasks, whose even simplest sub-problems have been beyond the ability of the best models we have right now. \n\n\n1. Multi-task Learning\nOne common concern among the reviewers is the lack of more diverse transfer experiments. We address this concern by performing an extensive set of multi-task experiments in our latest revision.  In particular, we trained one single network to control a broad range of diverse agents.\n\nWe create a Walker task-set which contains five 2d walkers. They have very different dynamics, from single legged hopper to two-legged ostrich with a tail and neck. Specifically, Walker-HalfHumanoid and Walker-Hopper are variants of Walker2d and Hopper, respectively, in the original MuJoCo Benchmarks. On the other hand, Walker-Horse (two-legged), Walker-Ostrich (two-legged), and Walker-Wolf (four-legged) are agents mimicking real animals. Just like real-animals, some of the agents have tails and a neck to help them to balance. The detailed schematic figures are in the appendix.\n\nWe refer to training  separate models of different weights for each agent as single-task learning, and sharing weights across multiple agents as multi-task learning. The results including our method and other competitors, e.g., MLP, are listed below:\n\nTABLE 1\n Model                    | HalfHum| Hopper | Ostrich |   Wolf    |   Horse    | Average\n\n    MLP    | Reward | 1775.75 | 1369.6 |  1198.9 | 1249.23 |  2084.1  |    /\n    MLP    |   Ratio   |   57.7%   |  62.0% |  48.2%  |  54.5%    |  69.7%     | 58.6%\n\nTreeNet  | Reward |   237.81 | 417.27 |  224.1   | 247.03   |  223.34    |    /\nTreeNet  |   Ratio   |   79.3%   | 98.0%  |  57.4%  | 141.2%  |   99.2%    |  94.8%\n\nNerveNet| Reward|  2536.52 | 2113.6 |  1714.6 | 2054.5   |  2343.6  |     /\nNerveNet|  Ratio   |   96.3%   | 101.8% |  98.8%  | 105.9%  |   106.4%  |  101.8%\n\n(Ratio indicates “the reward of multi-task” / “the reward of single-task baseline”)\n\nFrom the results, we can see that our method significantly outperforms other models. The MLP models failed to learn a shared representation over the different tasks. Their performance drops significantly when shifting from single-task to multi-task learning, while the performance of the NerveNet remains the same. We also show training curves in the updated version of the paper.\n\n\n2. Robustness \nTo  assess the model’s generalization ability, we added an experiment to evaluate how well the control policies can generalize from the training environment to slightly perturbed test environments, e.g. varying the mass or the torque of the walkers’ joints. \n\nAs pointed out by [1], the policy learned by MLP is very unstable and is typically overfit. Different from [1], where the authors improve the robustness via model ensembles, we show that NerveNet is able to improve robustness of the agent from the perspective of model’s structure, which means that NerveNet is able to improve robustness of the agent by exploiting priors and weight sharing in the model’s structure. \nIn this experiment, we perturbed the mass of the geometries (rigid bodies) in MuJoCo as well as the scale of the forces of the joints. We used the pretrained models with similar performance on the original task for both the MLP and NerveNet. We tested the performance in five agents from the “Walker” task set. The average performance is recorded in the figure below, and the specific details are summarized in the appendix of the latest paper revision.\nShown in the below table are the results of \"performance of perturbed agents\" / \"training performance\".\n\nTABLE 2\n     Model               | HalfHum  | Hopper |   Wolf    | Ostrich |  Horse   | Average\n\nMass |    MLP       |  33.28%    | 74.04%  | 94.68% | 59.23% | 40.61% | 60.37%\nMass | NerveNet |  95.87%   | 93.24%  | 90.13% |  80.2%  | 69.23% | 85.73%\n\nSTR    |     MLP      |   25.96%   | 21.77%  | 27.32% | 30.08%  | 19.80% | 24.99%\nSTR    | NerveNet |   31.11%   | 42.20%  | 42.84% | 31.41%  | 36.54% | 36.82%\n\nIn summary, we added (1) experiments on multi-task learning, (2) experiments on testing robustness, (3) included improved visualizations of the zero-shot learning experiments, and  added (4) more details in the appendix, e.g., hyper-parameters, the schematic figures of the “Walker” task-set agents.\n\n[1] Towards Generalization and Simplicity in Continuous Control\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1514643564127,"tcdate":1514643564127,"number":4,"cdate":1514643564127,"id":"SkNdxmSmf","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Comment","forum":"S1sqHMZCb","replyto":"BkjfRYLxz","signatures":["ICLR.cc/2018/Conference/Paper840/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper840/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"We thank the reviewer for the great suggestions regarding the quality of the paper, and we would like to bring your attention to the general comment above. We added new experiments in the latest revision.\n\nQ1: How does MLP with share weights among joints perform?\nA1: We name the variant proposed by the reviewer as MLP-Bind. Note that MLP-Bind and TreeNet are equivalent for the Snake agents, since the snakes only have one type of joint. We ran MLP-Bind for the zero-shot and fine-tuning experiments on centipedes. We summarize the results here: \n\n1. Zero-shot performances of MLP-Bind and MLPAA are very similar. Both models have limited performance in the zero-shot scenario. Attached below is a sample table for several transfer tasks in centipedes (full results in the appendix of the revised draft)\n\n2. For fine-tuning on ordinary centipedes from pretrained models, the performance is only slightly worse than when using MLP. In our experiment, in the two curves of transferring from CentipedeFour to CentipedeEight as well as CentipedeSix to CentipedeEight, MLP-Bind’s reward is 100-500 worse than MLPAA during fine-tuning.\n\n3. For the Crippled agents, the MLP-Bind agent is stuck at around 800 reward. This might be due to MLP-Bind not being able to efficiently exploit the information of crippled and well-functioning legs.\n\nFor the Average Reward:\n----------------------------------------------------------------------------\n    Task     |    MLPAA   |   MLP-Bind   |    NerveNet \n----------------------------------------------------------------------------\n4to6          |    109.4      |      62.13      |    139.6\n4to8          |    18.2        |      24.62      |    44.3\n----------------------------------------------------------------------------\n6to8          |    21.1        |      235.97    |    1674.9\n6to10        |    -42.4       |      18.65      |    940.0\n----------------------------------------------------------------------------\n4toCp06    |    -5.1         |      11.47      |    47.6\n4toCp08    |    5.1          |      7.34        |    40.0\n----------------------------------------------------------------------------\n6toCp08    |    36.5        |      29.09      |    523.6\n6toCp10    |    12.8        |      8.32        |    504.0\n----------------------------------------------------------------------------\n\nFor the average distance the agents could run in one episode (see updated version for the details of average distance, which is another metrics to evaluate how well the agents perform.)\n----------------------------------------------------------------------------\n    Task     |    MLPAA   |   MLP-Bind   |    NerveNet \n----------------------------------------------------------------------------\n4to6          |    545.3      |      62.13      |    577.3\n4to8          |    62.0        |      24.62      |    146.9\n----------------------------------------------------------------------------\n6to8          |    87.8        |      235.97    |    10612.6\n6to10        |    -17.0       |      18.65      |    6343.6\n----------------------------------------------------------------------------\n4toCp06    |    -22.5       |      11.47      |    91.1\n4toCp08    |    -26.9       |      7.34        |    80.1\n----------------------------------------------------------------------------\n6toCp08    |    138.3       |      29.09      |    3117.3\n6toCp10    |    13.6         |       8.32       |    3230.3\n----------------------------------------------------------------------------\n\nThe details of experiments we performed are updated in the appendix of the latest version.\n\n\nQ2: How the controller is shared? (“In the Output Model section, I am not sure how the controller is shared. It first says that \"Nodes with the same node type should share the instance of MLP\", which means all the \"joint\" nodes should share the same controller.”)\nA2: We clarified the Output Model section in the latest version. \nNervnet:  nodes of the same type (joint, root, body) share the same state update function, e.g., GRU weights. \nEvery node, regardless of its node type, shares the same output MLP instance.\n\n\nQ3: Typos and presentation regarding to Eq. (4).\nA3: We improved the clarity as per suggestions.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1514643507735,"tcdate":1514643507735,"number":3,"cdate":1514643507735,"id":"rkn4g7S7f","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Comment","forum":"S1sqHMZCb","replyto":"r1r7Vd9gf","signatures":["ICLR.cc/2018/Conference/Paper840/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper840/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"We thank the reviewer for the great suggestion regarding multi-task learning.\n\nQ1: “However, a wider range of experiments would have been helpful to judge the usefulness of the proposed policy representation … it would have been very nice to see multi-task learning that takes advantage of body structure”\nA1: We added the multi-task learning experiment. Please see the general comment above with additional experiments.\n\n\nQ2: Moving some details to the appendix.\nA2: We revised the paper and shortened the model’s section.\n\n\nQ3: Adding references.\nA3: Thanks for pointing out these two references. We included them in the latest version.\n\nWe believe that the focus of “Discrete Sequential Prediction of Continuous Actions for Deep RL” paper (action space discretization) and the focus of our paper (using structure information) are different. Combining these ideas might further boost the performance of the agents.\n\nFor the second paper, we agree that model-based control has been well studied, and this paper should be cited."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1514643458382,"tcdate":1514643423445,"number":2,"cdate":1514643423445,"id":"ryPylXB7G","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Comment","forum":"S1sqHMZCb","replyto":"Hy24AAnlM","signatures":["ICLR.cc/2018/Conference/Paper840/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper840/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"We thank the reviewer for the careful reading of our paper and suggestions.\n\nQ1: The problem the authors have chosen to tackle is perhaps a bit specific\nA1: Please see the general comment above with additional experiments.\n\n\nQ2: A brief statement of the paper's \"contributions\" is also needed.\nA2: We made the statement more clear in the latest version. Specifically, our main contribution is in exploring graph neural networks in reinforcement learning and investigating their ability to transfer structure. To the best of our knowledge, we are the first to address transfer learning for continuous control tasks.  We also make small contributions on the model side, i. e. GNNs. In particular, we introduce node type and associate an instance of an update function with each type. This fits the RL setting very well and also increases the model’s capacity. \n\n\nQ3: Abstract: I take issue with the phrase \"are significantly better than policies learned by other models.\nA3: We agree and will modify the wording in the abstract. Our main claims were with respect to transferability, since our model has significant improvement in the zero-shot and transfer learning tasks.\n\n\nQ4: Another figure is needed to better illustrate the algorithm. Relatedly, Sec. 2.2.2 is rather difficult to follow because of the lack of a figure or concrete example\nA4: We added a new figure (Fig. 2 in the newest revision) to illustrate how the input state is constructed, how messages are passed between the nodes and how the final policy is being output.\n\n\nQ5: Typo in Eq. (4) and other minor issues.\nA5: Thanks for pointing this out. We corrected them in the latest version.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512232960298,"tcdate":1512232913457,"number":1,"cdate":1512232913457,"id":"S1FRPUgWG","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Comment","forum":"S1sqHMZCb","replyto":"rJm-vMaxG","signatures":["ICLR.cc/2018/Conference/Paper840/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper840/Authors"],"content":{"title":"Connection with Message Passing Neural Network","comment":"Thanks for pointing out this paper!\nWe do think we can rephrase our model as a message passing neural network (MPNNs) except some subtle differences, like the message function could take representations of both head and tail of an edge as input arguments in MPNNs whereas ours only takes representation of head to compute the message.\nWe will add the discussion of these connections in the final version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512202786300,"tcdate":1512202733926,"number":2,"cdate":1512202733926,"id":"SJ8gfJebG","invitation":"ICLR.cc/2018/Conference/-/Paper840/Public_Comment","forum":"S1sqHMZCb","replyto":"rJm-vMaxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Message passing neural networks vs. graph neural networks","comment":"I think it is fair to frame their model as a graph neural network, as it closely resembles the \"local transition function\" proposed in the original graph neural network paper (Gori et al., 2009): http://ieeexplore.ieee.org/document/4700287/\n\nThe only architectural difference that the authors propose here is to use a gated per-node update function right after the local transition function is evaluated - this mostly resembles the work from Li et al., 2015: https://arxiv.org/abs/1511.05493 (which is cited)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512019707293,"tcdate":1512019707293,"number":1,"cdate":1512019707293,"id":"rJm-vMaxG","invitation":"ICLR.cc/2018/Conference/-/Paper840/Public_Comment","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["~George_Edward_Dahl1"],"readers":["everyone"],"writers":["~George_Edward_Dahl1"],"content":{"title":"Can you write your model as a message passing neural network?","comment":"Is it possible to write your model as a message passing neural network, as in http://proceedings.mlr.press/v70/gilmer17a.html ? It looks closely related and readers might benefit from any explicit connections that can be made."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1515642519443,"tcdate":1512005171833,"number":3,"cdate":1512005171833,"id":"Hy24AAnlM","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Review","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Though occasionally unclear, the authors present an interesting approach to solving a well-scoped problem.","rating":"7: Good paper, accept","review":"The authors present an interesting application of Graph Neural Networks to learning policies for controlling \"centipede\" robots of different lengths. They leverage the non-parametric nature of graph neural networks to show that their approach is capable of transferring policies to different robots more quickly than other approaches. The significance of this work is in its application of GNNs to a potentially practical problem in the robotics domain. The paper suffers from some clarity/presentation issues that will need to be improved. Ultimately, the contribution of this paper is rather specific, yet the authors show the clear advantage of their technique for improved performance and transfer learning on some agent types within this domain.\n\nSome comments:\n- Significant: A brief statement of the paper's \"contributions\" is also needed; it is unclear at first glance what portions of the work are the authors' own contributions versus prior work, particularly in the section describing the GNN theory.\n- Abstract: I take issue with the phrase \"are significantly better than policies learned by other models\", since this is not universally true. While there is a clear benefit to their technique for the centipede and snake models, the performance on the other agents is mostly comparable, rather than \"significantly better\"; this should be reflected in the abstract.\n- Figure 1 is instructive, but another figure is needed to better illustrate the algorithm (including how the state of the world is mapped to the graph state h, how these \"message\" are passed between nodes, and how the final graph states are used to develop a policy). This would greatly help clarity, particularly for those who have not seen GNNs before, and would make the paper more self-contained and easier to follow. The figure could also include some annotated examples of the input spaces of the different joints, etc. Relatedly, Sec. 2.2.2 is rather difficult to follow because of the lack of a figure or concrete example (an example might help the reader understand the procedure without having to develop an intuition for GNNs).\n- There is almost certainly a typo in Eq. (4), since it does not contain the aggregated message \\bar{m}_u^t.\n\nSmaller issues / typos:\n- Abstract: please spell out spell out multi-layer perceptrons (MLP).\n- Sec 2.2: \"servers\" should be \"serves\"\n- \"performance By\" on page 4 is missing a \".\"\n\nPros:\n- The paper presents an interesting application of GNNs to the space of reinforcement learning and clearly show the benefits of their approach for the specific task of transfer learning.\n- To the best of my knowledge, the paper presents an original result and presents a good-faith effort to compare to existing, alternative systems (showing that they outperform on the tasks of interest).\n\nCons:\n- The contributions of the paper should be more clearly stated (see comment above).\n- The section describing their approach is not \"self contained\" and is difficult for an unlearned reader to follow.\n- The problem the authors have chosen to tackle is perhaps a bit \"specific\", since the performance of their approach is only really shown to exceed the performance on agents, like centipedes or snakes, which have this \"modular\" quality.\n\nI certainly hope the authors improve the quality of the theory section; the poor presentation here brings down the rest of the paper, which is otherwise an easy read.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1515642519485,"tcdate":1511846941136,"number":2,"cdate":1511846941136,"id":"r1r7Vd9gf","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Review","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Investigates an under-explored idea, but evaluation could be more compelling","rating":"6: Marginally above acceptance threshold","review":"The submission proposes incorporation of additional structure into reinforcement learning problems. In particular, the structure of the agent's morphology. The policy is represented as a graph neural network over the agent's morphology graph and message passing is used to update individual actions per joint.\n\nThe exposition is fairly clear and the method is well-motivated. I see no issues with the mathematical correctness of the claims made in the paper. However, the paper could benefit from being shorter by moving some details to the appendix (such as much of section 2.1 and PPO description).\n\nRelated work section could consider the following papers:\n\n\"Discrete Sequential Prediction of Continuous Actions for Deep RL\"\nAnother approach that outputs actions per joint, although in a general manner that does not require morphology structure\n\n\"Generalized Biped Walking Control\"\nConsiders the task of interactively changing limb lengths (your size transfer task) in a zero-shot manner, albeit with a non-neural network controller\n\nThe experimental results investigate the effects of various algorithm parameters, which is appreciated. However, a wider range of experiments would have been helpful to judge the usefulness of the proposed policy representation. In addition to robustness to limb length and disability perturbations, it would have been very nice to see multi-task learning that takes advantage of body structure (such as learning to reach for target with arms while walking with legs and being able to learn those independently, for example).\n\nHowever, I do think using agent morphology is an under-explored idea and one that is general, since we tend to have access to this structure in continuous control tasks for the time being. As a result, I believe this submission would be of interest to ICLR community.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1515642519520,"tcdate":1511591442882,"number":1,"cdate":1511591442882,"id":"BkjfRYLxz","invitation":"ICLR.cc/2018/Conference/-/Paper840/Official_Review","forum":"S1sqHMZCb","replyto":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference/Paper840/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A nice paper that learns structured policy for control","rating":"7: Good paper, accept","review":"This paper proposes NerveNet to represent and learn structured policy for continuous control tasks. Instead of using the widely adopted fully connected MLP, this paper uses Graph Neural Networks to learn a structured controller for various MuJoco environments. It shows that this structured controller can be easily transferred to different tasks or dramatically speed up the fine-tuning of transfer.\n\nThe idea to build structured policy is novel for continuous control tasks. It is an exciting direction since there are inherent structures that should be exploited in many control tasks, especially for locomotion. This paper explores this less-studied area and demonstrates promising results.\n\nThe presentation is mostly clear. Here are some questions and a list of minor suggestions:\n1) In the Output Model section, I am not sure how the controller is shared. It first says that \"Nodes with the same node type should share the instance of MLP\", which means all the \"joint\" nodes should share the same controller. But later it says \"Two LeftHip should have a shared controller.\" What about RightHip? or Ankle? They all belongs to the same node type \"joint\". Am I missing something here? It seems that in this paper, weights sharing is an essential part of the structured policy, it would be great if it can be described in more details.\n\n2) In States Update of Propagation Model Section, it is not clear how the aggregated message is used in eq. (4).\n\n3) Typo in Caption of Table 1: CentipedeFour not CentipedeSix.\n\n4) If we just use MLP but share weights among joints (e.g. the weights from observation to action of all the LeftHips are constrained to be same), how would it compare to the method proposed in this paper?\n\nIn summary, I think that it is worthwhile to develop structured representation of policies for control tasks. It is analogue to use CNN that share weights between kernels for computer vision tasks. I believe that this paper could inspire many follow-up work. For this reason, I would recommend accepting this paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]}},{"tddate":null,"ddate":null,"tmdate":1514642836901,"tcdate":1509135763462,"number":840,"cdate":1509739069520,"id":"S1sqHMZCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1sqHMZCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"NerveNet: Learning Structured Policy with Graph Neural Networks","abstract":"We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by MLPs which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer. We demonstrate that policies learned by NerveNet are significantly better than policies learned by other models and are able to transfer even in a zero-shot setting.\n","pdf":"/pdf/a2113303259692cebedbfe522d13ef6fe2637a8d.pdf","TL;DR":"using graph neural network to model structural information of the agents to improve policy and transferability ","paperhash":"anonymous|nervenet_learning_structured_policy_with_graph_neural_networks","_bibtex":"@article{\n  anonymous2018nervenet:,\n  title={NerveNet: Learning Structured Policy with Graph Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sqHMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper840/Authors"],"keywords":["reinforcement learning","transfer learning","graph neural network"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}