{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222768474,"tcdate":1511835019265,"number":3,"cdate":1511835019265,"id":"rJQcSB5gG","invitation":"ICLR.cc/2018/Conference/-/Paper795/Official_Review","forum":"rJYFzMZC-","replyto":"rJYFzMZC-","signatures":["ICLR.cc/2018/Conference/Paper795/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting model to incorporate domain-specific knowledge in procedural language understanding","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper studies procedural language, which can be very useful in applications such as robotics or online customer support. The system is designed to model knowledge of the procedural task using actions and their effect on entities. The proposed solution incorporates a structured representation of domain-specific knowledge that appears to improve performance in two evaluated tasks: tracking entities as the procedure evolves, and generating sentences to complete a procedure. The method is interesting and presents a good amount of evidence that it works, compared to relevant baseline solutions. \n\nThe proposed tasks of tracking entities and generating sentences are also interesting given the procedural context, and the authors introduce a new dataset with dense annotations for evaluating this task. Learning happens in a weakly supervised manner, which is very interesting too, indicating that the model introduces the right bias to produce better results.\n\nThe manual selection and curation of entities for the domain are reasonable assumptions, but may also limit the applicability or generality from the learning perspective. This selection may also explain part of the better performance, as the right bias is not just in the model, but in the construction of the \"ontologies\" to make it work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Simulating Action Dynamics with Neural Process Networks","abstract":"Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics.   Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives.","pdf":"/pdf/b24f4a09db412c4d4adf8730ee514b669f3887e8.pdf","TL;DR":"We propose a new recurrent memory architecture that can track common sense state changes of entities by simulating the causal effects of actions.","paperhash":"anonymous|simulating_action_dynamics_with_neural_process_networks","_bibtex":"@article{\n  anonymous2018simulating,\n  title={Simulating Action Dynamics with Neural Process Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJYFzMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper795/Authors"],"keywords":["representation learning","memory networks","state tracking"]}},{"tddate":null,"ddate":null,"tmdate":1512222768511,"tcdate":1511788397058,"number":2,"cdate":1511788397058,"id":"r1Hu15Kxz","invitation":"ICLR.cc/2018/Conference/-/Paper795/Official_Review","forum":"rJYFzMZC-","replyto":"rJYFzMZC-","signatures":["ICLR.cc/2018/Conference/Paper795/AnonReviewer1"],"readers":["everyone"],"content":{"title":"good paper","rating":"9: Top 15% of accepted papers, strong accept","review":"SUMMARY.\n\nThe paper presents a novel approach to procedural language understanding.\nThe proposed model reads food recipes and updates the representation of the entities mentioned in the text in order to reflect the physical changes of the entities in the recipe.\nThe authors also propose a manually annotated dataset where each passage of a recipe is annotated with entities, actions performed over the entities, and the change in state of the entities after the action.\nThe authors tested their model on the proposed dataset and compared it with several baselines.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is very well written and easy to read.\nI enjoyed reading this paper, I found the proposed architecture very well thought for the proposed task.\nI would have liked to see a little bit more of analysis on the results, it would be interesting to see what are the cases the model struggles the most.\n\nI am wondering how the model would perform without intermediate losses i.e., entity selection loss and action selection loss.\nIt would also be interesting to see the impact of the amount of 'intermediate' supervision on the state change prediction.\n\nThe setup for generation is a bit unclear to me.\nThe authors mentioned to encode entity vectors with a biGRU, do the authors encode it in order of appearance in the text? would not it be better to encode the entities with some structure-agnostic model like Deep Sets?\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Simulating Action Dynamics with Neural Process Networks","abstract":"Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics.   Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives.","pdf":"/pdf/b24f4a09db412c4d4adf8730ee514b669f3887e8.pdf","TL;DR":"We propose a new recurrent memory architecture that can track common sense state changes of entities by simulating the causal effects of actions.","paperhash":"anonymous|simulating_action_dynamics_with_neural_process_networks","_bibtex":"@article{\n  anonymous2018simulating,\n  title={Simulating Action Dynamics with Neural Process Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJYFzMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper795/Authors"],"keywords":["representation learning","memory networks","state tracking"]}},{"tddate":null,"ddate":null,"tmdate":1512222768553,"tcdate":1511617326426,"number":1,"cdate":1511617326426,"id":"SJUEXlDxf","invitation":"ICLR.cc/2018/Conference/-/Paper795/Official_Review","forum":"rJYFzMZC-","replyto":"rJYFzMZC-","signatures":["ICLR.cc/2018/Conference/Paper795/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Worried about the generality of the model, the qualitative analysis, as well as a fair comparison to Recurrent Entity Networks and non-neural baselines","rating":"4: Ok but not good enough - rejection","review":"Summary\n\nThis paper presents Neural Process Networks, an architecture for capturing procedural knowledge stated in texts that makes use of a differentiable memory, a sentence and word attention mechanism, as well as learning action representations and their effect on entity representations. The architecture is tested for tracking entities in recipes, as well as generating the natural language description for the next step in a recipe. It is compared against a suit of baselines, such as GRUs, Recurrent Entity Networks, Seq2Seq and the Neural Checklist Model. While I liked the overall paper, I am worried about the generality of the model, the qualitative analysis, as well as a fair comparison to Recurrent Entity Networks and non-neural baselines.\n\nStrengths\n\nI believe the authors made a good effort in comparing against existing neural baselines (Recurrent Entity Networks, Neural Checklist Model) *for their task*. That said, it is unclear to me how generally applicable the method is and whether the comparison against Recurrent Entity Networks is fair (see Weaknesses).\nI like the ablation study.\n\nWeaknesses\n\nWhile I find the Neural Process Networks architecture interesting and I acknowledge that it outperforms Recurrent Entity Networks for the presented tasks, after reading the paper it is not clear to me how generally applicable the architecture is. Some design choices seem rather tailored to the task at hand (manual collection of actions MTurk annotation in section 3.1) and I am wondering where else the authors see their method being applied given that the architecture relies on all entities and actions being known in advance. My understanding is that the architecture could be applied to bAbI and CBT (the two tasks used in the Recurrent Entity Networks paper). If that is the case, a fair comparison to Recurrent Entity Networks would have been to test against Recurrent Entity Networks on these tasks too. If they the architecture cannot be applied in these tasks, the authors should explain why.\nI am not convinced by the qualitative analysis. Table 2 tells me that even for the best model the entity selection performance is rather unreliable (only 55.39% F1), yet all examples shown in Table 3 look really good, missing only the two entities oil (1) and sprinkles (3). This suggests that these examples were cherry-picked and I would like to see examples that are sampled randomly from the dev set. I have a similar concern regarding the generation task. First, it is not mentioned where the examples in Table 6 are taken from – is it the train, dev or test set? Second, the overall BLEU score seems quite low even for the best model, yet the examples in Table 6 look really good. In my opinion, a good qualitative analysis should also discuss failure cases. Since the BLEU score is so low here, you might also want to compare perplexity of the models.\nThe qualitative analysis in Table 5 is not convincing either. In Appendix A.1 it is mentioned that word embeddings are initialized from word2vec trained on the training set. My suspicion is that one would get the clustering in Table 4 already from those pretrained vectors, maybe even when pretrained on the Google news corpus. Hence, it is not clear what propagating gradients through the Neural Process Networks into the action embeddings adds, or put differently, why does it have to be a differentiable architecture when an NLP pipeline might be enough? This could easily be tested by another ablation where action embeddings are pretrained using word2vec and then fixed during training of the Neural Process Network. Moreover, in 3.3 it is mentioned that even the Action Selection is pretrained, which makes me wonder what is actually trained jointly in the architecture and what is not.\nI think the difficulty of the task at hand needs to be discussed at some point, ideally early in the paper. Until examples on page 7 are shown, I did not have a sense for why a neural architecture is chosen. For example, in 2.3 it is mentioned that for \"wash and cut\" the two functions fwash and fcut need to be selected. For this example, this seems trivial as the functions have the same name (and you could even have a function per name!). As far as I understand, the point of the action selector is to only have a fixed number of learned actions and multiple words (cut, slice etc.) should select the same action fcut. Otherwise (if there is little language ambiguity) I would not see the need for a complex neural architecture. Related to that, a non-neural baseline for the entity selection task that in my opinion definitely needs to be added is extracting entities using a pretrained NER system and returning all of them as the selection.\np2 Footnote 1: So if I understand this correctly, this work builds upon a dataset of over 65k recipes from Kiddon et al. (2016), but only for 875 of those detailed annotations were created?\n\nMinor Comments\n\np1: The statement \"most natural language understanding algorithms do not have the capacity …\" should be backed by reference.\np2: \"context representation ht\" – I would directly mention that this is a sentence encoding.\np3: 2.4: I have the impression what you are describing here is known in the literature as entity linking.\np3 Eq.3: Isn't c3*0 always a vector of zeros?\np4 Eq.6: W4 is an order-3 tensor, correct?\np4 Eq.8: What is YC and WC here and what are their dimensions? I am confused by the softmax, as my understanding (from reading the paragraph on the Action Selection Loss on p.5) was that the expression in the softmax here is a scalar (as it is done for every possible action), so this should be a sigmoid to allow for multiple actions to attain a probability of 1?\np5: \"See Appendix for details\" -> \"see Appendix C for details\"\np5 3.3: Could you elaborate on the heuristic for extracting verb mentions? Is only one verb mention per sentence extracted?\np5: \"trained to minimize cross-entropy loss\" -> \"trained to minimize the cross-entropy loss\"\np5 3.3: What is the global loss?\np6: \"been read (§2.5.\" -> \"been read (§2.5).\"\np6: \"We encode these vectors using a bidirectional GRU\" – I think you composing a fixed-dimensional vector from the entity vectors? What's eI?\np7: For which statement is (Kim et al. 2016) the reference? Surely, they did not invent the Hadamard product.\np8: \"Our model, in contrast\" use\" -> \"Our model, in contrast, uses\".\np8 Related Work: I think it is important to mention that existing architectures such as Memory Netwroks could, in principle, learn to track entities and devote part of their parameters to learn the effect of actions. What Neural Process Networks are providing is a strong inductive bias for tracking entities and learning the effect of actions that is useful for the task considered in this paper. As mentioned in the weaknesses, this might however come at the price of a less general model, which should be discussed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Simulating Action Dynamics with Neural Process Networks","abstract":"Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics.   Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives.","pdf":"/pdf/b24f4a09db412c4d4adf8730ee514b669f3887e8.pdf","TL;DR":"We propose a new recurrent memory architecture that can track common sense state changes of entities by simulating the causal effects of actions.","paperhash":"anonymous|simulating_action_dynamics_with_neural_process_networks","_bibtex":"@article{\n  anonymous2018simulating,\n  title={Simulating Action Dynamics with Neural Process Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJYFzMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper795/Authors"],"keywords":["representation learning","memory networks","state tracking"]}},{"tddate":null,"ddate":null,"tmdate":1509739096890,"tcdate":1509134977301,"number":795,"cdate":1509739094223,"id":"rJYFzMZC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJYFzMZC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Simulating Action Dynamics with Neural Process Networks","abstract":"Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics.   Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives.","pdf":"/pdf/b24f4a09db412c4d4adf8730ee514b669f3887e8.pdf","TL;DR":"We propose a new recurrent memory architecture that can track common sense state changes of entities by simulating the causal effects of actions.","paperhash":"anonymous|simulating_action_dynamics_with_neural_process_networks","_bibtex":"@article{\n  anonymous2018simulating,\n  title={Simulating Action Dynamics with Neural Process Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJYFzMZC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper795/Authors"],"keywords":["representation learning","memory networks","state tracking"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}