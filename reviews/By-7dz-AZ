{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222798356,"tcdate":1511981152987,"number":3,"cdate":1511981152987,"id":"H1YPgFhlf","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Review","forum":"By-7dz-AZ","replyto":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference/Paper868/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A reasonable if somewhat ad-hoc approach ","rating":"6: Marginally above acceptance threshold","review":"The authors consider the metrics for evaluating disentangled representations. They define three criteria: Disentanglement, Informativeness, and Completeness. They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights. Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.\n\nI think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at. Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me.  \n\nA second limitation of the work is the reliance on a \"true\" set of disentangled factors. We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors. Could the authors perhaps comment on how well these metrics would work in the semi-supervised case?\n\nOverall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response. \n\nMinor Commments:\n\n- Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption. \n\n- It is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the recent focus on models which can learn such representations, visual inspection remains the primary method for evaluating the degree of disentanglement achieved. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for quantitatively evaluating the quality of disentangled representations learned by different models. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and compare models on an equal basis. Experiments with the recent InfoGAN model for learning disentangled representations illustrate the appropriateness of the framework and provide a baseline for future work.","pdf":"/pdf/b2840e4750cf351b8396c960770cbe90d0fda1b3.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222798396,"tcdate":1511859898638,"number":2,"cdate":1511859898638,"id":"rk7pIjceG","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Review","forum":"By-7dz-AZ","replyto":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference/Paper868/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The idea is interesting but the way authors address it is neither fully clear nor convincing.","rating":"4: Ok but not good enough - rejection","review":"The paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data. \n\n*Quality* \nThe problem addressed is surely relevant in general terms. However, the contributed framework did not account for previously proposed metrics (such as equivariance, invariance and equivalence). Within the experimental results, only two methods are considered: although Info-GAN is a reliable competitor, PCA seems a little too basic to compete against. The choice of using noise-free data only is a limiting constraint (in [Chen et al. 2016], Info-GAN is applied to real-world data). \nFinally, in order to corroborate the quantitative results, authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix.\n\n*Clarity*\nThe explanation of the theoretical framework is not clear. In fact, Figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping. But, then, the authors missed to clarify how the definitions of D_i and C_j translate this requirement into math. \nAlso, the criterion of informativeness of Section 2 is split into two sub-criteria in Section 3.3, namely test set NRMSE and Zero-Shot NRMSE: such shift needs to be smoothed and better explained, possibly introducing it in Section 2.\n\n*Originality*\nThe paper does not allow to judge whether the three proposed criteria are original or not with respect to the previously proposed ones of [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. \n\n*Significance*\nThe significance of the proposed evaluation framework is not fully clear. The initial assumption of considering factors of variations related to graphics-generated data undermines the relevance of the work. Actually, authors only consider synthetic (noise-free) data belonging to one class only, thus not including the factors of variations related to noise and/or different classes.\n\nPROS: \nThe problem faced by the authors is interesting\n\nCONS:\nThe criteria of disentanglement, informativeness & completeness are not fully clear as they are presented.\nThe proposed criteria are not compared with previously proposed ones - equivariance, invariance and equivalence [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. Thus, it is not possible to elicit from the paper to which extent they are novel or how they are related..\nThe dataset considered is noise-free and considers one class only. Thus, several factors of variation are excluded a priori and this undermines the significance of the analysis.\nThe experimental evaluation only considers two methods, comparing Info-GAN, a state-of-the-art method, with a very basic PCA.\n\n\n**FINAL EVALUATION**\nThe reviewer rates this paper with a weak reject due to the following points.\n1) The novel criteria are not compared with existing ones [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015].\n2) There are two flaws in the experimental validation:\n\t2.1) The number of methods in comparison (InfoGAN and PCA) is limited.\n\t2.2) A synthetic dataset is only considered.\n\nThe reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the recent focus on models which can learn such representations, visual inspection remains the primary method for evaluating the degree of disentanglement achieved. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for quantitatively evaluating the quality of disentangled representations learned by different models. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and compare models on an equal basis. Experiments with the recent InfoGAN model for learning disentangled representations illustrate the appropriateness of the framework and provide a baseline for future work.","pdf":"/pdf/b2840e4750cf351b8396c960770cbe90d0fda1b3.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222798441,"tcdate":1511807706076,"number":1,"cdate":1511807706076,"id":"ByfkoCtlf","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Review","forum":"By-7dz-AZ","replyto":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference/Paper868/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper has good formal ideas about evaluating disentangling representations, a solid step forward on understanding learned representations. It would benefit from more thoughts on basic theory and more empirical work to be seen as a relevant benchmark going forward.","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThe authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.\nThe basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.\nThe authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.\nThe paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.\n\nSignificance:\nThe proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.\n\nClarity:\nThe paper is lucidly written and very understandable.\n\nQuality:\nThe authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.\nA few comments:\n1. How do the authors propose to deal with multimodal true latent factors? What if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data?\n2. Scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?\n3. the actual sources of variation are interpretable and explicit measurable quantities here. However, oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z (for instance the sentiment of a scene) even when these factors are known. How do the authors propose to move past narrow definitions of factors of variation and handle more complex variables? Arguably, disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well-behaved and relatively easy to quantify since they relate to image formation physics.\n4. For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation. I would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here, especially a variational autoencoder, beta-VAE and so on. Furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science.\n\nNovelty:\nPrevious papers like \"beta-VAE\" (Higgins et al. 2017) and \"Bayesian Representation Learning With Oracle Constraints\" by Karaletsos et al (ICLR 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does. It would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the recent focus on models which can learn such representations, visual inspection remains the primary method for evaluating the degree of disentanglement achieved. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for quantitatively evaluating the quality of disentangled representations learned by different models. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and compare models on an equal basis. Experiments with the recent InfoGAN model for learning disentangled representations illustrate the appropriateness of the framework and provide a baseline for future work.","pdf":"/pdf/b2840e4750cf351b8396c960770cbe90d0fda1b3.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739058779,"tcdate":1509136409483,"number":868,"cdate":1509739056126,"id":"By-7dz-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the recent focus on models which can learn such representations, visual inspection remains the primary method for evaluating the degree of disentanglement achieved. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for quantitatively evaluating the quality of disentangled representations learned by different models. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and compare models on an equal basis. Experiments with the recent InfoGAN model for learning disentangled representations illustrate the appropriateness of the framework and provide a baseline for future work.","pdf":"/pdf/b2840e4750cf351b8396c960770cbe90d0fda1b3.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}