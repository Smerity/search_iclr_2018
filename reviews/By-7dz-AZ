{"notes":[{"tddate":null,"ddate":null,"tmdate":1515163401494,"tcdate":1515163401494,"number":4,"cdate":1515163401494,"id":"S1-fkGaXf","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Comment","forum":"By-7dz-AZ","replyto":"ByfkoCtlf","signatures":["ICLR.cc/2018/Conference/Paper868/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper868/Authors"],"content":{"title":"Response","comment":"Thank you for your feedback. Please see our response to reviewer 2, which addresses the points made by all three reviewers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"ddate":null,"tddate":1515163331831,"tmdate":1515163419430,"tcdate":1515163299984,"number":3,"cdate":1515163299984,"id":"rJnsAbTQG","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Comment","forum":"By-7dz-AZ","replyto":"rk7pIjceG","signatures":["ICLR.cc/2018/Conference/Paper868/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper868/Authors"],"content":{"title":"Response","comment":"Thank you for your feedback. Please see our response to reviewer 2, which addresses the points made by all three reviewers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515007899560,"tcdate":1515007899560,"number":2,"cdate":1515007899560,"id":"rk4ik297f","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Comment","forum":"By-7dz-AZ","replyto":"H1YPgFhlf","signatures":["ICLR.cc/2018/Conference/Paper868/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper868/Authors"],"content":{"title":"Response to all reviewers (part 2)","comment":"R1:\n> The significance of the proposed evaluation framework is not fully clear. The\n> initial assumption of considering factors of variations related to\n> graphics-generated data undermines the relevance of the work.\n\nWe have further clarified the significance of the framework in Section\n1 & 5. The framework is not restricted to graphics-generated data,\nit could also be used e.g. with speech synthesis.\n\nR1:\n> But, then, the authors missed to clarify how the definitions of D_i\n> and C_j translate this requirement into math.\n\nThe descriptions of disentanglement and completeness on p 2&3\nmake it clear how D_i and C_j quantify the deviation from an\nideal bijective mapping.\n\nR1:\n> Also, the criterion of informativeness of Section 2 is split into two\n> sub-criteria in Section 3.3, namely test set NRMSE and Zero-Shot\n> NRMSE: such shift needs to be smoothed and better explained, possibly\n> introducing it in Section 2.\n\nWe thank the reviewer for pointing this out. We have now clarified (sec 4.1 final\nsentence) that the zero-shot inference task is a \"bonus\", and not a core\ncomponent of the framework.\n\nR1:\n> The dataset considered is noise-free and considers one class\n> only. Thus, several factors of variation are excluded a priori and\n> this undermines the significance of the analysis.\n\nIt would be easy to add noise (e.g. Gaussian) to the output of the\nrenderer, but we do not believe that this would have a substantial\neffect on the results. It would be interesting to expand the\nexperiments to cover more object classes, but we believe that the\nframework and experiments presented already constitute a substantial\nadvance.\n\nR3:\n> 1. How do the authors propose to deal with multimodal true latent\n> factors? What if multiple sets of z can generate the same observations\n> and how does the evaluation of disentanglement fairly work if the\n> underlying model cannot be uniquely recovered from the data?\n\nIf multiple sets of z can generate the same observations, then this\nshould be reflected in a (multimodal) distribution within the codes\nc. If this were present then it would be propagated via the mapping f\nfrom c to z into a distribution over z's. Current methods like InfoGAN\ntend to make a unimodal assumption about Q(c|x), but if this were\nmultimodal then the above mechanism would work, and one could use the\nobvious log-likelihood criterion log p(z|c) to train the\nregression network (e.g. like a mixture of experts, Jacobs et al\n1991). Of course the ordinary least squares criterion is just a\nspecial case of this with a Gaussian noise model for p(z|c).\nWe have also added a paragraph at the bottom of p3 concerning the\nrotation-of-factors case, for which the model is not identifiable.\n\nR1:\n> ... in order to corroborate the quantitative results, authors\n> should have reported some visual experiments in order to assess\n> whether a change in c_j really correspond to a change in the\n> corresponding factor of variation z_i according to the learnt monomial\n> matrix.\n\nPlease see Figure 6, as per the original submission.\n\nR3:\n> 3. the actual sources of variation are interpretable and explicit\n> measurable quantities here. However, oftentimes a source of variation\n> can be a variable that is hard or impossible to express in a simple\n> vector z (for instance the sentiment of a scene) even when these\n> factors are known. How do the authors propose to move past narrow\n> definitions of factors of variation and handle more complex variables?\n> Arguably, disentangling is a step towards concept learning and\n> concepts might be harder to formalize than the approach taken here\n> where in the experiment the variables are well-behaved and relatively\n> easy to quantify since they relate to image formation physics.\n\nIt is vital to be able to quantify disentangling wrt what R3 calls a\n\"simple\" vector z. The contribution of the paper is to do this. There\nmay well be more complex sources of latent structure, such as the\ninter-relationship of different objects in a scene. In our view\nthis can likely be handled by an appropriate hierarchical\nmodel with a vector of z's at the highest level, but this is\nan issue for future work.\n\nR2:\n> Tables 1 and 2 would be easier to unpack if the authors were to list\n> the names of the variables (i.e. azimuth instead of z_0) or at least\n> list what each variable is in the caption.\n\nfixed."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515007775053,"tcdate":1515007775053,"number":1,"cdate":1515007775053,"id":"H1PQy257z","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Comment","forum":"By-7dz-AZ","replyto":"H1YPgFhlf","signatures":["ICLR.cc/2018/Conference/Paper868/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper868/Authors"],"content":{"title":"Response to all reviewers (part 1)","comment":"We thank the reviewers for their helpful comments, and appreciate the\nview that \"defining metrics for evaluating the degree of\ndisentanglement in representations is great problem to look at\".\n\nTwo reviewers raise the issue that our work requires a \"true\" set of\ngenerative factors in order to carry out the evaluation. Our response\nis that if it is not possible to quantify disentanglement in this\nsituation, it will certainly be much more difficult to quantify it\nwhen the ground truth is not known, and this must be the first step.\nWe have now emphasized in the abstract, introduction and conclusion\nthat the method applies when the ground truth generative factors are\nknown.\n\nR1:\n> However, the contributed framework did not account for previously proposed\n> metrics (such a equivariance, invariance and equivalence).\n> ...\n> The paper does not allow to judge whether the three proposed criteria\n> are original or not with respect to the previously proposed ones of\n> [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014,\n> Jayaraman & Grauman 2015].\n> â€¦\n> The novel criteria are not compared with existing ones [Goodfellow et al.\n> 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman\n> 2015].\nand\nR3:\n> Previous papers like \"beta-VAE\" (Higgins et al. 2017) and \"Bayesian\n> Representation Learning With Oracle Constraints\" by Karaletsos et al\n> (ICLR 16) have followed similar experimental protocols inspired by the\n> same underlying idea of recovering known latent factors, but have\n> fallen short of proposing a formal framework like this paper does. It\n> would be good to add a section gathering such attempts at evaluation\n> previously made and trying to unify them under the proposed\n> framework.\n\nWe have added sec 3 to expand the coverage of related work. The\nrelationship to equivariance and invariance is covered in the last\nparagraph of sec 3; note that such properties arise naturally \nfrom a properly disentangled and informative representation.\n\nWe have expanded the comparison to Higgins et al. (2017) and\nKaraletsos et al (2016) in sec 3. We have also added here a paragraph\non similarities/differences to the work of Yang and Amari (1997) wrt\nthe evaluation of ICA, following comments we received on the paper.\n\nR2:\n> Within the experimental results, only two methods are considered:\n> although Info-GAN is a reliable competitor, PCA seems a little too\n> basic to compete against.\n> ...\n> The experimental evaluation only considers two methods, comparing\n> Info-GAN, a state-of-the-art method, with a very basic PCA.\nand\nR3:\n> The paper ultimately is light on comprehensive evaluation of popular models\n> on a variety of datasets and as such does not quite yield the insights it\n> could.\n> ...\n> For a paper introducing a formal experimental framework and metrics or\n> evaluation I find that the paper is light on experiments and evaluation. I\n> would hope that at the very least a broad range of generative models and\n> some recognition models are used to evaluate here, especially a variational\n> autoencoder, beta-VAE and so on.\n\nOur experiments highlight the differences between a baseline (PCA) and a\nstate-of-the-art method (InfoGAN). This contrastive\ncomparison demonstrates the appropriateness of the framework, with the\nthree criteria clearly explaining why InfoGAN's learnt code is superior to PCA's\nand the metric scores quantifying this level of superiority. We will make the\ncode and dataset publicly available on acceptance of the paper and hope this\nfacilitates further comparisons and eventually the establishment of quantitative\nbenchmarks for disentangled factor learning. We note e.g. that the authors of\nthe beta-VAE have not published their code, which has made conducting the\nrequested experiments more difficult.\n\nR3:\n> Furthermore the authors could consider ... offering a benchmark\n> experiment and code for the community to establish this as a means\n> of evaluation to maximize the impact of a paper aimed at\n> reproducibility and good science.\n\nWe will be happy to make the dataset and code publicly available\non acceptance of the paper, as now mentioned in the conclusion.\n\nR2:\n> though the way pseudo-distribution are define in terms of normalized weight\n> magnitudes is seems a little ad hoc to me.\n> ...\n> It is not entirely clear to me how the proposed metrics, whose\n> definitions all reference magnitudes of weights, generalize to the\n> case of random forests.\n\nWe thank the reviewer for this feedback. We have now clarified these points\nby defining the relative importances R_{ij} on p2, and discussing the definition\nof importances for random forests as per Breiman et al (1984) on p3."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642522747,"tcdate":1511981152987,"number":3,"cdate":1511981152987,"id":"H1YPgFhlf","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Review","forum":"By-7dz-AZ","replyto":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference/Paper868/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A reasonable if somewhat ad-hoc approach ","rating":"6: Marginally above acceptance threshold","review":"The authors consider the metrics for evaluating disentangled representations. They define three criteria: Disentanglement, Informativeness, and Completeness. They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights. Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.\n\nI think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at. Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me.  \n\nA second limitation of the work is the reliance on a \"true\" set of disentangled factors. We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors. Could the authors perhaps comment on how well these metrics would work in the semi-supervised case?\n\nOverall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response. \n\nMinor Commments:\n\n- Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption. \n\n- It is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515780443967,"tcdate":1511859898638,"number":2,"cdate":1511859898638,"id":"rk7pIjceG","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Review","forum":"By-7dz-AZ","replyto":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference/Paper868/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The idea is interesting and, despite the limited experimental setup, the proposed framework can ease future investigations ","rating":"6: Marginally above acceptance threshold","review":"The paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data. \n\n*Quality* \nThe problem addressed is surely relevant in general terms. However, the contributed framework did not account for previously proposed metrics (such as equivariance, invariance and equivalence). Within the experimental results, only two methods are considered: although Info-GAN is a reliable competitor, PCA seems a little too basic to compete against. The choice of using noise-free data only is a limiting constraint (in [Chen et al. 2016], Info-GAN is applied to real-world data). \nFinally, in order to corroborate the quantitative results, authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix.\n\n*Clarity*\nThe explanation of the theoretical framework is not clear. In fact, Figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping. But, then, the authors missed to clarify how the definitions of D_i and C_j translate this requirement into math. \nAlso, the criterion of informativeness of Section 2 is split into two sub-criteria in Section 3.3, namely test set NRMSE and Zero-Shot NRMSE: such shift needs to be smoothed and better explained, possibly introducing it in Section 2.\n\n*Originality*\nThe paper does not allow to judge whether the three proposed criteria are original or not with respect to the previously proposed ones of [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. \n\n*Significance*\nThe significance of the proposed evaluation framework is not fully clear. The initial assumption of considering factors of variations related to graphics-generated data undermines the relevance of the work. Actually, authors only consider synthetic (noise-free) data belonging to one class only, thus not including the factors of variations related to noise and/or different classes.\n\nPROS: \nThe problem faced by the authors is interesting\n\nCONS:\nThe criteria of disentanglement, informativeness & completeness are not fully clear as they are presented.\nThe proposed criteria are not compared with previously proposed ones - equivariance, invariance and equivalence [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]. Thus, it is not possible to elicit from the paper to which extent they are novel or how they are related..\nThe dataset considered is noise-free and considers one class only. Thus, several factors of variation are excluded a priori and this undermines the significance of the analysis.\nThe experimental evaluation only considers two methods, comparing Info-GAN, a state-of-the-art method, with a very basic PCA.\n\n\n**FINAL EVALUATION**\nThe reviewer rates this paper with a weak reject due to the following points.\n1) The novel criteria are not compared with existing ones [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015].\n2) There are two flaws in the experimental validation:\n\t2.1) The number of methods in comparison (InfoGAN and PCA) is limited.\n\t2.2) A synthetic dataset is only considered.\n\nThe reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed. \n\n**EVALUATION AFTER AUTHORS' REBUTTAL**\nThe reviewer has read the responses provided by the authors during the rebuttal period. In particular, with respect to the highlighted points 1 and 2, point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper. Despite the same level of clarification has not been reached for what concerns point 2, the proposed framework (although still limited in relevance due to the lack of more realistic settings) can be useful for the community as a benchmark to verify the level of disentanglement than newly proposed deep architectures can achieve. Finally, by also taking into account the positive evaluation provided by the fellow reviewers, the rating of the paper has been risen towards acceptance.   \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515796359633,"tcdate":1511807706076,"number":1,"cdate":1511807706076,"id":"ByfkoCtlf","invitation":"ICLR.cc/2018/Conference/-/Paper868/Official_Review","forum":"By-7dz-AZ","replyto":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference/Paper868/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper has good formal ideas about evaluating disentangling representations, a solid step forward on understanding learned representations. It would benefit from more thoughts on basic theory and more empirical work to be seen as a relevant benchmark going forward.","rating":"7: Good paper, accept","review":"****\nI acknowledge the author's comments and improve my score to 7.\n****\n\nSummary:\nThe authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.\nThe basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c.\nThe authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables.\nThe paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.\n\nSignificance:\nThe proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way.\n\nClarity:\nThe paper is lucidly written and very understandable.\n\nQuality:\nThe authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.\nA few comments:\n1. How do the authors propose to deal with multimodal true latent factors? What if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data?\n2. Scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?\n3. the actual sources of variation are interpretable and explicit measurable quantities here. However, oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z (for instance the sentiment of a scene) even when these factors are known. How do the authors propose to move past narrow definitions of factors of variation and handle more complex variables? Arguably, disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well-behaved and relatively easy to quantify since they relate to image formation physics.\n4. For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation. I would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here, especially a variational autoencoder, beta-VAE and so on. Furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science.\n\nNovelty:\nPrevious papers like \"beta-VAE\" (Higgins et al. 2017) and \"Bayesian Representation Learning With Oracle Constraints\" by Karaletsos et al (ICLR 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does. It would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515008131567,"tcdate":1509136409483,"number":868,"cdate":1509739056126,"id":"By-7dz-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By-7dz-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A framework for the quantitative evaluation of disentangled representations","abstract":"Recent AI research has emphasized the importance of learning  disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric.  While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).","pdf":"/pdf/be859db7900406f37d817c63db2a38ec35864ae2.pdf","paperhash":"anonymous|a_framework_for_the_quantitative_evaluation_of_disentangled_representations","_bibtex":"@article{\n  anonymous2018a,\n  title={A framework for the quantitative evaluation of disentangled representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-7dz-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper868/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}