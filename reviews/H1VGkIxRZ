{"notes":[{"tddate":null,"ddate":null,"tmdate":1515061519193,"tcdate":1515061519193,"number":5,"cdate":1515061519193,"id":"SJDGWFiQG","invitation":"ICLR.cc/2018/Conference/-/Paper267/Public_Comment","forum":"H1VGkIxRZ","replyto":"ryYGAGoXf","signatures":["~Siniša_Šegvić1"],"readers":["everyone"],"writers":["~Siniša_Šegvić1"],"content":{"title":"Thanks","comment":"Thank you for your clarification, I had missed the discussion in 5.2 and 5.3."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1515036176659,"tcdate":1515036176659,"number":5,"cdate":1515036176659,"id":"ryYGAGoXf","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Comment","forum":"H1VGkIxRZ","replyto":"r1OWfeiXf","signatures":["ICLR.cc/2018/Conference/Paper267/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper267/Authors"],"content":{"title":"Thanks for your comments","comment":"Thank you for your comments.  The analysis of the effects of temperature scaling can be found in Section 5.2, where we provide some insight into why changing T and delta can improve the detection performance.\n\nHere we provide some additional explanation. Suppose we consider two images where the difference between the largest output and the average of the rest of the outputs (denoted by U_1 in Section 2) is only slightly larger for the in-distribution image than  the out-of-distribution image.  In that same section, we have denoted the variance in the output by U_2, and have used Taylor's series to suggest that the soft-max score of the largest output is then determined by U_1 divided by T and U_2 divided by T^2. We also argue and provide empirical evidence to show that U_2 is larger for in-distribution images than out of distribution images. Thus, the soft-max score of the largest output of the in-distribution image can be smaller than the soft-max score of the largest output of the out-of-distribution image. To remedy this situation, Taylor's series suggests that increasing T will reduce the impact of U_2 and U_1 will dominate. Now, as we change T, we have to correspondingly change delta (since a larger T pushes all the soft-max scores towards 1/N) to be able to detect an in-distribution and an out-of-distribution image."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1515026776913,"tcdate":1515024896183,"number":4,"cdate":1515024896183,"id":"r1OWfeiXf","invitation":"ICLR.cc/2018/Conference/-/Paper267/Public_Comment","forum":"H1VGkIxRZ","replyto":"H1VGkIxRZ","signatures":["~Siniša_Šegvić1"],"readers":["everyone"],"writers":["~Siniša_Šegvić1"],"content":{"title":"Clarifying interaction between \\delta and T might improve the paper","comment":"Detecting outliers by perturbing the input is intuitively clear (and very nice!). Near inliers, the model has relatively low curvature due to strong training signal. Thus, anti-adversarial perturbation of an inlier is likely to increase the probability of the dominant class. On the other hand, the model may change arbitrarily near outliers. Hence, anti-adversarial perturbations are likely to produce less dependable effects there.\n\nHowever, I do not understand the advantage of simultaneously fitting the softmax temperature (T) *and* the softmax output threshold \\delta. It appears as these two parameters should cancel out each other, and yet Fig.3 suggests that somehow T=1000 is better than T=1 when \\delta is set for TPR=95%. Clarifying the interaction between \\delta and T might improve the paper.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1513996743416,"tcdate":1513996697871,"number":4,"cdate":1513996697871,"id":"ryfs-roff","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Comment","forum":"H1VGkIxRZ","replyto":"r1KVjuSlf","signatures":["ICLR.cc/2018/Conference/Paper267/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper267/Authors"],"content":{"title":"Response to Reviewer1","comment":"We thank the reviewer for the useful feedback. We address each point raised in detail below.\n\nR1: In the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet.\n\nWe have revised the last sentence of the abstract: “For example,ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.”\n\nR1: grammatical issues and some sentences need rewriting.\nWe have rewritten those sentences, and others, in the revised version. \n\nR1: I'm not convinced that the performance of the network for in-distribution images is unchanged. \n\nYes, the overall accuracy would have been changed if we ignored the results for in-distribution images that appear to be out-of-distribution.However, we meant to say that our method does not change the label predictions for in-distribution images, since one can always use the original image and pass it through the original neural network. We have replaced the word “performance” with “predictions” to avoid confusion.\n\nR1: Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution?\n\nWe provide empirical results on the correlation between difficult-to-classify images and difficult-to-detect images (Figure 16). We can observe that the images that are difficult to detect tend to be the images that are difficult to classify  (e.g., DenseNet can only achieve  around 50% test accuracy on the images having softmax scores below the threshold corresponding to 99% TPR,  while being able to achieve around 95.2% accuracy on the overall image set).\n\nR1: When you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again.\n\nWe have extensively studied the effect of $\\delta$ and have provided additional results in Appendix H. Also, as mentioned in the response to Reviewer 2, we no longer optimize over delta.\n\nR1: The reciprocal of the results between two datasets. \n\nWe provide the reciprocal of the results between CIFAR-10 and CIFAR-100 in Appendix I.  DenseNet can achieve 47.2% FPR at TPR 95% when the CIFAR-10 dataset is the in-distribution dataset and CIFAR-100 dataset is the out-of-distribution dataset, while achieving 81.4% FPR at TPR 95% when the CIFAR-100 dataset is the in-distribution and CIFAR-10 dataset is the out-of-distribution dataset. \n\nR1: What would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one? \n\nWe provide additional results in Appendix I (Figure 17), where we show the outputs of DenseNet on thirty classes for an image of apple from CIFAR-80 (in-distribution) and an image of red pepper from CIFAR-20 (out-distribution). We can observe that when the out-of-distribution images share a few common features with the in-distribution images (e.g., the image of apple is quite similar to the image of red pepper), the output distribution of the neural networks for the out-of-distribution images are sometimes similar to the output distribution for the in-distribution images.  \n\nR1: The method isn't sufficiently novel (although it is a novel use of existing methods).\n\nOur proposed method is inspired by the existing methods used in other tasks (temperature scaling used for distilling the knowledge in neural networks, Hinton et al., 2015, and adding small perturbations used for generating adversarial examples, Goodfellow et al. 2015). What is novel is the way in which we use perturbation: we do exactly the opposite of what Goodfellow et al. 2015 do; instead of adding, we actually subtract the perturbation suggested them. The fact that this, along with the temperature scaling, improves the out-of-distribution detection performance is surprising and novel. Further, our work also has merits in providing extensive experimental analysis and theoretical insights, and justifying the novel use case of these techniques for out-of-distribution image detection.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1513996654576,"tcdate":1513996654576,"number":3,"cdate":1513996654576,"id":"ryPuWHofM","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Comment","forum":"H1VGkIxRZ","replyto":"By__JgYef","signatures":["ICLR.cc/2018/Conference/Paper267/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper267/Authors"],"content":{"title":"Response to Reviewer2","comment":"We thank Reviewer 2 for the constructive and encouraging feedback!\n\nTo address your concern about delta, we are no longer optimizing with respect to this parameter. We tune the temperature (T), perturbation magnitude ($\\epsilon$) on an out-of-distribution image set (for a given in-distribution image set) and setting $\\delta$ to the threshold corresponding to the 95% TPR.  Our experiments in Appendix H appears to indicate that the choice of the out-of-distribution image set used to tune the parameters does not matter very much. Our method shows superior performance compared to the state-of-the-art whether we use Tiny-ImageNet (cropped) or Tiny-ImageNet (resized) or LSUN (resized) or iSUN (resized) or Gaussian noise or Uniform noise as the out-of-distribution  dataset during the parameter tuning process. We also note that, while we may use one of these datasets during the tuning process, the testing is performed against other out-of-distribution dataset as well.\n\nFollowing the suggestions, we extensively studied the effect of $\\delta$ and thereafter summarize our findings below.\nHow sensitive the method is to the threshold?\nIn Figure 13, we show how the thresholds affect FPR and TPR, where we can observe that the threshold corresponding to 95% TPR can produce small FPRs on all out-of-distribution datasets.\n\t\t\t\n(2) How much of the out of distribution dataset is used to determine this value, and what are the effects of this size during tuning?\t\t\t\t\nIn the main results reported in Table 2, we held out 1,000 images to tune the parameters and evaluated on the remaining 9,000 images. To further understand the effect of the tuning set size, we show in Figure 15 the detection performance as we vary the tuning set size, ranging from 200 to 2000. We evaluate the detection performance on the remaining 8,000 images. In general we found the performance tends to stabilize as the tuning set size varies.. \n\n(3) How does delta generalize across datasets? \nIn addition to the observation in Figure 13 (a) and (b) that the effect of $delta$ is quite similar across datasets, we further conducted experiments as suggested by the reviewer. Specifically, we set the threshold using one out of distribution dataset and then evaluate on a different one. All the results can be found in Appendix H (Figure 14). We observe that the parameters tuned on different out-of-distribution natural image sets have quite similar detection performance.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1513996587653,"tcdate":1513996587653,"number":2,"cdate":1513996587653,"id":"H1E4-rszf","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Comment","forum":"H1VGkIxRZ","replyto":"By0tIonxf","signatures":["ICLR.cc/2018/Conference/Paper267/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper267/Authors"],"content":{"title":"Response to Reviewer3","comment":"Thank you for the encouraging feedback on the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1515642422354,"tcdate":1511990917562,"number":3,"cdate":1511990917562,"id":"By0tIonxf","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Review","forum":"H1VGkIxRZ","replyto":"H1VGkIxRZ","signatures":["ICLR.cc/2018/Conference/Paper267/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Improvement on state-of-the-art for detecting out of distribution examples","rating":"9: Top 15% of accepted papers, strong accept","review":"Detecting out of distribution examples is important since it lets you know when neural network predictions might be garbage. The paper addresses this problem with a method inspired by adversarial training, and shows significant improvement over best known method, previously published in ICLR 2017.\n\nPrevious method used at the distribution of softmax scores as the measure. Highly peaked -> confidence, spread out -> out of distribution. The authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step. The small step is in the direction of gradient when top class activation is taken as the objective. This is also the gradient used to determine influence of predictors, and it's the gradient term used for adversarial training \"fast gradient sign\" method.\n\nTheir experiments show improvement across the board using DenseNet on collection of small size dataset (tiny imagenet, cifar, lsun). For instance at 95% threshold (detect 95% of out of distribution examples), their error rate goes down from 34.7% for the best known method, to 4.3% which is significant enough to prefer their method to the previous work.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1515642422391,"tcdate":1511747440111,"number":2,"cdate":1511747440111,"id":"By__JgYef","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Review","forum":"H1VGkIxRZ","replyto":"H1VGkIxRZ","signatures":["ICLR.cc/2018/Conference/Paper267/AnonReviewer2"],"readers":["everyone"],"content":{"title":"simple, effective technique","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new method for detecting out of distribution samples. The core idea is two fold: when passing a new image through the (already trained) classifier, first preprocess the image by adding a small perturbation to the image pushing it closer to the highest softmax output and second, add a temperature to the softmax. Then, a simple decision is made based on the output of the softmax of the perturbed image - if it is able some threshold then the image is considered in-distribution otherwise out-distribution.\n\nThis paper is well written, easy to understand and presents a simple and apparently effective method of detecting out of distribution samples. The authors evaluate on cifar-10/100 and several out of distribution datasets and this method outperforms the baseline by significant margins. They also examine the effects of the temperature and step size of the perturbation. \n \nMy only concern is that the parameter delta (threshold used to determine in/out distribution) is not discussed much. They seem to optimize over this parameter, but this requires access to the out of distribution set prior to the final evaluation. Could the authors comment on how sensitive the method is to this parameter? How much of the out of distribution dataset is used to determine this value, and what are the effects of this size during tuning? What happens if you set the threshold using one out of distribution dataset and then evaluate on a different one? This seems to be the central part missing to this paper and if the authors are able to address it satisfactorily I will increase my score. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1515768274818,"tcdate":1511521072594,"number":1,"cdate":1511521072594,"id":"r1KVjuSlf","invitation":"ICLR.cc/2018/Conference/-/Paper267/Official_Review","forum":"H1VGkIxRZ","replyto":"H1VGkIxRZ","signatures":["ICLR.cc/2018/Conference/Paper267/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"\n-----UPDATE------\n\nThe authors addressed my concerns satisfactorily. Given this and the other reviews I have bumped up my score from a 5 to a 6.\n\n----------------------\n\n\nThis paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs. This is a novel use of existing methods.\n\nSome roughly chronological comments follow:\n\nIn the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet.\n\nThe paper is quite well written aside from some grammatical issues. In particular, articles are frequently missing from nouns. Some sentences need rewriting (e.g. in 4.1 \"which is as well used by Hendrycks...\", in 5.2 \"performance becomes unchanged\").\n\n It is perhaps slightly unnecessary to give a name to your approach (ODIN) but in a world where there are hundreds of different kinds of GANs you could be forgiven.\n\nI'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images. I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications). Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution?\n\nWhen you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again.\n\nIn terms of experimentation it would be interesting to see the reciprocal of the results between two datasets. For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?\n\nSection 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results. This may just be a matter of taste.\n\nI did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one? (This is in part, addressed in the CIFAR80 20 experiments in the appendices).\n\nThis appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods).\n\nPros:\n- Baseline performance is exceeded by a large margin\n- Novel use of adversarial perturbation and temperature\n- Interesting analysis\n\nCons:\n- Doesn't introduce and novel methods of its own\n- Could do with additional experiments (as mentioned above)\n- Minor grammatical errors\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]}},{"tddate":null,"ddate":null,"tmdate":1513996482838,"tcdate":1509084940484,"number":267,"cdate":1509739392757,"id":"H1VGkIxRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1VGkIxRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks","abstract":"We consider the problem of detecting  out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can  separate the softmax score distributions of in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10 and Tiny-ImageNet) when the true positive rate is 95%.","pdf":"/pdf/9fde880b0d660cfb6a183a884fc6c2d98ae2fd65.pdf","paperhash":"anonymous|enhancing_the_reliability_of_outofdistribution_image_detection_in_neural_networks","_bibtex":"@article{\n  anonymous2018enhancing,\n  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1VGkIxRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper267/Authors"],"keywords":["Neural networks","out-of-distribution detection"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}