{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222691282,"tcdate":1512077165292,"number":3,"cdate":1512077165292,"id":"rJHuDgAlz","invitation":"ICLR.cc/2018/Conference/-/Paper561/Official_Review","forum":"rkPLzgZAZ","replyto":"rkPLzgZAZ","signatures":["ICLR.cc/2018/Conference/Paper561/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Complex network using heuristic structure, state reoresentations, and action selection for solving tasks inspired by psychology","rating":"6: Marginally above acceptance threshold","review":"The authors propose a kind of framework for learning to solve elemental tasks and then learning task switching in a multitask scenario. The individual tasks are inspired by a number of psychological tasks. Specifically, the authors use a pretrained convnet as raw statespace encoding together with previous actions and learn through stochastic optimization to predict future rewards for different actions. These constitute encapsulated modules for individual tasks. The authors test a number of different ways to construct the state representations as inputs to these module and report results from extensive simulations evaluating them. The policy is obtained through a heuristic, selecting actions with highest reward prediction variance across multiple steps of lookahead. Finally, two variants of networks are presented and evaluated, which have the purpose of selecting the appropriate module when a signal is provided to the system that a new task is starting.\n\nI find it particularly difficult to evaluate this manuscript. The presented simulation results are based on the described system, which is very complex and contains several, non-standard components and heuristic algorithms. \n\nIt would be good to motivate the action selection a bit further. E.g., the authors state that actions are sampled proportionally to the reward predictions and assure properties that are not necessarily intuitive, e.g. that a few reward in the future can should be equated to action values. It is also not clear under which conditions the proposed sampling of actions and the voting results in reward maximization. No statements are made on this other that empirically this worked best.  \n\nIs the integral in eq. 1 appropriate or should it be a finite sum?\n\nIt seems, that the narrowing of the spatial distribution relative to an \\epsilon -greedy policy would highly depend on the actual reward landscape, no? Is the maximum variance as well suited for exploration as for exploitation and reward maximization?\n\nWhat I find a bit worrisome is the ease with which the manuscript switches between  “inspiration” from psychology and neuroscience to plausibility of proposing algorithms to reinterpreting aimpoints as “salience” and feature extraction as “physical structure”. This necessarily introduces a number of \n\nOverall, I am not sure what I have learned with this paper. Is this about learning psychological tasks? New exploration policies? Arbitration in mixtures of experts? Or is the goal to engineer a network that can solve tasks that cannot be solved otherwise? I am a bit lost.\n\n\nMinor points:\n“can from”","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modular Continual Learning in a Unified Visual Environment","abstract":"A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a recurrent neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.","pdf":"/pdf/539b9cc50cd1acf859524e0fc56447c2537f5105.pdf","TL;DR":"We propose a neural module approach to continual learning using a unified visual environment with a large action space.","paperhash":"anonymous|modular_continual_learning_in_a_unified_visual_environment","_bibtex":"@article{\n  anonymous2018modular,\n  title={Modular Continual Learning in a Unified Visual Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkPLzgZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper561/Authors"],"keywords":["Continual Learning","Neural Modules","Interface Learning","Task Switching","Reinforcement Learning","Visual Decision Making"]}},{"tddate":null,"ddate":null,"tmdate":1512222691323,"tcdate":1512009194869,"number":2,"cdate":1512009194869,"id":"BJQgR1aef","invitation":"ICLR.cc/2018/Conference/-/Paper561/Official_Review","forum":"rkPLzgZAZ","replyto":"rkPLzgZAZ","signatures":["ICLR.cc/2018/Conference/Paper561/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting, albeit very dense, paper describing continuous learning over a set of different tasks with a flexible modular architecture and a large state and action space.","rating":"8: Top 50% of accepted papers, clear accept","review":"Reading this paper feels like reading at least two closely-related papers compressed into one, with overflow into the appendix (e.g. one about the EMS module, one about the the recurrent voting, etc).\n\nThere were so many aspects/components, that I am not entirely confident I fully understood how they all work together, and in fact I am pretty confident there was at least some part of this that I definitely did not understand. Reading it 5-20 more times would most likely help.\n\nFor example, consider the opening example of Section 3. In principle, this kind of example is great, and more of these would be very useful in this paper. This particular one raises a few questions:\n-Eq 5 makes it so that $(W \\Psi)$ and $(a_x)$ need to be positive or negative together.  Why use ReLu's here at all? Why not just $sign( (W \\Psi) a_x) $? Multiplying them will do the same thing, and is much simpler. I am probably missing something here, would like to know what it is... (Or, if the point of the artificial complexity is to give an example of the 3 basic principles, then perhaps point this out, or point out why the simpler version I just suggested would not scale up, etc)\n-what exactly, in this example, does $\\Psi$ correspond to? In prev discussion, $\\Psi$ is always written with subscripts to denote state history (I believe), so this is an opportunity to explain what is different here. \n-Nitpick: why is a vector written as $W$? (or rather, what is the point of bold vs non-bold here?)\n-a non-bold version of $Psi$, a few lines below, seems to correspond to the 4096 features of VGG's FC6, so I am still not sure what the bold version represents\n\n-The defs/eqns at the beginning of section 3.1 (Sc, CReLu, etc) were slightly hard to follow and I wonder whether there were any typos, e.g. was CReS meant to refer directly to Sc, but used the notation ${ReLu}^2$ instead? \n\nEach of these on its own would be easier to overlook, but there is a compounding effect here for me, as a reader, such that by further on in the paper, I am rather confused.\n\nI also wonder whether any of the elements described, have more \"standard\" interpretations/notations. For example, my slight confusion propagated further: after above point, I then did not have a clear intuition about $l_i$ in the EMS module. I get that symmetry has been built in, e.g. by the definitions of CReS and CReLu, etc, but I still don't see how it all works together, e.g. are late bottleneck architectures *exactly* the same as MLPs, but where inputs have simply been symmetrized, squared, etc? Nor do I have intuition about multiplicative symmetric interactions between visual features and actions, although I do get the sense that if I were to spend several hours implementing/writing out toy examples, it would clarify it significantly (in fact, I wouldn't be too surprised if it turns out to be fairly straightforward, as in my above comment indicating a seeming equivalence to simply multiplying two terms and taking the resulting sign). If the paper didn't need to be quite as dense, then I would suggest providing more elucidation for the reader, either with intuitions or examples or clearer relationships to more familiar formulations.\n\nLater, I did find that some of the info I *needed* in order to understand the results (e.g. exactly what is meant by a \"symmetry ablation\", how was that implemented?) was in fact in the appendices (of which there are over 8 pages).\n\nI do wonder how sensitive the performance of the overall system is to some of the details, like, e.g. the low-temp Boltzmann sampling rather than identity function, as described at the end of S2.\n\nMy confidence in this review is somewhere between 2 and 3.\n\nThe problem is an interesting one, the overall approach makes sense, it is clear the authors have done a very substantial  amount of work, and very diligently so (well-done!), some of the ideas are interesting and seem creative, but I am not sure I understand the glue of the details, and that might be very important here in order to assess it effectively.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modular Continual Learning in a Unified Visual Environment","abstract":"A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a recurrent neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.","pdf":"/pdf/539b9cc50cd1acf859524e0fc56447c2537f5105.pdf","TL;DR":"We propose a neural module approach to continual learning using a unified visual environment with a large action space.","paperhash":"anonymous|modular_continual_learning_in_a_unified_visual_environment","_bibtex":"@article{\n  anonymous2018modular,\n  title={Modular Continual Learning in a Unified Visual Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkPLzgZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper561/Authors"],"keywords":["Continual Learning","Neural Modules","Interface Learning","Task Switching","Reinforcement Learning","Visual Decision Making"]}},{"tddate":null,"ddate":null,"tmdate":1512222691366,"tcdate":1511820856538,"number":1,"cdate":1511820856538,"id":"HkxSRZcxM","invitation":"ICLR.cc/2018/Conference/-/Paper561/Official_Review","forum":"rkPLzgZAZ","replyto":"rkPLzgZAZ","signatures":["ICLR.cc/2018/Conference/Paper561/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A interesting scenario with several implications","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper comprises several ideas to study the continual learning problem. First, they show an ad-hoc designed environment, namely the Touchstream environment, in which both inputs and actions are represented in a huge space: as it happens with humans – for example when they are using a touch screen – the resolution of the input space, i.e. the images, is at least big as the resolution of the action space, i.e. where you click on the screen. This environment introduces the interesting problem of a direct mapping between input and actions. Second, they introduce an algorithm to solve this mapping problem in the Touchstream space. Specifically, the ReMaP algorithm learns to solve typical neuroscience tasks, by optimizing a computational module that facilitates the mapping in this space. The Early Bottleneck Multiplicative Symmetric (EMS) module extends the types of computation you might need to solve the tasks in the Touchstream space. Third, the authors introduce another module to learn how to switch from task to task in a dynamical way.\nThe main concern with this paper is about its length. While the conference does not provide any limits in terms of number of pages, the 13 pages for the main text plus other 8 for the supplementary material is probably too much. I am wondering if the paper just contains too much information for a single conference publication. \nAs a consequence, either the learning of the single tasks and the task switching experiments could have been addressed with further details. In case of single task learning, the tasks are relatively simple where k_beta = 1 (memory) and k_f = 2 (prediction) are sufficient to solve the tasks. It would have been interesting to evaluate the algorithm on more complex tasks, and how these parameters affect the learning. In case of the task switching a more interesting question would be how the network learns a sequence of tasks (more than one switch). \nOverall, the work is interesting, well described and the results are consistent. \nSome questions:\n- the paper starts with clear inspiration from Neuroscience, but nothing has been said on the biological plausibility of the ReMap, EMS and the recurrent neural voting;\n- animals usually learn in continuous-time, thus such tasks usually involve a time component, while this work is designed in a time step framework; could the author argument on that? (starting from Doya 2000)\n- specifically, the MTS and the Localization tasks involve working memory, thus a comparison with other working memory with reinforcement learning would make more sense that different degree of ablated modules. (for example Bakker 2002)\n\nMinor:\n- Fig 6 does not have letters\n- TouchStream is sometimes written as Touchstream\n\nRef.\nDoya, K. (2000). Reinforcement learning in continuous time and space. Neural Computation, 12(1), 219–245.\nBakker, B. (2002). Reinforcement Learning with Long Short-Term Memory. In T. G. Dietterich, S. Becker, & Z. Ghahramani (Eds.), (pp. 1475–1482). Presented at the Advances in Neural Information Processing Systems 14, MIT Press.\n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modular Continual Learning in a Unified Visual Environment","abstract":"A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a recurrent neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.","pdf":"/pdf/539b9cc50cd1acf859524e0fc56447c2537f5105.pdf","TL;DR":"We propose a neural module approach to continual learning using a unified visual environment with a large action space.","paperhash":"anonymous|modular_continual_learning_in_a_unified_visual_environment","_bibtex":"@article{\n  anonymous2018modular,\n  title={Modular Continual Learning in a Unified Visual Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkPLzgZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper561/Authors"],"keywords":["Continual Learning","Neural Modules","Interface Learning","Task Switching","Reinforcement Learning","Visual Decision Making"]}},{"tddate":null,"ddate":null,"tmdate":1509739235101,"tcdate":1509126735200,"number":561,"cdate":1509739232438,"id":"rkPLzgZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkPLzgZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Modular Continual Learning in a Unified Visual Environment","abstract":"A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a recurrent neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.","pdf":"/pdf/539b9cc50cd1acf859524e0fc56447c2537f5105.pdf","TL;DR":"We propose a neural module approach to continual learning using a unified visual environment with a large action space.","paperhash":"anonymous|modular_continual_learning_in_a_unified_visual_environment","_bibtex":"@article{\n  anonymous2018modular,\n  title={Modular Continual Learning in a Unified Visual Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkPLzgZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper561/Authors"],"keywords":["Continual Learning","Neural Modules","Interface Learning","Task Switching","Reinforcement Learning","Visual Decision Making"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}