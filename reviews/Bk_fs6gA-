{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642447197,"tcdate":1511824020697,"number":3,"cdate":1511824020697,"id":"rJT9cfqlz","invitation":"ICLR.cc/2018/Conference/-/Paper426/Official_Review","forum":"Bk_fs6gA-","replyto":"Bk_fs6gA-","signatures":["ICLR.cc/2018/Conference/Paper426/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting approach but flawed experiments","rating":"3: Clear rejection","review":"This paper proposes using long term memory to solve combinatorial optimization problems with binary variables. The authors do not exhibit much knowledge of combinatorial optimization literature (as has been pointed out by other readers) and ignore a lot of previous work by the combinatorial optimization community. In particular, evaluating on random instances is not a good measure of performance,  as has already been pointed out. The other issue is with the baseline solver, which also seems to be broken since their solution quality seems extremely poor. In light of these issues, I recommend reject.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]}},{"tddate":null,"ddate":null,"tmdate":1515642447235,"tcdate":1511822748363,"number":2,"cdate":1511822748363,"id":"ByEoHz5ez","invitation":"ICLR.cc/2018/Conference/-/Paper426/Official_Review","forum":"Bk_fs6gA-","replyto":"Bk_fs6gA-","signatures":["ICLR.cc/2018/Conference/Paper426/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP). The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP). In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations. This network is trained through supervised learning for the output and reinforcement learning for discrete operations. The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.\n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new.\n\n[Cons]\n- The paper is not clearly written (e.g., problem statement, notations, architecture description). So, it is hard to understand the core idea of this paper.\n- The proposed method and problem setting are not well-justified. \n- The results are not very convincing.\n\n# Novelty and Significance\n- The problem considered in this paper is new, but it is unclear why the problem should be formulated in such a way. To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem. I do not see why this should be formulated as a \"sequential\" decision problem. Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks. This does not require any \"memory\" because all necessary information is available to the network.\n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications. \n\n# Quality\n- The proposed reward function for training the memory controller sounds a bit arbitrary. The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network. In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective. It would be good to justify (empirically) the proposed reward function. \n- The results are not fully-convincing. If my understanding is correct, the LTMN is trained to predict the baseline solver's output. But, the LTMN significantly outperforms the baseline solver even in the training set. Can you explain why this is possible?\n\n# Clarity\n- The problem statement and model description are not described well. \n1) Is the network given a sequence of program/solution input? If yes, is it given during evaluation as well?\n2) Many notations are not formally defined. What is the output (o_t) of the network? Is it the optimal solution (x_t)? \n3) There is no mathematical definition of memory addressing mechanism used in this paper.\n- The overall objective function is missing. \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]}},{"tddate":null,"ddate":null,"tmdate":1515642447272,"tcdate":1511818244698,"number":1,"cdate":1511818244698,"id":"rJpWE-cgM","invitation":"ICLR.cc/2018/Conference/-/Paper426/Official_Review","forum":"Bk_fs6gA-","replyto":"Bk_fs6gA-","signatures":["ICLR.cc/2018/Conference/Paper426/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very hard to judge","rating":"4: Ok but not good enough - rejection","review":"Learning to solve combinatorial optimization problems using recurrent networks is a very interesting research topic. However, I had a very hard time understanding the paper. It certainly doesn’t help that I’m not familiar with the architectures the model is based on, nor with state-of-the-art integer programming solvers.\n\nThe architecture was described but not really motivated. The authors chose to study only random instances which are known to be bad representatives of real-world problmes, instead of picking a standard benchmark problem. Furthermore, the insights on how the network is actually solving the problems and how the proposed components contribute to the solution are minimal, if any.\n\nThe experimental issues (especially regarding the baseline) raised by the anonymous comments below were rather troubling; it’s a pity they were left unanswered.\n\nHopefully other expert reviewers will be able to provide constructive feedback.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]}},{"tddate":null,"ddate":null,"tmdate":1510558657737,"tcdate":1510558657737,"number":3,"cdate":1510558657737,"id":"Hy5pspL1f","invitation":"ICLR.cc/2018/Conference/-/Paper426/Public_Comment","forum":"Bk_fs6gA-","replyto":"rJjGPGcAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Strongly agree","comment":"Looking forward to a response from the authors.\n\nAnother signal that the baseline is broken - with 80 and 150 variables, the average cost is zero, which means that for all of the 1000 problems at these two sizes, their baseline returned the trivial all zeros solution.\n\nThe baseline is clearly broken (in addition to the many troubling concerns already pointed out in other comments)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]}},{"tddate":null,"ddate":null,"tmdate":1509807437080,"tcdate":1509725970831,"number":2,"cdate":1509725970831,"id":"rJjGPGcAZ","invitation":"ICLR.cc/2018/Conference/-/Paper426/Public_Comment","forum":"Bk_fs6gA-","replyto":"Bk_fs6gA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Interesting approach but very poor methodology","comment":"The authors propose to use a long-term memory network to solve combinatorial optimization problems, namely binary linear programs. \n\nUsing a long-term memory is definitely interesting, at it may capture a deeper understanding of the combinatorial problems at hand. This knowledge may in turn help improve the resolution process.\n\nThat being said, this works suffers from seemingly clear lack of knowledge of optimization-related literature and practices.\n\n* As was mentioned in a previous comment, there is no mention of existing literature on combinatorial optimization and integer linear optimization. The claim that \"the application of neural networks to combinatorial optimization has a long and distinguished history\" is only supported by references to works that are 30 years appart (mid 80s and mid 2010s).\n\n* The authors state that (sec. 3, 1st paragraph) \"a naive linear solver constructs the set of feasible solutions, [...] then iterates over [it] using the cost function to find the optimal solution\". This wrongly suggests that linear solvers go through explicit enumeration, which is not the case. Cutting planes and branch-and-bound techniques should be mentioned, or at least referred to (any textbook on linear programming would have a chapter on this).\n\n* The experimental procedure (section 5) goes against most good practices from the OR community:\n    - A set of randomly-generated instance is NOT representative of any real-life problem (see MIPlib for a well-known and broadly-used benchmark), and such instances are likely to be infeasible. This latter concern is not mentionned.\n    - In the generated dataset, \"at most 33% of coefficients are non-zeros\". Practical instances are much more sparse (see MIPlib instances)\n    - The baseline appears to be extremely poor, as was pointed out by a previous comment. 10 variables means at most 1024 solutions. Any solver that does not achieve optimality of such instances should not be considered as a baseline.\n    - The performance of the algorithms is evaluated using the average cost as a metric (with no mention of variance in the results). This is not a good metric for it is too sensitive to extreme values. Performance profiles (see Dolan and Moré 2002) are a more comprehensive tool for benchmarking optimization algorithms.\n\n\nAll in all, the lack of relevant litterature and poor methodology raise serious concerns over the contribution of the proposed approach."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]}},{"tddate":null,"ddate":null,"tmdate":1509118711654,"tcdate":1509118711654,"number":1,"cdate":1509118711654,"id":"Skx-mCeAZ","invitation":"ICLR.cc/2018/Conference/-/Paper426/Public_Comment","forum":"Bk_fs6gA-","replyto":"Bk_fs6gA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Missing related work","comment":"If you're going to compare against combinatorial optimization problems, you should cite work on combinatorial optimization - this is an active area of research and the google paper was rejected last year for ignoring previous work and overstating contributions. \n\nIs the COIN-OR package anywhere close to SOTA? I'd expect the industrial solvers like CPLEX and Gurobi to be far better, and they have free academic licenses available. You're showing an improvement in 60% of cases with only ten variables in Figure 2, but there's only 2^10 = 1024 possible variable values, meaning it's possible to brute force search through all the solutions and achieve the optimal value. The fact that your baseline solver doesn't do that suggests it's not a very strong baseline."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]}},{"tddate":null,"ddate":null,"tmdate":1509739309989,"tcdate":1509116688513,"number":426,"cdate":1509739307326,"id":"Bk_fs6gA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bk_fs6gA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Long Term Memory Network for Combinatorial Optimization Problems","abstract":"This paper introduces a framework for solving combinatorial optimization problems by learning from input-output examples of optimization problems. We introduce a new memory augmented neural model in which the memory is not resettable (i.e the information stored in the memory after processing an input example is kept for the next seen examples). We used deep reinforcement learning to train a memory controller agent to store useful memories. Our model was able to outperform hand-crafted solver on Binary Linear Programming (Binary LP). The proposed model is tested on different Binary LP instances with large number of variables (up to 1000 variables) and constrains (up to 700 constrains).","pdf":"/pdf/ac3226fc886cf21edcf75479646ba1278d5e37d2.pdf","TL;DR":"We propose a memory network model to solve Binary LP instances where the memory information is perseved for long-term use. ","paperhash":"anonymous|long_term_memory_network_for_combinatorial_optimization_problems","_bibtex":"@article{\n  anonymous2018long,\n  title={Long Term Memory Network for Combinatorial Optimization Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk_fs6gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper426/Authors"],"keywords":["Memory Networks","Combinatorial Optimization","Binary LP"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}