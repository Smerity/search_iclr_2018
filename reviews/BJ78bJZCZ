{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642453921,"tcdate":1511856547281,"number":3,"cdate":1511856547281,"id":"r1jjF59lM","invitation":"ICLR.cc/2018/Conference/-/Paper474/Official_Review","forum":"BJ78bJZCZ","replyto":"BJ78bJZCZ","signatures":["ICLR.cc/2018/Conference/Paper474/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An Extension to RWA","rating":"3: Clear rejection","review":"Summary:\nThis paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention. The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t.\n\nPros:\n- Addresses an issue of RWAs.\n\nCons:\n-The paper addresses a problem with an issue with RWAs. But it is not clear to me why would that be an important contribution.\n-The writing needs more work.\n-The experiments are lacking and the results are not good enough.\n\nGeneral Comments:\n\nThis paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address. I would still like to have a better justification on why should we care about RWA and fixing that model. \n\nThe writing of this paper seriously needs more work.  The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c.\n\nThe experiments are only on toyish and small scale tasks. According to the results the model doesn't really do better than a simple LSTM or GRU.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit","abstract":"Recurrent Neural Networks architectures excel at processing sequences by\nmodelling dependencies over different timescales. The recently introduced\nRecurrent Weighted Average (RWA) unit captures long term dependencies\nfar better than an LSTM on several challenging tasks. The RWA achieves\nthis by applying attention to each input and computing a weighted average\nover the full history of its computations. Unfortunately, the RWA cannot\nchange the attention it has assigned to previous timesteps, and so struggles\nwith carrying out consecutive tasks or tasks with changing requirements.\nWe present the Recurrent Discounted Attention (RDA) unit that builds on\nthe RWA by additionally allowing the discounting of the past.\nWe empirically compare our model to RWA, LSTM and GRU units on\nseveral challenging tasks. On tasks with a single output the RWA, RDA and\nGRU units learn much quicker than the LSTM and with better performance.\nOn the multiple sequence copy task our RDA unit learns the task three\ntimes as quickly as the LSTM or GRU units while the RWA fails to learn at\nall. On the Wikipedia character prediction task the LSTM performs best\nbut it followed closely by our RDA unit. Overall our RDA unit performs\nwell and is sample efficient on a large variety of sequence tasks.","pdf":"/pdf/eaa88eedd70fe9e6f654ed17af7007888ae02751.pdf","TL;DR":"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time","paperhash":"anonymous|efficiently_applying_attention_to_sequential_data_with_the_recurrent_discounted_attention_unit","_bibtex":"@article{\n  anonymous2018efficiently,\n  title={Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ78bJZCZ}\n}","keywords":["RNNs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper474/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642453956,"tcdate":1511817449608,"number":2,"cdate":1511817449608,"id":"rkzxWW5lf","invitation":"ICLR.cc/2018/Conference/-/Paper474/Official_Review","forum":"BJ78bJZCZ","replyto":"BJ78bJZCZ","signatures":["ICLR.cc/2018/Conference/Paper474/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage. The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions.\n\nThe proposed method is using Elman nets as the base RNN. I think the same method can be applied to GRUs or LSTMs. Some parameters might be redundant, however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination.\n\nIs there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA?\nOverall, the proposed method seems to be very useful for the RWA.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit","abstract":"Recurrent Neural Networks architectures excel at processing sequences by\nmodelling dependencies over different timescales. The recently introduced\nRecurrent Weighted Average (RWA) unit captures long term dependencies\nfar better than an LSTM on several challenging tasks. The RWA achieves\nthis by applying attention to each input and computing a weighted average\nover the full history of its computations. Unfortunately, the RWA cannot\nchange the attention it has assigned to previous timesteps, and so struggles\nwith carrying out consecutive tasks or tasks with changing requirements.\nWe present the Recurrent Discounted Attention (RDA) unit that builds on\nthe RWA by additionally allowing the discounting of the past.\nWe empirically compare our model to RWA, LSTM and GRU units on\nseveral challenging tasks. On tasks with a single output the RWA, RDA and\nGRU units learn much quicker than the LSTM and with better performance.\nOn the multiple sequence copy task our RDA unit learns the task three\ntimes as quickly as the LSTM or GRU units while the RWA fails to learn at\nall. On the Wikipedia character prediction task the LSTM performs best\nbut it followed closely by our RDA unit. Overall our RDA unit performs\nwell and is sample efficient on a large variety of sequence tasks.","pdf":"/pdf/eaa88eedd70fe9e6f654ed17af7007888ae02751.pdf","TL;DR":"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time","paperhash":"anonymous|efficiently_applying_attention_to_sequential_data_with_the_recurrent_discounted_attention_unit","_bibtex":"@article{\n  anonymous2018efficiently,\n  title={Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ78bJZCZ}\n}","keywords":["RNNs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper474/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642453991,"tcdate":1511674491632,"number":1,"cdate":1511674491632,"id":"BkEYMCPlG","invitation":"ICLR.cc/2018/Conference/-/Paper474/Official_Review","forum":"BJ78bJZCZ","replyto":"BJ78bJZCZ","signatures":["ICLR.cc/2018/Conference/Paper474/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Improvement upon a rarely used technique still not showing great results yet.","rating":"4: Ok but not good enough - rejection","review":"The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor. While the RWA was an interesting idea with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods.\n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original. On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU - except for MultiCopy where it trains faster, but not to better results and it looks like the difference is between few and very-few training steps anyway. The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM. So the results are not strongly convincing, and the paper lacks any mention of newer work on attention. This year strong improvements over state-of-the-art have been achieved using attention for translation (\"Attention is All You Need\") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition). To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit","abstract":"Recurrent Neural Networks architectures excel at processing sequences by\nmodelling dependencies over different timescales. The recently introduced\nRecurrent Weighted Average (RWA) unit captures long term dependencies\nfar better than an LSTM on several challenging tasks. The RWA achieves\nthis by applying attention to each input and computing a weighted average\nover the full history of its computations. Unfortunately, the RWA cannot\nchange the attention it has assigned to previous timesteps, and so struggles\nwith carrying out consecutive tasks or tasks with changing requirements.\nWe present the Recurrent Discounted Attention (RDA) unit that builds on\nthe RWA by additionally allowing the discounting of the past.\nWe empirically compare our model to RWA, LSTM and GRU units on\nseveral challenging tasks. On tasks with a single output the RWA, RDA and\nGRU units learn much quicker than the LSTM and with better performance.\nOn the multiple sequence copy task our RDA unit learns the task three\ntimes as quickly as the LSTM or GRU units while the RWA fails to learn at\nall. On the Wikipedia character prediction task the LSTM performs best\nbut it followed closely by our RDA unit. Overall our RDA unit performs\nwell and is sample efficient on a large variety of sequence tasks.","pdf":"/pdf/eaa88eedd70fe9e6f654ed17af7007888ae02751.pdf","TL;DR":"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time","paperhash":"anonymous|efficiently_applying_attention_to_sequential_data_with_the_recurrent_discounted_attention_unit","_bibtex":"@article{\n  anonymous2018efficiently,\n  title={Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ78bJZCZ}\n}","keywords":["RNNs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper474/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739282819,"tcdate":1509122378906,"number":474,"cdate":1509739280162,"id":"BJ78bJZCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ78bJZCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit","abstract":"Recurrent Neural Networks architectures excel at processing sequences by\nmodelling dependencies over different timescales. The recently introduced\nRecurrent Weighted Average (RWA) unit captures long term dependencies\nfar better than an LSTM on several challenging tasks. The RWA achieves\nthis by applying attention to each input and computing a weighted average\nover the full history of its computations. Unfortunately, the RWA cannot\nchange the attention it has assigned to previous timesteps, and so struggles\nwith carrying out consecutive tasks or tasks with changing requirements.\nWe present the Recurrent Discounted Attention (RDA) unit that builds on\nthe RWA by additionally allowing the discounting of the past.\nWe empirically compare our model to RWA, LSTM and GRU units on\nseveral challenging tasks. On tasks with a single output the RWA, RDA and\nGRU units learn much quicker than the LSTM and with better performance.\nOn the multiple sequence copy task our RDA unit learns the task three\ntimes as quickly as the LSTM or GRU units while the RWA fails to learn at\nall. On the Wikipedia character prediction task the LSTM performs best\nbut it followed closely by our RDA unit. Overall our RDA unit performs\nwell and is sample efficient on a large variety of sequence tasks.","pdf":"/pdf/eaa88eedd70fe9e6f654ed17af7007888ae02751.pdf","TL;DR":"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time","paperhash":"anonymous|efficiently_applying_attention_to_sequential_data_with_the_recurrent_discounted_attention_unit","_bibtex":"@article{\n  anonymous2018efficiently,\n  title={Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ78bJZCZ}\n}","keywords":["RNNs"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper474/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}