{"notes":[{"tddate":null,"ddate":null,"tmdate":1512290867926,"tcdate":1512290867926,"number":3,"cdate":1512290867926,"id":"By245E-Wf","invitation":"ICLR.cc/2018/Conference/-/Paper711/Public_Comment","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"I think there was previous work on certifiable regions around an input. ","comment":"I am trying to understand the relation of this paper and \n\nMeasuring neural net robustness with constraints\nby Bastani et al. NIPS 2016\n\nIn that paper, as far as I understand, the authors do the following: They are given some input image x1 and some neural network ( multiple layers, only ReLU activations) and lets say the output is also of the form sgn( w^T x ) so a linear function of the previous layer. \nThe paper obtains a polytope around x1 that provably will produce the same output as x1. Further they require that all the relus keep the same sign (which is a limitation but it critically allows them to get linear inequalities in x). \n\nSo they can now solve an LP to find the l_inf best possible epsilon that guarantees the same output label around x1. \nIn that sense this is a certifiable region around a given input, for multiple layer neural nets. \n\nHow does the authors' work compare ?\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1512222723690,"tcdate":1511908063209,"number":3,"cdate":1511908063209,"id":"SkwJQwogM","invitation":"ICLR.cc/2018/Conference/-/Paper711/Official_Review","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["ICLR.cc/2018/Conference/Paper711/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The main idea of  using upper bound (as opposed to lower bound) is reasonable. However, I find there are some limitations/weakness of the proposed method:","rating":"5: Marginally below acceptance threshold","review":"This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer. The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time.\n\nThe main idea of  using upper bound (as opposed to lower bound) is reasonable. However, I find there are some limitations/weakness of the proposed method:\n1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer)\n2. SDP while tractable, would still require very expensive computation to solve exactly.\n3. The relaxation seems a bit loose - in particular, in above step 2 and 3, the authors replace the gradient value by a global upper bound on that, which to me seems can be pretty loose.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1512222723735,"tcdate":1511814636411,"number":2,"cdate":1511814636411,"id":"BJVgLg9xf","invitation":"ICLR.cc/2018/Conference/-/Paper711/Official_Review","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["ICLR.cc/2018/Conference/Paper711/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Innovative rigorous defense against adversarial attacks on NNs","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors propose a new defense against security attacks on neural networks. The attack model involves a standard l_inf norm constraint. Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice. Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks. The approach is evaluated for several attacks on MNIST data.\n\nFirst of all, the paper is very well written and structured. As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic). The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. An innovative training criterion based on that certificate is proposed. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks.\n\nIn summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation. For me, it is a clear accept. The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1512222725539,"tcdate":1511604647926,"number":1,"cdate":1511604647926,"id":"SJlhZp8gf","invitation":"ICLR.cc/2018/Conference/-/Paper711/Official_Review","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["ICLR.cc/2018/Conference/Paper711/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A path towards a principled approach to train a robust classifier","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied.\nWhile the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive.\n\nHowever the new bound is an \"upper\" bound of the worst-case performance which is very different from the conventional sampling based \"lower\" bounds. Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier.\nThis paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound).\nIn conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier, and the paper is clearly written and easy to follow.\n \nThere are possible future directions to be developed.\n\n1. Apply the sum-of-squares (SOS) method.\nThe paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy. One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound.\nThe paper already mentions about this direction and it would be interesting to see the experimental results.\n\n2. Develop a similar relaxation for deep neural networks.\nThe author already mentioned that they are pursuing this direction. While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1510698801860,"tcdate":1510698801860,"number":2,"cdate":1510698801860,"id":"r15EJeKyG","invitation":"ICLR.cc/2018/Conference/-/Paper711/Official_Comment","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["ICLR.cc/2018/Conference/Paper711/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper711/Authors"],"content":{"title":"Addressing comments/questions","comment":"Thanks for the comments and questions. \n\n1. When run on the CPU, our training takes about 1.5 times the training time of Madry et al. \nScipy's max eigen vector computation runs on the CPU by default. We estimate that using a GPU implementation of Lancoz should also result in our training taking about 1.5 times the time taken by Madry et al. on the GPU. In general, our training is much slower than normal training. However, it's possible to speed up things using simple tricks. For example, using warm starts for the max eigen vector computations, by initializing with the solution of the previous iteration. \n\n2. For multi-layered networks, optimizing for the worst-case adversarial example subject to ReLU constraints can be written as a different Quadratic Program (where the ReLU constraints are encoded by quadratic constraints). This Quadratic Program can then be relaxed to a semidefinite program, like in our paper. We are currently exploring  this idea empirically. \n\n3. The Madry et al. paper considers experiments with 10^5 restarts to exhaustively understand the optimization landscape. The actual attacks use between 1 and 20 restarts. [Table 1 on page 12]. On the networks that we considered in our  paper, we didn't find any decrease in accuracy on increasing the number of random restarts beyond 5. \n\nA really small value of reg parameter results in a network that has high clean test accuracy but low robustness, and similarly a very large value led to a network that had really low clean accuracy. However, for intermediate values of regularization, we observed that the classification loss (multiclass hinge) and regularization loss balance each other such that the worst case adversarial accuracy @ \\eps = 0.1 remains nearly the same.  [the adversarial accuracy depends on the ratio of the multi class hinge loss to regularization loss which remains constant when the reg parameter is on the order of 0.05; we report results for this value in the paper]."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1510511988600,"tcdate":1510511988600,"number":1,"cdate":1510511988600,"id":"SJT_HG81G","invitation":"ICLR.cc/2018/Conference/-/Paper711/Official_Comment","forum":"Bys4ob-Rb","replyto":"Byvbr5WJM","signatures":["ICLR.cc/2018/Conference/Paper711/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper711/Authors"],"content":{"title":"Discussion of a NIPS'17 paper","comment":"Hi Seong Joon Oh, \n\nThanks for the pointer to the work by Matthias Hein, Maksym Andriushchenko. We'll cite this work in the next version of our paper. They propose a general bound on perturbations in the p-norm, that are necessary to cause misclassification, but only show how to compute this bound (for two layer neural networks) when p = 2.\nIn our work, we consider the attack model where p = \\infty. This makes a significant difference in the computations involved, where spectral-type bounds hold for p = 2; but optimizing over the L_\\infty ball is typically more complex; we show how to efficiently do this in our work. \n\nAnother key point of difference in that Hein and Andruishchenko use a \"proxy\" (Equation 6) for the actual lower bound in the proposed training algorithm.\nIn general, there is no discussion on how this proxy relates to the actual derived bound (Equation 5). In our work, we propose a training algorithm that efficiently maximizes the lower bound on perturbation proposed (or equivalently, in the language of our paper, minimizing an upper bound on the adversarial loss). "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1510535506029,"tcdate":1510510605064,"number":2,"cdate":1510510605064,"id":"SyBzxMU1z","invitation":"ICLR.cc/2018/Conference/-/Paper711/Public_Comment","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comments/Questions","comment":"Great work!\n\nSome questions:\n1. I'm curious -- how does the training time compare to that of Madry et. al.? Also, how much longer does this take than just normal training -- given that you have to compute the maximum eigen vector at each update? \n\n2. Also, thoughts on generalizing this to multi-layered networks?\n\n3. The Madry et. al. paper seems to consider 10^5 adv. examples or so, for training and attack. Are 5 random restarts sufficient to arrive at strong conclusions?\n \nGiven that the number of linear regions around a single point is fairly large, looks like 5 would be small. But, since the comparisons are fair (5 restarts for each defense), this seems very promising. \n\nAlso, how does regularization affect the accuracy?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1510216958918,"tcdate":1510216958918,"number":1,"cdate":1510216958918,"id":"Byvbr5WJM","invitation":"ICLR.cc/2018/Conference/-/Paper711/Public_Comment","forum":"Bys4ob-Rb","replyto":"Bys4ob-Rb","signatures":["~Seong_Joon_Oh1"],"readers":["everyone"],"writers":["~Seong_Joon_Oh1"],"content":{"title":"Discussion of a NIPS'17 paper","comment":"Thanks for the interesting paper! \nI wanted to leave a pointer to another NIPS'17 accepted paper:\n\nhttps://arxiv.org/abs/1705.08475\nFormal Guarantees on the Robustness of a Classifier against Adversarial Manipulation\nMatthias Hein, Maksym Andriushchenko\n\nLike the submission, Hein et al. also derive Lipschitz bound for neural networks with 1 hidden layer (see sec2.3) which looks similar to eq4&5 in the submission (modulo Lp norm used etc.). Indeed, this submission goes one step further to derive more bounds, but it would still be nice to discuss the difference.\n\nAlso small note: the main paper is 10 pages. In my view many intermediate inequalities (or proofs) can be deferred to appendix -- to highlight the main argument."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]}},{"tddate":null,"ddate":null,"tmdate":1509739147166,"tcdate":1509133106892,"number":711,"cdate":1509739144498,"id":"Bys4ob-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bys4ob-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Certified Defenses against Adversarial Examples ","abstract":"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no that perturbs each pixel by at most $\\epsilon = 0.1$ can cause more than $35\\%$ test error.\n","pdf":"/pdf/8a61a5836917253717ab05d490f9d3227af92f08.pdf","TL;DR":"We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.","paperhash":"anonymous|certified_defenses_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018certified,\n  title={Certified Defenses against Adversarial Examples },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys4ob-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper711/Authors"],"keywords":["adversarial examples","certificate of robustness","convex relaxations"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}