{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222568024,"tcdate":1511844633072,"number":3,"cdate":1511844633072,"id":"Hk-Qowclf","invitation":"ICLR.cc/2018/Conference/-/Paper135/Official_Review","forum":"r1tJKuyRZ","replyto":"r1tJKuyRZ","signatures":["ICLR.cc/2018/Conference/Paper135/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An interesting framework but more real-world experiments needed","rating":"5: Marginally below acceptance threshold","review":"Summary:\n\nThis paper proposes an encoder-decoder framework for learning latent representations of sets of elements. The model utilizes the neural attention mechanism for set inputs proposed in (Vinyals et al., ICLR 2016) to encode a set into a fixed-length latent representation, and then employs an LSTM decoder to reconstruct the original set of elements, in which a stable matching algorithm is used to match decoder outputs to input elements. Experimental results on synthetic datasets show that the model learns meaningful representations and effectively handles permutation invariance.\n\nMajor Concerns:\n\n1. Although the employed Gale-Shapely algorithm facilitates permutation-invariant set reconstruction, it has O(n^2) computational complexity during each back-propagation iteration, which might prevent it from scaling to sets of fairly big sizes. \n\n2. The experiments are only evaluated on synthetic datasets, and applications of the set autoencoder to real-world applications or scientific problems will make this work more interesting and significant.\n\n3. The main contribution of this work is the adoption of the stable matching algorithm in the decoder. A strong set autoencoder baseline will be, the encoder employs the neural attention mechanism proposed in (Vinyals et al., ICLR 2016), but the decoder just uses a standard LSTM as in a seq2seq framework. Comparisons to this baseline will reveal the contribution of the stable matching procedure in the whole  framework of  the set autoencoder for learning representations. \n\nMinor issues:\n\nOn page 5, above Section 4, d_j -> o_j ?\n\nthe footnote on page 5: we not consider -> we do not consider?\n\non page 6 and 7,   6.000, 1.000 and 10.000 training examples ->  6000, 1000 and 10,000 training examples","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Set Autoencoder: Unsupervised Representation Learning for Sets","abstract":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements. It is closely related to sequence-to-sequence models, which learn fixed-sized latent representations for sequences, and have been applied to a number of challenging supervised sequence tasks such as machine translation, as well as unsupervised representation learning for sequences.\nIn contrast to sequences, sets are permutation invariant. The proposed set autoencoder considers this fact, both with respect to the input as well as the output of the model. On the input side, we adapt a recently-introduced recurrent neural architecture using a content-based attention mechanism. On the output side, we use a stable marriage algorithm to align predictions to labels in the learning phase.\nWe train the model on synthetic data sets of point clouds and show that the learned representations change smoothly with translations in the inputs, preserve distances in the inputs, and that the set size is represented directly. We apply the model to supervised tasks on the point clouds using the fixed-size latent representation. For a number of difficult classification problems, the results are better than those of a model that does not consider the permutation invariance. Especially for small training sets, the set-aware model benefits from unsupervised pretraining.","pdf":"/pdf/88d92cecf5edf21d619debc1f58a660c3b597987.pdf","TL;DR":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements.","paperhash":"anonymous|the_set_autoencoder_unsupervised_representation_learning_for_sets","_bibtex":"@article{\n  anonymous2018the,\n  title={The Set Autoencoder: Unsupervised Representation Learning for Sets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1tJKuyRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper135/Authors"],"keywords":["set","unsupervised learning","representation learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222568063,"tcdate":1511793580536,"number":2,"cdate":1511793580536,"id":"B1EnXjFxG","invitation":"ICLR.cc/2018/Conference/-/Paper135/Official_Review","forum":"r1tJKuyRZ","replyto":"r1tJKuyRZ","signatures":["ICLR.cc/2018/Conference/Paper135/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good preliminary results","rating":"5: Marginally below acceptance threshold","review":"Summary\nThis paper proposes an autoencoder for sets. An input set is encoded into a\nfixed-length representation using an attention mechanism (previously proposed by\n[1]). The decoder generates the output sequentially and the generated sequence\nis matched to the best-matching ordering of the target output set.\nExperiments are done on synthetic datasets to demonstrate properties of the\nlearned representation.\n\nPros\n- Experiments show that the autoencoder helps improve classification accuracy\n  for small training set sizes on the shape classification task.\n- The analysis of how the decoder generates data is insightful.\n\nCons\n- The experiments are on toy datasets only. Given the availability of point\n  cloud data sets, for example, KITTI which has a widely used benchmark for\npoint cloud based object detection, it would make the paper stronger if this\nmodel was benchmarked against published baselines.\n\n- The autoencoder does not seem to help much on the regression tasks where even\n  for the smaller training set size setting, directly using the encoder to solve\nthe task often works best. Even finetuning is unable to recover from the\npretrained weights. Therefore, it seems that the decoder (which is the novel\naspect of this work) is perhaps not working well, or is not well suited to the\nregression tasks being considered.\n\n- The classification task, for which the learned representations work well\n  empirically, seems to be geared towards representing object shape. It doesn't\nreally require remembering each point. On the other hand, the regression tasks\nthat could require remembering the points don't seem to be benefit much from the\nautoencoder pretraining. This suggests that while the model is able to represent\noverall shape, it has a hard time remembering individual elements of the set.\nThis seems like a drawback, since a general \"set auto-encoder\" should be able\nto perform a wide variety of tasks on the input set which could require remembering\nthe set's elements.\n\nQuality\nThis paper describes the proposed model quite well and provides encouraging\npreliminary results.\n\nClarity\nThe paper is easy to understand.\n\nOriginality\nThe novelty in the model is using a matching algorithm to find the best ordering\nof the target output set to match with the sequentially generated decoder\noutput. However, the paper makes a choice of one ranking based matching scheme\nand does not compare to other alternatives.\n\nSignificance\nThis paper proposes a way of learning representations of sets which will be of\nbroad interest across the machine learning community. These models are likely to\nbecome more relevant with increasing prevelance of point cloud data.\n\nReferences\n[1] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to\nsequence for sets. arXiv preprint arXiv:1511.06391.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Set Autoencoder: Unsupervised Representation Learning for Sets","abstract":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements. It is closely related to sequence-to-sequence models, which learn fixed-sized latent representations for sequences, and have been applied to a number of challenging supervised sequence tasks such as machine translation, as well as unsupervised representation learning for sequences.\nIn contrast to sequences, sets are permutation invariant. The proposed set autoencoder considers this fact, both with respect to the input as well as the output of the model. On the input side, we adapt a recently-introduced recurrent neural architecture using a content-based attention mechanism. On the output side, we use a stable marriage algorithm to align predictions to labels in the learning phase.\nWe train the model on synthetic data sets of point clouds and show that the learned representations change smoothly with translations in the inputs, preserve distances in the inputs, and that the set size is represented directly. We apply the model to supervised tasks on the point clouds using the fixed-size latent representation. For a number of difficult classification problems, the results are better than those of a model that does not consider the permutation invariance. Especially for small training sets, the set-aware model benefits from unsupervised pretraining.","pdf":"/pdf/88d92cecf5edf21d619debc1f58a660c3b597987.pdf","TL;DR":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements.","paperhash":"anonymous|the_set_autoencoder_unsupervised_representation_learning_for_sets","_bibtex":"@article{\n  anonymous2018the,\n  title={The Set Autoencoder: Unsupervised Representation Learning for Sets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1tJKuyRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper135/Authors"],"keywords":["set","unsupervised learning","representation learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222568104,"tcdate":1511539386670,"number":1,"cdate":1511539386670,"id":"rk7TfpBlG","invitation":"ICLR.cc/2018/Conference/-/Paper135/Official_Review","forum":"r1tJKuyRZ","replyto":"r1tJKuyRZ","signatures":["ICLR.cc/2018/Conference/Paper135/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"This paper mostly extends Vinyals et al, 2015 paper (\"Order Matters\") on how to represent sets as input and/or output of a deep architecture.\n\nAs far as I understood, the set encoder is the same as the one in \"Order Matters\". If not, it would be useful to underline the differences.\n\nThe decoder, on the other hand, is different and relies on a loss that is based on an heuristic to find the current best order (based on an ordering, or mapping W, found using the Gale-Shapely algorithm). Does this mean that Algorithm 1 needs to be run for every training (and test) example? if so, it is important to note what is the effective complexity of running it?\n\nThe experimental section is interesting, but in the end a bit disappointing: although a new artificial dataset is proposed to evaluate sets, it is unclear how different are the findings from those in the \"Order Matters\" paper:\n- the first set of results (in Section 4.1) confirms that the set encoder is important (which was also in the other paper I believe)\n- the second set of results (Section 4.2) shows that in some cases, an auto-encoder is also useful: this is mostly the case when the supervised data is small compared to the availability of a much larger unsupervised data (of sets). This is interesting (and novel compared to the \"Order Matters\" paper) but corresponds to known findings from most previous work on semi-supervised learning: pre-training is only useful when only a very small supervised data exists, and quickly becomes irrelevant. This is not specific to sets.\n\nFinally, It would have been very interesting to see experiments on real data concerned with sets.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Set Autoencoder: Unsupervised Representation Learning for Sets","abstract":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements. It is closely related to sequence-to-sequence models, which learn fixed-sized latent representations for sequences, and have been applied to a number of challenging supervised sequence tasks such as machine translation, as well as unsupervised representation learning for sequences.\nIn contrast to sequences, sets are permutation invariant. The proposed set autoencoder considers this fact, both with respect to the input as well as the output of the model. On the input side, we adapt a recently-introduced recurrent neural architecture using a content-based attention mechanism. On the output side, we use a stable marriage algorithm to align predictions to labels in the learning phase.\nWe train the model on synthetic data sets of point clouds and show that the learned representations change smoothly with translations in the inputs, preserve distances in the inputs, and that the set size is represented directly. We apply the model to supervised tasks on the point clouds using the fixed-size latent representation. For a number of difficult classification problems, the results are better than those of a model that does not consider the permutation invariance. Especially for small training sets, the set-aware model benefits from unsupervised pretraining.","pdf":"/pdf/88d92cecf5edf21d619debc1f58a660c3b597987.pdf","TL;DR":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements.","paperhash":"anonymous|the_set_autoencoder_unsupervised_representation_learning_for_sets","_bibtex":"@article{\n  anonymous2018the,\n  title={The Set Autoencoder: Unsupervised Representation Learning for Sets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1tJKuyRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper135/Authors"],"keywords":["set","unsupervised learning","representation learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739466274,"tcdate":1509030112670,"number":135,"cdate":1509739463629,"id":"r1tJKuyRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1tJKuyRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Set Autoencoder: Unsupervised Representation Learning for Sets","abstract":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements. It is closely related to sequence-to-sequence models, which learn fixed-sized latent representations for sequences, and have been applied to a number of challenging supervised sequence tasks such as machine translation, as well as unsupervised representation learning for sequences.\nIn contrast to sequences, sets are permutation invariant. The proposed set autoencoder considers this fact, both with respect to the input as well as the output of the model. On the input side, we adapt a recently-introduced recurrent neural architecture using a content-based attention mechanism. On the output side, we use a stable marriage algorithm to align predictions to labels in the learning phase.\nWe train the model on synthetic data sets of point clouds and show that the learned representations change smoothly with translations in the inputs, preserve distances in the inputs, and that the set size is represented directly. We apply the model to supervised tasks on the point clouds using the fixed-size latent representation. For a number of difficult classification problems, the results are better than those of a model that does not consider the permutation invariance. Especially for small training sets, the set-aware model benefits from unsupervised pretraining.","pdf":"/pdf/88d92cecf5edf21d619debc1f58a660c3b597987.pdf","TL;DR":"We propose the set autoencoder, a model for unsupervised representation learning for sets of elements.","paperhash":"anonymous|the_set_autoencoder_unsupervised_representation_learning_for_sets","_bibtex":"@article{\n  anonymous2018the,\n  title={The Set Autoencoder: Unsupervised Representation Learning for Sets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1tJKuyRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper135/Authors"],"keywords":["set","unsupervised learning","representation learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}