{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642454853,"tcdate":1511811077432,"number":3,"cdate":1511811077432,"id":"Byp-dy9gG","invitation":"ICLR.cc/2018/Conference/-/Paper481/Official_Review","forum":"rJ8rHkWRb","replyto":"rJ8rHkWRb","signatures":["ICLR.cc/2018/Conference/Paper481/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Another model for learning to embed words as a function of their characters","rating":"4: Ok but not good enough - rejection","review":"This paper presents a new model for composing representations of characters into word embeddings. The starting point of their argument is to include position-specific embeddings of characters rather than just position-independent characters. By adding together position-specific vectors, reasonable results are obtained.\n\nThis is an interesting result, but I have a few recommendations to improve the paper.\n1) It is a bit hard to assess since it is not evaluated on a standard datasets. There are a number standard datasets for open vocabulary language modeling. E.g., the MWC corpus (http://k-kawakami.com/research/mwc), or even the Penn Treebank (although it is conventionally modeled in closed vocabulary form).\n2) There are many existing models for composing characters into words. In addition to those cited in the paper, see the citations listed below. Comparison with those is crucial in a paper like this.\n3) Since the predictions are done at the word type level, it is unclear how vocabulary set of the corpus is determined, and what is done with OOV word types at test time (while it is possible to condition on them using the technique in the paper, it is not possible to use this technique for generation).\n4) The analysis is interesting, but a more intuitive explanation would be to show nearest neighbor plots.\n\nSome missing citations:\n\nComposing characters into words:\n\ndos Santos and Zadrozny. (2014 ICML) http://proceedings.mlr.press/v32/santos14.pdf\nLing et al. (2015 EMNLP) Finding Function in Form. https://arxiv.org/abs/1508.02096\n\nAdditionally, using explicit positional features in modeling language has been used:\nVaswani et al. (2017) Attention is all you need https://arxiv.org/abs/1706.03762\nand a variety of other sources.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Fully Connected Network for Composing Word Embeddings from Characters","abstract":"This work introduces a simple network for producing character aware word embeddings. Position agnostic and position aware character embeddings are combined to produce an embedding vector for each word. The learned word representations are shown to be very sparse and facilitate improved results on language modeling tasks, despite using markedly fewer parameters, and without the need to apply dropout. A final experiment suggests that weight sharing contributes to sparsity, increases performance, and prevents overfitting.","pdf":"/pdf/bad9d4f9f13e8d0e641a83b65c7504461ffbd5a2.pdf","TL;DR":"A fully connected architecture is used to produce word embeddings from character representations, outperforms traditional embeddings and provides insight into sparsity and dropout.","paperhash":"anonymous|a_simple_fully_connected_network_for_composing_word_embeddings_from_characters","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Fully Connected Network for Composing Word Embeddings from Characters},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ8rHkWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper481/Authors"],"keywords":["natural language processing","word embeddings","language models","neural network","deep learning","sparsity","dropout"]}},{"tddate":null,"ddate":null,"tmdate":1515642454891,"tcdate":1511807647628,"number":2,"cdate":1511807647628,"id":"HkDiq0Flf","invitation":"ICLR.cc/2018/Conference/-/Paper481/Official_Review","forum":"rJ8rHkWRb","replyto":"rJ8rHkWRb","signatures":["ICLR.cc/2018/Conference/Paper481/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Position aware character embeddings can be sparse and are less prone to overfitting on language modeling.","rating":"5: Marginally below acceptance threshold","review":"The paper uses both position agnostic and position aware embeddings for tokens in a language modeling task. To obtain token embeddings, they concatenate two embeddings: the sum of character embeddings and the sum of (character, position) embeddings, the former being position agnostic and the latter being position aware. In a language modeling task, they find that using a combination of both improves perplexity over the standard token embedding baseline with fewer parameters. \n\nThe paper shows that the character embeddings are more sparse, measured with the Gini coefficient, than token embeddings and are more robust to overfitting. They also find that while dropout increases overall sparsity, it makes a few tokens homogenous. The paper does not give a crisp answer to why such sparsity patterns are observed. \n\nThe paper falls a bit  short both empirically and technically. While their technique is interesting, they do not compare it to the baseline of using convolutions over characters. More empirical evidence is needed for the technique to be adopted by the community.  On the theory side, they should dig deeper into the reasons for sparsity and how it might help to train better models. \n\nIf the papers shows that the approach can work well in machine translation or language modeling of morphologically rich languages, it might encourage practitioners to use the technique. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Fully Connected Network for Composing Word Embeddings from Characters","abstract":"This work introduces a simple network for producing character aware word embeddings. Position agnostic and position aware character embeddings are combined to produce an embedding vector for each word. The learned word representations are shown to be very sparse and facilitate improved results on language modeling tasks, despite using markedly fewer parameters, and without the need to apply dropout. A final experiment suggests that weight sharing contributes to sparsity, increases performance, and prevents overfitting.","pdf":"/pdf/bad9d4f9f13e8d0e641a83b65c7504461ffbd5a2.pdf","TL;DR":"A fully connected architecture is used to produce word embeddings from character representations, outperforms traditional embeddings and provides insight into sparsity and dropout.","paperhash":"anonymous|a_simple_fully_connected_network_for_composing_word_embeddings_from_characters","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Fully Connected Network for Composing Word Embeddings from Characters},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ8rHkWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper481/Authors"],"keywords":["natural language processing","word embeddings","language models","neural network","deep learning","sparsity","dropout"]}},{"tddate":null,"ddate":null,"tmdate":1515642454960,"tcdate":1511629162992,"number":1,"cdate":1511629162992,"id":"By7uW7Pef","invitation":"ICLR.cc/2018/Conference/-/Paper481/Official_Review","forum":"rJ8rHkWRb","replyto":"rJ8rHkWRb","signatures":["ICLR.cc/2018/Conference/Paper481/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors implement a model to obtain an embedding for a word given its characters, but fail to compare to relevant previous work or even any published results, making the validity of their claims difficult to asses.","rating":"3: Clear rejection","review":"The authors propose a neural network architecture which takes the characters of a word as input along with their positions, and output a word embedding. They then use these as inputs to a GRU language model, which is evaluated on two medium size data sets made from a series of novels and the Project Gutenberg Canada books respectively.\n\nWhile the idea has merit, the experimental protocol is too flawed to draw any reliable conclusions. Why use Wheel of Time, which is not in the public domain, rather than e.g. text8? Why not train the model to convergence (Figure 3)? Do the learned embeddings exhibit any morphological significance, or does the model only serve a regularization purpose?\n\nAs for the model itself: are the position agnostic character embeddings actually helpful in the spelling model? Does the model have the expressivity to learn the same embeddings as a look-up table?\n\nThe authors are also missing a significant amount of relevant literature on the topic of building word embeddings from characters, for example:\nFinding Function in Form: Compositional Character Models for Open Vocabulary Word Representation, Ling et al., 2015\nEnriching Word Vectors with Subword Information, Bojanowski et al. 2017\nCompositional Morphology for Word Representations and Language Modelling, Botha and Blunsom 2014\n\nPros:\n- Valid idea\n\nCons:\n- Too many missing references\n- Some modeling choices lack justification\n- Experiments do not provide meaningful comparisons and are not reproducible\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Simple Fully Connected Network for Composing Word Embeddings from Characters","abstract":"This work introduces a simple network for producing character aware word embeddings. Position agnostic and position aware character embeddings are combined to produce an embedding vector for each word. The learned word representations are shown to be very sparse and facilitate improved results on language modeling tasks, despite using markedly fewer parameters, and without the need to apply dropout. A final experiment suggests that weight sharing contributes to sparsity, increases performance, and prevents overfitting.","pdf":"/pdf/bad9d4f9f13e8d0e641a83b65c7504461ffbd5a2.pdf","TL;DR":"A fully connected architecture is used to produce word embeddings from character representations, outperforms traditional embeddings and provides insight into sparsity and dropout.","paperhash":"anonymous|a_simple_fully_connected_network_for_composing_word_embeddings_from_characters","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Fully Connected Network for Composing Word Embeddings from Characters},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ8rHkWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper481/Authors"],"keywords":["natural language processing","word embeddings","language models","neural network","deep learning","sparsity","dropout"]}},{"tddate":null,"ddate":null,"tmdate":1514136053367,"tcdate":1509123390122,"number":481,"cdate":1509739276199,"id":"rJ8rHkWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJ8rHkWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Simple Fully Connected Network for Composing Word Embeddings from Characters","abstract":"This work introduces a simple network for producing character aware word embeddings. Position agnostic and position aware character embeddings are combined to produce an embedding vector for each word. The learned word representations are shown to be very sparse and facilitate improved results on language modeling tasks, despite using markedly fewer parameters, and without the need to apply dropout. A final experiment suggests that weight sharing contributes to sparsity, increases performance, and prevents overfitting.","pdf":"/pdf/bad9d4f9f13e8d0e641a83b65c7504461ffbd5a2.pdf","TL;DR":"A fully connected architecture is used to produce word embeddings from character representations, outperforms traditional embeddings and provides insight into sparsity and dropout.","paperhash":"anonymous|a_simple_fully_connected_network_for_composing_word_embeddings_from_characters","_bibtex":"@article{\n  anonymous2018a,\n  title={A Simple Fully Connected Network for Composing Word Embeddings from Characters},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ8rHkWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper481/Authors"],"keywords":["natural language processing","word embeddings","language models","neural network","deep learning","sparsity","dropout"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}