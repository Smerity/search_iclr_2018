{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222632845,"tcdate":1512220198895,"number":3,"cdate":1512220198895,"id":"Hkk48Xg-f","invitation":"ICLR.cc/2018/Conference/-/Paper383/Official_Review","forum":"rk4Fz2e0b","replyto":"rk4Fz2e0b","signatures":["ICLR.cc/2018/Conference/Paper383/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Incremental improvement on graph neural networks with heuristic graph partitioning","rating":"5: Marginally below acceptance threshold","review":"Since existing GNNs are not computational efficient when dealing with large graphs, the key engineering contributions of the proposed method, GPNN, are a partitioning and the associated scheduling components. \n\nThe paper is well written and easy to follow. However, related literature for message passing part is inadequate. \n\nI have two concerns. The primary one is that the method is incremental and rather heuristic. For example, in Section 2.2, Graph Partition part, the authors propose to \"first randomly sample the initial seed nodes biased towards nodes which are labeled and have a large out-degree\", they do not give any reasons for the preference of that kind of nodes. \n\nThe second one is that of the experimental evaluation. GPNN is on par with other methods on small graphs such as citation networks, performs comparably to other methods, and only clearly outperforms on distantly-supervised entity extraction dataset. Thus, it is  not clear if GPNN is more effective than others in general. As for experiments on DIEL dataset, the authors didn't compare to GCN due to the simple reason that GCN ran out of memory. However, vanilla GCN could be trivially partitioned and propagating just as shown in this paper. I think such experiment is crucial, without which I cannot assess this method properly.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Partition Neural Networks for Semi-Supervised Classification","abstract":"We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with spectral partitioning and also propose a modified multi-seed flood fill for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.","pdf":"/pdf/a5312ae7aa7d1856eaac948fd0523467f5044851.pdf","paperhash":"anonymous|graph_partition_neural_networks_for_semisupervised_classification","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Partition Neural Networks for Semi-Supervised Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4Fz2e0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper383/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222632892,"tcdate":1511742027411,"number":2,"cdate":1511742027411,"id":"r1Q8qCdgf","invitation":"ICLR.cc/2018/Conference/-/Paper383/Official_Review","forum":"rk4Fz2e0b","replyto":"rk4Fz2e0b","signatures":["ICLR.cc/2018/Conference/Paper383/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Partitioning for better message passing - maybe?","rating":"5: Marginally below acceptance threshold","review":"The authors investigate different message passing schedules for GNN learning.  Their proposed approach is to partition the graph into disjoint subregions, pass many messages on the sub regions and pass fewer messages between regions (an approach that is already considered in related literature, e.g., the BP literature), with the goal of minimizing the number of messages that need to be passed to convey information between all pairs of nodes in the network.  Experimentally, the proposed approach seems to perform comparably to existing methods (or slightly worse on average in some settings).  The paper is well-written and easy to read.  My primary concern is with novelty.  Many similar ideas have been floating around in a variety of different message-passing communities.  With no theoretical reason to prefer the proposed approach, it seems like it may be of limited interest to the community if speed is its only benefit (see detailed comments below).\n\nSpecific comments:\n\n1)  \"When information from any one node has reached all other nodes in the graph for the first time, this problem is considered as solved.\"\n\nPerhaps it is my misunderstanding of the way in which GNNs work, but isn't the objective actually to reach a set of fixed point equations.  If so, then simply propagating information from one side of the graph may not be sufficient.\n\n2)  The experimental results in Section 4.4 are almost impossible to interpret.  Perhaps it is better to plot number of edges updated versus accuracy?  This at least would put them on equal footing.   In addition, the experiments that use randomness should be repeated and plotted on average (just in case you happened to pick a bad schedule).\n\n3)  More generally, why not consider random schedules (i.e., just pick a random edge, update, repeat) or random partitions?  I'm not certain that a fixed set will perform best independent of the types of updates being considered, and random schedules, like the fully synchronous case for an important baseline (especially if update speed is all you care about).\n\nTypos:\n\n-pg. 6, \"Thm. 2\" -> \"Table 2\"","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Partition Neural Networks for Semi-Supervised Classification","abstract":"We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with spectral partitioning and also propose a modified multi-seed flood fill for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.","pdf":"/pdf/a5312ae7aa7d1856eaac948fd0523467f5044851.pdf","paperhash":"anonymous|graph_partition_neural_networks_for_semisupervised_classification","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Partition Neural Networks for Semi-Supervised Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4Fz2e0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper383/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222632944,"tcdate":1511732487502,"number":1,"cdate":1511732487502,"id":"HkaZrhuez","invitation":"ICLR.cc/2018/Conference/-/Paper383/Official_Review","forum":"rk4Fz2e0b","replyto":"rk4Fz2e0b","signatures":["ICLR.cc/2018/Conference/Paper383/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Extends the GNN framework to handle large graphs by running async updates on subgraphs derived by using graph partitioning algorithms. Results demonstrated on semi-supervised task. ","rating":"6: Marginally above acceptance threshold","review":"Graph Neural Networks are methods using NNs to deal with graph data (each data point has some features, and there is some known connectivity structure among nodes) for problems such as semi-supervised classification. They can also be viewed as an abstraction and generalizations of RNNs to arbitrary graphs. As such they assume each unit has inputs from other nodes, as well as from some stored representation of a state and upon receiving all its information and executing a computation on the values of these inputs and its internal state, it can update the state as well as propagate information to neighbouring nodes. \n\nThis paper deals with the question of computing over very large input graphs where learning becomes computationally problematic (eg hard to use GPUs, optimization gets difficult due to gradient issues, etc). The proposed solution is to partition the graph into sub graphs, and use a schedule alternating between performing intra and inter graph partitions operations. To achieve that two things need to be determined - how to partition the graph, and which schedules to choose. The authors experiment with existing and somewhat modified solutions for each of these problems and present results that show that for large graphs, these methods are indeed effective and achieve state-of-the-art/improved results over existing methos. \n\nThe main critique is that this feels more of an engineering solution to running such GNNs on large graphs than a research innovations. The proposed algorithms are straight forward and/or utilize existing algorithms, and introduce many hyper parameters and ad-hoc decisions (the scheduling to choose for instance). In addition, they do not satisfy any theoretical framework, or proposed in the context of a theoretical framework than has guarantees of mathematical properties that are desirable. As such it is likely of use for practitioners but not a major research contribution. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph Partition Neural Networks for Semi-Supervised Classification","abstract":"We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with spectral partitioning and also propose a modified multi-seed flood fill for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.","pdf":"/pdf/a5312ae7aa7d1856eaac948fd0523467f5044851.pdf","paperhash":"anonymous|graph_partition_neural_networks_for_semisupervised_classification","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Partition Neural Networks for Semi-Supervised Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4Fz2e0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper383/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739332997,"tcdate":1509110395845,"number":383,"cdate":1509739330335,"id":"rk4Fz2e0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk4Fz2e0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Graph Partition Neural Networks for Semi-Supervised Classification","abstract":"We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with spectral partitioning and also propose a modified multi-seed flood fill for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.","pdf":"/pdf/a5312ae7aa7d1856eaac948fd0523467f5044851.pdf","paperhash":"anonymous|graph_partition_neural_networks_for_semisupervised_classification","_bibtex":"@article{\n  anonymous2018graph,\n  title={Graph Partition Neural Networks for Semi-Supervised Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk4Fz2e0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper383/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}