{"notes":[{"tddate":null,"ddate":null,"tmdate":1516197332030,"tcdate":1516197332030,"number":6,"cdate":1516197332030,"id":"ByhCSC3EM","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Comment","forum":"B1KFAGWAZ","replyto":"S141qTNVG","signatures":["ICLR.cc/2018/Conference/Paper1051/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1051/Authors"],"content":{"title":"Response","comment":"Thanks for the updates. \n\nWe have updated the argument about differences from CommNet as follows, emphasizing the independently and recurrently reasoning property of the master agent. \n\n\"This design represents an initial version of the proposed master-slave framework, however it does not facilitate an independently reasoning master agent which takes in messages from all agents step by step and processes such information in an recurrent manner.\"\n\n\nTaking the advice regarding the master's global information, we add another baseline \"CommNet + OccupancyMap\" which takes its original state as well as the occupancy map explicitly. The results show that CommNet performs better with this additional global information. However, with an explicit global planner (the \"master\" agent) designed in our model, such information seems better utilized to facilitate learning of more powerful collaborative polices. More details please refer to Table 1 and the comments in the updated paper. Regardless, note that, all information revealed from the extra occupancy map is by definition already included in all agents' state. To be specific, the occupancy map only includes the positions and IDs of our units without any of the enemies. And it is mentioned in the original CommNet paper* section 4.3.1 and 4.3.3 that the absolute location of each agent is already included in its input state features. Thus the occupancy map adds no extra information to the total information of all agents, but organizes them in an explicit global manner.\n\nSoftmax/Gaussian action layers are added to Figure 4.\n\nWe sincerely appreciate the constructive suggestions. \n\n\n*Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pp. 2244–2252, 2016."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515669980297,"tcdate":1515669980297,"number":5,"cdate":1515669980297,"id":"S141qTNVG","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Comment","forum":"B1KFAGWAZ","replyto":"H1aoqUTmz","signatures":["ICLR.cc/2018/Conference/Paper1051/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1051/AnonReviewer2"],"content":{"title":"...","comment":"I am not sure if this really clarifies much.\n\n\"CommNet pass back the sum of all agents’ hidden state directly to each agent at the next time step (this direct pass is what we meant by handcrafted information); whereas our applied an LSTM to process such information from time to time and therefore formulates an independently thinking master agent\"\n\n-I am not sure if I understand correctly, but I think this says \"we use LSTMs rather than multilayer feedforward networks for the components f_i\", is that correct? (Fig. 1 clearly shows that you do communicate at every timestep, I am not sure why this is less handcrafted)\n\n\"we applied a GCM\"\n-Yes, I did recognize the GCM as a novel feature in my review. Perhaps its merit could be investigated by comparing \"with CommNet with 1 extra agent that takes in the same information as your 'master'.\" ?\n\nI don't understand why the Gaussian/softmax layers are not clarified in the updated paper. \"on the top layer\" is just not so clear.\n\nThe standard errors look good, as such the evaluation does imply that there are merits to the overall approach. However, I still think that in the current form, it is not quite clear exactly what explains this difference in performance.\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1516197629896,"tcdate":1515183096547,"number":4,"cdate":1515183096547,"id":"BkbZ3Ip7f","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Comment","forum":"B1KFAGWAZ","replyto":"B1KFAGWAZ","signatures":["ICLR.cc/2018/Conference/Paper1051/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1051/Authors"],"content":{"title":"Updates to the paper","comment":"We summarize the updates made to the paper as follow:\n\n1. Add one more StarCraft micromanagement task (both task description and new evaluation results etc.) \"Dragoons vs. Zealots\" where heterogeneous agents are involved.\n\n2. A new experiment is included in section 4.4 to compare models of different centralization level (both the curves in Figure 6 (c) and the related analysis).\n\n3. Add a visualization of the master agent's hidden state to demonstrate the effectiveness of occupancy map and GCM module in our proposal (and the related analysis).\n\n4. Update table 1 and table 2 with standard errors, follow the results of GMEZO reported in the original paper.\n\n5. Fix some typos and details according to reviewers' suggestions.\n\n6. Statement about CommNet has been revised in Section 2.\n\n7.  Results of a new baseline \"CommNet + Occupancy Map\" are provided in Table 1.\n\n8. Softmax/Gaussian action layers have been added to Figure 4."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515182757045,"tcdate":1515182757045,"number":3,"cdate":1515182757045,"id":"H1aoqUTmz","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Comment","forum":"B1KFAGWAZ","replyto":"HyuecsNxf","signatures":["ICLR.cc/2018/Conference/Paper1051/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1051/Authors"],"content":{"title":"Response to review","comment":"There are two key differences from CommNet: 1) CommNet pass back the sum of all agents’ hidden state directly to each agent at the next time step (this direct pass is what we meant by handcrafted information); whereas our applied an LSTM to process such information from time to time and therefore formulates an independently thinking master agent; 2) when passing back the processed information, we applied a GCM unit to effectively merge information/“thoughts” from both the master agent and the slave agents. Both independent thinking LSTM and the GCM merging module are learnable and their effectiveness has been shown when compared to the CommNet model. We highly recognize CommNet and our proposal was directly motivated from CommNet. We have refined some of the statement to avoid misleading.\n\n* This seems to suggest that the only thing that the master can communicate is action information? It this the case?\n\nThis is not the case. The master will pass along its hidden states to the GCM module which will process such information and that from the slave agents to finally output actions.\n\n* What are the standard errors?\n\nStandard errors are provided in the updated tables.\n\n* The section 3.2 explains standard things (policy gradient), but the details are a bit unclear. In particular, I do not see how the Gaussian/softmax layers are integrated; they do not seem to appear in figure 4?\n\nThe Gaussian/softmax layers are actually after the hidden state output of the GCM module (to generate master's action) and each slave agent's RNN module (to generate slave's action).\n\n* I cannot understand figure 7 without more explanation. (The background is all black - did something go wrong with the pdf?)\n\nThanks for your reminding. The black background is due to the command line output of the combat task environment. As a simple game environment based on MazeBase (Sukhbaatar et al. 2015), the visualization of this task is printed to command line as it was originally implemented. Thus the two examples are obtained by directly cropping command line output. The yellow and green dashes/arrows are added by us to illusrate the behaviours of agents. We will try to replace this demonstration to a more reader-friendly version in the camera-ready version if accepted.\n\nAnd the details suggestions are very helpful,  we fixed them in the updated version. Thanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515182142704,"tcdate":1515182142704,"number":2,"cdate":1515182142704,"id":"S1vBOLaQG","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Comment","forum":"B1KFAGWAZ","replyto":"HynlSGDxM","signatures":["ICLR.cc/2018/Conference/Paper1051/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1051/Authors"],"content":{"title":"Response to review","comment":"The overall problem statement of the MARL problem we consider here should be formulated as an MDP (supposed we do not take “fog of war” etc. into account). \n\nAlthough each agent can only observe the environment partially, when considering the global optimal planning, we should view these problems from a centralized perspective and formulate it as one global MDP where the action of such MDP is defined as the concatenation of all agents’ actions. However since such an action space can be very large when the number of agents increases, therefore people tend to apply decentralized perspective, where each agent can be viewed as having its own MDP and than try to implement certain communication protocols between agents to facilitate collaborations in-between. In this regard, our work proposed to apply a master-slave communication architecture and to realize such architecture with learnable neural networks so that the final communication protocols are learned from many times of RL try and errors in the same way as the parameters of any RL models.\n\nTechnically speaking, we have no assumption of synchronous and noise-free communication. Regarding communication, as mentioned above, since the final protocols will be driven by training data, we almost do not have any assumptions except for pre-defined the certain hyper-parameters such as the dimensionality etc. As for synchronousness, our model does not make such an assumption, e.g. the master agent can take empty or zero values if any of the slave agents have no output at certain time steps. \n\nRegarding advances to RL research, we provide a novel practical MARL solution which encodes the idea  of master-slave communication architecture. There are currently few theoretical results in this paper, especially on properties of the global MDP statements. However, motivated from the concept of combining both the centralized and decentralized perspectives of MARL, we are working on another draft explaining how effective communications between agents helps finding better optima of the global MDP. \n\nAs for implicit global state representations, this is a very good suggestion, we have added some analysis where we tried to visualize and understand the hidden state of the master agent. More details can be found in sec 4.5."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515182079471,"tcdate":1515181980852,"number":1,"cdate":1515181980852,"id":"BkSjwU6Xz","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Comment","forum":"B1KFAGWAZ","replyto":"B1qTwXcgz","signatures":["ICLR.cc/2018/Conference/Paper1051/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1051/Authors"],"content":{"title":"Response to review","comment":"We appreciate the positive comments and address the concerns as follows. \n\nRegarding experimental evaluation, thanks to the suggestion, we have added uncertainty results aka std in the latest tables. The training was carried out on multiple seeds and we chose the best 5 runs and computed the statistics (which is somehow standard since training of RL is not always stable). \n\nThe suggestion to compare with more centralized baselines is enlightening, we manage to realized a simple centralized baseline based on one modification from our own model. More detailed results are provided in the newly section 4.4. In a nutshell, this method performs rather poorly probably due to scalability issues, but we agree there are still room to explore in this regard. Regarding the performance of GMEZO, we have originally reported the reproduced results from the BicNet paper for consistency considerations, in the updated version we clarify this point and have now reported the highest ones from both. Still ours are consistently superior.\n\nIt is slightly subjective when discussing how novel a proposal is. What we can confirm is that we are the first to explicitly explore the master-slave communication architecture for MARL and effectively show its superiority to existing communication proposals such as that in BicNet and CommNet. The proposed GCM is a novel communication processor. Although the initial experimental results didn’t strongly justify its advantages, we have tried our more challenging settings such as heterogeneous starcraft tasks etc. We included one such case and related analysis in the updated version. We agree and has never denied that master-salve and LSTM cells are existing wisdoms, our major contribution is the novel and effective instantiation in the MARL settings, out of which we reported very promising performance on challenging tasks such as starcraft micromanagements.\n\nAll detailed comments are very helpful, we have properly addressed them in the updated version. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515642380028,"tcdate":1511827394245,"number":3,"cdate":1511827394245,"id":"B1qTwXcgz","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Review","forum":"B1KFAGWAZ","replyto":"B1KFAGWAZ","signatures":["ICLR.cc/2018/Conference/Paper1051/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Somewhat encouraging results on StarCraft, method is incremental, paper needs work","rating":"4: Ok but not good enough - rejection","review":"The paper presents results across a range of cooperative multi-agent tasks, including a simple traffic simulation and StarCraft micro-management. The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS-MARL. \nA gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each agent.  This contribution is added to the logits coming from each agent. \n\nPros: \n-The results on StarCraft are encouraging and present state of the art performance if reproducible.\n\nCons:\n-The experimental evaluation is not very thorough:\nNo uncertainty of the mean is stated for any of the results. 100 evaluation runs is very low. It is furthermore not clear whether training was carried out on multiple seeds or whether these are individual runs. \n\n-BiCNet and CommNet are both aiming to learn communication protocols which allow decentralized execution. Thus they represent weak baselines for a fully centralized method such as MS-MARL. \nThe only fully centralized baseline in the paper is GMEZO, however results stated are much lower than what is reported in the original paper (eg. 63% vs 79% for M15v16). The paper is missing further centralized baselines. \n\n-It is unclear to what extends the novelty of the paper (specific architecture choices) are required. For example, the gating mechanism for producing the action logits is rather complex and seems to only help in a subset of settings (if at all).\n\nDetailed comments:\n\"For all tasks, the number of batch per training epoch is set to 100.\"\nWhat does this mean?\n\nFigure 1: \nThis figure is very helpful, however the colour for M->S is wrong in the legend. \n\nTable 2:\nGMEZO win rates are low compared to the original publication. \nWhat many independent seeds where used for training? What are the confidence intervals? How many runs for evaluation? \n\n\nFigure 4:\nB) What does it mean to feed two vectors into a Tanh? This figure currently very unclear. What was the rational for choosing a vanilla RNN for the slave modules?\n\nFigure 5:\na) What was the rational for stopping training of CommNet after 100 epochs? The plot looks like CommNet is still improving. \nc) This plot is disconcerting. Training in this plot is very unstable. The final performance of the method ('ours') does not match what is stated in 'Table 2'. I wonder if this is due to the very small batch size used (\"a small batch size of 4 \").\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515642380064,"tcdate":1511625971962,"number":2,"cdate":1511625971962,"id":"HynlSGDxM","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Review","forum":"B1KFAGWAZ","replyto":"B1KFAGWAZ","signatures":["ICLR.cc/2018/Conference/Paper1051/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients.\nAuthors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines.\n\nOverall, I had a very confusing feeling when reading the paper. First, authors do not formulate what exactly is the problem statement for MARL. Is it an MDP or poMDP? How do different agents perceive their time, is it synchronized or not? Do they (partially) share the incentive or may have completely arbitrary rewards?\nWhat is exactly the communication protocol?\n\nI find this question especially important for MARL, because the assumption on synchronous and noise-free communication, including gradients is too strong to be useful in many practical tasks.\n\nSecond, even though the proposed architecture proved to perform empirically better that the considered baselines, the extent to which it advances RL research is unclear to me.\nCurrently, it looks \n\nBased on that, I can’t recommend acceptance of the paper.\n\nTo make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients).\nIt would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1515642380103,"tcdate":1511467504329,"number":1,"cdate":1511467504329,"id":"HyuecsNxf","invitation":"ICLR.cc/2018/Conference/-/Paper1051/Official_Review","forum":"B1KFAGWAZ","replyto":"B1KFAGWAZ","signatures":["ICLR.cc/2018/Conference/Paper1051/AnonReviewer2"],"readers":["everyone"],"content":{"title":"results seem nice, but novelty less clear","rating":"5: Marginally below acceptance threshold","review":"This paper investigates multiagent reinforcement learning  making used of a \"master slave\" architecture (MSA). On the positive side, the paper is mostly well-written, seems technically correct, and there are some results that indicate that the MSA is working quite well on relatively complex tasks. On the negative side, there seems to be relatively limited novelty: we can think of MSA as one particular communication (i.e, star) configuration one could use is a multiagent system. One aspect does does strike me as novel is the \"gated composition module\", which allows differentiation of messages to other agents based on the receivers internal state. (So, the *interpretation* of the message is learned). I like this idea, however, the results are mixed, and the explanation given is plausible, but far from a clearly demonstrated answer.\n\nThere are some important issues that need clarification:\n\n* \"Sukhbaatar et al. (2016) proposed the “CommNet”, where broadcasting communication channel among all agents were set up to share a global information which is the summation of all individual agents. [...] however the summed global signal is hand crafted information and does not facilitate an independently reasoning master agent.\"\n-Please explain what is meant here by 'hand crafted information', my understanding is that the f^i in figure 1 of that paper are learned modules?\n-Please explain what would be the differences with CommNet with 1 extra agent that takes in the same information as your 'master'.\n\n\n*This relates also to this: \n\n\"Later we empirically verify that, even when the overall in-\nformation revealed does not increase per se, an independent master agent tend to absorb the same\ninformation within a big picture and effectively helps to make decision in a global manner. Therefore\ncompared with pure in-between-agent communications, MS-MARL is more efficient in reasoning\nand planning once trained. [...] \nSpecifically, we compare the performance among the CommNet model, our\nMS-MARL model without explicit master state (e.g. the occupancy map of controlled agents in this\ncase), and our full model with an explicit occupancy map as a state to the master agent. As shown in\nFigure 7 (a)(b), by only allowed an independently thinking master agent and communication among\nagents, our model already outperforms the plain CommNet model which only supports broadcast-\ning communication of the sum of the signals.\"\n\n-Minor: I think that the statement \"which only supports broadcast-ing communication of the sum of the signals\" is not quite fair: surely they have used a 1-channel communication structure, but it would be easy to generalize that.\n\n-Major: When I look at figure 4D, I see that the proposed approach *also* only provides the master with the sum (or really mean) with of the individual messages...? So it is not quite clear to me what explains the difference.\n\n\n*In 4.4, it is not quite clear exactly how the figure of master and slave actions is created. This seems to suggest that the only thing that the master can communicate is action information? It this the case?\n\n* In table 2, it is not clear how significant these differences are. What are the standard errors?\n\n* The section 3.2 explains standard things (policy gradient), but the details are a bit unclear. In particular, I do not see how the Gaussian/softmax layers are integrated; they do not seem to appear in figure 4?\n\n* I cannot understand figure 7 without more explanation. (The background is all black - did something go wrong with the pdf?)\n\n\n\n\nDetails:\n* references are wrongly formatted throughout. \n\n* \"In this regard, we are among the first to combine both the centralized perspective and the decentralized perspective\"\nThis is a weak statement (E.g., I suppose that in the greater scheme of things all of us will be amongst the first people that have walked this earth...)\n\n\n* \"Therefore they tend to work more like a commentator analyzing and criticizing the play, rather than\na coach coaching the game.\"\n-This sounds somewhat vague. Can it be made crisper?\n\n* \"Note here that, although we explicitly input an occupancy map to the master agent, the actual infor-\nmation of the whole system remains the same.\"\nThis is a somewhat peculiar statement. Clearly, the distribution of information over the agents is crucial. For more insights on this one could refer to the literature on decentralized POMDPs.\n\n\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]}},{"tddate":null,"ddate":null,"tmdate":1516160206924,"tcdate":1509138073170,"number":1051,"cdate":1510092360236,"id":"B1KFAGWAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1KFAGWAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning","abstract":"Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspective is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetics experiments and when applied to challenging StarCraft  micromanagement tasks.","pdf":"/pdf/6025584f4ce2e8f2e6a1110498fccea98cc54a47.pdf","TL;DR":"We revisit the idea of the master-slave architecture in multi-agent deep reinforcement learning and outperforms state-of-the-arts.","paperhash":"anonymous|revisiting_the_masterslave_architecture_in_multiagent_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting The Master-Slave Architecture In Multi-Agent Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1KFAGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1051/Authors"],"keywords":["Deep Reinforcement Learning","Multi-Agent Reinforcement Learning","StarCraft Micromanagement Tasks"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}