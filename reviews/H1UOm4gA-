{"notes":[{"tddate":null,"ddate":null,"tmdate":1512414606064,"tcdate":1512414606064,"number":3,"cdate":1512414606064,"id":"HJLqaM7bz","invitation":"ICLR.cc/2018/Conference/-/Paper235/Official_Review","forum":"H1UOm4gA-","replyto":"H1UOm4gA-","signatures":["ICLR.cc/2018/Conference/Paper235/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting contribution","rating":"6: Marginally above acceptance threshold","review":"The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks. Agents working in this setting therefore, learn the language of the \"teacher\" and efficiently ground words to their respective concepts in the environment. The work also propose a neat model motivated by the environment and outperform various baselines. \n\nFurther, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase. \n\nThe robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right? As the paper says, a difference of one word can lead to completely different goals and so, the noise robustness experiments seem to test for the biases learned by the agent in some sense (which is not desirable). Is there any justification for why this method of injecting noise was chosen ? Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ?  \n\nOverall, I think the paper proposes an interesting environment and task that is of interest to the community in general. The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Grounded Language Acquisition and Generalization in 2D Environment","abstract":"We build a virtual agent for learning language in a 2D maze-like environment. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the environment, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably extrapolates to interpret navigation commands that may contain not only new word combinations but also new words not appeared in training commands. These new words are transferred from question answering. This language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms four comparison methods on interpreting zero-shot sentences. We additionally demonstrate human-interpretable intermediate outputs of the model.","pdf":"/pdf/bc7282375d9a7b8c6df1784bccc627dc80d8277d.pdf","TL;DR":"Training an agent in a 2D virtual world for grounded language acquisition and generalization.","paperhash":"anonymous|interactive_grounded_language_acquisition_and_generalization_in_2d_environment","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Grounded Language Acquisition and Generalization in 2D Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1UOm4gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper235/Authors"],"keywords":["grounded language learning","zero-shot language learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222592840,"tcdate":1511820417476,"number":2,"cdate":1511820417476,"id":"B1KF2Z5xf","invitation":"ICLR.cc/2018/Conference/-/Paper235/Official_Review","forum":"H1UOm4gA-","replyto":"H1UOm4gA-","signatures":["ICLR.cc/2018/Conference/Paper235/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review from AnonReviewer3","rating":"6: Marginally above acceptance threshold","review":"[Overview]\nIn this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user's questions as well. To address this problem, the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images. Specifically, By this way, the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly. In the experiments, the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations, including in-domain and out-of-domain cases.\n\n[Strengths]\n\n1. I think this paper proposed interesting tasks to combine the vision, language, and actions. As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments. The authors should release the dataset to prompt the research in this area.\n\n2. The authors proposed a simple method to ground the language on visual input. Specifically, the authors grounded each word in a sentence to all locations of the visual map, and then perform a simple concept detection upon it. Then, the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well.\n\n3. From the experiments, it is shown that the proposed model outperforms several baseline methods in both normal tasks and out-of-domain ones. According to the visualizations, the interpreter could generate meaningful attention map given a textual query.\n\n[Weakness]\n\n1. The definition of explicit grounding is a bit misleading. Though the grounding or attention is performed for each word at each location of the visual map. It is a still kind of soft-attention, except that is performed for each word in a sentence. As far as I know, this has been done in several previous works, such as: (a). Hierarchical question-image co-attention for visual question answering (https://scholar.google.com/scholar?oi=bibs&cluster=15146345852176060026&btnI=1&hl=en). Lu et al. NIPS 2016. (b). Graph-Structured Representations for Visual Question Answering. Teney et al. arXiv 2016. At most recent, we have seen some more explicit way for visual grounding like: (c). Bottom-up and top-down attention for image captioning and VQA (https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017.\n\n2. Since the model is aimed at grounding the language on the vision based on interactions, it is worth to show how well the final model could ground the text words to each of the visual objects. Say, show the affinity matrix between the words and the objects to indicate the correlations.\n\n[Summary]\n\nI think this is a good paper which integrates vision, language, and actions in a virtual environment. I would foresee more and more works will be devoted to this area, considering its close connection to our daily life. To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc. According to the visualization, the model could attend the right region of the image for finishing a navigation and QA task. As I said, the authors should rephrase the definition of explicit grounding, to make it clearly distinguished with the previous work I listed above. Also, the authors should definitely show the grounding attention results of words and visual signal jointly, i.e., showing them together in one figure instead of separately in Figure 9 and Figure 10.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Grounded Language Acquisition and Generalization in 2D Environment","abstract":"We build a virtual agent for learning language in a 2D maze-like environment. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the environment, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably extrapolates to interpret navigation commands that may contain not only new word combinations but also new words not appeared in training commands. These new words are transferred from question answering. This language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms four comparison methods on interpreting zero-shot sentences. We additionally demonstrate human-interpretable intermediate outputs of the model.","pdf":"/pdf/bc7282375d9a7b8c6df1784bccc627dc80d8277d.pdf","TL;DR":"Training an agent in a 2D virtual world for grounded language acquisition and generalization.","paperhash":"anonymous|interactive_grounded_language_acquisition_and_generalization_in_2d_environment","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Grounded Language Acquisition and Generalization in 2D Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1UOm4gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper235/Authors"],"keywords":["grounded language learning","zero-shot language learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222592883,"tcdate":1511548215406,"number":1,"cdate":1511548215406,"id":"B1JBH18gf","invitation":"ICLR.cc/2018/Conference/-/Paper235/Official_Review","forum":"H1UOm4gA-","replyto":"H1UOm4gA-","signatures":["ICLR.cc/2018/Conference/Paper235/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper introduces a new task that combines elements of instruction following\nand visual question answering: agents must accomplish particular tasks in an\ninteractive environment while providing one-word answers to questions about\nfeatures of the environment. To solve this task, the paper also presents a new\nmodel architecture that effectively computes a low-rank attention over both\npositions and feature indices in the input image. It uses this attention as a\ncommon bottleneck for downstream predictors that select actions and answers to\nquestions. The paper's main claim is that this model architecture enables strong\ngeneralization: it allows the model to succeed at the instruction following task\neven when given words it has only seen in QA contexts, and vice-versa.\nExperiments show that on the navigation task, the proposed approach outperforms\na variety of baselines under both a normal data condition and one requiring\nstrong generalization.\n\nOn the whole, I think this paper does paper does a good job of motivating the\nproposed modeling decisions. The approach is likely to be useful for other\nresearchers working on related problems. I have a few questions about the\nevaluation, but most of my comments are about presentation.\n\nEVALUATION\n\nIs it really the case that no results are presented for the QA task, or am I\nmisreading one of the charts here? Given that this paper spends a lot of time\nmotivating the QA task as part of the training scenario, I was surprised not to\nsee it evaluated. \n\nAdditionally, when I first read the paper I thought that the ZS1 experiments\nfeatured no QA training at all. However, your response to one of the sibling\ncomments suggests that it's still a \"mixed\" training setting where the sampled\nQA and NAV instances happen to cover the full space. This should be made more\nclear in the paper. It would be nice to know (1) how the various models perform\nat QA in both ZS1 and ZS2 settings, and (2) what the actual performance is NAV\nalone (even if the results are terrible).\n\nMODEL PRESENTATION\n\nI found section 2 difficult to read: in particular, the overloading of \\Phi\nwith different subscripts for different output types, the general fact that\ne.g. x and \\Phi_x are used interchangeably, and the large number of different\nvariables. My best suggestions are to drop the \\Phis altogether and consider\nusing text subscripts rather than coming up with a new name for every variable,\nbut there are probably other things that will also help.\n\nOTHER NOTES\n\n- This paper needs serious proofreading---just in the first few pages the errors\n  I noticed were \"in 2D environment\" (in the title!), \"such capability\", \"this\n  characteristics\", \"such language generalization problem\", \"the agent need to\",\n  \"some early pioneering system\", \"commands is\". I gave up on keeping track at\n  this point but there are many more.\n\n- \\phi in Fig 2 should be explained by the caption.\n\n- Here's another good paper to cite for the end of 2.2.1:\n  https://arxiv.org/pdf/1707.00683.pdf.\n\n- The mechanism in 2.2.4 feels a little like\n  http://aclweb.org/anthology/D17-1015\n\n- I don't think the content on pages 12, 13, and 14 adds much to the\n  paper---consider moving these to an appendix.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Grounded Language Acquisition and Generalization in 2D Environment","abstract":"We build a virtual agent for learning language in a 2D maze-like environment. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the environment, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably extrapolates to interpret navigation commands that may contain not only new word combinations but also new words not appeared in training commands. These new words are transferred from question answering. This language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms four comparison methods on interpreting zero-shot sentences. We additionally demonstrate human-interpretable intermediate outputs of the model.","pdf":"/pdf/bc7282375d9a7b8c6df1784bccc627dc80d8277d.pdf","TL;DR":"Training an agent in a 2D virtual world for grounded language acquisition and generalization.","paperhash":"anonymous|interactive_grounded_language_acquisition_and_generalization_in_2d_environment","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Grounded Language Acquisition and Generalization in 2D Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1UOm4gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper235/Authors"],"keywords":["grounded language learning","zero-shot language learning"]}},{"tddate":null,"ddate":null,"tmdate":1510604901901,"tcdate":1510604901901,"number":1,"cdate":1510604901901,"id":"rkCvlKvkf","invitation":"ICLR.cc/2018/Conference/-/Paper235/Official_Comment","forum":"H1UOm4gA-","replyto":"r1a6A4PJz","signatures":["ICLR.cc/2018/Conference/Paper235/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper235/Authors"],"content":{"title":"Thanks for your comments! They are really good questions and helpful.","comment":"1. The formula of the module A was not contained in the paper,\nprimarily due to the cut down of pages. It is just a feedforward\nsub-network that approximates the value function and generates the action\ndistribution, given the representation q (Fig. 2). However, as you\nhave asked, we now think it might be a good idea to add it back in the\npaper.\n\n2. Continuing on your first question, our RL method is simply\ncombining AC and Experience Replay (ER). ER is mainly used to\nstabilize AC while maintaining sample efficient. This might result in\nsome conflict between off-policy and on-policy, since the experiences\nsampled from the replay buffer were not generated by the current\npolicy. However, we find that this works well in practice (perhaps\nbecause of the small replay buffer). Similar work was also proposed\nrecently:\n\n\tSample Efficient Actor-Critic With Experience Replay, Wang et al, ICLR 2017.\n\nwhich is more sophisticated compared to ours.\n\nMore specifically, for every minibatch sampled from the replay buffer,\nwe have the following gradient:\n\n-\\sum_{k=0}^K(\\nabla_{\\theta}\\log\\pi_{\\theta}(a_k|x_k)+\\nabla_{\\theta}v_{\\theta}(x_k))(r+\\gamma\n v_{\\theta'}(x_k')-v_{\\theta}(x_k))\n\nwhere k is the sample index in the batch, (x_k, a_k, x_k') is the\nsampled transition, v is the value function to learn, \\theta is the\ncurrent parameters, \\theta' is the target parameters that have update\ndelay as in ER, and \\gamma is the discount factor. This gradient\nmaximizes the expected reward while minimizes the TD error.\n\n3. We did try only training NAV without QA. However, it only worked to\nsome extent for small-size maps like 3x3 or 5x5, with a much smaller\namount of object classes. It was difficult to converge on the current\nsetting of 7x7 maps with 119 object classes. Thus QA is important for\nthe learning. It is not uncommon to see some auxiliary tasks used for\na better convergence in RL problems, for example, language prediction\nand some other cost functions were used in parallel with RL:\n\n\tGrounded language learning in a simulated 3d world, Hermann et al,\n\tarxiv 1706.06551, 2017\n\nNote that QA only helps understanding of questions. The NAV commands\nand action control still need to be learned from RL. Questions and\ncommands are disjoint sets of sentences. The only common part is some\nlocal word or phrase patterns. More importantly, QA offers the\nopportunity to assess the transferring ability of the model across\ntasks, denoted as ZS2 in the paper.\n\nWe believe that such task of jointly learning language and vision is\nchallenging. Even for children, it is very likely that they learn from\na mixture of signals of the environment instead from a single task.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Grounded Language Acquisition and Generalization in 2D Environment","abstract":"We build a virtual agent for learning language in a 2D maze-like environment. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the environment, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably extrapolates to interpret navigation commands that may contain not only new word combinations but also new words not appeared in training commands. These new words are transferred from question answering. This language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms four comparison methods on interpreting zero-shot sentences. We additionally demonstrate human-interpretable intermediate outputs of the model.","pdf":"/pdf/bc7282375d9a7b8c6df1784bccc627dc80d8277d.pdf","TL;DR":"Training an agent in a 2D virtual world for grounded language acquisition and generalization.","paperhash":"anonymous|interactive_grounded_language_acquisition_and_generalization_in_2d_environment","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Grounded Language Acquisition and Generalization in 2D Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1UOm4gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper235/Authors"],"keywords":["grounded language learning","zero-shot language learning"]}},{"tddate":null,"ddate":null,"tmdate":1510588100794,"tcdate":1510588100794,"number":1,"cdate":1510588100794,"id":"r1a6A4PJz","invitation":"ICLR.cc/2018/Conference/-/Paper235/Public_Comment","forum":"H1UOm4gA-","replyto":"H1UOm4gA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Questions about your RL setup","comment":"Nice work!\n\nI have a few questions about the technical details of your reinforcement learning experiments:\n\n- I did not find the formula for the module A of the model which is mentioned in Equation (1). Is it contained in the paper?\n- According to Section 3 you are using an actor-critic method. Meanwhile, your Appendix D says that you are using \"Experience Replay\". Can you please provide more details on what specific RL approach was used?\n- According to Appendix D you train NAV and VQA pathways in parallel. Did you try training NAV only? Does you RL approach to grounding work in the absence of the additional signal from VQA?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Interactive Grounded Language Acquisition and Generalization in 2D Environment","abstract":"We build a virtual agent for learning language in a 2D maze-like environment. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the environment, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably extrapolates to interpret navigation commands that may contain not only new word combinations but also new words not appeared in training commands. These new words are transferred from question answering. This language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms four comparison methods on interpreting zero-shot sentences. We additionally demonstrate human-interpretable intermediate outputs of the model.","pdf":"/pdf/bc7282375d9a7b8c6df1784bccc627dc80d8277d.pdf","TL;DR":"Training an agent in a 2D virtual world for grounded language acquisition and generalization.","paperhash":"anonymous|interactive_grounded_language_acquisition_and_generalization_in_2d_environment","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Grounded Language Acquisition and Generalization in 2D Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1UOm4gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper235/Authors"],"keywords":["grounded language learning","zero-shot language learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739413901,"tcdate":1509077869883,"number":235,"cdate":1509739411257,"id":"H1UOm4gA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1UOm4gA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Interactive Grounded Language Acquisition and Generalization in 2D Environment","abstract":"We build a virtual agent for learning language in a 2D maze-like environment. The agent sees surrounding environment images, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher’s language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the environment, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably extrapolates to interpret navigation commands that may contain not only new word combinations but also new words not appeared in training commands. These new words are transferred from question answering. This language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms four comparison methods on interpreting zero-shot sentences. We additionally demonstrate human-interpretable intermediate outputs of the model.","pdf":"/pdf/bc7282375d9a7b8c6df1784bccc627dc80d8277d.pdf","TL;DR":"Training an agent in a 2D virtual world for grounded language acquisition and generalization.","paperhash":"anonymous|interactive_grounded_language_acquisition_and_generalization_in_2d_environment","_bibtex":"@article{\n  anonymous2018interactive,\n  title={Interactive Grounded Language Acquisition and Generalization in 2D Environment},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1UOm4gA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper235/Authors"],"keywords":["grounded language learning","zero-shot language learning"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}