{"notes":[{"tddate":null,"ddate":null,"tmdate":1512164544955,"tcdate":1512164544955,"number":3,"cdate":1512164544955,"id":"S1YphBy-f","invitation":"ICLR.cc/2018/Conference/-/Paper594/Official_Comment","forum":"SyX0IeWAW","replyto":"r13qQpKxM","signatures":["ICLR.cc/2018/Conference/Paper594/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper594/Authors"],"content":{"title":"Proposed Changes","comment":"Thanks for taking the time to review and give feedback. We’ve addressed the main points and proposed some changes in the next version to clear up explanations and reasoning.\n\n> Algorithm 1 makes it sound like...parallel?\n\nWhile the core algorithm can be run sequentially, we use a multi-core setup in experiments to speed up the process. There may be some confusion in the description -- after the group resets theta, a new task is sampled, and all cores learn on this same new task. Therefore at any given time we are optimizing for 10 tasks in parallel, but these tasks are constantly being re-sampled from the distribution. We will clarify this in the next revision.\n\n> Is the number of sup-policies pre-defined/hard-coded/hand-designed? What happens if you have too many/not enough?\n\nFor simplicity’s sake, we pre-define the number of sub-policies, treating it as a hyperparameter. In the Future Work section, we describe a potential method for condensing multiple sub-policies into a single network, allowing the agent to learn any distribution of sub-policies.\n\nWith a small number of sub-policies, the agent may be less robust to new tasks (as it learns fewer behaviors). With a large number of sub-policies, it takes longer to train agents.\n\n> The argumentation in Sect. 5 is vague...time.\n\nThe point about not updating phi too much is correct (we address it below). A key point in the Sect.5 argument is that sub-policies should only be trained in conjunction with a strong master policy, which is the rationale behind the warmup period.\n\n> I am not convinced that the above is solved by staggering the tasks in the asynchronous setting...\n\nIn practice, we use a small phi learning rate (0.0003) compared to the theta learning rate (0.01), as defined in the 6.1 Experimental Setup. Our goal here is that small changes in the representation (phi) are negligible in the short-run training of theta, but will build up in the long-run.\n\nWe’ll add in this reasoning behind the learning-rate choices in the next revision.\n\n> Another interesting experiment would be to test how much the system unlearns, e.g., by optimizing for a task, switching to a few other tasks, freezing phi and testing if the first task can still achieve the same performance\n> The plots Fig. 4/7 are a bit unclear. My guess is \"full training\" means learning from scratch as described in Sect. 6.1, \"sampled tasks\" means trying whether the learned sub-policies also work for a previously unseen task. Here again the question: What happens if you freeze phi? How well do phi updated on the new tasks work on the original ones? Related question: Why is there no plot on the combination task (Fig. 7) and full training on Four Rooms?\n\nYou’re correct on the meaning of “full training” and “sampled tasks”. We’ll add a description in the caption to clear things up. \n\nRegarding the freeze phi experiment: When running the “Sampled Task” experiments, only theta is trained, so phi is frozen. If phi was overfitting/unlearning on every new task, the agent would perform poorly on an unseen “Sampled Task”.\n\nThe plot for the combination task is uninteresting since the rewards are so different. The different trials (MLSH Transfer, Shared Policy Transfer, Single Policy) never pass each other in performance.\n\nOn four rooms, we don’t include a full training since the base methods compared (PPO and Actor Critic) have vastly different sample efficiencies. Instead, we just train until both baselines have reached convergence.\n\n> Sect. 6.4 \"series of tasks\" is a bit unclear\n\nThanks -- we’ll clarify this to “series of tasks involving robotic locomotion in the physics domain”\n\n> Sect. 6.4: Why is the ratio of warm-up and training so different compared to the 2D bandits? How much influence does this parameter have on the performance of the approach?\n\nThe physics domain has a more complicated learning task for sub-policies compared to the 2D task, so training is naturally slower. However, master policies have the same learning task in both situations (select a sub-policy). So we give more training updates per warmup in the physics task. While it’s important to have a warmup period (as shown in Fig 4, MLSH performs worse when not including a warmup), the ratio doesn’t need to be precise. It’s always more accurate to have a long warmup period and short training period, but the agent will take longer to train. We’ll add this intuition in the next revision."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1512164293367,"tcdate":1512164293367,"number":2,"cdate":1512164293367,"id":"ByaasHJWM","invitation":"ICLR.cc/2018/Conference/-/Paper594/Official_Comment","forum":"SyX0IeWAW","replyto":"rkCZR_3xz","signatures":["ICLR.cc/2018/Conference/Paper594/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper594/Authors"],"content":{"title":"Clarification","comment":"The idea of learning a hierarchy of sub-policies has been explored in past work, many of which we cite and discuss in Section 2:\n\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. arXiv preprint arXiv:1609.05140, 2016.\n\nCarlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement learning. In International Conference on Learning Representations, 2017.\n\nRichard S Sutton, Doina Precup, , and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. In Artificial intelligence, 1999.\n\nIn contrast to many previous works, our method aims to learn sub-policies automatically, without the need for hand engineering (in the paper you mentioned, they design running and leaping policies). In addition, we focus on the idea of sharing sub-policies over distributions of tasks, rather than on single tasks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1511980550478,"tcdate":1511980550478,"number":2,"cdate":1511980550478,"id":"rkCZR_3xz","invitation":"ICLR.cc/2018/Conference/-/Paper594/Public_Comment","forum":"SyX0IeWAW","replyto":"SyX0IeWAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Unclear about novelty","comment":"The work is interesting but it is unclear how novel it is. There exists similar work that learns something very similar to the hierarchy in this paper.\n\nX. B. Peng, G. Berseth, and M. Van de Panne. 2016. Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning. ACM Transactions on Graphics (Proc. SIGGRAPH 2016) 35, 5. \n\nHowever, in this previous work the sub-policy is not a neural network but having the sub-policy not be a neural network is not a novel idea."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222695421,"tcdate":1511829462215,"number":3,"cdate":1511829462215,"id":"r1RR1Vclf","invitation":"ICLR.cc/2018/Conference/-/Paper594/Official_Review","forum":"SyX0IeWAW","replyto":"SyX0IeWAW","signatures":["ICLR.cc/2018/Conference/Paper594/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper proposes a novel method for inducing temporal hierarchical structure in a specialized multi-task setting.","rating":"7: Good paper, accept","review":"This paper proposes a novel hierarchical reinforcement learning method for a fairly particular setting.  The setting is one where the agent must solve some task for many episodes in a sequence, after which the task will change and the process repeats.  The proposed solution method splits the agent into two components, a master policy which is reset to random initial weights for each new task, and several sub-policies (motor primitives) that are selected between by the master policy every N steps and whose weights are not reset on task switches.  The core idea is that the master policy is given a relatively easy learning task of selecting between useful motor primitives and this can be efficiently learned from scratch on each new task, whereas learning the motor primitives occurs slowly over many different tasks.  To push this motivation into the learning process, the master policy is updated always but the sub-policies are only updated after an extended warmup period (called the joint-update or training period).  This experiments include both small domains (moving to 2D goals and four-rooms) and more complex physics simulations (4-legged ants and humanoids).  In both the simple and complex domains, the proposed method (MLSH) is able to robustly achieve good performance.\n\nThis approach to obtaining complex structured behavior appears impressive despite the amount of temporal structure that must be provided to the method (the choice of N, the warmup period, and the joint-update period).  Relying on the temporal structure for the hierarchy, and forcing the master policy to be relearned from scratch for each new task may be problematic in general, but this work shows that in some complex settings, a simple temporal decomposition may be sufficient to encourage the development of reusable motor primitives and to also enable quick learning of meta-policies over these motor-primitives.  Moreover, the results show that these temporal hierarchies are helpful in these domains, as the corresponding non-hierarchical methods failed on the more challenging tasks.\n\nThe paper could be improved in some places (e.g. unclear aliases of joint-update or training periods, describing how the parameters were chosen, and describing what kinds of sub-policies are learned in these domains when different parameter choices are made).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222695464,"tcdate":1511802215373,"number":2,"cdate":1511802215373,"id":"HyJuHTteM","invitation":"ICLR.cc/2018/Conference/-/Paper594/Official_Review","forum":"SyX0IeWAW","replyto":"SyX0IeWAW","signatures":["ICLR.cc/2018/Conference/Paper594/AnonReviewer2"],"readers":["everyone"],"content":{"title":"very vague paper","rating":"3: Clear rejection","review":"Please see my detailed comments in the \"official comment\"\n\nQuality\n======\nThe idea is interesting, the theory is hand-wavy at best, the experiments show that it works but don't evaluate many interesting/relevant aspects. It is also unclear how much tuning is involved.\n\nClarity\n=====\nThe paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) leaving many details up the the reader's best guess.\n\nOriginality\n=========\nThe idea looks original.\n\nSignificance\n==========\nIf it works as advertised this approach would mean a drastic speedup on previously unseen task from the same distribution.\n\nPros and Cons\n============\n+ interesting idea\n- we do everything asynchronously and in parallel and it magically works\n- many open questions / missing details","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1511802260844,"tcdate":1511801747983,"number":1,"cdate":1511801747983,"id":"r13qQpKxM","invitation":"ICLR.cc/2018/Conference/-/Paper594/Official_Comment","forum":"SyX0IeWAW","replyto":"SyX0IeWAW","signatures":["ICLR.cc/2018/Conference/Paper594/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper594/AnonReviewer2"],"content":{"title":"Please improve algorithm description","comment":"The paper proposes to learn sub-policies (or motor primitives, options, etc.) jointly with a higher level policy by optimizing for multiple tasks simultaneously\n\n- Algorithm 1 makes it sound like all the learning happens sequentially, later on we learn that a multi-core setup is used. The details on this remain very vague. As far as I can guess we have 10 groups (with 12 cores each). All cores within a group are assigned the exact same task (shared theta), the parameters phi for the sub-policies is shared between all nodes. The text reads like the parameters theta are forgotten and the learning thereof is restarted with the exact same task. Hence we optimize for exactly 10 tasks in parallel?\n\n- Is the number of sup-policies pre-defined/hard-coded/hand-designed? What happens if you have too many/not enough?\n\n- The argumentation in Sect. 5 is vague and holds only if the tasks are learned sequentially. To me it sounds like you need to ensure that you don't update phi too much, otherwise it might unlearn something useful for the previous tasks. In the 2D moving bandit problem this seems to be achieved by only updating phi for a small amount of time.\n\n- I am not convinced that the above is solved by staggering the tasks in the asynchronous setting. While still in the warm-up phase (i.e,. learning theta) the agents associated to a certain task need to cope with the fact that the phi is changed simultaneously, hence they have to play catch-up with the changing representation while trying to improve their performance.\n\n- Another interesting experiment would be to test how much the system unlearns, e.g., by optimizing for a task, switching to a few other tasks, freezing phi and testing if the first task can still achieve the same performance\n\n- The plots Fig. 4/7 are a bit unclear. My guess is \"full training\" means learning from scratch as described in Sect. 6.1, \"sampled tasks\" means trying whether the learned sub-policies also work for a previously unseen task. Here again the question: What happens if you freeze phi? How well do phi updated on the new tasks work on the original ones? Related question: Why is there no plot on the combination task (Fig. 7) and full training on Four Rooms?\n\n- Sect. 6.4 \"series of tasks\" is a bit unclear\n\n- Sect. 6.4: Why is the ratio of warm-up and training so different compared to the 2D bandits? How much influence does this parameter have on the performance of the approach?\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222695503,"tcdate":1511800875717,"number":1,"cdate":1511800875717,"id":"BJN4gTtlM","invitation":"ICLR.cc/2018/Conference/-/Paper594/Official_Review","forum":"SyX0IeWAW","replyto":"SyX0IeWAW","signatures":["ICLR.cc/2018/Conference/Paper594/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Metalearning with shared hierarchies","rating":"4: Ok but not good enough - rejection","review":"This paper considers the reinforcement learning problem setup in which an agent must solve not one, but a set of tasks in some domain, in which the state space and action space are fixed. The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution.\n\nI found the paper to be generally well written and the key ideas easy to understand on first pass. The authors should be commended for this. Aside from a few minor grammatical issues (e.g. missing articles here and there), the writing cannot be too strongly faulted.\n\nThe problem setup is of general interest to the community. Metalearning in the multitask setup seems to be gaining attention and is certainly a necessary  step towards building rapidly adaptable agents.\n\nWhile the concepts were clearly introduced, I think the authors need to make, much more strongly, the case that the method is actually valuable. In that vein, I would have liked to see more work done on elucidating how this method works ‘under the hood’. For example, it is not at all clear how the number of sub policies affects performance (one would imagine that there is a clear trade off), nor how this number should be chosen. It seems obvious that this choice would also affect the subtle dynamics between holding the master policy constant while updating the sub policies and vice versa. While the authors briefly touch on some of these issues in the rationale section, I found these arguments largely unsubstantiated. Moreover, this leads to a number of unjustified hyper-parameters in the method which I suspect would affect the training catastrophically without significant fine-tuning.\n\nThere are also obvious avenues to be followed to check/bolster the intuitions behind the method. By way of example, my sense is that the procedure described in the paper uncovers a set of sub policies that form a `good’ cover for the task space - if so simply plotting out what they policies look like (or better yet how they adapt in time) would be very insightful (the rooms domain is perhaps a good candidate for this).\n\nWhile the key ideas are clearly articulated  the practical value of the procedure is insufficiently motivated. The paper would benefit hugely from additional analysis.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739212263,"tcdate":1509127883330,"number":594,"cdate":1509739209585,"id":"SyX0IeWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyX0IeWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"META LEARNING SHARED HIERARCHIES","abstract":"We develop a metalearning approach for learning hierarchically structured poli- cies, improving sample efficiency on unseen tasks through the use of shared primitives—policies that are executed for large numbers of timesteps. Specifi- cally, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.","pdf":"/pdf/768a11a20240b095d4055a6dfb00b038103c22e4.pdf","TL;DR":"learn hierarchal sub-policies through end-to-end training over a distribution of tasks","paperhash":"anonymous|meta_learning_shared_hierarchies","_bibtex":"@article{\n  anonymous2018meta,\n  title={META LEARNING SHARED HIERARCHIES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyX0IeWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper594/Authors"],"keywords":["hierarchal reinforcement learning","meta-learning"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}