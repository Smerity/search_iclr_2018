{"notes":[{"tddate":null,"ddate":null,"tmdate":1512273132884,"tcdate":1512079512092,"number":8,"cdate":1512079512092,"id":"S1ljgW0gf","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"rkiav21gf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"question about experiments running time","comment":"I am wondering how long your method needs to run when using 10 dynamics in some experiments such as Swimmer-v1 and Humanoid-v1? \nCould you please tell us and also provide a description of your experiment facilities. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1512222733064,"tcdate":1511834760313,"number":3,"cdate":1511834760313,"id":"Hkg9Vrqlz","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Review","forum":"SJJinbWRZ","replyto":"SJJinbWRZ","signatures":["ICLR.cc/2018/Conference/Paper719/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A nice baseline in the epic model-based vs. model-free battle","rating":"7: Good paper, accept","review":"This paper presents a simple model-based RL approach, and shows that with a few small tweaks to more \"typical\" model-based procedures, the methods can substantially outperform model-free methods on continuous control tasks.  In particular, the authors show that by 1) using an ensemble of models instead of a single models, 2) using TRPO to optimize the policy based upon these models (rather that analytical gradients), and 3) using the model ensemble to validate when to stop policy optimization, then a simple model-based approach actually can outperform model-free methods.\n\nOverall, I think this is a nice paper, and worth accepting.  There is very little actually new here, of course: the actual model-based method is entirely standard except with the additions above (which are also all fairly standard approaches in isolation).  But at a higher level, the fact that such simple model-based approaches work better than somewhat complex model free approaches actually is the point of the paper to me.  While the general theme of model-based RL outperforming model-free RL is not new (Atkeson and Santamaria (1997) comes to a similar conclusion) its good to see this same pattern demonstrated \"officially\" on modern RL benchmarks, especially since the _completely_ naive strategy of using a single model and more standard policy optimization doesn't perform as well.\n\nNaturally, there is some question as to whether the work here is novel enough to warrant publication, but I think the overall message of the paper is strong enough to overcome fairly minimal contribution from an algorithmic perspective.  I did also have a few general concerns that I think could be discussed with a bit more detail in the paper:\n1) The choice of this particular model ensemble to represent uncertainty seems rather ad-how.  Why is it sufficient to simply learn N models with different initial weights?  It seems that the likely cause for this is that the random initial weights may lead to very different behavior in the unobserved parts of the space (i.e., portions of the state space where we have no samples), and thus.  But it seems like there are much more principled ways of overcoming this same problem, e.g. by using an actual Bayesian neural net, directly modeling uncertainty in the forward model, or using generative model approaches.  There's some discussion of this point in the introduction, but I think a bit more explanation about why the model ensemble is expected to work well for this purpose.\n2) Likewise, the fact the TRPO outperforms more standard gradient methods is somewhat surprising.  How is the model ensemble being treated during BPTT?  In the described TRPO method, the authors use a different model at each time step, sampling uniformly.  But it seems like a single model is used for each rollout in the proposed BPTT method?  If so, it's not surprising that this approach performs worse.  But it seems like one could backprop through the different per-timestep models just as easily, and it would remove one additional source of difference between the two settings.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1512222733107,"tcdate":1511806595687,"number":2,"cdate":1511806595687,"id":"SJ3tICFlz","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Review","forum":"SJJinbWRZ","replyto":"SJJinbWRZ","signatures":["ICLR.cc/2018/Conference/Paper719/AnonReviewer2"],"readers":["everyone"],"content":{"title":"well-written, good experiments, but limited novelty, doubts about time-complexity, and use of ensembles over BNNs","rating":"6: Marginally above acceptance threshold","review":"The authors combine an ensemble of DNNs as model for the dynamics with TRPO. The ensemble is used in two steps:\nFirst to collect imaginary roll-outs for TRPO and secondly to estimate convergence of the algorithm. The experiments indicate superior performance over the baselines.\n\nThe paper is well-written and the experiments indicate good results. However, idea of using ensembles in the context of \n(model-based) RL  is not novel, and it comes at the cost of time complexity.  Therefore, the method should utilize \nthe advantage an ensemble provides to its full extent. \nThe main strength of an ensemble is to provide lower test error, but also some from of uncertainty estimate given by the spread of the predictions. The authors mainly utilize the first, but to a lesser extent the second advantage (the imaginary roll-outs will  utilize the spread to generate possible outcomes). Ideally the exploration should also be guided by the uncertainty (such as VIME).\n\nRelated, what where the arguments in favor of an ensemble compared to Bayesian neural networks (possibly even as simple as using MH-dropout)? BNNs provide a stronger theoretical justification that the predictive uncertainty is meaningful.\n\nCan the authors comment on the time-complexity of the proposed methods compared to the baselines? In Fig. 2  the x-axis is the time step of the  real data. But I assume it took a different amount of time for each method to reach step t. The same argument can be made for Fig. 4. It seems here that in snake the larger ensembles reach convergence the quickest, but I expect this effect to be reversed when considering actual training time.\n\nIn total I think this paper can provide a useful addition to the literature.  However, the proposed approach does not have strong novelty and I am not fully convinced if the additional burden on time complexity outweighs the improved performance.\n\nMinor:  In Sec. 2: \"Both of these approaches assume a fixed dataset of samples which are collected\nbefore the algorithm starts operating.\"  This is incorrect, while these methods consider the domain of fixed datasets, the algorithms themselves are not limited to this context.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1512222733151,"tcdate":1511722370633,"number":1,"cdate":1511722370633,"id":"rkoFpFOlz","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Review","forum":"SJJinbWRZ","replyto":"SJJinbWRZ","signatures":["ICLR.cc/2018/Conference/Paper719/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Solid contribution in relevant field","rating":"7: Good paper, accept","review":"Summary:\nThe paper proposes to use ensembles of models to overcome a typical problem when training on a learned model: That the policy learns to take advantage of errors of the model.\nThe models use the same training data but are differentiated by a differente parameter initialization and by training on differently drawn minibatches.\nTo train the policy, at each step the next state is taken from an uniformly randomly drawn model.\nFor validation the policy is evaluated on all models and training is stopped early if it doesn't improve on enough of them. \n\nWhile the idea to use an ensemble of deep neural networks to estimate their uncertainty is not new, I haven't seen it yet in this context. They successfully show in their experiments that typical levels of performance can be achieved using much less samples from the real environment.\n\nThe reduction in required samples is over an order of magnitude for simple environments (Mujoco Swimmer). However, (as expected for model based algorithms) both the performance as well as the reduction in sample complexity gets worse with increasing complexity of the environment. It can still successfully tackle the Humanoid Mujoco task but my guess is that that is close to the upper limit of this algorithm?\n\nOverall the paper is a solid and useful contribution to the field.\n\n*Quality:*\nThe paper is clearly shows the advantage of the proposed method in the experimental section where it compares to several baselines (and not only one, thank you for that!). \n\nThings which in my opinion aren't absolutely required in the paper but I would find interesting and useful (e.g. in the appendix) are:\n1. How does the runtime (e.g. number of total samples drawn from both the models and the real environment, including for validation purpuses) compare?\nFrom the experiments I would guess that MB-TRPO is about two to three orders of magnitude slower, but having this information would be useful.\n2. For more complex environments it seems that training is becoming less stable and performance degradates, especially for the Humanoid environment. A plot like in figure 4 (different number of models) for the humanoid environment could be interesting? Additionally maybe a short discussion where the major problem for further scaling lies? For example: Expressiveness of the models? Required number of models / computation feasibility? Etc... This is not necessarily required for the paper but would be interesting.\n\n*Originality & Significance:*\nAs far as I can tell, none of the fundamental ideas are new. However, they are combined in an interesting, novel way that shows significant performance improvements.\nThe problem the authors tackle, namely learning a deep neural network model for model based RL, is important and relevant. As such, the paper contributes to the field and should be accepted.\n\n*Smaller questions and notes:*\n- Longer training times for MB-TRPO, in particular for Ant and Humanoid would have been intersting if computationionally feasibly.\n- Could this in principle be used with Q-learning as well (instead of TRPO) if the action space is discrete? Or is there an obvious reason why not that I am missing?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511409595903,"tcdate":1511398910912,"number":6,"cdate":1511398910912,"id":"S1vW09QlG","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Comment","forum":"SJJinbWRZ","replyto":"HkWtjGlxG","signatures":["ICLR.cc/2018/Conference/Paper719/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper719/Authors"],"content":{"title":"Re: Relationship to EPOpt","comment":"Thank you for your response.\n\nWith the goal of keeping the paper at an 8-page length, we only provided a limited literature survey, comparing to what we thought to be the most related recent work on model based RL. We would be happy to include a comprehensive section on related work in the final version, including a comparison to DAgger and other SysId methods. \n\nWe are also starting to investigate a real robot implementation of our ideas. Naturally, there are many challenges in getting model based RL to work on real hardware, and we intend this to be part of a different publication. Because of the hardware difficulty, a lot of recent work have also tested their algorithms on challenging OpenAI benchmark and open source their code. We found this to be useful to quickly and fairly compare our algorithm to other state-of-the-art methods.\n\nA difficulty in model learning in model-based setting is the coupling between the model and the policy, i.e., in order to learn a good model we need to learn a good policy and vice versa. In the paper we show that using a single model doesn't allow us to learn a good policy due to overfitting. As a result, the model cannot provide accurate enough predictions for optimizing the policy as shown - see the learning curve in figure 4. We attach the videos showing the model prediction vs the real environment in the case of a single model below. https://drive.google.com/open?id=1FzHQgosQNfbHsKXVewYrhuLrOq8jVEqH\n\nAs commented in a different thread, we intend to open source our code, with the hope that other researcher can try our method on various other problem domains."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511331749792,"tcdate":1511331749792,"number":5,"cdate":1511331749792,"id":"B1RjDqMlf","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Comment","forum":"SJJinbWRZ","replyto":"ryRzHczeM","signatures":["ICLR.cc/2018/Conference/Paper719/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper719/Authors"],"content":{"title":"Re: replicating results","comment":"We feed both s and a into the input. They don't need to have the same dimension."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511331094314,"tcdate":1511331094314,"number":7,"cdate":1511331094314,"id":"ryRzHczeM","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"rJBiAuGxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Replicating results","comment":"Thanks, hmmm, I know it is a MLP but how do you deal with s and a since they have different dimensions. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511325341094,"tcdate":1511325341094,"number":4,"cdate":1511325341094,"id":"rJBiAuGxG","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Comment","forum":"SJJinbWRZ","replyto":"BJLAKoxeG","signatures":["ICLR.cc/2018/Conference/Paper719/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper719/Authors"],"content":{"title":"Re: replicating results","comment":"D contains all the data collected so far. The model is a feed-forward neural network with two hidden layers."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511310780809,"tcdate":1511205326449,"number":6,"cdate":1511205326449,"id":"BJLAKoxeG","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"rkiav21gf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: Re: replicating results","comment":"Thanks, I am wondering dataset D consist of only on-policy data or all previous collected data, could you clarify this? Another question is what is the structure of f(s, a) is, you only mentioned it has hidden sizes 1024-1024 and ReLU nonlinearities. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511169045611,"tcdate":1511168888823,"number":5,"cdate":1511168888823,"id":"HkWtjGlxG","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"S16EX33yG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relationship to EPOpt","comment":"Thanks for the response. Your point about model expressiveness is well taken. However, the justifications still seem inadequate primarily because the chosen tasks and results do not adequately represent your premise.\n\nOn the question of physics based models vs DNN models -- this is still wide open. While DNN models are more expressive they might require orders of magnitude more samples to train. In addition, ability to generalize to states that have not been sufficiently visited is also hard to reason -- it is precisely for this reason that DAGGER and related approaches aggregate the data sets and slowly update the policy. Physics based models suffer less from this issue due to having the right priors. Thus, while it might be possible that in the large sample case expressive models might win owing to their capacity, these are not the regimes typical robotics problems operate in. Of course, if actual hardware results were presented, your proposed motivations and premise would have been well justified, but this is not the case.\n\nOn the difficulty on model learning -- it is not clear if there have been negative results in recent literature. The proposed way to learn the model is not very different from the vanilla approach of DAGGER for System ID -- are there any particular differences from simply aggregating the data sets and minimizing L2 loss? My guess is that not many people actually tried or implemented this correctly. An analysis of how accurate the learned model is would also be very revealing. For example, predict next state using learned model and visualize this in the simulator -- do we see smooth transitions or are the state transitions physically implausible? I could imagine that the learned models are not very accurate for prediction, and hence was not pursued rigorously. However, the models might be sufficient for policy improvement -- if this is the case, it might be an interesting insight.\n\nI should emphasize that my intention is not to be overly critical of the work. Model based RL is indeed a promising approach and your paper is one of the first to actually show good positive results with it in recent times. However, my concern is that many related works have been ignored, and the method is somewhat oversold. This is of course not entirely negative -- if a simple method works well but has been ignored by the community, it is worth pointing this out."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511143362662,"tcdate":1511143362662,"number":3,"cdate":1511143362662,"id":"rkiav21gf","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Comment","forum":"SJJinbWRZ","replyto":"SJK0YrJlz","signatures":["ICLR.cc/2018/Conference/Paper719/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper719/Authors"],"content":{"title":"Re: replicating results","comment":"Thank you for your questions. The standard deviation of parameter noise is proportional to the difference between the current parameters and the previous ones. We use 3.0 for the proportional ratio.\nThe dataset D consists of both training and validation sets. After new data are collected, we split them and put them into each set. We plan to release the codebase in the future."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1511138704554,"tcdate":1511115216850,"number":4,"cdate":1511115216850,"id":"SJK0YrJlz","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"SytosZKJM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"replicating results","comment":"I am trying to replicate your results, it is unclear to me what exactly the value of standard derivation for perturbing policy parameters is(see A.1.1DATA COLLECTION), there only states that it is proportional to the absolute difference. Another question in A.1.1 is that 'we split the collected data using a 2-to-1 ratio for training and validation datasets' while in algorithm.2 it seems that you collect all previous data in D and use it to train, do you split current collected data or dataset D.  Could you please help clarify this? BTW, would you like to open source  codebase in the future?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1510954321521,"tcdate":1510945589314,"number":2,"cdate":1510945589314,"id":"S16EX33yG","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Comment","forum":"SJJinbWRZ","replyto":"SJY0Oo5yM","signatures":["ICLR.cc/2018/Conference/Paper719/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper719/Authors"],"content":{"title":"Re: Relationship to EPOpt?","comment":"Thank you for your question. Here is the explanation.\n\nThe use of model uncertainty to deal with the discrepancy between the model and real world dynamics is a mature idea that dates back to much earlier than the EPOpt paper. Robust MDPs [1,2], ensemble methods [3], Bayesian approaches such as PILCO [4], among others, all share the same idea - use data to estimate some form of model uncertainty, and find a policy that works well against all models in the uncertainty set, with the hope that this policy will also work well in the real world model. \n\nThe differences between these works is in the models that they learn - discrete models in [1,2], Gaussian processes in [4], and physical models with a small set of parameters in [3] and EPOpt. \n\nTo date, learning dynamics models with deep neural networks for non-trivial tasks has been notoriously hard. For example, in [5], Nagabandi et al. proposed to use model-free fine tuning after model-based training, and in [6] Gal et al. showed primary results on cartpole.\n\nThe promise in using neural network models is their expressiveness -- which can scale up to complex dynamics for which Gaussian processes are not applicable, while writing down an analytical physics model is too challenging. This would be the case, for example, in a real-world robot that needs to handle deformable objects. For such domains, writing down a parametric physical model, as was done in EPOpt, can be problematic. \n\nSo, while the main difference with prior work on model uncertainty is in our DNN model, our contribution is in showing that, for the first time, such expressive models can be used to solve challenging control tasks. This should not be waived off as a minor difference, as getting DNNs to learn useful dynamics models has been the focus of many recent studies [5,6,7].\n\nIntuitively, the challenge in model based RL with DNNs is that as the models become more expressive, the harder it becomes to control generalization errors, which the policy optimization tends to exploit (thus leading to a failure in the real-world execution). To put things in numbers, our DNN models have thousands of tunable parameters. The models in the EPOpt paper had at most 4.\n\n[1] Bagnell, J.A., Ng, A.Y. and Schneider, J.G., 2001. Solving uncertain Markov decision processes.\n[2] Nilim, A. and El Ghaoui, L., 2005. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5), pp.780-798.\n[3] Mordatch, I., Lowrey, K. and Todorov, E., 2015, September. Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on(pp. 5307-5314). IEEE.\n[4] Deisenroth, M. and Rasmussen, C.E., 2011. PILCO: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11) (pp. 465-472).\n[5] Nagabandi, A., Kahn, G., Fearing, R.S. and Levine, S., 2017. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. arXiv preprint arXiv:1708.02596.\n[6] Gal, Y., McAllister, R.T. and Rasmussen, C.E., 2016, April. Improving PILCO with bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop (Vol. 951, p. 2016).\n[7] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T. and Tassa, Y., 2015. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952).\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1510811857011,"tcdate":1510811857011,"number":3,"cdate":1510811857011,"id":"SJY0Oo5yM","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"SJJinbWRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Relationship to EPOpt?","comment":"This paper from last year's ICLR also considers an ensemble of models and uses TRPO to find a robust policy with reduced sample complexity. https://arxiv.org/abs/1610.01283\n\nCan you comment in the connections? It seems very relevant, and this line of work should be cited and discussed in the paper. Domain randomization based approaches also fall under this bucket. The only difference I see is that EPOpt uses a physics based model representation whereas DNN models are used here. However, this difference is extremely minor, since the way the model is updated is identical in both -- gradient descent or MAP (in Bayesian case). Is the method proposed here simply EPOpt with DNN function approximator for the model?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"ddate":null,"tddate":1511479017431,"tmdate":1511479030267,"tcdate":1510707424146,"number":2,"cdate":1510707424146,"id":"HJdkWGt1f","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"SytosZKJM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for your reply","comment":"Thank you for your explanation. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1510706080866,"tcdate":1510706080866,"number":1,"cdate":1510706080866,"id":"SytosZKJM","invitation":"ICLR.cc/2018/Conference/-/Paper719/Official_Comment","forum":"SJJinbWRZ","replyto":"rkm6BWFkf","signatures":["ICLR.cc/2018/Conference/Paper719/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper719/Authors"],"content":{"title":"Re: policy learning method choice","comment":"Thank you for your comment. Since we do not care about the fictitious sample complexity, we do not find that PPO consistently improves the real sample complexity. We also noticed that the hyperparameters in PPO are more tricky to tune at least in the model-based setting, whereas TRPO can work well out-of-the-box."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1510704571499,"tcdate":1510704571499,"number":1,"cdate":1510704571499,"id":"rkm6BWFkf","invitation":"ICLR.cc/2018/Conference/-/Paper719/Public_Comment","forum":"SJJinbWRZ","replyto":"SJJinbWRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"policy learning method choice","comment":"I was wondering what is the difference in results between using TRPO and PPO for policy learning. PPO seems to be more stable and sample efficient than TRPO. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]}},{"tddate":null,"ddate":null,"tmdate":1509739142713,"tcdate":1509133463263,"number":719,"cdate":1509739140054,"id":"SJJinbWRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJJinbWRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Model-Ensemble Trust-Region Policy Optimization","abstract":"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  They tend to suffer from high sample complexity, however, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks","pdf":"/pdf/f37abcda609fe36ffb95bc1a8dfc4e73f60eafe3.pdf","TL;DR":"Deep Model-Based RL that works well.","paperhash":"anonymous|modelensemble_trustregion_policy_optimization","_bibtex":"@article{\n  anonymous2018model-ensemble,\n  title={Model-Ensemble Trust-Region Policy Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJJinbWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper719/Authors"],"keywords":["model-based reinforcement learning","model ensemble","reinforcement learning","model bias"]},"nonreaders":[],"replyCount":17,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}