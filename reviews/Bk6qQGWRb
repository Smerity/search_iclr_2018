{"notes":[{"tddate":null,"ddate":null,"tmdate":1515187813660,"tcdate":1515187813660,"number":13,"cdate":1515187813660,"id":"SkCDCP6Qz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"SkMlZJ9gG","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"We thank AnonReviewer1 for a clear and constructive review. We are encouraged that you recognize the importance of the problem addressed and the novelty of the methods\n\nRegarding the posterior distribution, we apologize for being imprecise. As we noted to another reviewer, indeed this is not a true posterior but rather an approximation of the Q values that has an explicit smooth (approximate) representation of the uncertainty of the state-action values. We agree it can be off in the described situation but since BDQN runs BLR frequently and uses a moving window of replay buffer it cannot have a severe effect on BDQN performance. Indeed one interesting finding of our work is that this simple approach yields surprisingly large empirical benefits.  \n\nThanks for raising this interesting point. We actually discuss the effect of sampling frequency in the appendix. In the episodic RL, it is enough to Thompson sample the model at the beginning of each episode (theoretically, more frequent sampling does not change the existing bounds). For Atari games we use T^{sample} equal to 1000 which is at the same order of game episode horizon. In the appendix, we added a further discussion about the effect of resampling frequency, the insight about how its design, and what T_sample may be best for the RL problems with shorter or longer horizon.\n\nWe would like to encourage the reviewer to look at the latest update of the draft where we added our results on more games and currently, we have 15 games. Unfortunately, due to the high cost of deep RL experiment, we were not able to run bdqn on all the Atari games. Regarding the baseline,\nwe choose DDQN as a simple baseline that is quite similar to BDQN except in the last layer and in the fit, and we have clarified this. \nWe ran Bootstrap DQN as another baseline for 5 games, but unfortunately, despite extensive experimentation and design choices, we were not able to reproduce their results. For some games, we received the return of less than the return of random policy. For honesty and clarity, since November, we put our Bootstrap DQN implementation public as well. \nCurrently, in the TS RL literature, it is one of the biggest challenges in the community to provide a significant improvement, as promised by TS, over DDQN, you can find almost all of the TS based cited literature in our paper, they compare against dqn and ddqn. It is roughly known, also discussed and confirmed with the authors of some of these Thompson sampling works, that neither of the proposed approaches produced much beyond the modest gains of DDQN on Atari games, except, as you correctly point out the current proposed BDQN approach which provides a significant improvement over this baseline. \n\n\nRegarding the game Atlantis, the behavior of bdqn on this game was interesting for us as well.  We elaborate more on the scores of this game in the appendix. To get the score of 3.2M after 20M samples, we enforced the mentioned limit. When we removed this limit, the score gets to 62M after 15M samples. \n\nRegarding finding the optimal Q function and policy, the reviewer is right and we like to thank you also for your point on it. For the described grid world, in order to simplify the example, we assume that the game is deterministic, game horizon H is the episode length as well and only the true Q is the most optimistic function in the set. In this case, any other Q, even those which take the agent to the destination have a non zero Bellman error (defined in https://arxiv.org/abs/1610.09512), therefore the agent wants to eliminate them from the set. We have revised the statement.\n\nWe already discussed the experiments in the response to the AC. We ran the mentioned experiment again, and as AnonRev3 confirmed, the current scores are similar to the baseline. Furthermore, in our comparison tables, we compare against the scores of DDQN from its original papers during its evaluation time as well.\n\nWe apologize and we have corrected accordingly. We choose the 2008 paper due to its theoretical analysis and have updated accordingly.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515187609736,"tcdate":1515187609736,"number":12,"cdate":1515187609736,"id":"r1fs6DpXf","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"rJVfxQ2lf","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thanks for your thoughtful review of our paper. We appreciate it\n\n“Sampling from posterior”: We apologize for being imprecise. As we noted to another reviewer, we have an approximate algorithm in which we use Bayesian linear regression to fit a function to the Q values in a way that allows some uncertainty over the resulting state-action values to be fit, and therefore sampled. We have updated our discussion accordingly. Other algorithms like Bootstrapped DQN also empirically compute an approximation over the posterior of Q values. We believe that our approach gains benefit from two features: (1) from computing an exact linear regression fit to the last layer, which can be more data efficient than single step updates (though in comparing to episodic replay it may vary); and (2) from an explicit (approximate) representation of uncertainty over the Q-values that can be easily sampled and used to inform exploration. \n\nRegarding the hyperparameters, as we also noted in the area chair review, the provided hyperparameter tuning is been used for the tuning of the extra parameters of BDQN and contains a few short run of BDQN. We revised the corresponding section of the paper and made it clear that the performed HPO was simple, quick, and indication of the robustness of BDQN. About per game parameter, we use a fixed size of replay sample for BLR among all the games.\n\nWe apologize if the tables have appeared misleading. We were merely trying to illustrate the performance of BDQN vs DDQN after the same number of samples, and in addition, the BDQN in its earlier stage vs DDQN after 200M samples during its evaluation time (reduced epsilon) which is reported at the original paper. We also have been asked and advised to add the comparison to human score, samples complexity of reaching the human scores and sample complexity of reaching DDQN^\\dagger score.\n\n\nRegarding BBQ work, as we mentioned above, we believe that our approach gains benefit from two features: (1) from computing an exact linear regression fit to the last layer and (2) from an explicit (approximate) representation of uncertainty over the Q-values. In the TS deep RL literature, mostly, two lines of works for Thompson sampling have been studied. One is variational inference based method,e.g.\nEfficient Dialogue Policy Learning with BBQ-Networks\nDropout as a Bayesian Approximation\n\nAnd the second one is the confidence based, e.g.\nDeep exploration via randomized value functions\nDeep exploration via bootstrapped dqn\nThe uncertainty bellman equation and exploration\nEtc.\nWhere the objective functions in these two lines of works, as the reviewer also mentioned, are different and BDQN is in the second category. In addition, these two following papers:\nDeep exploration via bootstrapped dqn\nDropout Inference in Bayesian Neural Networks with Alpha-divergences\nArgue that how the variational inference methods can severely underestimate model uncertainty.\n\nWe appreciate the reviewer for mentioning the typo in our figure2. The figure 2 is updated and the episode count was a typo, it is the step count. We compare the computation cost of BDQN  and DDQN, but computing equation 6 just involves inverting a matrix of 512 by 512, every 100k time step, which is computationally negligible. We have a detailed discussion on the computation cost in the appendix.\n\na_\\tau is the action taken at time step \\tau, we restated it in the main text.  \n\nRegarding the use of W in the update of theta, we are grateful to the reviewer for the careful review of our paper. The concern in use of W for feature representation update is really a keen observation and we elaborated it in the appendix. As the reviewer mentioned, we should not change this W too frequently, since it forces the network to spend the network capacity for providing a feature representation which is good for the variety of different draws of W. At the same time, it provides a noisy gradient since the cost surface changes fast and prevent the model from learning. On the other hand, the update of W, used for the update of theta, should not happen barely as well since every 100K the posterior gets updated and the feature representation should not overfit to a single W, and consequently overfit to a fixed set of skills. \n\nWe thank the reviewer for the feedback on the clarity of section 5, we apologize and we have updated this section.\n\nWe added Snoek et al to our paper. Thanks for mentioning it.\n\nWe agree that reproducibility is critical and we had released our code including hyperparameter settings in November. Also, the graphs, learned models (model parameters), returns per episode, any remaining output of the experiments and the required material for reproducing the plots are provided publicly."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515185983063,"tcdate":1515185983063,"number":11,"cdate":1515185983063,"id":"HkPHDDpXG","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"HkHX9bm-M","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thank you very much for your careful and constructive feedbacks, they helped us to carve the face of our paper in a strong way. We are glad that you appreciated both the method and the clarity of exposition. \n \nWe completely agree with your comments with respect to RLSVI and Bootstrapped DQN,  we have revised the discussion accordingly and presented bdqn as a direct extension of RLSVI. \n\nYou are right about our approach is also an approximate method of a posterior. We apologize for being imprecise and we have updated our language to be more careful in our discussion and presentation of Bayesian linear regression.\n\nRegarding your point about the number of frames to run, for the game pong, we have run the experiments for a longer period of time, but in order to observe the difference between bdqn and ddqn performances, we had to plot just for the beginning of the game. For some games, we stopped when the plots reached the area around the plateau and for some games, we ran out resources to continue to 100M. \nWe would like to share with the reviewer that all the returns per episode, frame counts, and clipped reward return arrays are shared publicly and one can easily reproduce these plots and compare the score in different time steps. We also added a few more columns for the sample complexity and comparison to human scores. \n\nRegarding “running for the same number of samples”, we should mention that in the reported tables, the scores in the first column (BDQN) and the second column (DDQN) are the scores after the same number of samples (provided in the last column). For each game, both bdqn and ddqn are run for the same number of samples. The DDQN^\\dagger is also the score of DDQN reported in the original DDQN paper during the evaluation phase.\n\nRegarding the baseline, as you mentioned, the current plots and scores are updated.\n\nRegarding re-deviation of BLR, we tried to balance between deriving the standard method while making the paper self-contained. We received feedback that it is helpful to have BLR derivation, e.g. Rev2  suggested to elaborate more on BLR part.\n\n\nAgain thanks to your feedback, we made another constructive change, with help of other TS researchers, in the presentation of our paper and overview of related work. \nWe start off with PSRL, and how randomised value function and  RLSVI leveraged it. Then how bootstrap dqn extend the idea to deep learning, followed by the noisy net, bbq, shallow UBE and LS-DQN. Finally, we explain that bdqn is an extension to RLSVI and follows the same idea. \n\nStill, we would be happy to get more feedback from you to further improve our paper.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515183854005,"tcdate":1515183854005,"number":10,"cdate":1515183854005,"id":"S1Ux1vpmG","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"HkUmeK2WG","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Empirical scores of DDQN","comment":"Area Chair:\nThanks for the review! We really appreciate it. \n\nRegarding the score of DDQN, we ran the experiments, e.g. for the game Pong, again and reported the result in the current draft. As the AnonReviewer1 also mentioned after visiting the current draft, the current results are similar to the original paper. We are grateful to all the reviewers for their constructive suggestion and we believe that it made the paper stronger.\n\nIt would be helpful to mention that, on the score tables in the main draft, we also compared against the score of DDQN^\\dagger, which is the reported score of DDQN from the original DDQN paper (copied) during the evaluation phase where evaluation epsilon is set to 0.001 after 200M samples. For BDQN, we did not design any evaluation phase. We already elaborated it more in the main text in order to make it more clear.\n\n\nFor the choice of hyper parameters, we used the hyper parameters used in DQN and DDQN, which are tuned through an exhaustive hyperparameter tuning procedure in the original papers for these algorithms.\n\nRegarding the hyperparameter tuning of extra parameters of BDQN, one of the main reason why we talked about it in the main text, aside from being honest about BDQN, was to deliver the point that it is not exhaustive and it is a simple tuning procedure as another proof of the superiority of Thompson sampling over epsilon-greedy. The whole process of hyperparameter tuning, including coding, contains a few runs of BDQN, each for a few hours. We stated that a further hyperparameter tuning can be done for BDQN to provide even more exciting results. We, already, discussed it in the main draft.\n\nThe BDQN code is available to the public and is online since November. To preserve double-blind status, we won't post the GitHub link here but it's not too hard to find.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515182060344,"tcdate":1515182060344,"number":9,"cdate":1515182060344,"id":"Hy4eOLaXM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"BkisdVq7f","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"LS-DQN","comment":"Thanks for the thoughtful review, follow-up of our paper and thank you for mentioning this interesting and related work. This work, which just came out recently and appeared at NIPS, is interestingly very similar to our last layer regression narrative and we already included a discussion about it in our paper. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515181762414,"tcdate":1515181762414,"number":8,"cdate":1515181762414,"id":"HJ5TIU6Qz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"General reply to reviewers and area chair","comment":"Dear reviewers and area chair\n\nWe would like to thank the reviewers and area chair for their thoughtful responses to our paper. We are grateful to each of you for critical suggestions that helped us to significantly improve our paper. Please find individual replies to each of the reviews in the respective threads.\n\nFurthermore, since the Area Chair provided the abstract of the main concerns in the reviews, we would like to ask all the reviewers to consider looking at the area chair’s review and our reply.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1514977443404,"tcdate":1514977443404,"number":7,"cdate":1514977443404,"id":"BkisdVq7f","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/AnonReviewer3"],"content":{"title":"[Late] another paper to discuss / cite","comment":"\"Shallow Updates for Deep Reinforcmeent Learning\"\nhttps://arxiv.org/pdf/1705.07461.pdf\n\nThis work on LS-DQN follows a (relatively) similar narrative of using a Least-Squares RL on the last layer of DQN.\nYou might consider that this paper on Bayesian DQN is similar to an RLSVI-version of LS-DQN.\n\nI was not aware of this until recently."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1513649095270,"tcdate":1513649095270,"number":6,"cdate":1513649095270,"id":"rk1A7xIGz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"B1Y_pIHfM","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Detailed uncertainty approximation","comment":"Thank you for helping us in addressing this comment. We agree that adding a further discussion in a detailed comparison of UBE and BDQN makes the current draft stronger. \n\nAs the reviewer3 mentioned, UBE provides a Bellman-like equation for uncertainty and learns a shallow network in order to approximate the uncertainty. While in BDQN, the uncertainty is approximated using BLR. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1513610608995,"tcdate":1513610608995,"number":5,"cdate":1513610608995,"id":"B1Y_pIHfM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"ryfx6L7fM","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/AnonReviewer3"],"content":{"title":"This doesn't feel like a convincing answer to me","comment":"Both of these algorithms use a linear approximation to the final layer of a neural network and the covariance matrix (X^T X) to quantify confidence sets.\n\nThe question of whether you term this \"Bayesian\" or \"Frequentist\" uncertainty feels misleading and a little pretentious. In both cases they are being used to guide an agent's exploration, the epistemological roots of Bayes/Frequency don't seem like the pressing issue.\n\nThe relevant issue is how to propagate uncertainty over the value function over multiple time steps. One does this via a sampling procedure and the other by attempting to \"learn\" an approximating neural network from \"shallow\" = one step uncertainty. This would be good to discuss in the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1513479452731,"tcdate":1513479402186,"number":4,"cdate":1513479402186,"id":"ryfx6L7fM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"B1MgfN_WM","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Different source of uncertainties","comment":"Thanks for your interest in our paper!\nThe mentioned paper is an interesting line of work on uncertainty measure for exploration but the source of uncertainty in this paper is different from BDQN. As described in both papers, if one approximates Q(x , a), the mean of random return in state x after taking action a, as a linear function of features\n\nQ(x,a) = w*\\phi\n\nthen the random return is distributed as follows\n\nQ(x,a)+\\epsilon = w^\\top\\phi+epsilon\n\nFrom a frequentist perspective, given the data, one can minimize bellman residual (a mean square error in this setting), find a fixed point of it and estimate w^*. Due to the noise \\epsilon in the random return and the approximated generative model, the estimated w^* has a frequentist uncertainty and the authors use this uncertainty to randomize over the actions. As it is well known in the linear regression setting, if the noise gets big, the confidence interval gets big as well.\n\nOn the other hand, in our setting, the source of uncertainty comes from the agent posterior belief on the Q function and the Thompson sampling is applied over approximated posterior distribution. We approximate the generative model with the Bayesian approach where the parameter w is assigned a prior. The uncertainty on the Q function comes from the posterior belief of w (Eq6) where the randomness in the return is captured by Eq7. Our uncertainty comes from belief in w which is constructed from both randomnesses in return and the prior belief in w. \n\nTo be more abstract, the mentioned paper exploits the frequentist uncertainty while in our setting we exploit Bayesian belief to construct the uncertainty.\n\nCheers,\nAuthors\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1513029743120,"tcdate":1513029662088,"number":3,"cdate":1513029662088,"id":"HkUmeK2WG","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/Area_Chair"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Area_Chair"],"content":{"title":"Lack of empirical rigor?","comment":"All the reviews seem to question the empirical rigor of this work.  AnonReviewer3 commented that the implemented baseline DDQN didn't seem to match prior work.  AnonReviewer2 also had concerns that the hyperparameter tuning of the baseline DDQN was weaker than their method.  Similarly AnonReviewer1 asked for stronger validation and stronger baselines and points out e.g. \"It’s a bit suspicious that it doesn’t achieve 20+ in Pong\". \n\nIn light of recent revelations in deep reinforcement learning (i.e. https://arxiv.org/pdf/1709.06560.pdf), this seems like a significant issue that is prevalent.  Could the reviewers and authors comment about whether they feel that the empirical evidence presented in this work is strong enough to justify that this paper should not be subject to the criticism presented in the aforementioned paper?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1512747497874,"tcdate":1512747497874,"number":3,"cdate":1512747497874,"id":"B1MgfN_WM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Public_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Related works in the literature","comment":"I think this is a very nice paper. I wanted to ask you about an interesting connection between this work and another recent work on RL exploring (https://arxiv.org/pdf/1709.05380.pdf), in particular the use of the last layer NN statistics to generate uncertainty combined with Thompson sampling. Can you comment on the differences and similarities between your work and this? Thanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515642515054,"tcdate":1512409628860,"number":3,"cdate":1512409628860,"id":"HkHX9bm-M","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Review","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting approach for Thompson sampling in DQN, some concerns over baseline.","rating":"5: Marginally below acceptance threshold","review":"(Last minute reviewer brought in as a replacement).\n\nThis paper proposed \"Bayesian Deep Q-Network\" as an approach for exploration via Thompson sampling in deep RL.\nThis algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.\nThe agent then samples from this posterior for an approximate Thompson sampling.\nExperimental results show that this outperforms an epsilon-greedy baseline.\n\nThere are several things to like about this paper:\n- The problem of efficient exploration with deep RL is important and under-served by practical algorithms. This seems like a good algorithm in many ways.\n- The paper is mostly clear and well written.\n- The experimental results are impressive in their outperformance.\n\nHowever, there are also some issues, many of which have already been raised:\n- The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example).\n- There are some loose and misleading descriptions of the algorithm computing \"the posterior\" when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away.\n- The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI. The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph. RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.\n- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.\n\nOverall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network.\nHowever, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach.\nFor example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example).\n\nIn it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly.\n\n\n===============================================================\n\nRevising my review following the rebuttal period and also the (ongoing) revisions to the paper.\n\nI've been disappointed by the authors have incorporated the feedback/reviews - I expected something a little more clear / honest. Given the ongoing review decisions/issues I'm putting my review slightly below accept.\n\n## Relation to literature on \"randomized value functions\"\nIt's really wrong to present BDQN as is if it's the first attempt at large-scale approximations to Thompson sampling (and then slip in a citation to RLSVI as a BDQN-like algorithm). This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this. Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm! If you say something like \"they did not estimate a true posterior\" then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior. In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN.\n\n## Quality/science of experiments\nThe experimental results have been updated, and the performance of the baseline now seems much more reasonable. However, the procedure for \"selecting arbitrary number of frames\" to report performance seems really unnecessary... it would be clear that BDQN is outperforming DDQN... you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific. This type of stuff smells like overfitting!","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515642515094,"tcdate":1511956491623,"number":2,"cdate":1511956491623,"id":"rJVfxQ2lf","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Review","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea but lack of baselines, strong theoretical justification and reference to previous work","rating":"5: Marginally below acceptance threshold","review":"\nThe authors describe how to use Bayesian neural networks with Thompson sampling\nfor efficient exploration in q-learning. The Bayesian neural networks are only\nBayesian in the last layer. That is, the authors learn all the previous layers\nby finding point estimates. The Bayesian learning of the last layer is then\ntractable since it consists of a linear Gaussian model. The resulting method is\ncalled BDQL. The experiments performed show that the proposed approach, after\nhyper-parameter tuning, significantly outperforms the epsilon-greedy\nexploration approaches such as DDQN.\n\nQuality:\n\nI am very concern about the authors stating on page 1 \"we sample from the\nposterior on the set of Q-functions\". I believe this statement is not correct.\nThe Bayesian posterior distribution is obtained by combining an assumed\ngenerative model for the data, data sampled from that model and some prior\nassumptions. In this paper there is no generative model for the data and the\ndata obtained is not actually sampled from the model. The data are just targets\nobtained by the q-learning rule. This means that the authors are adapting\nQ-learning methods so that they look Bayesian, but in no way they are dealing\nwith a principled posterior distribution over Q-functions. At least this is my\nopinion, I would like to encourage the authors to be more precise and show in\nthe paper what is the exact posterior distribution over Q-functions and show\nhow they approximate that distribution, taking into account that a posterior\ndistribution is obtained as $p(theta|D) \\propto p(D|theta)p(\\theta)$. In the\ncase addressed in the paper, what is the likelihood $p(D|\\theta)$ and what are\nthe modeling assumptions that explain how $D$ is generated by sampling from a\nmodel parameterized by \\theta?\n\nI am also concerned about the hyper-parameter tuning for the baselines. In\nsection 5 (choice of hyper-parameters) the authors describe a quite exhaustive\nhyper-parameter tuning procedure for BDQL. However, they do not mention whether\nthey perform a similar hyper-parameter tuning for DDQN, in particular for the\nparameter epsilon which will determine the amount of exploration. This makes me\nwonder if the comparison in table 2 is fair. Especially, because the authors\ntune the amount of data from the replay-buffer that is used to update their\nposterior distribution. This will have the effect of tuning the width of their\nposterior approximation which is directly related to the amount of exploration\nperformed by Thompson sampling. You can, therefore, conclude that the authors are\ntuning the amount of exploration that they perform on each specific problem.\nIs that also being done for the baseline DDQN, for example, by tuning epsilon in\neach problem?\n\nThe authors also report in table 2 the scores obtained for DDQN by Osband et\nal. 2016. What is the purpose of including two rows in table 2 with the same\nmethod? It feels a bit that the authors want to hide the fact that they only\ncompare with a singe epsilon-greedy baseline (DDQN). Epsilon-greedy methods\nhave already been shown to be less efficient than Bayesian methods with\nThompson sampling for exploration in q learning (Lipton et al. 2016).\n\nThe authors do not compare with variational approaches to Bayesian learning\n(Lipton et al. 2016). They indicate that since Lipton et al. \"do not\ninvestigate the Atari games, we are not able to have their method as an\nadditional baseline\". This statement seems completely unjustified. The authors\nshould clearly include a description of why Lipton's approach cannot be applied\nto the Atari games or include it as a baseline. \n\nThe method proposed by the authors is very similar to Lipton's approach. The\nonly difference is that Lipton et al. use variational inference with a\nfactorized Gaussian distribution to approximate the posterior on all the\nnetwork weights. The authors by contrast, perform exact Bayesian inference, but\nonly on the last layer of their neural network. It would be very useful to know\nwhether the exact linear Gaussian model in the last layer proposed by the\nauthors has advantages with respect to a variational approximation on all the\nnetwork weights. If Lipton's method would be expensive to apply to large-scale\nsettings such as the Atari games, the authors could also compare with that\nmethod in smaller and simpler problems.\n\nThe plots in Figure 2 include performance in terms of episodes. However, it\nwould also be useful to know how much is the extra computational costs of\nthe proposed method. One could imagine that computing the posterior\napproximation in equation 6 has some additional cost. How do BDQN and DDQN\ncompare when one takes into account running time and not episode count into\naccount?\n\nClarity:\n\nThe paper is clearly written. However, I found a lack of motivation for the\nspecific design choices made to obtain equations 9 and 10. What is a_t in\nequation 9? The parameters \\theta are updated just after equation 10 by\nfollowing the gradient of the loss in which the weights of the last layer are\nfixed to a posterior sample, instead of the posterior mean. Is this update rule\nguaranteed to produce convergence of \\theta? I could imagine that at different\ntimes, different posterior samples of the weights will be used to compute the\ngradients. Does this create any instability in learning? \n\nI found the paragraph just above section 5 describing the maze-like\ndeterministic game confusing and not very useful. The authors should improve\nthis paragraph.\n\nOriginality:\n\nThe proposed approach in which the weights in the last layer of the neural\nnetwork are the only Bayesian ones is not new. The same method was proposed in\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... &\nAdams, R. (2015, June). Scalable Bayesian optimization using deep neural\nnetworks. In International Conference on Machine Learning (pp. 2171-2180).\n\nwhich the authors fail to cite. The use of Thompson sampling for efficient\nexploration in deep Q learning is also not new since it has been proposed by\nLipton et al. 2016. The main contribution of the paper is to combine these two\nmethods (equations 6-10) and evaluate the results in the large-scale setting of\nATARI games, showing that it works in practice.\n\nSignificance:\n\nIt is hard to determine how significant the work is since the authors only\ncompare with a single baseline and leave aside previous work on efficient\nexploration with Thompson sampling based on variational approximations.\n\nAs far as the method is described, I believe it would be impossible to\nreproduce their results because of the complexity of the hyper-parameter tuning\nperformed by the authors. I would encourage the authors to release code that can\ndirectly generate Figure 2 and table 2.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511860526666,"tcdate":1511860501223,"number":2,"cdate":1511860501223,"id":"BJpzYoqlz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"rJnxeAYeM","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"RLSVI","comment":"Hi Ian\nThank you for your comment. We are aware of your work on RLSVI, and we agree that if we make the BDQN episodic and do not update the feature representation it is exactly RLSVI. We will elaborate it more on the main draft. \n\nRegarding using RLSVI or Bayesian regression at each layer, that is an interesting extension to BDQN and we have left it for the future work. \n\nFor the revision, we are adding a further plot to the game Atlantis. In DDQN paper ‘’Deep Reinforcement Learning with Double Q-learning’’, the score for this game is 65k and you can notice that, Fig. 2, the BDQN agent suddenly starts to learn a significantly better policy which gives an average score of 3.2M, then stays there, with no improvement. We investigated this stopping in the improvement by looking at the episode length. We realized that the agent reaches the maximum episode length limit of openaigym which is 100k. After removing this limit, surprisingly it got a score of 62M after 15M samples which is almost 1000x higher than the reported one in DDQN paper.\n\nAbout resampling from posterior of W, actually in the older version of the algorithm, there was another parameter \\tilde{W} which was sampled from the same distribution as W, but more frequently (every \\tilde{T} time step). We used \\tilde{W} to make decisions and used W (samples every T^{sample}) in order to update the feature representation in Bellman residual equation (line 14 in the Alg). We tried \\tilde{T} of 1, 10, 100, and  T^{sample} time steps for game of Assault during the hyperparameter tuning period, but did not observe any significant difference. That’s why we, for simplicity, removed it from the setting and just kept W. It looks for Atari games, sampling more frequency does not make much difference (T^{sample} is in the same (or less) order as episode length (H) for many Atari games), but for the RL problems with shorter horizon and especially deterministic transition, we believe it makes a difference. We are adding a further discussion in the appendix about sampling frequency of \\tilde{W} and address how crucial the choice of \\tilde{T} could be in different RL settings.\n\nCheers\nAuthors \n\n“To preserve double-blind status, we won't post the GitHub link here.”\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511861494283,"tcdate":1511847988774,"number":1,"cdate":1511847988774,"id":"H1pN_dqlz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"ByuOaTteM","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Baseline","comment":"Thanks for the comment. For the baseline, we used the implementation described in DDQN paper ‘’Deep Reinforcement Learning with Double Q-learning’’, and the available code in \nhttps://github.com/kazizzad/Double-DQN-MxNet-Gluon.git\nSince there's always also a variance across runs, we ran the code again, where we got a score close to what mentioned in DDQN paper. We'll update the plot for the revision. Thanks for mentioning it.\n\nCheers,\nAuthors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515642515136,"tcdate":1511809257556,"number":1,"cdate":1511809257556,"id":"SkMlZJ9gG","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Review","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A nice algorithm. Decent preliminary results, but some more validation would be welcome.","rating":"6: Marginally above acceptance threshold","review":"The authors propose a new algorithm for exploration in Deep RL. They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action. Posterior weights are sampled to select actions during execution (Thompson Sampling style). I generally liked the paper and the approach, here are some more detailed comments.\n\nUnlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets. It’s not immediately clear what the semantics of this posterior are then. Take for example the case where a particular transition (s,a,r,s’) gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced. \n\nPrevious applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode. But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling? It would be useful to comment on that aspect.\n\nThe method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN). DDQN wasn’t proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods). The authors argue they could not reproduce Osband’s bootstrapped DQN, which is also TS-based, but you could at least have reported their scores.  \n\nOn these games versus (their implementation of) DDQN, the results seem encouraging. But it would be good to know whether the approach works well across games and is competitive against other stronger baselines. Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice.\n\nIn addition, your reported score on Atlantis of ~2M seems too big. Did you cap the max episode time to 30mins? As is done in the baselines usually.\n\n\nMinor things:\n-“TS finds the true Q-function very fast” But that contradicts the previous statements, I think you mean something different. If TS does not select certain actions, the Q-function would not be updated for these actions. It might find the optimal policy quickly though, even though it doesn’t resolve the entire value function completely.\n-Which epsilon did you use for evaluation of DDQN in the experiments? It’s a bit suspicious that it doesn’t achieve 20+ in Pong.\n-The history of how to go from a Bellman equation to a sample-based update seems a bit distorted. Sample-based RL did not originate in 2008. Also, DQN does not optimize the Bellman residual, it’s a TD update. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511804916790,"tcdate":1511804916790,"number":2,"cdate":1511804916790,"id":"rJnxeAYeM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Public_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["~Ian_Osband1"],"readers":["everyone"],"writers":["~Ian_Osband1"],"content":{"title":"Connection to RLSVI","comment":"Cool work!\n\nI wanted to highlight a deeper connection between your work and the algorithm RLSVI that you already cite, but maybe didn't realize the deeper connection: https://arxiv.org/pdf/1402.0635.pdf.\n\nIf you run BDQN with a linear architecture and T^{sample} = H finite horizon problem, my understanding is that BDQN is exactly the same as RLSVI. Certainly RLSVI is presented in that paper for a linear architecture, but the general approach of Randomized Least Squares Value Iteration is not specific to that architecture https://searchworks.stanford.edu/view/11891201.\n\nIt is very interesting though that you get better performance using this \"last-layer\" approach to RLSVI, rather than something like Bootstrap/Ensemble. Maybe one way to present this is as an effective way to extend RLSVI to multi-layer architectures.\n\nBy the way, I find it surprising that resampling the noisy W in this way so infrequently is not simply learned away by the SGD... can you comment on this?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511804273087,"tcdate":1511804273087,"number":1,"cdate":1511804273087,"id":"ByuOaTteM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Public_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Question marks on the baseline","comment":"Your baseline results of DDQN seem strange to me, particularly the result on Pong.\nIt seems like these results are quite different from (for example) https://github.com/openai/baselines\n\nCan you comment on this?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1515188137646,"tcdate":1509135253282,"number":810,"cdate":1509739086346,"id":"Bk6qQGWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/b8252cd7f7fce3fe5d9f9f1c54a91e8ff44cf650.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]},"nonreaders":[],"replyCount":19,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}