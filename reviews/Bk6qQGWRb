{"notes":[{"tddate":null,"ddate":null,"tmdate":1512409628860,"tcdate":1512409628860,"number":3,"cdate":1512409628860,"id":"HkHX9bm-M","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Review","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting approach for Thompson sampling in DQN, some concerns over baseline.","rating":"6: Marginally above acceptance threshold","review":"(Last minute reviewer brought in as a replacement).\n\nThis paper proposed \"Bayesian Deep Q-Network\" as an approach for exploration via Thompson sampling in deep RL.\nThis algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.\nThe agent then samples from this posterior for an approximate Thompson sampling.\nExperimental results show that this outperforms an epsilon-greedy baseline.\n\nThere are several things to like about this paper:\n- The problem of efficient exploration with deep RL is important and under-served by practical algorithms. This seems like a good algorithm in many ways.\n- The paper is mostly clear and well written.\n- The experimental results are impressive in their outperformance.\n\nHowever, there are also some issues, many of which have already been raised:\n- The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example).\n- There are some loose and misleading descriptions of the algorithm computing \"the posterior\" when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away.\n- The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI. The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph. RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.\n- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.\n\nOverall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network.\nHowever, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach.\nFor example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example).\n\nIn it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1512222774535,"tcdate":1511956491623,"number":2,"cdate":1511956491623,"id":"rJVfxQ2lf","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Review","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea but lack of baselines, strong theoretical justification and reference to previous work","rating":"5: Marginally below acceptance threshold","review":"\nThe authors describe how to use Bayesian neural networks with Thompson sampling\nfor efficient exploration in q-learning. The Bayesian neural networks are only\nBayesian in the last layer. That is, the authors learn all the previous layers\nby finding point estimates. The Bayesian learning of the last layer is then\ntractable since it consists of a linear Gaussian model. The resulting method is\ncalled BDQL. The experiments performed show that the proposed approach, after\nhyper-parameter tuning, significantly outperforms the epsilon-greedy\nexploration approaches such as DDQN.\n\nQuality:\n\nI am very concern about the authors stating on page 1 \"we sample from the\nposterior on the set of Q-functions\". I believe this statement is not correct.\nThe Bayesian posterior distribution is obtained by combining an assumed\ngenerative model for the data, data sampled from that model and some prior\nassumptions. In this paper there is no generative model for the data and the\ndata obtained is not actually sampled from the model. The data are just targets\nobtained by the q-learning rule. This means that the authors are adapting\nQ-learning methods so that they look Bayesian, but in no way they are dealing\nwith a principled posterior distribution over Q-functions. At least this is my\nopinion, I would like to encourage the authors to be more precise and show in\nthe paper what is the exact posterior distribution over Q-functions and show\nhow they approximate that distribution, taking into account that a posterior\ndistribution is obtained as $p(theta|D) \\propto p(D|theta)p(\\theta)$. In the\ncase addressed in the paper, what is the likelihood $p(D|\\theta)$ and what are\nthe modeling assumptions that explain how $D$ is generated by sampling from a\nmodel parameterized by \\theta?\n\nI am also concerned about the hyper-parameter tuning for the baselines. In\nsection 5 (choice of hyper-parameters) the authors describe a quite exhaustive\nhyper-parameter tuning procedure for BDQL. However, they do not mention whether\nthey perform a similar hyper-parameter tuning for DDQN, in particular for the\nparameter epsilon which will determine the amount of exploration. This makes me\nwonder if the comparison in table 2 is fair. Especially, because the authors\ntune the amount of data from the replay-buffer that is used to update their\nposterior distribution. This will have the effect of tuning the width of their\nposterior approximation which is directly related to the amount of exploration\nperformed by Thompson sampling. You can, therefore, conclude that the authors are\ntuning the amount of exploration that they perform on each specific problem.\nIs that also being done for the baseline DDQN, for example, by tuning epsilon in\neach problem?\n\nThe authors also report in table 2 the scores obtained for DDQN by Osband et\nal. 2016. What is the purpose of including two rows in table 2 with the same\nmethod? It feels a bit that the authors want to hide the fact that they only\ncompare with a singe epsilon-greedy baseline (DDQN). Epsilon-greedy methods\nhave already been shown to be less efficient than Bayesian methods with\nThompson sampling for exploration in q learning (Lipton et al. 2016).\n\nThe authors do not compare with variational approaches to Bayesian learning\n(Lipton et al. 2016). They indicate that since Lipton et al. \"do not\ninvestigate the Atari games, we are not able to have their method as an\nadditional baseline\". This statement seems completely unjustified. The authors\nshould clearly include a description of why Lipton's approach cannot be applied\nto the Atari games or include it as a baseline. \n\nThe method proposed by the authors is very similar to Lipton's approach. The\nonly difference is that Lipton et al. use variational inference with a\nfactorized Gaussian distribution to approximate the posterior on all the\nnetwork weights. The authors by contrast, perform exact Bayesian inference, but\nonly on the last layer of their neural network. It would be very useful to know\nwhether the exact linear Gaussian model in the last layer proposed by the\nauthors has advantages with respect to a variational approximation on all the\nnetwork weights. If Lipton's method would be expensive to apply to large-scale\nsettings such as the Atari games, the authors could also compare with that\nmethod in smaller and simpler problems.\n\nThe plots in Figure 2 include performance in terms of episodes. However, it\nwould also be useful to know how much is the extra computational costs of\nthe proposed method. One could imagine that computing the posterior\napproximation in equation 6 has some additional cost. How do BDQN and DDQN\ncompare when one takes into account running time and not episode count into\naccount?\n\nClarity:\n\nThe paper is clearly written. However, I found a lack of motivation for the\nspecific design choices made to obtain equations 9 and 10. What is a_t in\nequation 9? The parameters \\theta are updated just after equation 10 by\nfollowing the gradient of the loss in which the weights of the last layer are\nfixed to a posterior sample, instead of the posterior mean. Is this update rule\nguaranteed to produce convergence of \\theta? I could imagine that at different\ntimes, different posterior samples of the weights will be used to compute the\ngradients. Does this create any instability in learning? \n\nI found the paragraph just above section 5 describing the maze-like\ndeterministic game confusing and not very useful. The authors should improve\nthis paragraph.\n\nOriginality:\n\nThe proposed approach in which the weights in the last layer of the neural\nnetwork are the only Bayesian ones is not new. The same method was proposed in\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... &\nAdams, R. (2015, June). Scalable Bayesian optimization using deep neural\nnetworks. In International Conference on Machine Learning (pp. 2171-2180).\n\nwhich the authors fail to cite. The use of Thompson sampling for efficient\nexploration in deep Q learning is also not new since it has been proposed by\nLipton et al. 2016. The main contribution of the paper is to combine these two\nmethods (equations 6-10) and evaluate the results in the large-scale setting of\nATARI games, showing that it works in practice.\n\nSignificance:\n\nIt is hard to determine how significant the work is since the authors only\ncompare with a single baseline and leave aside previous work on efficient\nexploration with Thompson sampling based on variational approximations.\n\nAs far as the method is described, I believe it would be impossible to\nreproduce their results because of the complexity of the hyper-parameter tuning\nperformed by the authors. I would encourage the authors to release code that can\ndirectly generate Figure 2 and table 2.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511860526666,"tcdate":1511860501223,"number":2,"cdate":1511860501223,"id":"BJpzYoqlz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"rJnxeAYeM","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"RLSVI","comment":"Hi Ian\nThank you for your comment. We are aware of your work on RLSVI, and we agree that if we make the BDQN episodic and do not update the feature representation it is exactly RLSVI. We will elaborate it more on the main draft. \n\nRegarding using RLSVI or Bayesian regression at each layer, that is an interesting extension to BDQN and we have left it for the future work. \n\nFor the revision, we are adding a further plot to the game Atlantis. In DDQN paper ‘’Deep Reinforcement Learning with Double Q-learning’’, the score for this game is 65k and you can notice that, Fig. 2, the BDQN agent suddenly starts to learn a significantly better policy which gives an average score of 3.2M, then stays there, with no improvement. We investigated this stopping in the improvement by looking at the episode length. We realized that the agent reaches the maximum episode length limit of openaigym which is 100k. After removing this limit, surprisingly it got a score of 62M after 15M samples which is almost 1000x higher than the reported one in DDQN paper.\n\nAbout resampling from posterior of W, actually in the older version of the algorithm, there was another parameter \\tilde{W} which was sampled from the same distribution as W, but more frequently (every \\tilde{T} time step). We used \\tilde{W} to make decisions and used W (samples every T^{sample}) in order to update the feature representation in Bellman residual equation (line 14 in the Alg). We tried \\tilde{T} of 1, 10, 100, and  T^{sample} time steps for game of Assault during the hyperparameter tuning period, but did not observe any significant difference. That’s why we, for simplicity, removed it from the setting and just kept W. It looks for Atari games, sampling more frequency does not make much difference (T^{sample} is in the same (or less) order as episode length (H) for many Atari games), but for the RL problems with shorter horizon and especially deterministic transition, we believe it makes a difference. We are adding a further discussion in the appendix about sampling frequency of \\tilde{W} and address how crucial the choice of \\tilde{T} could be in different RL settings.\n\nCheers\nAuthors \n\n“To preserve double-blind status, we won't post the GitHub link here.”\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511861494283,"tcdate":1511847988774,"number":1,"cdate":1511847988774,"id":"H1pN_dqlz","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Comment","forum":"Bk6qQGWRb","replyto":"ByuOaTteM","signatures":["ICLR.cc/2018/Conference/Paper810/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper810/Authors"],"content":{"title":"Baseline","comment":"Thanks for the comment. For the baseline, we used the implementation described in DDQN paper ‘’Deep Reinforcement Learning with Double Q-learning’’, and the available code in \nhttps://github.com/kazizzad/Double-DQN-MxNet-Gluon.git\nSince there's always also a variance across runs, we ran the code again, where we got a score close to what mentioned in DDQN paper. We'll update the plot for the revision. Thanks for mentioning it.\n\nCheers,\nAuthors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1512222774579,"tcdate":1511809257556,"number":1,"cdate":1511809257556,"id":"SkMlZJ9gG","invitation":"ICLR.cc/2018/Conference/-/Paper810/Official_Review","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference/Paper810/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A nice algorithm. Decent preliminary results, but some more validation would be welcome.","rating":"6: Marginally above acceptance threshold","review":"The authors propose a new algorithm for exploration in Deep RL. They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action. Posterior weights are sampled to select actions during execution (Thompson Sampling style). I generally liked the paper and the approach, here are some more detailed comments.\n\nUnlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets. It’s not immediately clear what the semantics of this posterior are then. Take for example the case where a particular transition (s,a,r,s’) gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced. \n\nPrevious applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode. But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling? It would be useful to comment on that aspect.\n\nThe method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN). DDQN wasn’t proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods). The authors argue they could not reproduce Osband’s bootstrapped DQN, which is also TS-based, but you could at least have reported their scores.  \n\nOn these games versus (their implementation of) DDQN, the results seem encouraging. But it would be good to know whether the approach works well across games and is competitive against other stronger baselines. Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice.\n\nIn addition, your reported score on Atlantis of ~2M seems too big. Did you cap the max episode time to 30mins? As is done in the baselines usually.\n\n\nMinor things:\n-“TS finds the true Q-function very fast” But that contradicts the previous statements, I think you mean something different. If TS does not select certain actions, the Q-function would not be updated for these actions. It might find the optimal policy quickly though, even though it doesn’t resolve the entire value function completely.\n-Which epsilon did you use for evaluation of DDQN in the experiments? It’s a bit suspicious that it doesn’t achieve 20+ in Pong.\n-The history of how to go from a Bellman equation to a sample-based update seems a bit distorted. Sample-based RL did not originate in 2008. Also, DQN does not optimize the Bellman residual, it’s a TD update. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511804916790,"tcdate":1511804916790,"number":2,"cdate":1511804916790,"id":"rJnxeAYeM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Public_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["~Ian_Osband1"],"readers":["everyone"],"writers":["~Ian_Osband1"],"content":{"title":"Connection to RLSVI","comment":"Cool work!\n\nI wanted to highlight a deeper connection between your work and the algorithm RLSVI that you already cite, but maybe didn't realize the deeper connection: https://arxiv.org/pdf/1402.0635.pdf.\n\nIf you run BDQN with a linear architecture and T^{sample} = H finite horizon problem, my understanding is that BDQN is exactly the same as RLSVI. Certainly RLSVI is presented in that paper for a linear architecture, but the general approach of Randomized Least Squares Value Iteration is not specific to that architecture https://searchworks.stanford.edu/view/11891201.\n\nIt is very interesting though that you get better performance using this \"last-layer\" approach to RLSVI, rather than something like Bootstrap/Ensemble. Maybe one way to present this is as an effective way to extend RLSVI to multi-layer architectures.\n\nBy the way, I find it surprising that resampling the noisy W in this way so infrequently is not simply learned away by the SGD... can you comment on this?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1511804273087,"tcdate":1511804273087,"number":1,"cdate":1511804273087,"id":"ByuOaTteM","invitation":"ICLR.cc/2018/Conference/-/Paper810/Public_Comment","forum":"Bk6qQGWRb","replyto":"Bk6qQGWRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Question marks on the baseline","comment":"Your baseline results of DDQN seem strange to me, particularly the result on Pong.\nIt seems like these results are quite different from (for example) https://github.com/openai/baselines\n\nCan you comment on this?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]}},{"tddate":null,"ddate":null,"tmdate":1509739089008,"tcdate":1509135253282,"number":810,"cdate":1509739086346,"id":"Bk6qQGWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bk6qQGWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Efficient Exploration through Bayesian   Deep Q-Networks","abstract":"We propose Bayesian Deep Q-Network  (BDQN), a  practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, DDQN.","pdf":"/pdf/1d1cb39194766a3769f802860ec9205f18260390.pdf","TL;DR":"Using Bayesian regression to estimate the posterior over Q-functions and deploy Thompson Sampling as a targeted exploration strategy with efficient trade-off the exploration and exploitation","paperhash":"anonymous|efficient_exploration_through_bayesian_deep_qnetworks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Exploration through Bayesian   Deep Q-Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk6qQGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper810/Authors"],"keywords":["Deep RL","Thompson Sampling","Posterior update"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}