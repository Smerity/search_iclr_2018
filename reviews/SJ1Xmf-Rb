{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222769558,"tcdate":1512047006629,"number":3,"cdate":1512047006629,"id":"HkvoWK6ef","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Review","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Suprizingly good results with an rather simple architecture. Is the comparison to SotA fair??","rating":"6: Marginally above acceptance threshold","review":"\nThis paper addresses the problem of incremental class learning with brain inspired memory system. This relies on 1/ hippocampus like system relying on a temporary memory storage and probabilistic neural network classifier, 2/ a prefrontal cortex-like ladder network architecture, performing joint autoencoding and classification, 3/ an amygdala-like classifier that combines the decision of both structures. The experiments suggests that the approach performs better than state-of-the-art incremental learning approaches, and approaches offline learning.\nThe paper is well written. The main issue I have with the approach is the role of the number of examples stored in hippocampus and its implication for the comparison to state-of-the art approaches.\nComments:\nIt seems surprising to me that the network manages to outperform other approaches using such a simplistic network for hippocampus (essentially a Euclidian distance based classifier). I assume that the great performance is due to the fact that a lot of examples per classes are stored in hippocampus. I could not find an investigation of the effect of this number on the performance. I assume this number corresponds to the mini-batch size (450). I would like that the authors elaborate on how fair is the comparison to methods such as iCaRL, which store very little examples per classes according to Fig. 2. I assume the comparison must take into account the fact that FearNet stores permanently relatively large covariance matrices for each classes.\nOverall, the hippocampus structure is the weakness of the approach, as it is so simple that I would assume it cannot adapt well to increasingly complex tasks. Also, making an analogy with hippocampus for such architecture seems a bit exaggerated.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.","pdf":"/pdf/52722fb81022d805f2e1cc44690f82ff7a957c68.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222769596,"tcdate":1511813058534,"number":2,"cdate":1511813058534,"id":"rJ96Jgclf","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Review","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting cognitive science take, tested on modern datasets","rating":"6: Marginally above acceptance threshold","review":"I quite liked the revival of the dual memory system ideas and the cognitive (neuro) science inspiration. The paper is overall well written and tackles serious modern datasets, which was impressive, even though it relies on a pre-trained, fixed ResNet (see point below).\n\nMy only complaint is that I felt I couldn’t understand why the model worked so well. A better motivation for some of the modelling decisions would be helpful. For instance, how much the existence (and training) of a BLA network really help — which is a central new part of the paper, and wasn’t in my view well motivated. It would be nice to compare with a simpler baseline, such as a HC classifier network with reject option. I also don’t really understand why the proposed pseudorehearsal works so well. Some formal reasoning, even if approximate, would be appreciated.\n\nSome additional comments below:\n\n- Although the paper is in general well written, it falls on the lengthy side and I found it difficult at first to understand the flow of the algorithm. I think it would be helpful to have a high-level pseudocode presentation of the main steps.\n\n- It was somewhat buried in the details that the model actually starts with a fixed, advanced feature pre-processing stage (the ResNet, trained on a distinct dataset, as it should). I’m fine with that, but this should be discussed. Note that there is evidence that the neuronal responses in areas as early as V1 change as monkeys learn to solve discrimination tasks. It should be stressed that the model does not yet model end-to-end learning in the incremental setting.\n\n- p. 4, Eq. 4, is it really necessary to add a loss for the intermediate layers, and not only for the input layer? I think it would be clearer to define the \\mathcal{L} explictily somewhere. Also, shouldn’t the sum start at j=0?","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.","pdf":"/pdf/52722fb81022d805f2e1cc44690f82ff7a957c68.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222769638,"tcdate":1511657538545,"number":1,"cdate":1511657538545,"id":"HyirgqDgM","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Review","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper presents an interesting problem of incremental classification inspired by the dual memory-system of brain. I feel the paper  explicitly describes the problem and explains the proposed methodology in great detail.","rating":"7: Good paper, accept","review":"Quality: The paper presents a novel solution to an incremental classification problem based on a dual memory system. The proposed solution is inspired by the memory storage mechanism in brain.\n\nClarity: The problem has been clearly described and the proposed solution is described in detail. The results of numerical experiments and the real data analysis are satisfactory and clearly shows the superior performance of the method compared to the existing ones.\n\nOriginality: The solution proposed is a novel one based on a dual memory system inspired by the memory storage mechanism in brain. The memory consolidation is inspired by the mechanisms that occur during sleep. The numerical experiments showing the FearNet performance with sleep frequency also validate the comparison with the brain memory system.\n\nSignificance: The work discusses a significant problem of incremental classification. Many of the shelf deep neural net methods require storage of previous training samples too and that slows up the application to larger dataset. Further the traditional deep neural net also suffers from the catastrophic forgetting. Hence, the proposed work provides a novel and scalable solution to the existing problem.\n\npros: (a) a scalable solution to the incremental classification problem using a brain inspired dual memory system\n          (b) mitigates the catastrophic forgetting problem using a memory consolidation by pseudorehearsal.\n          (c) introduction of a subsystem that allows which memory system to use for the classification\n\ncons: (a)  How FearNet would perform if imbalanced classes are seen in more than one study sessions?\n          (b) Storage of class statistics during pseudo rehearsal could be computationally expensive. How to cope with that?\n          (c) How FearNet would handle if there are multiple data sources?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.","pdf":"/pdf/52722fb81022d805f2e1cc44690f82ff7a957c68.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739093600,"tcdate":1509135127357,"number":802,"cdate":1509739090937,"id":"SJ1Xmf-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.","pdf":"/pdf/52722fb81022d805f2e1cc44690f82ff7a957c68.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}