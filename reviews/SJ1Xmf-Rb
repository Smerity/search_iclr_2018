{"notes":[{"tddate":null,"ddate":null,"tmdate":1515792271211,"tcdate":1515792271211,"number":5,"cdate":1515792271211,"id":"rkvqPjUVz","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Comment","forum":"SJ1Xmf-Rb","replyto":"SkTJreuzM","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper802/AnonReviewer3"],"content":{"title":"Well revised and the previous issues have been clarified","comment":"I am happy with the revision! My concerns regarding the FearNet mechanism have been properly addressed. The issue of class imbalance, computational hurdles in storage and the issue of multiple data modalities have been addressed appropriately."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513780452891,"tcdate":1513780452891,"number":4,"cdate":1513780452891,"id":"SkTJreuzM","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Comment","forum":"SJ1Xmf-Rb","replyto":"HyirgqDgM","signatures":["ICLR.cc/2018/Conference/Paper802/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper802/Authors"],"content":{"title":"Comments for Reviewer #3","comment":"\nReviewer #3: How FearNet would perform if imbalanced classes are seen in more than one study sessions?\n\nAuthors: FearNet generates a balanced number of pseudoexamples during its sleep phase and when updating BLA, so class imbalance is not an issue.  To test this, we did an experiment with CIFAR-100 where we selected a random number of samples from each class (20-500) so that the class distribution was imbalanced.  We would expect a slight degradation in performance because we aren’t using as many samples to train FearNet as we did in the paper (i.e., the model doesn’t generalize as well for the test set.)  The results (\\omega_{base} = 0.884, \\omega_{new} = 0.729, and \\omega_{all} = 0.897) indicate that FearNet is robust to imbalanced class distributions.\n\nReviewer #3: Storage of class statistics during pseudo rehearsal could be computationally expensive. How to cope with that?\n\nAuthors: We agree that storing class statistics is a major bottleneck, but FearNet still manages to be less memory intensive than other models.  In Table 5, we show that the storage cost for FearNet is still lower than previous methods.  In Table 6, we show that FearNet still outperforms other methods when only a diagonal covariance matrix is stored for each class, while decreasing storage costs by 65%. \n\nReviewer #3: How FearNet would handle if there are multiple data sources?\n\nAuthors: We assume that the reviewer is referring to FearNet’s ability to handle multiple data modalities.  In Table 4, we explored FearNet’s ability to simultaneously learn audio and visual information.  The results showed that FearNet was able to simultaneously learn datasets with very different data representations.  \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513780240684,"tcdate":1513780240684,"number":3,"cdate":1513780240684,"id":"HyFfVxOzG","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Comment","forum":"SJ1Xmf-Rb","replyto":"rJ96Jgclf","signatures":["ICLR.cc/2018/Conference/Paper802/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper802/Authors"],"content":{"title":"Comments for Reviewer #2","comment":"\nReviewer #2: My only complaint is that I felt I couldn’t understand why the model worked so well. A better motivation for some of the modelling decisions would be helpful. For instance, how much the existence (and training) of a BLA network really help — which is a central new part of the paper, and wasn’t in my view well motivated. It would be nice to compare with a simpler baseline, such as a HC classifier network with reject option. \n\nAuthors: We do explore how the BLA effects FearNet performance in an ablation study shown in Table 3.  We actually tried different variants for BLA before settling on the model that we used in the paper. We have included the results of the other variants in the supplemental material to help justify our decisions. \n\nReviewer #2: I also don’t really understand why the proposed pseudorehearsal works so well. Some formal reasoning, even if approximate, would be appreciated.\n\nAuthors: Rehearsal and psuedorehearsal are old ideas from the 1990s. We have added more justification for why they help alleviate catastrophic forgetting in Section 2 and in the discussion.\n\nReviewer #2: Although the paper is in general well written, it falls on the lengthy side and I found it difficult at first to understand the flow of the algorithm. I think it would be helpful to have a high-level pseudocode presentation of the main steps.\n\nAuthors: The high-level pseudocode for FearNet’s train and predict functionality is a great idea.  We have included this in the supplemental material of our revised version. \n\nReviewer #2: It was somewhat buried in the details that the model actually starts with a fixed, advanced feature pre-processing stage (the ResNet, trained on a distinct dataset, as it should). I’m fine with that, but this should be discussed. Note that there is evidence that the neuronal responses in areas as early as V1 change as monkeys learn to solve discrimination tasks. It should be stressed that the model does not yet model end-to-end learning in the incremental setting.\n\nAuthors: We have included the following sentence in the beginning of Section 4, “In this paper, we use pre-trained embeddings of the input (e.g., ResNet).”  We think representation learning is an important next step, and it will be incorporated into FearNet 2.0, which is currently in its early planning stages.\n\nReviewer #2: p. 4, Eq. 4, is it really necessary to add a loss for the intermediate layers, and not only for the input layer? I think it would be clearer to define the \\mathcal{L} explictily somewhere. Also, shouldn’t the sum start at j=0?\n\nAuthors: Thank you for pointing that out.  We have fixed Eq. 4 and defined the \\mathcal{L} term to make it clear that we are computing the MSE loss between the output of each hidden layer and the input/output of the mPFC autoencoder.  The rationale for using MSE losses at the intermediate layers stem from Valpola (2015), where he showed that errors in deeper layers had a harder time being corrected because they were further away from the training signal (i.e., data layer).  Adding the multi-layer loss forces the autoencoder to correct errors at every layer.  A good autoencoder fit is important for our framework because it is directly related to the fidelity of the pseudoexamples being generated for sleep phases.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513780257972,"tcdate":1513780075507,"number":2,"cdate":1513780075507,"id":"HkmO7eOGf","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Comment","forum":"SJ1Xmf-Rb","replyto":"HkvoWK6ef","signatures":["ICLR.cc/2018/Conference/Paper802/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper802/Authors"],"content":{"title":"Comments for Reviewer #1","comment":"\nReviewer #1: It seems surprising to me that the network manages to outperform other approaches using such a simplistic network for hippocampus (essentially a Euclidian distance based classifier). I assume that the great performance is due to the fact that a lot of examples per classes are stored in hippocampus.\n\nAuthors: Our comparison to the state-of-the-art is valid, and we have thoroughly checked our results.  The HC network is simple, and we are comparing not only it, but the entire network to the state-of-the-art.  We chose to implement HC using nearest neighbor density estimation because it is able to make inferences from new data immediately without expensive loops through the training data and often works well when data is scarce (low-shot learning), and it has the perfect properties for enabling us to use pseudorehearsal for consolidating information from HC to mPFC. Psuedorehearsal requires mixing the raw recently observed data with generated examples of data observed long ago.  Our HC model can be thought of as a simple buffer that can enable inference to be made until the information is transferred to mPFC, at which point the HC model is erased. After “sleeping,” FearNet does not store old values in HC because those memories now reside in the mPFC network.  FearNet uses its BLA module to determine where the memory resides.  Since FearNet could erroneously predict the network where the memory resides, we don’t believe that HC artificially inflates FearNet performance.  We tested this by performing the incremental learning experiment for the 1-nearest neighbor (1-NN) for all three datasets (see Table 2 in the revised manuscript). FearNet outperformed 1-NN because 1-NN was unable to generalize to the test data as well as FearNet.  Additionally, compared to FearNet, 1-NN is significantly less memory efficient (Table 5) and very slow at making predictions.\n\nReviewer #1: I could not find an investigation of the effect of this number on the performance. I assume this number corresponds to the mini-batch size (450).\n\nAuthors: We did investigate FearNet performance as a function of how many classes are learned (stored in HC) before its sleep phase is performed (see Fig. 5 in the discussion).  The mini-batch is only for 1) the sleep phase and 2) updating BLA.  To make this clearer, we added “We investigate FearNet’s performance as a function of how much data is stored in HC in Section 6.2.” to the end of Section 4.1.\n\nReviewer #1: I would like that the authors elaborate on how fair is the comparison to methods such as iCaRL, which store very little examples per classes according to Fig. 2. I assume the comparison must take into account the fact that FearNet stores permanently relatively large covariance matrices for each classes.\n\nAuthors: For CIFAR-100, iCaRL stores 2,000 exemplars for replay.  At the beginning, they are able to store most/all of the exemplars (there are 500 per class available) since the buffer maxes out at 2,000.  As time moves on, that number decreases as it has to make room for new classes.  By the end, there are 20 exemplars per class.  We have re-written the last paragraph in Section 2 to clarify this point.  In comparison, our model stores the mean/covariance matrix for each class, and then generates new “exemplars” (pseudoexamples) during sleep. Note that our method still outperforms iCaRL and other methods when only a diagonal covariance is stored, as discussed in Section 6.2 (see Table 6).  Using MLP type architectures for iCaRL and FearNet, we showed that storing class statistics is still more memory efficient than storing these exemplars (see Table 5).  Our future work will focus on using generative models that don’t require class statistics for pseudorehearsal.  \n\nReviewer #1: Overall, the hippocampus structure is the weakness of the approach, as it is so simple that I would assume it cannot adapt well to increasingly complex tasks. Also, making an analogy with hippocampus for such architecture seems a bit exaggerated.\n\nAuthors: We agree that HC could be improved, and we included in our future work that we want to replace HC with a semi-parametric model, instead of an entirely non-parametric model.  We also agree that the low-level operations that occur in the individual FearNet modules (e.g., HC) are not entirely analogous to operations that occur in the brain; and to be fair, we don’t make that claim.  Our main inspiration for FearNet is 1) the brain’s dual-memory architecture for rapid acquisition of new information and long-term storage of old information, 2) how mammalian brains consolidate recent memories to long term storage during sleep, and 3) the recent and remote recall pathways that BLA uses. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513779910781,"tcdate":1513779910781,"number":1,"cdate":1513779910781,"id":"Bk1Cfe_MM","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Comment","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper802/Authors"],"content":{"title":"Revised Manuscript Available!","comment":"First, we would like to thank the reviewers for their valuable feedback.  Their comments have helped us to improve the original manuscript.  All three reviewers expressed that they liked our new brain-inspired algorithm for incremental class learning.  We identified two main issues that they raised: 1) the reviewers wanted more justification for architectural decisions (with a focus on HC and BLA); and 2) the reviewers wanted more explanation for why pseudorehearsal works for mitigating catastrophic forgetting during incremental class learning. Additionally, the reviewers suggested a number of minor changes that will make the paper clearer and enable others to better reproduce our work, although we will make all of our code available once the paper is accepted.  We address each reviewer comment individually.  Please let us know if there are any other questions/concerns regarding our revised manuscript.  Thank you!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642513368,"tcdate":1512047006629,"number":3,"cdate":1512047006629,"id":"HkvoWK6ef","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Review","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Suprizingly good results with an rather simple architecture. Is the comparison to SotA fair??","rating":"6: Marginally above acceptance threshold","review":"\nThis paper addresses the problem of incremental class learning with brain inspired memory system. This relies on 1/ hippocampus like system relying on a temporary memory storage and probabilistic neural network classifier, 2/ a prefrontal cortex-like ladder network architecture, performing joint autoencoding and classification, 3/ an amygdala-like classifier that combines the decision of both structures. The experiments suggests that the approach performs better than state-of-the-art incremental learning approaches, and approaches offline learning.\nThe paper is well written. The main issue I have with the approach is the role of the number of examples stored in hippocampus and its implication for the comparison to state-of-the art approaches.\nComments:\nIt seems surprising to me that the network manages to outperform other approaches using such a simplistic network for hippocampus (essentially a Euclidian distance based classifier). I assume that the great performance is due to the fact that a lot of examples per classes are stored in hippocampus. I could not find an investigation of the effect of this number on the performance. I assume this number corresponds to the mini-batch size (450). I would like that the authors elaborate on how fair is the comparison to methods such as iCaRL, which store very little examples per classes according to Fig. 2. I assume the comparison must take into account the fact that FearNet stores permanently relatively large covariance matrices for each classes.\nOverall, the hippocampus structure is the weakness of the approach, as it is so simple that I would assume it cannot adapt well to increasingly complex tasks. Also, making an analogy with hippocampus for such architecture seems a bit exaggerated.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515794434610,"tcdate":1511813058534,"number":2,"cdate":1511813058534,"id":"rJ96Jgclf","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Review","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting cognitive science take, tested on modern datasets","rating":"7: Good paper, accept","review":"I quite liked the revival of the dual memory system ideas and the cognitive (neuro) science inspiration. The paper is overall well written and tackles serious modern datasets, which was impressive, even though it relies on a pre-trained, fixed ResNet (see point below).\n\nMy only complaint is that I felt I couldn’t understand why the model worked so well. A better motivation for some of the modelling decisions would be helpful. For instance, how much the existence (and training) of a BLA network really help — which is a central new part of the paper, and wasn’t in my view well motivated. It would be nice to compare with a simpler baseline, such as a HC classifier network with reject option. I also don’t really understand why the proposed pseudorehearsal works so well. Some formal reasoning, even if approximate, would be appreciated.\n\nSome additional comments below:\n\n- Although the paper is in general well written, it falls on the lengthy side and I found it difficult at first to understand the flow of the algorithm. I think it would be helpful to have a high-level pseudocode presentation of the main steps.\n\n- It was somewhat buried in the details that the model actually starts with a fixed, advanced feature pre-processing stage (the ResNet, trained on a distinct dataset, as it should). I’m fine with that, but this should be discussed. Note that there is evidence that the neuronal responses in areas as early as V1 change as monkeys learn to solve discrimination tasks. It should be stressed that the model does not yet model end-to-end learning in the incremental setting.\n\n- p. 4, Eq. 4, is it really necessary to add a loss for the intermediate layers, and not only for the input layer? I think it would be clearer to define the \\mathcal{L} explictily somewhere. Also, shouldn’t the sum start at j=0?","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642513439,"tcdate":1511657538545,"number":1,"cdate":1511657538545,"id":"HyirgqDgM","invitation":"ICLR.cc/2018/Conference/-/Paper802/Official_Review","forum":"SJ1Xmf-Rb","replyto":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference/Paper802/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper presents an interesting problem of incremental classification inspired by the dual memory-system of brain. I feel the paper  explicitly describes the problem and explains the proposed methodology in great detail.","rating":"7: Good paper, accept","review":"Quality: The paper presents a novel solution to an incremental classification problem based on a dual memory system. The proposed solution is inspired by the memory storage mechanism in brain.\n\nClarity: The problem has been clearly described and the proposed solution is described in detail. The results of numerical experiments and the real data analysis are satisfactory and clearly shows the superior performance of the method compared to the existing ones.\n\nOriginality: The solution proposed is a novel one based on a dual memory system inspired by the memory storage mechanism in brain. The memory consolidation is inspired by the mechanisms that occur during sleep. The numerical experiments showing the FearNet performance with sleep frequency also validate the comparison with the brain memory system.\n\nSignificance: The work discusses a significant problem of incremental classification. Many of the shelf deep neural net methods require storage of previous training samples too and that slows up the application to larger dataset. Further the traditional deep neural net also suffers from the catastrophic forgetting. Hence, the proposed work provides a novel and scalable solution to the existing problem.\n\npros: (a) a scalable solution to the incremental classification problem using a brain inspired dual memory system\n          (b) mitigates the catastrophic forgetting problem using a memory consolidation by pseudorehearsal.\n          (c) introduction of a subsystem that allows which memory system to use for the classification\n\ncons: (a)  How FearNet would perform if imbalanced classes are seen in more than one study sessions?\n          (b) Storage of class statistics during pseudo rehearsal could be computationally expensive. How to cope with that?\n          (c) How FearNet would handle if there are multiple data sources?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513779767715,"tcdate":1509135127357,"number":802,"cdate":1509739090937,"id":"SJ1Xmf-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ1Xmf-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"FearNet: Brain-Inspired Model for Incremental Learning","abstract":"Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie  methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing  training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall.  FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.\n","pdf":"/pdf/8615b9df0a0d8f3392508783f7998e58593e0115.pdf","TL;DR":"FearNet is a memory efficient neural-network, inspired by memory formation in the mammalian brain, that is capable of incremental class learning without catastrophic forgetting.","paperhash":"anonymous|fearnet_braininspired_model_for_incremental_learning","_bibtex":"@article{\n  anonymous2018fearnet:,\n  title={FearNet: Brain-Inspired Model for Incremental Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ1Xmf-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper802/Authors"],"keywords":["Incremental Learning","Lifelong Learning","Supervised Learning","Catastrophic Forgetting","Brain-Inspired","Neural Networks"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}