{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222719591,"tcdate":1511866687222,"number":3,"cdate":1511866687222,"id":"SkDHZacef","invitation":"ICLR.cc/2018/Conference/-/Paper684/Official_Review","forum":"BJubPWZRW","replyto":"BJubPWZRW","signatures":["ICLR.cc/2018/Conference/Paper684/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposes a multi-view semi-supervised method","rating":"7: Good paper, accept","review":"This paper proposes a multi-view semi-supervised method. For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap. Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data.\n\nTo be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here). As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term. Otherwise, the model can't learn anything.\n\nMy understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss). In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance? If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1. One obvious merit is that the unlabeled data is utilized more efficiently, k times better.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cross-View Training for Semi-Supervised Learning","abstract":"We present Cross-View Training (CVT), a simple but effective method for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a \"``teacher\") to produce soft targets. The model then learns from these soft targets (acting as a ``\"student\"). We deviate from prior work by adding multiple auxiliary student softmax layers to the model. The input to each student layer is a sub-network of the full model that has a restricted view of the input  (e.g., only seeing one region of an image). The students can learn from the teacher (the full model) because the teacher sees more of each example. Concurrently, the students improve the quality of the representations used by the teacher as they learn to make predictions with limited data. We propose variants of our method for CNN image classifiers and BiLSTM sequence taggers. When combined with Virtual Adversarial Training, CVT improves upon the current state-of-the-art on semi-supervised CIFAR-10 and semi-supervised SVHN. We also apply CVT to train semi-supervised sequence taggers on four natural language processing tasks using hundreds of millions of sentences of unlabeled data. The resulting models improve upon or are competitive with the current state-of-the-art on every task.\n","pdf":"/pdf/a8e013d87cc144d74ec5caff0768d6fb880b4285.pdf","TL;DR":"Self-training with different views of the input gives excellent results for semi-supervised image recognition and sequence tagging.","paperhash":"anonymous|crossview_training_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018cross-view,\n  title={Cross-View Training for Semi-Supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJubPWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper684/Authors"],"keywords":["semi-supervised learning","image recognition","sequence tagging"]}},{"tddate":null,"ddate":null,"tmdate":1512361084297,"tcdate":1511851139789,"number":2,"cdate":1511851139789,"id":"HJhFVtqez","invitation":"ICLR.cc/2018/Conference/-/Paper684/Official_Review","forum":"BJubPWZRW","replyto":"BJubPWZRW","signatures":["ICLR.cc/2018/Conference/Paper684/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Experimental results are not impressive","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a ’Cross View training’ approach to semi-supervised learning. In the teacher-student framework for semi-supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model. The auxiliary softmax layers take different views of the input for prediction.\n\nPros:\n1. A simple approach to encourage better representations learned from unlabeled examples. \n\n2. Experiments are comprehensive.\n\nCons:\n\n0. The whole paper just presented strategies and empirical results. There are no discussions of insights and why the proposed strategy work, for what cases it will work, and for what cases it will not work? Why? \n\n1. The addition of auxiliary layers improves Sequence Tagging results marginally. \n\n2. The claim of cross-view for sequence tagging setting is problematic. Because the task is per-position tagging, those added signals are essentially not part of the examples, but the signals of its neighbors. \n\n3. Adding n^2 linear layers for image classification essentially makes the model much larger. It is unfair to compare to the baseline models with much fewer parameters. \n\n4. The \"CVT, no noise\" should be compared to \"CVT, random noise\", then to \"CVT, adversarial noise\". The current results show that the improvements are mostly from VAT, instead of CVT. \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Cross-View Training for Semi-Supervised Learning","abstract":"We present Cross-View Training (CVT), a simple but effective method for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a \"``teacher\") to produce soft targets. The model then learns from these soft targets (acting as a ``\"student\"). We deviate from prior work by adding multiple auxiliary student softmax layers to the model. The input to each student layer is a sub-network of the full model that has a restricted view of the input  (e.g., only seeing one region of an image). The students can learn from the teacher (the full model) because the teacher sees more of each example. Concurrently, the students improve the quality of the representations used by the teacher as they learn to make predictions with limited data. We propose variants of our method for CNN image classifiers and BiLSTM sequence taggers. When combined with Virtual Adversarial Training, CVT improves upon the current state-of-the-art on semi-supervised CIFAR-10 and semi-supervised SVHN. We also apply CVT to train semi-supervised sequence taggers on four natural language processing tasks using hundreds of millions of sentences of unlabeled data. The resulting models improve upon or are competitive with the current state-of-the-art on every task.\n","pdf":"/pdf/a8e013d87cc144d74ec5caff0768d6fb880b4285.pdf","TL;DR":"Self-training with different views of the input gives excellent results for semi-supervised image recognition and sequence tagging.","paperhash":"anonymous|crossview_training_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018cross-view,\n  title={Cross-View Training for Semi-Supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJubPWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper684/Authors"],"keywords":["semi-supervised learning","image recognition","sequence tagging"]}},{"tddate":null,"ddate":null,"tmdate":1512222719671,"tcdate":1511809028670,"number":1,"cdate":1511809028670,"id":"Bkp-xJ5xf","invitation":"ICLR.cc/2018/Conference/-/Paper684/Official_Review","forum":"BJubPWZRW","replyto":"BJubPWZRW","signatures":["ICLR.cc/2018/Conference/Paper684/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper presents a so-called cross-view training for semi-supervised deep models. This paper suffers from several weaknesses, e.g., lack of novelty, technical flaw and no significant improvement over the existing approaches.","rating":"2: Strong rejection","review":"This paper presents a so-called cross-view training for semi-supervised deep models. Experiments were conducted on various data sets and experimental results were reported.\n\nPros:\n* Studying semi-supervised learning techniques for deep models is of practical significance.\n\nCons:\n* The novelty of this paper is marginal. The use of unlabeled data is in fact a self-training process. Leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retrieval. \n* The proposed approach suffers from a technical weakness or flaw. For the self-labeled data, the prediction of each view is enforced to be same as the assigned self-labeling. However, since each view related to a sub-region of the image (especially when the model is not so deep), it is less likely for this region to contain the representation of the concepts (e.g., some local region of an image with a horse may exhibit only grass); enforcing the prediction of this view to be the same self-labeled concepts (e.g,“horse”) may drive the prediction away from what it should be ( e..g, it will make the network to predict grass as horse). Such a flaw may affect the final performance of the proposed approach.\n* The word “view” in this paper is misleading. The “view” in this paper is corresponding to actually sub-regions in the images\n* The experimental results indicate that the proposed approach fails to perform better than the compared baselines in table 2, which reduces the practical significance of the proposed approach. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cross-View Training for Semi-Supervised Learning","abstract":"We present Cross-View Training (CVT), a simple but effective method for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a \"``teacher\") to produce soft targets. The model then learns from these soft targets (acting as a ``\"student\"). We deviate from prior work by adding multiple auxiliary student softmax layers to the model. The input to each student layer is a sub-network of the full model that has a restricted view of the input  (e.g., only seeing one region of an image). The students can learn from the teacher (the full model) because the teacher sees more of each example. Concurrently, the students improve the quality of the representations used by the teacher as they learn to make predictions with limited data. We propose variants of our method for CNN image classifiers and BiLSTM sequence taggers. When combined with Virtual Adversarial Training, CVT improves upon the current state-of-the-art on semi-supervised CIFAR-10 and semi-supervised SVHN. We also apply CVT to train semi-supervised sequence taggers on four natural language processing tasks using hundreds of millions of sentences of unlabeled data. The resulting models improve upon or are competitive with the current state-of-the-art on every task.\n","pdf":"/pdf/a8e013d87cc144d74ec5caff0768d6fb880b4285.pdf","TL;DR":"Self-training with different views of the input gives excellent results for semi-supervised image recognition and sequence tagging.","paperhash":"anonymous|crossview_training_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018cross-view,\n  title={Cross-View Training for Semi-Supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJubPWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper684/Authors"],"keywords":["semi-supervised learning","image recognition","sequence tagging"]}},{"tddate":null,"ddate":null,"tmdate":1509739161022,"tcdate":1509132032040,"number":684,"cdate":1509739158357,"id":"BJubPWZRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJubPWZRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Cross-View Training for Semi-Supervised Learning","abstract":"We present Cross-View Training (CVT), a simple but effective method for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a \"``teacher\") to produce soft targets. The model then learns from these soft targets (acting as a ``\"student\"). We deviate from prior work by adding multiple auxiliary student softmax layers to the model. The input to each student layer is a sub-network of the full model that has a restricted view of the input  (e.g., only seeing one region of an image). The students can learn from the teacher (the full model) because the teacher sees more of each example. Concurrently, the students improve the quality of the representations used by the teacher as they learn to make predictions with limited data. We propose variants of our method for CNN image classifiers and BiLSTM sequence taggers. When combined with Virtual Adversarial Training, CVT improves upon the current state-of-the-art on semi-supervised CIFAR-10 and semi-supervised SVHN. We also apply CVT to train semi-supervised sequence taggers on four natural language processing tasks using hundreds of millions of sentences of unlabeled data. The resulting models improve upon or are competitive with the current state-of-the-art on every task.\n","pdf":"/pdf/a8e013d87cc144d74ec5caff0768d6fb880b4285.pdf","TL;DR":"Self-training with different views of the input gives excellent results for semi-supervised image recognition and sequence tagging.","paperhash":"anonymous|crossview_training_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018cross-view,\n  title={Cross-View Training for Semi-Supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJubPWZRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper684/Authors"],"keywords":["semi-supervised learning","image recognition","sequence tagging"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}