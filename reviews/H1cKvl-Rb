{"notes":[{"tddate":null,"ddate":null,"tmdate":1515185878931,"tcdate":1515185878931,"number":8,"cdate":1515185878931,"id":"BkyyDvTXG","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"Revised manuscript","comment":"Dear reviewers, we have taken your feedback into account and revised the manuscript. A new manuscript has been uploaded. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515185728046,"tcdate":1515185728046,"number":7,"cdate":1515185728046,"id":"SkOHUDamG","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"HySlgMqMf","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"Thank you for reproducing the results","comment":"We would like to thank you for reproducing and validating the results of our paper. In our implementation, gradients from the multiple heads are first averaged before passing into the gradient update of the convolutional layers.  "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515185292845,"tcdate":1515185292845,"number":6,"cdate":1515185292845,"id":"S1S54DamM","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"H14cttFXM","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"Thank you for reproducing the results","comment":"We would like to thank you for reproducing and validating the results of our paper. Regarding the hyperparameter $\\lambda$ in UCB exploration, it is set to $\\lambda = 0.1$ uniformly for all games evaluated as stated in the middle of Page 5 of the draft. No game-specific fine-tuning was done in the experiments."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515035385929,"tcdate":1515035385929,"number":5,"cdate":1515035385929,"id":"SJMbiMiXf","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"rkTJ2wYeG","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"Reply to the review","comment":"We thank the reviewer’s comments and address in the following: \n\n1. Bootstrapped DQN samples one Q network from the ensemble applies it for a whole episode for exploration. We hypothesize that this intuition of deep exploration, by consistently using one Q function in each episode, does not guarantee that each Q function’s exploration is beneficial nor efficient. For example, each Q function deviates from the ensembled Q and accumulates inefficiency in a long episode. Although our proposed methods use the same network structure of bootstrapped DQN, the goal is very different: we build exploration bonus based on the uncertainty or discrepancy of Q ensembles. In UCB exploration, exploration bonus is based on the uncertainty of Q ensembles and encourages the agent to reduce the uncertainty in Q values. \n\n2. Ensemble uncertainty is due to Q networks being parametrized with deep neural networks, which introduces nonconvexity in Bellman update. Thus, even though the Q networks are trained with the same samples, their parameters do not converge to the same. We also experimented with training each Q network with independently sampled transitions from the reply buffer, and did not observe improved performance. We don’t think the optimization method (SGD/Adam) plays a key role. This phenomenon that bagging worsens the performance of deep ensembles is also observed in supervised training setting. [Lee et al, 2015] observed that supervised learning trained with deep ensembles with random initializations perform better than bagging for deep ensembles. [Balaji et al, 2017] used deep ensembles for uncertainty estimates and also observed that bagging deteriorated performance in their experiments. We will revise and clarify the source of uncertainty from the ensembles. \n\n3. We will modify/shorten the derivation on pages 3 and 4.\n\n4. On efficient exploration in RL, our proposed two algorithms use the Q functions directly while prior works construct exploration bonus using state-visitation counts, which are not tied to the rewards that agents seek to maximize. Our goal is to construct methods that reduce the inefficiency of prior algorithms where learning can be wasted on visiting irrelevant states. Thus, by improving upon bootstrapped DQN and comparing with state-visitation count-based methods such as A3C+, we demonstrate that this direction of exploration based on Q-values is promising, and different from hyperparameter tuning. Due to compute constraint, we trained the proposed algorithms on each game with 40 million frames, less than 200 million frames used in prior works. Thus games that typically require more frames to learn do not show big improvement in our experiments. \n\nReferences: \t\t\t\t \t\t\t\t\t\t\t\nS. Lee, S. Purushwalkam, M. Cogswell, D. Crandall, and D. Batra. Why M heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314, 2015. \n\t\t\t\t\nLakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. \"Simple and scalable predictive uncertainty estimation using deep ensembles.\" Advances in Neural Information Processing Systems. 2017.\t\t\t \t\t\t\t\n\t\t\t"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515035277781,"tcdate":1515035277781,"number":4,"cdate":1515035277781,"id":"BkI9qfsmG","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"B13fzyclG","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"Reply to the review","comment":"We thank the reviewer’s comment and address in the following:\n\n1. We will modify/shorten the derivation on pages 3 and 4.\n\n2. We observed the Q values for actions chosen according to Alg 1 and Alg 2. These Q values correspond to good actions. During learning, the Q values for such good actions gradually increase. The discrepancies between the Q values also increase in absolute values. But normalized by the mean Q value from different Q networks, the discrepancies gradually decrease. \n\n3. In Alg 1 and Alg 2, epsilon-greedy is not used, such that we can isolate the effects of exploration using Ensemble Voting or UCB exploration only. We did not experiment with adding epsilon-greedy on top of Alg 1, but agree that it will be an interesting experiment to see whether epsilon-greedy helps or hurts exploration on top of Alg 1. \n\n4. Besides action selection, bootstrapped DQN allows each Q network to be trained with different samples (using a masking mechanism), even though in bootstrapped DQN’s Atari experiments, all Q networks are trained using the same samples. In Alg 1, Q networks only use random initialization and trained with the same samples. We also experimented with training Q networks with independently drawn samples, which deteriorated the performance. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515035475373,"tcdate":1515035194791,"number":3,"cdate":1515035194791,"id":"SyQS9Go7f","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"r1prpe1ZM","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"Reply to the review","comment":"We thank the reviewer’s comments. We address in the following:\n\n1. We first comment that the improvement from our proposed methods is significant. We used a strong Double DQN baseline, which achieves competitive or better learning results trained with 40 million frames, compared with prior published results [Van Hasselt, et al, 2016] trained with 200 million frames.  Improvement of proposed methods is significant over this strong Double DQN baseline. Table 2 in Appendix B shows that Ensemble Voting performs better than Double DQN in 37 out of 49 games evaluated, and UCB Exploration performs better than Double DQN in 38 out of 49 games evaluated. In addition, UCB Exploration performs better than Ensemble Voting in 35 out of 49 games evaluated. We will include such comparison in the results section. \n\n2. We also compared to bootstrapped DQN as shown in Figure 1, Figure 2, and the results Table 2 in Appendix B. \n\n3. A3C+ represents one line of research comprised of multiple works where the agent constructs exploration bonus based on state visitation counts. As discussed in Section 2.2, the exploration bonus from these methods does not depend on the reward, thus the exploration may focus on irrelevant aspects of the environment. In comparison, our exploration bonus depend on the Q values directly. We chose A3C+ to compare our method of reward-based exploration bonus against count-based exploration bonus and demonstrate that this reward/Q values-based approach of constructing exploration bonus is promising. \n\n4. Bootstrapped DQN samples one Q network from the ensemble applies it for a whole episode for exploration. We hypothesize that this intuition of deep exploration, by consistently using one Q function in each episode, does not guarantee that exploration is beneficial nor efficient. For example, each Q function deviates from the ensembled-Q. Although our proposed methods use the same network structure of bootstrapped DQN, the goal is very different: we build exploration bonus based on the uncertainty or discrepancy of Q ensembles. UCB exploration bonus is based on the uncertainty of Q ensembles and encourages the agent to reduce the uncertainty in Q values. \n\n5. The INFOGAIN section attempts another approach of exploration using Q-ensembles. However, the improvement of this method is less consistently across the board. This could be due to the approximations we made in constructing the INFOGAIN exploration bonus. We document the results of the experiment in the Appendix for potential future interest in this direction.\n\n\n6. We will modify/shorten the derivation on pages 3 and 4.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515010143403,"tcdate":1515009873854,"number":2,"cdate":1515009873854,"id":"HyqLPh9Qz","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"SJsA9mLfz","signatures":["ICLR.cc/2018/Conference/Paper600/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/Authors"],"content":{"title":"reply to clarifications","comment":"We first comment on the significance of the performance improvement. The Double DQN baseline we used is a very fine-tuned baseline, achieving competitive rewards compared with prior published results such as the original Double DQN paper [Van Hasselt, et al. 2016]. In fact, in many games, our results trained with 40-million-frames are already higher than prior results trained with 200-million-frames. For common hyperparameters, our Ensemble Voting and UCB Exploration algorithms used the same as those in our Double DQN implementation.  Both methods achieved significant improved performance over Double DQN. Table 2 in Appendix B shows that Ensemble Voting performs better than Double DQN in 37 out of 49 games evaluated, and UCB Exploration performs better than Double DQN in 38 out of 49 games evaluated. In addition, UCB Exploration performs better than Ensemble Voting in 35 out of 49 games evaluated. We will expand the Results section and include these comparisons. \n\nWe do assume that the reward function is deterministic given state and action. We will state out this assumption more clearly in the notations (Section 2.1). We will also replace `r` with `r(s, a)` to make the reward’s dependency on the state and action more clear. We will abbreviate the derivations on pages 3 and 4  and rewrite based on the feedback. \n\nRegarding the upper confidence bound, we use the different network to construct an empirical variance of the estimated Q values for each (s, a) combination. As the Q functions are parametrized by a deep neural network, they do not converge to the same parameters when initialized independently and trained with the same samples due to nonconvexity, thus leading to varied Q value empirical estimates from the Q networks. The uncertainty comes from the variance of the empirical estimates. \n\nWe also try constructing the empirical variance from Q networks initialized randomly and trained with samples drawn independently from the replay buffer.  However, this approach does not lead to better performance compared with random initialization only. Thus we conclude the discrepancies in the Q-networks created by independent random initializations contain very useful information for exploration. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1514932668821,"tcdate":1514932620442,"number":4,"cdate":1514932620442,"id":"H14cttFXM","invitation":"ICLR.cc/2018/Conference/-/Paper600/Public_Comment","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"ICLR 2018 Reproducibility Challenge","comment":"We reproduce the experiments in the paper \"UCB EXPLORATION VIA Q-ENSEMBLES\" and verify the main conclusions. Our full report can be found at https://github.com/yifjiang/UCB-review/blob/master/Reproducing%20UCB%20EXPLORATION%20VIA%20Q-ENSEMBLES.pdf. Here is a summary of our work.\n\nThe original paper employed the UCB method on bootstrapped DQN and did an experiment on 49 Atari games. We implemented the baseline model, Double DQN, as well as one of the proposed models, UCB Exploration, upon OpenAI baseline models. Due to the constraints on time and computing resources, we attempted to replicate the results on one game (UpNDown). In addition, we also evaluated the models on a simpler environment, CartPole. We got similar results as the original paper on UpNDown. UCB Exploration outperforms Double DQN in this environment. However, the UCB Exploration method does not perform as well as Double DQN in the CartPole environment.\n\nOverall, Our experiments show that the original paper is reproducible. The hyperparameter table provided in the original paper greatly helps the reproduction and improves the soundness of the paper.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1513918445387,"tcdate":1513918445387,"number":3,"cdate":1513918445387,"id":"HySlgMqMf","invitation":"ICLR.cc/2018/Conference/-/Paper600/Public_Comment","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"ICLR Reproducibility Challenge Summary","comment":"We attempted to replicate this paper as part of the ICLR Reproducibility Challenge. We built our own implementation of this algorithm on top of OpenAI's existing Double DQN baseline. We attempted to replicate the results on three environments: Space Invaders, Breakout, and UpNDown. In the first two games our results appear to validate the paper's baseline-relative performance, although the specific scores we achieved were quite different. However on the UpNDown environment we were unable to achieve success using their algorithm, doing far worse than the baseline or results in other papers. The cause of our failure to replicate in UpNDown is still unclear. It's plausibly due implementation differences, such as in exactly how Adam was used train the convolutional layers shared by the multiple heads, or whether gradient normalization was in fact used. Based on the score differences between our baselines across all experiments, and since our UCB was implemented on top of our Double DQN baseline, implementation differences between baselines probably also play a role.\n\nThe full report is here: https://drive.google.com/file/d/1QVAmKK1ijZkYXeHxdyb6YrP0-K2CXQht/view?usp=sharing\nThe report appendix includes a link to our codebase."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1513695698993,"tcdate":1513695698993,"number":1,"cdate":1513695698993,"id":"rysAtjUMz","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Comment","forum":"H1cKvl-Rb","replyto":"r1w5R7Ufz","signatures":["ICLR.cc/2018/Conference/Paper600/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper600/AnonReviewer1"],"content":{"title":"RE: UCB","comment":"Sorry for the confusion, I meant *not* considered a Bayesian strategy of course... I've edited my review."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1513664143539,"tcdate":1513664143539,"number":2,"cdate":1513664143539,"id":"r1w5R7Ufz","invitation":"ICLR.cc/2018/Conference/-/Paper600/Public_Comment","forum":"H1cKvl-Rb","replyto":"B13fzyclG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"UCB","comment":"Regarding your comment on\n\"There is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I’m not sure this is justified and necessary here. Moreover, the UCB strategy is generally considered a Bayesian strategy, so I wasn’t convinced by the link to Bayesian RL in this paper.\"\n\nAs you mentioned, it is not clear to me as well what is the purpose if the derivations on pages 3 and 4 where it ends up to equation (12). But regarding your latter statement, could you please point me to a reference which says UCB strategy is a Bayesian strategy?\n\nCheers"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1513663201231,"tcdate":1513663201231,"number":1,"cdate":1513663201231,"id":"SJsA9mLfz","invitation":"ICLR.cc/2018/Conference/-/Paper600/Public_Comment","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Few clarifications","comment":"In the main text, the authors mentioned that \n\"A sufficient condition for (8) is to maximize the lower-bound of the posterior distribution in (9) by ensuring the indicator function in (9) to hold.\" \nThe input to the indicator function looks like the second moment of a random variable. Could you elaborate when it happens? I am not sure it can be achieved for stochastic reward. Is that correct?\nFurthermore, the paper suggests that \"We can replace (8) with the update (10)\". Can you comment on why this is the case? Is it again for the deterministic reward? In Alg2, line 9, when you update according (12), you mean using TD update to reduce the Bellman residual? If the answer is yes, then I am not sure that I understood the message out of derivation in pages 3 and 4.\n\nThe authors introduce UCB exploration using Q-ensemble and mentioned that they extend the intuition of UCB algorithms in order to propose algorithm 2. But, as the reviewer 2 also touched upon it, I could not find a justification why the variance of k networks, trained using the same procedure but different initialization resembles upper confidence bound. Could you please comment on that?\n\nIn light of recent revelations in deep reinforcement learning (i.e. https://arxiv.org/pdf/1709.06560.pdf) and lack of significant improvement of the two proposed methods over DDQN, that would be helpful if the authors could comment about whether they feel their empirical results is an evidence of the significance of their methods.\n\nNotation. In many places in the equations, e.g. first equation in page 2,  the authors used r as a reward, but I guess it should be r(s,a). \n\nThanks.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515642477344,"tcdate":1512144196589,"number":3,"cdate":1512144196589,"id":"r1prpe1ZM","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Review","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["ICLR.cc/2018/Conference/Paper600/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement.","rating":"5: Marginally below acceptance threshold","review":"This paper introduces a number of different techniques for improving exploration in deep Q learning. The main technique is to use UCB (upper confidence bound) to speedup exploration. The authors also introduces \"Ensemble voting\" facilitate exploitation.\n\nThis paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement. The techniques introduced are a small permutation of previous results. The baselines are not particularly strong either.\n\nThe paper appeared to have be rushed. The presentation is not always clear.\n\nI also have the following questions I hope the authors could help me with:\n\n1. I failed to understand how Eqn (5). Could you please clarify.\n\n2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration.\n\n3. Why comparing to A3C+ which is not necessarily better than A3C in final performance?\n\n4. Why not comparing to Bootstrapped DQN since the proposed method is based on it?\n\n5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits?\n\n6. If there is a section on INFOGAIN exploration, why not mention it in the main text?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515642477380,"tcdate":1511809556522,"number":2,"cdate":1511809556522,"id":"B13fzyclG","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Review","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["ICLR.cc/2018/Conference/Paper600/AnonReviewer1"],"readers":["everyone"],"content":{"title":"New exploration method for DeepRL with some good results.","rating":"7: Good paper, accept","review":"The authors propose a new exploration algorithm for Deep RL. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style.\n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I’m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn’t convinced by the link to Bayesian RL in this paper.\n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? \n-How does the distribution of Q values look like during different phases of learning?\n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.\n-What’s different between Alg 1 and bootstrapped DQN (other than the action selection)?\n\nMinor things:\n-Missing propto in Eq 7?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere…\n-it looks more a Bellman residual update as written in (11).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515642477416,"tcdate":1511779300719,"number":1,"cdate":1511779300719,"id":"rkTJ2wYeG","invitation":"ICLR.cc/2018/Conference/-/Paper600/Official_Review","forum":"H1cKvl-Rb","replyto":"H1cKvl-Rb","signatures":["ICLR.cc/2018/Conference/Paper600/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A good paper that shows UCB-ensemble outperforms Thompson Sampling ensemble (like bootstrapped DQN) in experiments.","rating":"6: Marginally above acceptance threshold","review":"This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning.\nThe algorithm then chooses optimistically over the distribution induced by the ensemble.\nThis leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN.\n\nThere are several things to like about this paper:\n- It is a clear paper, with a simple message and experiments that back up the claims.\n- The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants.\n- It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \"ensemble voting\" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?)\n\nOn the other hand:\n- The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer.\n- Something feels wrong/hacky/incomplete about just doing \"ensemble\" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \"random initialization + SGD/Adam + specific network architecture\" to maintain this idea of uncertainty. For example, this wouldn't work for linear value functions!\n- I think the original bootstrapped DQN used \"ensemble voting\" at test time, so maybe you should change the labels or the way this is introduced/discussed. It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \"raw\" bootstrapped DQN) and UCB still looks like it does better.\n- I'm not convinced that page 4 and the \"Bayesian\" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \"this is similar to particle filter\" and add the mathematical derivation after, rather than as if it was some complex formula derived. If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead.\n- I think this paper might miss the point of the \"bigger\" problem of efficient exploration in RL... or even how to get \"deep\" exploration with deep RL. Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \"sub-human\" games you might hope.)\n\nOverall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari.\nThe scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark.\nPerhaps this will encourage people to dig deeper into some of these issues... I vote accept.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]}},{"tddate":null,"ddate":null,"tmdate":1515184938286,"tcdate":1509128066493,"number":600,"cdate":1509739206122,"id":"H1cKvl-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1cKvl-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"UCB EXPLORATION VIA Q-ENSEMBLES","abstract":"We show how an ensemble of $Q^*$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the $Q$-learning setting. We propose an exploration strategy based on upper-confidence bounds (UCB). Our experiments show significant gains on the Atari benchmark. ","pdf":"/pdf/ef93188320781ea7a18c0eb58633fbc18071d095.pdf","TL;DR":"Adapting UCB exploration to ensemble Q-learning improves over prior methods such as Double DQN, A3C+ on Atari benchmark","paperhash":"anonymous|ucb_exploration_via_qensembles","_bibtex":"@article{\n  anonymous2018ucb,\n  title={UCB EXPLORATION VIA Q-ENSEMBLES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1cKvl-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper600/Authors"],"keywords":["Reinforcement learning","Q-learning","ensemble method","upper confidence bound"]},"nonreaders":[],"replyCount":16,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}