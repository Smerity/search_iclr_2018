{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642378239,"tcdate":1513373946726,"number":3,"cdate":1513373946726,"id":"BkmW-pbMG","invitation":"ICLR.cc/2018/Conference/-/Paper103/Official_Review","forum":"SJD8YjCpW","replyto":"SJD8YjCpW","signatures":["ICLR.cc/2018/Conference/Paper103/AnonReviewer4"],"readers":["everyone"],"content":{"title":"limited novelty","rating":"4: Ok but not good enough - rejection","review":"This paper has limited novelty, the ideas has been previously proposed in HashedNet and Deep Compression. The experimental section is week, with only mnist and cifar results it's not convincing to the community whether this method is general. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Balanced and Deterministic Weight-sharing Helps Network Performance","abstract":"Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.","pdf":"/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf","TL;DR":"Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.","paperhash":"anonymous|balanced_and_deterministic_weightsharing_helps_network_performance","_bibtex":"@article{\n  anonymous2018balanced,\n  title={Balanced and Deterministic Weight-sharing Helps Network Performance},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJD8YjCpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper103/Authors"],"keywords":["Weight-sharing","Weight sharing","Weight tying","neural networks","entropy","hash function","hash table","balance","sparse","sparsity","hashednets"]}},{"tddate":null,"ddate":null,"tmdate":1515642378278,"tcdate":1511816888696,"number":2,"cdate":1511816888696,"id":"rybTRlqgz","invitation":"ICLR.cc/2018/Conference/-/Paper103/Official_Review","forum":"SJD8YjCpW","replyto":"SJD8YjCpW","signatures":["ICLR.cc/2018/Conference/Paper103/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A framework for studying weight sharing","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a general framework for studying weight sharing in neural networks. They further suggest two hash functions and study the role of different properties of these hash functions in the performance.\n\nThe paper is well-written and clear. It is a follow-up on Chen et al. (2015) which introduced HashedNets. Therefore, the idea of using hash functions is not novel. This paper suggests a framework to study different hash functions. However, the experimental results do not seem adequate to validate this framework. One issue here is lack of a baseline for performance comparison. Otherwise, the significance of the results is not clear.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Balanced and Deterministic Weight-sharing Helps Network Performance","abstract":"Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.","pdf":"/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf","TL;DR":"Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.","paperhash":"anonymous|balanced_and_deterministic_weightsharing_helps_network_performance","_bibtex":"@article{\n  anonymous2018balanced,\n  title={Balanced and Deterministic Weight-sharing Helps Network Performance},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJD8YjCpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper103/Authors"],"keywords":["Weight-sharing","Weight sharing","Weight tying","neural networks","entropy","hash function","hash table","balance","sparse","sparsity","hashednets"]}},{"tddate":null,"ddate":null,"tmdate":1515642378318,"tcdate":1511772690519,"number":1,"cdate":1511772690519,"id":"rJqGz8tlf","invitation":"ICLR.cc/2018/Conference/-/Paper103/Official_Review","forum":"SJD8YjCpW","replyto":"SJD8YjCpW","signatures":["ICLR.cc/2018/Conference/Paper103/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The manuscript contains few insights","rating":"4: Ok but not good enough - rejection","review":"The manuscript advocates to study the weight sharing in a more systematic way by proposing ArbNets which defines the weight sharing function as a hash function. In this framework, any existing neural network architectures, including CNN and RNN, could be incorporated into ArbNets.\n\nThe manuscript is not well written. There are multiple grammar errors and typos. Content-wise, it is already well known that CNN and RNN can be expressed as general MLP with weight sharing. The introduction of ArbNets does not bring much value or insight to this area. So it seems that most content before experimental section is common sense.\n\nIn the experimental section, it is interesting to see how different hash function with different level of entropy can affect the performance of neural nets. However, this single observation cannot enrich the whole manuscript. Two questions:\n(1) What is the definition of sparsity here, and how is it controlled?\n(2) There seems to be a step change in Figure 3. All the results are either between 10 to 20, or near 50. And the blue line goes up and down. Is this expected?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Balanced and Deterministic Weight-sharing Helps Network Performance","abstract":"Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.","pdf":"/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf","TL;DR":"Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.","paperhash":"anonymous|balanced_and_deterministic_weightsharing_helps_network_performance","_bibtex":"@article{\n  anonymous2018balanced,\n  title={Balanced and Deterministic Weight-sharing Helps Network Performance},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJD8YjCpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper103/Authors"],"keywords":["Weight-sharing","Weight sharing","Weight tying","neural networks","entropy","hash function","hash table","balance","sparse","sparsity","hashednets"]}},{"tddate":null,"ddate":null,"tmdate":1509739482872,"tcdate":1508976975008,"number":103,"cdate":1509739480210,"id":"SJD8YjCpW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJD8YjCpW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Balanced and Deterministic Weight-sharing Helps Network Performance","abstract":"Weight-sharing plays a significant role in the success of many deep neural networks, by increasing memory efficiency and incorporating useful inductive priors about the problem into the network. But understanding how weight-sharing can be used effectively in general is a topic that has not been studied extensively. Chen et al. (2015) proposed HashedNets, which augments a multi-layer perceptron with a hash table, as a method for neural network compression. We generalize this method into a framework (ArbNets) that allows for efficient arbitrary weight-sharing, and use it to study the role of weight-sharing in neural networks. We show that common neural networks can be expressed as ArbNets with different hash functions. We also present two novel hash functions, the Dirichlet hash and the Neighborhood hash, and use them to demonstrate experimentally that balanced and deterministic weight-sharing helps with the performance of a neural network.","pdf":"/pdf/34b1c5d539a7372128f2df9faa496f8cda146e3c.pdf","TL;DR":"Studied the role of weight sharing in neural networks using hash functions, found that a balanced and deterministic hash function helps network performance.","paperhash":"anonymous|balanced_and_deterministic_weightsharing_helps_network_performance","_bibtex":"@article{\n  anonymous2018balanced,\n  title={Balanced and Deterministic Weight-sharing Helps Network Performance},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJD8YjCpW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper103/Authors"],"keywords":["Weight-sharing","Weight sharing","Weight tying","neural networks","entropy","hash function","hash table","balance","sparse","sparsity","hashednets"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}