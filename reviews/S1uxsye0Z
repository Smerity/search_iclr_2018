{"notes":[{"tddate":null,"ddate":null,"tmdate":1515785200985,"tcdate":1515785200985,"number":5,"cdate":1515785200985,"id":"rJYg3KLEf","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Comment","forum":"S1uxsye0Z","replyto":"Bk15lpF1G","signatures":["ICLR.cc/2018/Conference/Paper204/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper204/Authors"],"content":{"title":"About the questions on the rebuttal raised by Reviewer #3:","comment":"Thanks very much for your careful examination. We do appreciate it.\n(1) If you look at the final Rademacher complexity bound we are proving, it has no absolute value inside the supremum. The contraction lemma is applied to the Rademacher complexity without absolute value. That is why equation (7) comes after the contraction. We understand this is confusing. We will make it clear in the next version.\n(2) As you mentioned, if we take expectation with respect to r, then f^L is not a function of r any more. Actually in our definition, the final prediction function f^L is a deterministic function (since we take the expectation w.r.t. r)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1515113734077,"tcdate":1515113734077,"number":4,"cdate":1515113734077,"id":"S1CZ6BnQM","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Comment","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper204/Authors"],"content":{"title":"We have posted a revision of the draft","comment":"Update list:\n\n1. added a subsection 6.5 to the appendix to empirically demonstrate the relations between the stochastic objective and the deterministic approximation. (minor comments from Reviewer#2)\n2. added a subsection 6.6 to the appendix to empirically show the stability of the dropout rate convergence. (suggestion 4 from Reviewer #1)\n3. added one paragraph of descriptions of the terms used in our bound following the theorem 3.1 as suggested by Reviewer #1 (Q1). \n4. added two cases when our bounds are tight. (the last paragraph in section 3.1) This responds to the second concern raised by Review #2.\n5. added the definition of the empirical Rademacher complexity to section 3.1 as suggested by Reviwer #3.\n6. added a paragraph about the different notations used for vectors and scalars in subsection 6.1. (second paragraph of the proof) This is to respond the questions 2 and 3 raised by Reviewer #3.\n7. fixed all the typos pointed out by Reviewer #3. (comment 1 and 2 from Reviewer #3)\n8. added references of pages and chapters suggested by Reviewer #3.\n9. fixed the tiny column issues mentioned by Review #2 \n10. for the first concern raised by Review #2, we suggest reading section 6.3 in our appendix.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1513377798818,"tcdate":1513377798818,"number":3,"cdate":1513377798818,"id":"SJkzxRWzz","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Comment","forum":"S1uxsye0Z","replyto":"Bk15lpF1G","signatures":["ICLR.cc/2018/Conference/Paper204/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper204/Authors"],"content":{"title":"some explanations on the questions raised by reviewer #3","comment":"Thanks very much for your review and comments. \n\nAbout your major comments \n\n(1):\nThanks for your suggestion, we will include the definition of empirical Rademacher complexity in our revision.\n\n(2) and (3):\nAs we stated in the first paragraph of our proof 6.1, we treat the functions fed into the neurons of the l-th layer as one class of functions. Therefore, f^L(x;W) is a vector as you correctly pointed out, but f^L(x;w) is a scalar. So each dimension of f^L(x;W) is viewed as one instance coming from the same function class f^L(x;w). Similar ways of proof have been adopted in Wan et al. (2013). We are sorry about the confusion. We will add more descriptions about it to make that clear in our revision. \n\n(4)\nIt is a good question. The dependency on the number of classes comes from the contraction lemma. However, what we proved is only a weak bound on the Rademacher complexity. We are still working on further tightening the bound. For now, we are not sure if we can reduce the dependency on the number of classes to sub-linear. We hope this work will also open additional research directions and future extensions to the community. You are always welcome to add to it.\n\nAbout your minor comments:\n\n(1) (2) Thanks for the careful examination. We will fix the typos in the next version.\n\n(3) Thanks for the comments. \nContraction lemma (Shalev-Shwartz & Ben-David, 2014) is a variant of the Lemma 26.9 located on page 381, Chapter 26.\nLemma 26.11 in Shalev-Shwartz & Ben-David (2014) is located on page 383, Chapter 26.2. \nWe will add the chapters and pages to the proof. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1513377943276,"tcdate":1513377431289,"number":2,"cdate":1513377431289,"id":"ry1sRp-ff","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Comment","forum":"S1uxsye0Z","replyto":"HkOUXRFlz","signatures":["ICLR.cc/2018/Conference/Paper204/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper204/Authors"],"content":{"title":"some thoughts about the questions raised by reviewer #1","comment":"Thanks very much for your encouraging comments and helpful suggestions. \n\n1. The upper bound suggests that layers affect the complexity in a multiplicative way. An extreme case as we described in the last paragraph of section 3.2 is, if the dropout retain rates for one layer are all zeros, then the empirical Rademacher complexity for the whole network is zero since the network is doing random guess for predictions. In this case the bound is tight. We will put more descriptions about the terms in our bound.\n\n2. This is an interesting suggestion. Norm based regularizers currently used are imposed on the weights of each layer without considering the retain rates and the regularization is done on each layer independently. We suggest organizing them in a systematic way. \n\n3. In terms of running time, the proposed framework takes one additional backpropagation compared to its standard deep network counterpart. In practice, we find the running time per epoch after introducing the regularizer is approximately 1.6 to 1.9 times that of the current standard deep network. \n\n4: Thanks for the great suggestions.  There are some potential issues with drawing the confidence intervals for the retain rate of a particular neuron. For example, permuting the neurons does not change the network structure but it may lead to some identifiability issues. Instead to demo the stability of the algorithm we may add a plot showing the histograms of the theta with different initializations.  \n\n5. This is an excellent question! In fact, we are conducting additional evaluations to verify this pattern. We had some preliminary empirical observations that, as the layer goes higher, fewer neurons get high retain rates . This is somewhat consistent with the fact that people tend to set the number of neurons smaller for higher layers. We still need more experiments to tell if this is a general pattern.\n\n6. This is another great question. It is also related to an on-going follow-up work we are currently investigating as stated in the conclusion and future work section of our paper. If we use the setting of p=\\infty and q=1, the L1 norm regularizer may produce sparse retain rates. Subsequently, we could prune the corresponding neuron. Therefore we could use the algorithm as a way to determine the number of neurons used in hidden layers, i.e., we can use the regularizer to tune the network architecture. Similarly, if we use p=1 and q=\\infty, then we can expect sparse coefficients on W due to the property of the L1 norm, in this way the regularizer can also be used to prune the internal neural connections. \n\n7. Currently we do not have any theory for choosing p and q. As we stated above, one way is to choose p and q based on the sparsity desire. If we would like to impose sparsity on the number of neurons to fire,we may set q=1 to promote sparse retain rates. On the other hand, if we would like to impose sparsity on the number of internal connections, i.e., have a sparse coefficient matrix W, we may set p=1 instead."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1513377022230,"tcdate":1513377022230,"number":1,"cdate":1513377022230,"id":"S18-Tp-zz","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Comment","forum":"S1uxsye0Z","replyto":"Syx39Bsgf","signatures":["ICLR.cc/2018/Conference/Paper204/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper204/Authors"],"content":{"title":"some comments about the questions raised by reviewer#2","comment":"Thanks very much for your valuable comments and helpful suggestions. \n\nQ: Why adding Rad as a regularizer reasonable? Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad?\nA: These are great questions. We agree we do not have a rigorous way to prove adding an approximate upper bound to the objective can lead to any theoretical guarantee as you correctly pointed out.  The theorem of the upper bound in the paper is rigorous but why adding the upper bound to the objective can help is heuristic and empirical.\n\nOn the other hand, adding an approximate term that is related to the upper bound of the Rademacher complexity is not something new. For example, the squared L2 norm regularizer used in the ridge regression, though there are explanations such as Bayesian priors, can be interpreted as a term related to the upper bound of the Rademacher complexity of linear classes . People are already using it. Similarly, the L1 regularizer used in LASSO can also be interpreted as a term related to the Rademacher complexity bound. We have put a section in the Appendix (Section 6.3) to somewhat justify it in a heuristic way. \n\nQ: The actual resulting regularizer turns out to be… rather loose bound…\nA: We agree that the bound proved in the paper could be a bit loose. Still in some extreme cases it is tight. For example, as we indicated in the paragraph before Section 3.3, if the retain rates in one layer are all zeros, the model always makes random guess for prediction. In this case the empirical Rademacher complexity is zero and our bound is tight. In general, even if the bound is loose, it still gives some justification on the norms used in today’s neural network regularizations. Additionally, it leads to a systematic way of weighting the norms as well as the retain rates.\n\nMinor comments:\nQ: While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions. After all, the gradient used for regular learning is not based on the mean prediction, but rather the samples.\nA: This is an excellent question. As we stated in Section 3.3, “this is an approximation to the true f^L(x;W, θ)”. Using the mean is purely an approximation used for the sake of optimization efficiency. By design we should use the samples. However empirically we found that optimizing based on the mean (instead of the actual sampling) still leads to a decrease of the objective. We will add additional figures to better illustrate the point in our next revision.\n\nQ: tiny columns surrounding figures are ugly and hard to read\nA: Thanks for the suggestion. We will fix it in our revision.\n\nQ: dropout rate is perhaps more common than retain rate\nA: We use the retain rate instead just to make the upper bound look less messy. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1516045505823,"tcdate":1511901864415,"number":3,"cdate":1511901864415,"id":"Syx39Bsgf","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Review","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/AnonReviewer2"],"readers":["everyone"],"content":{"title":"does not actual say why using Rademacher as a regularizer is theoretically justified, and why the loose bound is reasonable","rating":"5: Marginally below acceptance threshold","review":"==Main comments\n\nThe authors connect dropout parameters to a bound of the Rademacher complexity (Rad) of the network. While it is great to see deep learning techniques inspired by learning theory, I think the paper makes too many leaps and the Rad story is ultimately unconvincing.  Perhaps it is better to start with the resulting regularizer, and the interesting direct optimization of dropout parameters. In its current form, the following leaps problematic and were not addressed in the paper:\n\n1) Why is is adding Rad as a regularizer reasonable? Rad is usually hard to compute, and most useful for bounding the generalization error. It would be interesting if it also turns out to be a good regularizer, but the authors do not say why nor cite anything. Like the VC dimension, Rad itself depends on the model class, and cannot be directly optimized. Even if you can somehow optimize over the model class, these quantities give very loose bounds, and do not equal to generalization error. For example, I feel even just adding the actual generalization error bound is more natural. Would it make sense to just add Rad to the objective in this way for a linear model?\n\n2) Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad? The actual resulting regularizer turns out to be a weight penalty but this seems to be a rather loose bound that might not have too much to do with Rad anymore. There should be some analysis on how loose this bound is, and if this looseness matter at all. \n\nThe empirical results themselves seem reasonable, but the results are not actually better than simpler methods in the corresponding tasks, the interpretation is less confident. Afterall, it seems that the proposed method had several parameters that were turned, where the analogous parameters are not present in the competing methods. And the per unit dropout rates are themselves additional parameters, but are they actually good use of parameters?\n\n==Minor comments\n\nThe optimization is perhaps also not quite right, since this requires taking the gradient of the dropout parameter in the original objective. While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions. The gradient used for regular learning is not based on the mean prediction, but rather the samples.\n\ntiny columns surrounding figures are ugly and hard to read\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1515642408576,"tcdate":1511805775704,"number":2,"cdate":1511805775704,"id":"HkOUXRFlz","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Review","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is an important piece of work that relates complexity of networks' learnability to dropout rates in backpropagation. This paper answers some critical questions about dropout learning.","rating":"7: Good paper, accept","review":"An important contribution. The paper is well written. Some questions that needs to be better answered are listed here.\n1. The theorem is difficult to decipher. Some remarks needs to be included explaining the terms on the right and what they mean with respect to learnability or complexity. \n2. How does the regularization term in eq (2) relate to the existing (currently used) norm based regularizers in deep network learning? It may be straight forward but some small simulation/plots explaining this is important. \n3. Apart from the accuracy results, the change in computational time for working with eq (2), rather than using existing state-of-the-art deep network optimization needs to be reported? How does this change vary with respect to dataset and network size (beyond the description of scaled regularization in section 4)?\n4. Confidence intervals needs to be computed for the retain-rates (reported as a function of epoch). This is critical both to evaluate the stability of regularizers as well as whether the bound from theorem is strong. \n5. Did the evaluations show some patterns on the retain rates across different layers? It seems from Figure 3,4 that retain rates in lower layers are more closer to 1 and they decrease to 0.5 as depth increases. Is this a general pattern? \n6. It has been long known that dropout relates to non-negative weighted averaging of partially learned neural networks and dropout rate of 0.5 provides best dymanics. The evaluations say that clearly 0.5 for all units/layers us not correct. What does this mean in terms of network architecture? Is it that some layers are easy to average (nothing is learned there, so dropped networks have small variance), while some other layers are sensitive? \n7. What are some simple guidelines for choosing the values of p and q? Again it appears p=q=2 is the best, but need confidence intervals here to say anything substantial. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1515766489376,"tcdate":1510752390645,"number":1,"cdate":1510752390645,"id":"Bk15lpF1G","invitation":"ICLR.cc/2018/Conference/-/Paper204/Official_Review","forum":"S1uxsye0Z","replyto":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference/Paper204/AnonReviewer3"],"readers":["everyone"],"content":{"title":"mathematical analysis seems not sound","rating":"5: Marginally below acceptance threshold","review":"This paper studies the adjustment of dropout rates which is a useful tool to prevent the overfitting of deep neural networks. The authors derive a generalization error bound in terms of dropout rates. Based on this, the authors propose a regularization framework to adaptively select dropout rates. Experimental results are also given to verify the theory.\n\nMajor comments:\n(1) The Empirical Rademacher complexity is not defined. For completeness, it would be better to define it at least in the appendix.\n(2) I can not follow the inequality (5). Especially, according to the main text, f^L is a vector-valued function . Therefore, it is not clear to me the meaning of \\sum\\sigma_if^L(x_i,w) in (5).\n(3) I can also not see clearly the third equality in (9). Note that f^l is a vector-valued function. It is not clear to me how it is related to a summation over j there.\n(4) There is a linear dependency on the number of classes in Theorem 3.1. Is it possible to further improve this dependency?\n\nMinor comments:\n(1) Section 4: 1e-3,1e-4,1e-5 is not consistent with 1e^{-3}, 1e^{-4},1e^{-5}\n(2) Abstract: there should be a space before \"Experiments\".\n(3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1\n\nSummary:\nThe mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis.\n\n----------------------------\nAfter Rebuttal:\nThank you for revising the paper. I think there are still some possible problems. \nLet us consider eq (12) in the appendix on the contraction property of Rademacher complexity (RC).\n(1) Since you consider a variant of RC with absolute value inside the supermum, to my best knowledge, the contraction property (12) should involve an additional factor of 2, see, e.g., Theorem 12 of \"Rademacher and Gaussian Complexities: Risk Bounds and Structural Results\" by Bartlett and Mendelson. Since you need to apply this contraction property L times, there should be a factor of 2^L in the error bound. This make the bound not appealing for neural networks with a moderate L.\n(2) Second, the function g involves an expectation w.r.t. r before the activation function. I am not sure whether this existence of expectation w.r.t. r would make the contraction property applicable in this case.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]}},{"tddate":null,"ddate":null,"tmdate":1515113461892,"tcdate":1509059311749,"number":204,"cdate":1509739428381,"id":"S1uxsye0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1uxsye0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adaptive Dropout with Rademacher Complexity Regularization","abstract":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art\ndeep learning algorithms impose dropout strategy to prevent feature co-adaptation.\nHowever, choosing the dropout rates remains an art of heuristics or\nrelies on empirical grid-search over some hyperparameter space. In this work,\nwe show the network Rademacher complexity is bounded by a function related\nto the dropout rate vectors and the weight coefficient matrices. Subsequently, we\nimpose this bound as a regularizer and provide a theoretical justified way to trade-off\nbetween model complexity and representation power. Therefore, the dropout\nrates and the empirical loss are unified into the same objective function, which is\nthen optimized using the block coordinate descent algorithm. We discover that\nthe adaptively adjusted dropout rates converge to some interesting distributions\nthat reveal meaningful patterns.Experiments on the task of image and document\nclassification also show our method achieves better performance compared to the\nstate-of-the-art dropout algorithms.","pdf":"/pdf/9441f616cac348cd06b585c987e3c56dd4bdca1f.pdf","TL;DR":"We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound.","paperhash":"anonymous|adaptive_dropout_with_rademacher_complexity_regularization","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Dropout with Rademacher Complexity Regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1uxsye0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper204/Authors"],"keywords":["model complexity","regularization","deep learning","model generalization","adaptive dropout"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}