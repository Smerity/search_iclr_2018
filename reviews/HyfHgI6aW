{"notes":[{"tddate":null,"ddate":null,"tmdate":1512276509423,"tcdate":1512270581380,"number":1,"cdate":1512270581380,"id":"rkpxjJ-WM","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Comment","forum":"HyfHgI6aW","replyto":"HJBOB_oxf","signatures":["ICLR.cc/2018/Conference/Paper73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper73/Authors"],"content":{"title":"Official Reply to AnonReviewer3","comment":"Dear Reviewer,\n\nThank you for your detailed review. We would like to answer some of the points raised by you in our response here :\n\n1) We agree that on a first pass it might look like the structure of the domains looks very similar. However, while writing this paper our focus was on the environments/domains that one might encounter in robotics and real world applications. We choose to demonstrate the feasibility of our architecture in such 2D/2.5D worlds and look to answer problems faced by other learning architectures in such worlds. Further, we added the graph experiment in section 3.3 to break up some of the structural similarity between the domains.  The graph experiment differs from the other domains in that the state space is no longer 2 dimensional. Further, the number of states observed by the agent and the number of valid actions varies as the agent visits each node. This is because the action space now depends on the number of vertices connected to the current node that the agent is in. Additionally, the action space in these graphs is also no longer a choice between up/down/left/right. The agent has to learn to pick the correct next node to visit and there are N-1 choices (where N is the number of nodes). \n\n\n2) We completely agree with the reviewer that remembering a map might hamper the ability of the work to be extended to other domains which might not have explicit geometric structure. This is in fact one of the limitations of \"Cognitive Mapping and Planning for Visual Navigation\" by Gupta et. al where learning an explicit top down map of the 3d environment might not be possible in some domains. Instead, in our work, we maintain a belief estimate over the environment which is represented in the external memory as a set of activations. We would like to draw the reviewer's attention to Fig 13 in the appendix. In this figure, we show the map estimate stored in the memory for the tunnel task. As one can see, the information stored in the memory does not correspond to the geometric structure of the environment. Instead, our proposed architecture learns to output different activations when the critical parts of the environment are observed by the agent. In the tunnel task, the memory exhibits one kind of activations when the agent observes the end of the tunnel, and when it turns out of the tunnel. For all other events, the memory shows no change in its activations. Additionally, when looking at the read/write weights when entering and exiting the tunnel, we see that the write weights are activated till the agent sees the end of the tunnel and the read weights are activated when the agent turns around. Thus, planning by remembering important events encountered in the environment allows us to use the proposed planner to domains where geometric structure might not exist. This is also something we wished to demonstrate by planning on graphs where there is no such explicit structure. \n\nWe thank the reviewer for pointing out the typos and other potentially confusing statements and will address them in the updated version. \n\n\nWe hope, this answers some of your concerns about our paper. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/89ebf0b1505d7e5e9b946ebdf1becada8503d5b0.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1512222737011,"tcdate":1511983101790,"number":3,"cdate":1511983101790,"id":"r1IWuK2lf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Review","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference/Paper73/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clever idea with strong supporting experimental evidence, but paper is missing key details about the approach","rating":"9: Top 15% of accepted papers, strong accept","review":"The paper presents a method for navigating in an unknown and partially observed environment is presented. The proposed approach splits planning into two levels: 1) local planning based on the observed space and 2) a global planner which receives the local plan, observation features, and access to an addressable memory to decide on which action to select and what to write into memory. \n\nThe contribution of this work is the use of value iteration networks (VINs) for local planning on a locally observed map that is fed into a learned global controller that references history and a differential neural computer (DNC), local policy, and observation features select an action and update the memory. The core concept of learned local planner providing additional cues for a global, memory-based planner is a clever idea and the thorough analysis clearly demonstrates the benefit of the approach.\n\nThe proposed method is tested against three problems: a gridworld, a graph search, and a robot environment. In each case the proposed method is more performant than the baseline methods.  The ablation study of using LSTM instead of the DNC and the direct comparison of CNN + LSTM support the authors’ hypothesis about the benefits of the two components of their method. While the author’s compare to DRL methods with limited horizon (length 4), there is no comparison to memory-based RL techniques. Furthermore, a comparison of related memory-based visual navigation techniques on domains for which they are applicable should be considered as such an analysis would illuminate the relative performance over the overlapping portions problem domains  For example, analysis of the metric map approaches on the grid world or of MACN on their tested environments.\n\nPrior work in visual navigation in partially observed and unknown environments have used addressable memory (e.g., Oh et al.) and used VINs (e.g., Gupta et al.) to plan as noted. In discussing these methods, the authors state that these works are not comparable as they operate strictly on discretized 2d spaces. However, it appears to the reviewer that several of these methods can be adapted to higher dimensions and be applicable at least a subclass (for the euclidean/metric map approaches) or the full class of the problems (for Oh et al.), which appears to be capable to solve non-euclidean tasks like the graph search problem. If this assessment is correct, the authors should differentiate between these approaches more thoroughly and consider empirical comparisons. The authors should further consider contrasting their approach with “Neural SLAM” by Zhang et al.\n\nA limitation of the presented method is requirement that the observation “reveals the labeling of nearby states.” This assumption holds in each of the examples presented: the neighborhood map in the gridworld and graph examples and the lidar sensor in the robot navigation example. It would be informative for the authors to highlight this limitation and/or identify how to adapt the proposed method under weaker assumptions such as a sensor that doesn’t provide direct metric or connectivity information such as a RGB camera. \n\nMany details of the paper are missing and should be included to clarify the approach and ensure reproducible results. The reviewer suggests providing both more details in the main section of the paper and providing the precise architecture including hyperparameters in the supplementary materials section. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/89ebf0b1505d7e5e9b946ebdf1becada8503d5b0.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1512222737053,"tcdate":1511912813168,"number":2,"cdate":1511912813168,"id":"HJBOB_oxf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Review","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference/Paper73/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper proposes a novel neural network architecture for planning in partially observable environments with sparse rewards. It uses a differentiable memory module to maintain an estimate of the geometry of the partially observable state, and splits planning into a two-level hierarchical process. Experiments in several domains demonstrate the validity of the proposed architecture. ","rating":"6: Marginally above acceptance threshold","review":"The paper addresses the important problem of planning in partially observable environments with sparse rewards, and the empirical verification over several domains is convincing. My main concern is that the structure of these domains is very similar - essentially, a graph where only neighboring vertices are directly observable, and because of this, the proposed architecture might not be applicable to planning in general POMDPs (or, in their continuous counterparts, state-space models). The authors claim that what is remembered by the planner does not take the form of a map, but isn't the map estimate \\hat{m} introduced at the end of Section 2.1 precisely such a map? From Section 2.4, it appears that these map estimates are essential in computing the low-level policies from which the final, high-level policy is computed. If the ability to maintain and use such local maps is essential for this method, its applicability is likely restricted to this specific geometric structure of domains and their observability. \n\nSome additional comments:\n\nP. 2, Section 2.1: does H(s) contain 0s for non-observable and 1s for observable states? If yes, please state it.\n\nP. 3: the concatenation of state and observation histories is missing from the definition of the transition function.\n\nP. 3, Eq. 1: overloaded notation - if T is the transition function for the large MDP on histories, it should not be used for the transition function between states. Maybe the authors meant to use f() for that transition?\n\nP. 3, Eq. 3: the sum is over i, but it is not clear what i indexes.\n\nP.3, end of Section 2.1: when computing the map estimate \\hat{m}, shouldn't the operator be min, that is, a state is assumed to be open (0), unless one or more observations show that it is blocked (-1)?\n\nP.5: the description of the reward function is inconsistent - is it 0 at the goal state, or >0?\n\nP. 11, above Fig. 9: typo, \"we observe that the in the robot world\"\n \n ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/89ebf0b1505d7e5e9b946ebdf1becada8503d5b0.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1512222737095,"tcdate":1511377643485,"number":1,"cdate":1511377643485,"id":"H1QljSQxz","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Review","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference/Paper73/AnonReviewer1"],"readers":["everyone"],"content":{"title":"What about strong motion-planning baselines?","rating":"4: Ok but not good enough - rejection","review":"Summary:\n\nA method is proposed for robot navigation in partially observable scenarios. E.g. 2D navigation in a grid world from start to goal but the robot can only sense obstacles in a certain radius around it. A learning-based method is proposed here which takes the currently discovered partial map as input to convolutional layers and then passes through K-iterations of a VIN module to a final controller. The controller takes as input both the convolutional features, the VIN module and has access to a differential memory module. A linear layer takes inputs from both the controller and memory and predicts the next step of the robot. This architecture is termed as MACN.\n\nIn experiments on 2D randomly generated grid worlds, general graphs and a simulated ground robot with a lidar, it is shown that memory is important for navigating partially observable environments and that the VIN module is important to the architecture since a CNN replacement doesn't perform as well. Also larger start-goal distances can be better handled by increasing the memory available.\n\nComments:\n\n- My main concern is that there are no non-learning based obvious baselines like A*, D*, D*-Lite and related motion planners which have been used for this exact task very successfully and run on real-world robots like the Mars rover. In comparison to the size of problems that can be handled by such planners the experiments shown here are much smaller and crucially the network can output actions which collide with obstacles while the search-based planners by definition will always produce feasible paths and require no training data. I would like to see in the experimental tables, comparison to path lengths produced by MACN vs. those produced by D*-Lite or Multi-Heuristic A*. While it is true that motion-planning will keep the entire discovered map in memory for the problem sizes shown here (2D maps: 16x16, 32x32, 64x64 bitmaps, general graphs: 9, 16, 25, 36 nodes) that is on the order of a few kB memory only. For the 3D simulated robot which is actually still treated as a 2D task due to the line lidar scanner MxN bitmap is not specified but even a few Mb is easily handled by modern day embedded systems. I can see that perhaps when map sizes exceed say tens of Gbs then perhaps MACN's memory will be smaller to obtain similar performance since it may learn better map compression to better utilize the smaller budget available to it. But experiments at that scale have not been shown currently.\n\n- Figure 1: There is no sensor (lidar or camera or kinect or radar) which can produce the kind of sensor observations shown in 1(b) since they can't look beyond occlusions. So such observations are pretty unrealistic.\n\n- \"The parts of the map that lie within the range of the laser scanner are converted to obstacle-free ...\": How are occluded regions marked?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/89ebf0b1505d7e5e9b946ebdf1becada8503d5b0.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1509739501632,"tcdate":1508888634246,"number":73,"cdate":1509739498973,"id":"HyfHgI6aW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/89ebf0b1505d7e5e9b946ebdf1becada8503d5b0.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}