{"notes":[{"tddate":null,"ddate":null,"tmdate":1514301525950,"tcdate":1514301525950,"number":5,"cdate":1514301525950,"id":"ByRUdkx7f","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Comment","forum":"HyfHgI6aW","replyto":"BJGZMyhWf","signatures":["ICLR.cc/2018/Conference/Paper73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper73/Authors"],"content":{"title":"Neural SLAM: Learning to Explore with External Memory","comment":"Thank you for bringing to our attention this work. This is definitely very interesting and we have included a pointer \nto your work in our related work section. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1514301461271,"tcdate":1514301461271,"number":4,"cdate":1514301461271,"id":"r1pfOkgmf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Comment","forum":"HyfHgI6aW","replyto":"r1IWuK2lf","signatures":["ICLR.cc/2018/Conference/Paper73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper73/Authors"],"content":{"title":"Official reply to AnonReviewer2","comment":"Dear Reviewer,\n\nWe would first off like to thank you for your strong support and feedback on our paper. Your detailed reviews will definitely help us in improving our paper. \n\nWe would like to answer some of the points raised by you in our response here:\n\nWe set up the CNN+Memory architecture to emulate the FRMQN from Oh et. al's work as closely as possible. The DNC actually improves upon the read, write and context architecture described in the paper. Further, in our experiments, we found that when training the CNN+Memory architecture with supervised learning, the network performed worse than our MACN. We hypothesized that if supervised learning was unable to learn a reasonable policy, then any reinforcement learning paradigm with sparse rewards would definitely do worse. \n\n\nWe would like to thank you for bringing to our attention the work of Zhang et. al - \"Neural SLAM\". To the best of our understanding, this paper focuses on using a SLAM formulation in a deep reinforcement learning paradigm which helps in exploration. Exploration is one topic that we have not explored in this work since we assume that there is always a path to the goal. In future work, we intend to extend our network to be trained with reinforcement learning instead of supervised learning. In such a setting, a Neural SLAM style architecture might help with exploration when the environment presents sparse rewards. \n\nWe have added a note to Section 5 regarding the need for perfect labeling of nearby states. We agree additional work is required to model sensors such as an RGB camera where such direct labeling might not be possible. The focus of this paper is to investigate the feasibility of a hierarchical learning scheme for planning in partially observable environments and hence we assume perfect sensors. In future work, using real-world sensors that do not always give a perfect labeling of nearby states will be one of our goals.\n\nWe have included all details one would need to reproduce our work in Section 2.4 under the computation graph. Further, experiment specific details are included in the appendix. It might be possible to present these in a more reader friendly format such as a table in the camera-ready version of the paper if required? Additionally, we intend to make our code publicly available. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1513066602933,"tcdate":1513066602933,"number":3,"cdate":1513066602933,"id":"BJ7deMpZf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Comment","forum":"HyfHgI6aW","replyto":"rkpxjJ-WM","signatures":["ICLR.cc/2018/Conference/Paper73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper73/Authors"],"content":{"title":"Updated Paper ","comment":"Dear Reviewer,\n\nWe have updated our paper to address the typos you had pointed out in our paper. \n\nAdditionally,  would like to answer one of the questions you had raised \n\n\"P.3, end of Section 2.1: when computing the map estimate \\hat{m}, shouldn't the operator be min, that is, a state is assumed to be open (0), unless one or more observations show that it is blocked (-1)?\"\n\nIf one were to consider a 1x1 in which we have 2 observations over time. if both obs are zero, the sum is zero and the max is zero thus indicating that the state is open.  \n\nif both obs are -1, sum is -2. The max is -1 indicating state is blocked. \nIf one of the observations is -1 and the other 0, the sum is -1 and the max is still -1 telling us that the state is blocked. \n\nAll other typos have been addressed in the paper. \nWe hope this answers your concerns about our paper and you re consider our paper favorably. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1513098465548,"tcdate":1513065037716,"number":2,"cdate":1513065037716,"id":"Hk8IcbTbz","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Comment","forum":"HyfHgI6aW","replyto":"H1QljSQxz","signatures":["ICLR.cc/2018/Conference/Paper73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper73/Authors"],"content":{"title":"Official Reply to AnonReviewer1","comment":"Dear Reviewer, \n\nWe would like to thank you for your detailed review. We would like to answer some of the points raised by you in our response here :\n\nWe have tried to address the reviewers concerns about comparing with motion planning baselines by adding another subsection. We agree it is useful to compare to A* and have added comparisons. We request the reviewer to look at Section 3.5 in the latest version. \n\nThat being said, we would like to point out several things here :\n\n1) There is a key difference between our approach and using a planning algorithm such as A*. A* is a model-based approach where you need to explicitly know beforehand knowledge about the cost, transition probabilities of the agent and explicitly construct a map. The motivation behind using a model-free approach such as ours is that these transition probabilities and cost function (in our case reward map -> low reward near obstacles and high reward in open space) is learned by the agent. \n\nThis is, in fact, the biggest motivation for using end to end learning approaches!  One does not need to explicitly know the model beforehand and can use the neural network to approximate it. Our proposed model learns the transition probabilities the cost map and the learns how to plan on the local map. \n\n2) Another advantage of using a model-free approach such as ours as opposed to A* is that our model learns a compact representation of the environment. This can be seen in Experiment 2 (grid world with tunnels)  and Section B in the Appendix. In the case of the tunnel environment, we can make the tunnels arbitrarily long (say 500 units in length). A* would have to expand all nodes going into the tunnel and would need to remember the entire map. \nHowever with our approach, we can use the same memory size for both 20 length tunnels as well as 500 since it records only the events where we enter and exit the tunnel as well as the end of the tunnel. Further, these events were not hand engineered. Instead, the network learned what events were important to understand the topology of a tunnel.  '\n\nWe absolutely agree with the reviewer that our sensor models are simplistic and we assume perfect models. In this work, we are focused on learning how to navigate a partially observable environment when an architecture consisting of a differentiable planner and memory are used. In future work, we would focus on extending our work to model sensor effects such as noise, occlusions. \n\nWe hope this answers some of your concerns about our paper and you reconsider our paper more favorably. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"ddate":null,"tddate":1512989185912,"tmdate":1512989373456,"tcdate":1512989178492,"number":1,"cdate":1512989178492,"id":"BJGZMyhWf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Public_Comment","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Neural SLAM: Learning to Explore with External Memory ","comment":"You might be interested to take a look\n\nNeural SLAM: Learning to Explore with External Memory \n(https://arxiv.org/pdf/1706.09520.pdf)\n\nWe present an approach for agents to learn representations of a global map from sensor data, to aid their exploration in new environments. To achieve this, we embed procedures mimicking that of traditional simultaneous localization and mapping (SLAM) into the soft attention based addressing of external memory architectures, in which the external memory acts as an internal representation of the environment for the agent. This structure encourages the evolution of SLAMlike behaviors inside a completely differentiable deep neural network. We show that this approach can help reinforcement learning agents to successfully explore new environments where long-term memory is essential. We validate our approach in both challenging grid-world environments and preliminary Gazebo experiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1512276509423,"tcdate":1512270581380,"number":1,"cdate":1512270581380,"id":"rkpxjJ-WM","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Comment","forum":"HyfHgI6aW","replyto":"HJBOB_oxf","signatures":["ICLR.cc/2018/Conference/Paper73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper73/Authors"],"content":{"title":"Official Reply to AnonReviewer3","comment":"Dear Reviewer,\n\nThank you for your detailed review. We would like to answer some of the points raised by you in our response here :\n\n1) We agree that on a first pass it might look like the structure of the domains looks very similar. However, while writing this paper our focus was on the environments/domains that one might encounter in robotics and real world applications. We choose to demonstrate the feasibility of our architecture in such 2D/2.5D worlds and look to answer problems faced by other learning architectures in such worlds. Further, we added the graph experiment in section 3.3 to break up some of the structural similarity between the domains.  The graph experiment differs from the other domains in that the state space is no longer 2 dimensional. Further, the number of states observed by the agent and the number of valid actions varies as the agent visits each node. This is because the action space now depends on the number of vertices connected to the current node that the agent is in. Additionally, the action space in these graphs is also no longer a choice between up/down/left/right. The agent has to learn to pick the correct next node to visit and there are N-1 choices (where N is the number of nodes). \n\n\n2) We completely agree with the reviewer that remembering a map might hamper the ability of the work to be extended to other domains which might not have explicit geometric structure. This is in fact one of the limitations of \"Cognitive Mapping and Planning for Visual Navigation\" by Gupta et. al where learning an explicit top down map of the 3d environment might not be possible in some domains. Instead, in our work, we maintain a belief estimate over the environment which is represented in the external memory as a set of activations. We would like to draw the reviewer's attention to Fig 13 in the appendix. In this figure, we show the map estimate stored in the memory for the tunnel task. As one can see, the information stored in the memory does not correspond to the geometric structure of the environment. Instead, our proposed architecture learns to output different activations when the critical parts of the environment are observed by the agent. In the tunnel task, the memory exhibits one kind of activations when the agent observes the end of the tunnel, and when it turns out of the tunnel. For all other events, the memory shows no change in its activations. Additionally, when looking at the read/write weights when entering and exiting the tunnel, we see that the write weights are activated till the agent sees the end of the tunnel and the read weights are activated when the agent turns around. Thus, planning by remembering important events encountered in the environment allows us to use the proposed planner to domains where geometric structure might not exist. This is also something we wished to demonstrate by planning on graphs where there is no such explicit structure. \n\nWe thank the reviewer for pointing out the typos and other potentially confusing statements and will address them in the updated version. \n\n\nWe hope, this answers some of your concerns about our paper. \n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1515642498755,"tcdate":1511983101790,"number":3,"cdate":1511983101790,"id":"r1IWuK2lf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Review","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference/Paper73/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Clever idea with strong supporting experimental evidence, but paper is missing key details about the approach","rating":"9: Top 15% of accepted papers, strong accept","review":"The paper presents a method for navigating in an unknown and partially observed environment is presented. The proposed approach splits planning into two levels: 1) local planning based on the observed space and 2) a global planner which receives the local plan, observation features, and access to an addressable memory to decide on which action to select and what to write into memory. \n\nThe contribution of this work is the use of value iteration networks (VINs) for local planning on a locally observed map that is fed into a learned global controller that references history and a differential neural computer (DNC), local policy, and observation features select an action and update the memory. The core concept of learned local planner providing additional cues for a global, memory-based planner is a clever idea and the thorough analysis clearly demonstrates the benefit of the approach.\n\nThe proposed method is tested against three problems: a gridworld, a graph search, and a robot environment. In each case the proposed method is more performant than the baseline methods.  The ablation study of using LSTM instead of the DNC and the direct comparison of CNN + LSTM support the authors’ hypothesis about the benefits of the two components of their method. While the author’s compare to DRL methods with limited horizon (length 4), there is no comparison to memory-based RL techniques. Furthermore, a comparison of related memory-based visual navigation techniques on domains for which they are applicable should be considered as such an analysis would illuminate the relative performance over the overlapping portions problem domains  For example, analysis of the metric map approaches on the grid world or of MACN on their tested environments.\n\nPrior work in visual navigation in partially observed and unknown environments have used addressable memory (e.g., Oh et al.) and used VINs (e.g., Gupta et al.) to plan as noted. In discussing these methods, the authors state that these works are not comparable as they operate strictly on discretized 2d spaces. However, it appears to the reviewer that several of these methods can be adapted to higher dimensions and be applicable at least a subclass (for the euclidean/metric map approaches) or the full class of the problems (for Oh et al.), which appears to be capable to solve non-euclidean tasks like the graph search problem. If this assessment is correct, the authors should differentiate between these approaches more thoroughly and consider empirical comparisons. The authors should further consider contrasting their approach with “Neural SLAM” by Zhang et al.\n\nA limitation of the presented method is requirement that the observation “reveals the labeling of nearby states.” This assumption holds in each of the examples presented: the neighborhood map in the gridworld and graph examples and the lidar sensor in the robot navigation example. It would be informative for the authors to highlight this limitation and/or identify how to adapt the proposed method under weaker assumptions such as a sensor that doesn’t provide direct metric or connectivity information such as a RGB camera. \n\nMany details of the paper are missing and should be included to clarify the approach and ensure reproducible results. The reviewer suggests providing both more details in the main section of the paper and providing the precise architecture including hyperparameters in the supplementary materials section. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1515642498837,"tcdate":1511912813168,"number":2,"cdate":1511912813168,"id":"HJBOB_oxf","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Review","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference/Paper73/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper proposes a novel neural network architecture for planning in partially observable environments with sparse rewards. It uses a differentiable memory module to maintain an estimate of the geometry of the partially observable state, and splits planning into a two-level hierarchical process. Experiments in several domains demonstrate the validity of the proposed architecture. ","rating":"6: Marginally above acceptance threshold","review":"The paper addresses the important problem of planning in partially observable environments with sparse rewards, and the empirical verification over several domains is convincing. My main concern is that the structure of these domains is very similar - essentially, a graph where only neighboring vertices are directly observable, and because of this, the proposed architecture might not be applicable to planning in general POMDPs (or, in their continuous counterparts, state-space models). The authors claim that what is remembered by the planner does not take the form of a map, but isn't the map estimate \\hat{m} introduced at the end of Section 2.1 precisely such a map? From Section 2.4, it appears that these map estimates are essential in computing the low-level policies from which the final, high-level policy is computed. If the ability to maintain and use such local maps is essential for this method, its applicability is likely restricted to this specific geometric structure of domains and their observability. \n\nSome additional comments:\n\nP. 2, Section 2.1: does H(s) contain 0s for non-observable and 1s for observable states? If yes, please state it.\n\nP. 3: the concatenation of state and observation histories is missing from the definition of the transition function.\n\nP. 3, Eq. 1: overloaded notation - if T is the transition function for the large MDP on histories, it should not be used for the transition function between states. Maybe the authors meant to use f() for that transition?\n\nP. 3, Eq. 3: the sum is over i, but it is not clear what i indexes.\n\nP.3, end of Section 2.1: when computing the map estimate \\hat{m}, shouldn't the operator be min, that is, a state is assumed to be open (0), unless one or more observations show that it is blocked (-1)?\n\nP.5: the description of the reward function is inconsistent - is it 0 at the goal state, or >0?\n\nP. 11, above Fig. 9: typo, \"we observe that the in the robot world\"\n \n ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1515642498904,"tcdate":1511377643485,"number":1,"cdate":1511377643485,"id":"H1QljSQxz","invitation":"ICLR.cc/2018/Conference/-/Paper73/Official_Review","forum":"HyfHgI6aW","replyto":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference/Paper73/AnonReviewer1"],"readers":["everyone"],"content":{"title":"What about strong motion-planning baselines?","rating":"4: Ok but not good enough - rejection","review":"Summary:\n\nA method is proposed for robot navigation in partially observable scenarios. E.g. 2D navigation in a grid world from start to goal but the robot can only sense obstacles in a certain radius around it. A learning-based method is proposed here which takes the currently discovered partial map as input to convolutional layers and then passes through K-iterations of a VIN module to a final controller. The controller takes as input both the convolutional features, the VIN module and has access to a differential memory module. A linear layer takes inputs from both the controller and memory and predicts the next step of the robot. This architecture is termed as MACN.\n\nIn experiments on 2D randomly generated grid worlds, general graphs and a simulated ground robot with a lidar, it is shown that memory is important for navigating partially observable environments and that the VIN module is important to the architecture since a CNN replacement doesn't perform as well. Also larger start-goal distances can be better handled by increasing the memory available.\n\nComments:\n\n- My main concern is that there are no non-learning based obvious baselines like A*, D*, D*-Lite and related motion planners which have been used for this exact task very successfully and run on real-world robots like the Mars rover. In comparison to the size of problems that can be handled by such planners the experiments shown here are much smaller and crucially the network can output actions which collide with obstacles while the search-based planners by definition will always produce feasible paths and require no training data. I would like to see in the experimental tables, comparison to path lengths produced by MACN vs. those produced by D*-Lite or Multi-Heuristic A*. While it is true that motion-planning will keep the entire discovered map in memory for the problem sizes shown here (2D maps: 16x16, 32x32, 64x64 bitmaps, general graphs: 9, 16, 25, 36 nodes) that is on the order of a few kB memory only. For the 3D simulated robot which is actually still treated as a 2D task due to the line lidar scanner MxN bitmap is not specified but even a few Mb is easily handled by modern day embedded systems. I can see that perhaps when map sizes exceed say tens of Gbs then perhaps MACN's memory will be smaller to obtain similar performance since it may learn better map compression to better utilize the smaller budget available to it. But experiments at that scale have not been shown currently.\n\n- Figure 1: There is no sensor (lidar or camera or kinect or radar) which can produce the kind of sensor observations shown in 1(b) since they can't look beyond occlusions. So such observations are pretty unrealistic.\n\n- \"The parts of the map that lie within the range of the laser scanner are converted to obstacle-free ...\": How are occluded regions marked?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]}},{"tddate":null,"ddate":null,"tmdate":1513063850619,"tcdate":1508888634246,"number":73,"cdate":1509739498973,"id":"HyfHgI6aW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyfHgI6aW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Memory Augmented Control Networks","abstract":"Planning problems in partially observable environments cannot be solved directly with convolutional networks and require some form of memory. But, even memory networks with sophisticated addressing schemes are unable to learn intelligent reasoning satisfactorily due to the complexity of simultaneously learning to access memory and plan. To mitigate these challenges we propose the Memory Augmented Control Network (MACN). The network splits planning into a hierarchical process. At a lower level, it learns to plan in a locally observed space. At a higher level, it uses a collection of policies computed on locally observed spaces to learn an optimal plan in the global environment it is operating in. The performance of the network is evaluated on path planning tasks in environments in the presence of simple and complex obstacles and in addition, is tested for its ability to generalize to new environments not seen in the training set.","pdf":"/pdf/00c8121b4eff79831a96f49e0ed92218ec69d232.pdf","TL;DR":"Memory Augmented Network to plan in partially observable environments. ","paperhash":"anonymous|memory_augmented_control_networks","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Augmented Control Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyfHgI6aW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper73/Authors"],"keywords":["planning","memory networks","deep learning","robotics"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}