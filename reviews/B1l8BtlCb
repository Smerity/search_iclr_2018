{"notes":[{"tddate":null,"ddate":null,"tmdate":1514974076366,"tcdate":1514974076366,"number":4,"cdate":1514974076366,"id":"S1VYsm9mM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Comment","forum":"B1l8BtlCb","replyto":"HJ-eMYYxz","signatures":["ICLR.cc/2018/Conference/Paper335/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper335/Authors"],"content":{"title":"Thoughtful review and interesting questions","comment":"As explained in the response to Reviewer 2, we decided to standardize on a single model size for the WMT experiments, but acknowledge that an evaluation of comparative performance at different sizes would be a worthwhile follow-up. However, direct comparison of the NAT at one model size to an autoregressive Transformer at a different model size may not be especially informative, because our NAT relies on an autoregressive teacher with the same model size in order to initialize the encoder.\n\nThe presence of the model distillation step also suggests that a meaningful BLEU improvement from larger NAT model size is unlikely, since only the NPD step allows the NAT to outperform its autoregressive teacher, even in the best case.\n\nWe believe that the NAT's gap in performance between the English/German language pair and the English/Romanian language pair suggests that it is sensitive to the degree of reordering; we agree that it would be worthwhile to follow up on this hypothesis with pairs like Japanese/English or Turkish/English that exhibit even more reordering than German/English does.\n\nThe external aligner we used produces fairly noisy results; our experiments with using the attention weights from an autoregressive Transformer as a (potentially more powerful) alignment model resulted in somewhat worse performance, suggesting that the dependence on alignment quality may not be straightforward.\n\nWe can revise our description of the positional attention layer."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1514973554880,"tcdate":1514973554880,"number":3,"cdate":1514973554880,"id":"B1oOt7qQf","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Comment","forum":"B1l8BtlCb","replyto":"B1Zh3McgM","signatures":["ICLR.cc/2018/Conference/Paper335/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper335/Authors"],"content":{"title":"Thoughtful review with many excellent points","comment":"The reviewer has brought up an interesting point about comparison of models at different sizes.\n\nWe agree that the gap on English/German WMT14 is large enough that a relatively smaller autoregressive Transformer, especially on short sentences and with highly optimized inference kernels, might achieve similar latency and accuracy to the NAT. But no amount of kernel optimization or model size reduction can change the sequential nature of autoregressive translation; the autoregressive latency will always be proportional to the sentence length and the NAT will be faster when sequences are sufficiently long. The non-autoregressive Transformer can also benefit from low-level optimizations like quantization; we believe we compared the two on an even footing by using similar implementations for both.\n\nAlso, while the original Transformer paper provided a strong set of baseline hyperparameters for the autoregressive architecture given a particular model size, we would need to conduct a significant amount of additional search to identify the right parameter settings for other model sizes. Instead we chose to focus our computational resources on the ablation study and more language pairs.\n\nWe think the difference between the EN<–>DE and EN<–>RO results may be the result of a greater need for long-distance (clause-level) reordering between English and German (which are closely related languages with significant differences in sentence structure) than between English and Romanian (which, while less closely related, have more similarities in word order); this is an interesting direction for future research.\n\nAs for computation time, we are making the assumption that a significant amount of parallelism is available and the primary metric is the latency on the critical path. This is not necessarily the case for every deployment of machine translation in practice, but it is a metric on which existing neural MT systems perform particularly poorly. Given that assumption, the additional computation needed for NPD, while potentially significant in terms of throughput, would not result in more than a doubling of latency."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1514972750018,"tcdate":1514972750018,"number":2,"cdate":1514972750018,"id":"S1U8U7qXM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Comment","forum":"B1l8BtlCb","replyto":"rJKwhzhxM","signatures":["ICLR.cc/2018/Conference/Paper335/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper335/Authors"],"content":{"title":"Reviewer raises good points","comment":"We agree that the methodology presented in this paper contains several moving parts and a few techniques, such as external alignment supervision, sequence-level knowledge distillation, and fine-tuning using reinforcement learning, that could be considered “tricks.” All of these techniques are targeted at solving the multi-modality problem introduced when performing non-autoregressive translation, but if there’s a particular part of the pipeline that you feel is unclear, we would be happy to improve the description and explanation.\n\nAlso, we would argue that our approach introduces relatively few additional hyperparameters over the original Transformer, primarily the inclusion of the various fine-tuning losses and the number of fertility samples used at inference time. While we did not conduct an exhaustive search over e.g. a range of possible values for the weights on each fine-tuning loss, we tried to present a reasonably comprehensive set of ablations to identify the effect of each part of our methodology. \n\nWe also agree that IBM 2 might not be the best possible choice of fertility inference model, since the model itself only considers fertility implicitly (as part of the alignment process) and not explicitly like IBM 3+. Our decision to use IBM 2 was based on the availability, performance, and ease of integration of a popular existing implementation (fast_align) of that particular alignment model. Meanwhile, the use of fertility supervision in the first place can be justified from two perspectives: from the variational inference perspective, an external alignment model provides a very simple and tractable proposal distribution; at a higher level, fertility supervision simply turns a difficult, unsupervised learning problem into an easier, supervised one."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1515642433715,"tcdate":1511955553224,"number":3,"cdate":1511955553224,"id":"rJKwhzhxM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Review","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference/Paper335/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Fast inference for transformer nmt","rating":"6: Marginally above acceptance threshold","review":"This paper can be seen as an extension of the paper \"attention is all you need\" that will be published at nips in a few weeks (at the time I write this review). \n\nThe goal here is to make the target sentence generation non auto regressive. The authors propose to introduce a set of latent variables to represent the fertility of each source words. The number of target words can be then derived and they're all predicted in parallel.\n\nThe idea is interesting and trendy. However, the paper is not really stand alone. A lot of tricks are stacked to reduce the performance degradation. However, they're sometimes to briefly described to be understood by most readers. \n\nThe training process looks highly elaborate with a lot of hyper parameters. Maybe you could comment on this. \n\nFor instance, the use fertility supervision during training could be better motivated and explained. Your choice of IBM 2 is wired since it doesn't include fertility. Why not IBM 4, for instance ? How you use IBM model for supervision. This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1515642433756,"tcdate":1511824553213,"number":2,"cdate":1511824553213,"id":"B1Zh3McgM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Review","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference/Paper335/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review of \"Non-autoregressive neural machine translation\"","rating":"7: Good paper, accept","review":"This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models). The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described). The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search. \n\nOverall, this is an interesting paper. It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores. Table 1 gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as NAT -- losing 2-5 BLEU points on WMT14 is significant.  While the Ro->En results are good, this particular language pair has not been used much by others; it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why WMT14 en->de and de->en are not improving more. Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1515642433798,"tcdate":1511784943295,"number":1,"cdate":1511784943295,"id":"HJ-eMYYxz","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Review","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference/Paper335/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting work","rating":"7: Good paper, accept","review":"This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words. The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs. To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations. For training, apply knowledge distilation for better training followed by fine tuning by reinforce. Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding.\n\nThe motivation is clear and proposed methods are very sound. Experiments are carried out very carefully.\n\nI have only minor concerns to this paper:\n\n- The experiments are designed to achieve comparable BLEU with improved latency. I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough.\n\n- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment.\n\n- What is the impact of the external word aligner quality? For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner. \n\n- The positional attention is rather unclear and it would be better to revise it. Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1511299081549,"tcdate":1511299081549,"number":1,"cdate":1511299081549,"id":"Sk-GdGflM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Official_Comment","forum":"B1l8BtlCb","replyto":"SJ7QvnZlM","signatures":["ICLR.cc/2018/Conference/Paper335/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper335/Authors"],"content":{"title":"Thanks!","comment":"You're right about the first error; that's a typo. For the second point, the fertility sequence is [2, 0, 1] because our analysis (and the network) counts the period/full stop as a third source/target token."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1511274267393,"tcdate":1511274267393,"number":2,"cdate":1511274267393,"id":"SJ7QvnZlM","invitation":"ICLR.cc/2018/Conference/-/Paper335/Public_Comment","forum":"B1l8BtlCb","replyto":"B1l8BtlCb","signatures":["~Ozan_Caglayan1"],"readers":["everyone"],"writers":["~Ozan_Caglayan1"],"content":{"title":"Two small glitches","comment":"Page 3, Paragraph 1: \"The factorization by length introduced .... first and third property but not the **first.**\"\nPage 6,  4.1.: Why is the fertility sequence [2,0,1] ? If I understood fertility correctly, I think it should have been [2,0] since number of tokens in the source sentence is 2."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]}},{"tddate":null,"ddate":null,"tmdate":1509739358259,"tcdate":1509098824198,"number":335,"cdate":1509739355600,"id":"B1l8BtlCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1l8BtlCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Non-Autoregressive Neural Machine Translation","abstract":"Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English–German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English–Romanian.","pdf":"/pdf/2918f051b465c39c49b9ef88c96683fcca4bbd79.pdf","TL;DR":"We introduce the first NMT model with fully parallel decoding, reducing inference latency by 10x.","paperhash":"anonymous|nonautoregressive_neural_machine_translation","_bibtex":"@article{\n  anonymous2018non-autoregressive,\n  title={Non-Autoregressive Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1l8BtlCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper335/Authors"],"keywords":["machine translation","non-autoregressive","transformer","fertility"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}