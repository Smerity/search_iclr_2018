{"notes":[{"tddate":null,"ddate":null,"tmdate":1512451593398,"tcdate":1512451593398,"number":1,"cdate":1512451593398,"id":"B1WzCoQZz","invitation":"ICLR.cc/2018/Conference/-/Paper936/Public_Comment","forum":"By-IifZRW","replyto":"Skf5I79gf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Strongly agree","comment":"I agree with this reviewer. Much of the mathematical derivation has been worked out before, even much of the uncertainty propagation part. I would add that [1] reviews many of the papers relying on these derivations.\n\nWhile the paper proposes an interesting model, I believe the paper can't really be accepted without any experimental verification.\n\n[1] http://jmlr.org/papers/volume17/damianou16a/damianou16a.pdf"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Neurons","abstract":"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\nFirst, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\nSecond, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\nThird, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\nThe resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\nLike a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","pdf":"/pdf/ade247befee7fc59f5293b8e372be246ef9e3cc3.pdf","TL;DR":"We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference.","paperhash":"anonymous|gaussian_process_neurons","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-IifZRW}\n}","keywords":["gaussian process neuron activation function stochastic transfer function learning variational bayes probabilistic"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper936/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222823050,"tcdate":1511866003792,"number":3,"cdate":1511866003792,"id":"Bkhq035gz","invitation":"ICLR.cc/2018/Conference/-/Paper936/Official_Review","forum":"By-IifZRW","replyto":"By-IifZRW","signatures":["ICLR.cc/2018/Conference/Paper936/AnonReviewer3"],"readers":["everyone"],"content":{"title":"GPN Review","rating":"7: Good paper, accept","review":"This paper investigates probabilistic activation functions that can be structured in a manner similar to traditional neural networks whilst deriving an efficient implementation and training regime that allows them to scale to arbitrarily sized datasets.\n\nThe extension of Gaussian Processes to Gaussian Process Neurons is reasonably straight forward, with the crux of the paper being the path taken to extend GPNs from intractable to tractable.\nThe first step, virtual observations, are used to provide stand ins for inputs and outputs of the GPN.\nThese are temporary and are later made redundant.\nTo avoid the intractable marginalization over latent variables, the paper applies variational inference to approximate the posterior within the context of given training data.\nOverall the process by which GPNs are made tractable to train leverages many recent and not so recent techniques.\n\nThe resulting model is theoretically scalable to arbitrary datasets as the total model parameters are independent of the number of training samples.\nIt is unfortunate but understandable that the GPN model experiments are confined to another paper.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Neurons","abstract":"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\nFirst, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\nSecond, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\nThird, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\nThe resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\nLike a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","pdf":"/pdf/ade247befee7fc59f5293b8e372be246ef9e3cc3.pdf","TL;DR":"We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference.","paperhash":"anonymous|gaussian_process_neurons","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-IifZRW}\n}","keywords":["gaussian process neuron activation function stochastic transfer function learning variational bayes probabilistic"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper936/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222823096,"tcdate":1511827082340,"number":2,"cdate":1511827082340,"id":"Skf5I79gf","invitation":"ICLR.cc/2018/Conference/-/Paper936/Official_Review","forum":"By-IifZRW","replyto":"By-IifZRW","signatures":["ICLR.cc/2018/Conference/Paper936/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Intriguing but incomplete work. No experimental validation.","rating":"4: Ok but not good enough - rejection","review":"In Bayesian neural networks, a deterministic or parametric activation is typically used. In this work, activation functions are considered random functions with a GP prior and are inferred from data.\n\n\n- Unnecessary complexity\n\nThe presentation of the paper is unnecessarily complex. It seems that authors spend extra space creating problems and then solving them. Although some of the derivations in Section 3.2.2 are a bit involved, most of the derivations up to that point (which is already in page 6) follow preexisting literature.\n\nFor instance, eq. (3) proposes one model for p(F|X). Eq. (8) proposes a different model for p(F|X), which is an approximation to the previous one. Instead, the second model could have been proposed directly, with the appropriate citation from the literature, since it isn't new. Eq. (13) is introduced as a \"solution\" to a non-existent problem, because the virtual observations are drawn from the same prior as the real ones, so it is not that we are \"coming up\" with a convenient GP prior that turns out to produce a computationally tractable solution, we are just using the prior on the observations consistently.\n\nIn general, the authors seem to use \"approximately equal\" and \"equal\" interchangeably, which is incorrect. There should be a single definition for p(F|X). And there should be a single definition for L_pred. The expression for L_pred given in eq. (20) (exact) and eq. (41) (approximate) do not match and yet both are connected with an equality (or proportionality), which they shouldn't.\n\nQ(A) is sometimes taken to mean the true posterior (i.e., eq. (31)), sometimes a Gaussian approximation (i.e., eq (32) inside the integral), and both are used interchangeably.\n\n\n- Incorrect references to the literature\n\nPage 3: \"using virtual observations (originally proposed by Quiñonero-Candela & Rasmussen (2005) for sparse approximations of GPs)\"\n\nThe authors are citing as the origin of virtual observations a survey paper on the topic. Of course, that survey paper correctly attributes the origin to [1].\n\nPage 4: \"we apply the technique of variational inference Wainwright et al. (2008)\".\n\nHow can variational inference be attributed to (again) a survey paper on the topic from 2008, when for instance [2] appeared in 2003?\n\n\n- Correctness of the approach\n\nCan the authors guarantee that the variational bound that they are introducing (as defined in eqs. (19) and (41)) is actually a variational bound? It seems to me that the approximations made to Q(A) to propagate the uncertainty are breaking the bounding guarantee. If it is no longer a lower bound, what is the rationale behind maximizing it?\n\nThe mathematical basis for this paper is actually introduced in [3] and a single-layer version of the current model is developed in [4]. However, in [4] the authors manage to avoid the additional Q(A) approximation that breaks the variational bound. The authors should contrast their approach with [4] and discuss if and why that additional central limit theorem application is necessary.\n\n\n- No experiments\n\nThe use of a non-parametric definition for the activation function should be contrasted with the use of a parametric one. With enough data, both might produce similar results. And the parameter sharing in the parametric one might actually be beneficial. With no experiments at all showing the benefit of this proposal, this paper cannot be considered complete.\n\n\n- Minor errors:\n\nEq. (4), for consistency, should use the identity matrix for the covariance matrix definition.\nEq. (10) uses subscript d where it should be using subscript n\nEq. (17) includes p(X^L|F^L) in the definition of Q(...), but it shouldn't. That was particularly misleading, since if we take eq. (17) to be correct (which I did at first), then p(X^L|F^L) cancels out and should not appear in eq. (20).\nEq. (23) uses Q(F|A) to mean the same as P(F|A) as far as I understand. Then why use Q?\n\n\n- References\n\n[1] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.\n[2] Beal, M.J. Variational Algorithms for Approximate Bayesian Inference.\n[3] M.K. Titsias and N.D. Lawrence. Bayesian Gaussian process latent variable model. \n[4] M. Lázaro-Gredilla. Bayesian warped Gaussian processes.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Neurons","abstract":"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\nFirst, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\nSecond, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\nThird, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\nThe resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\nLike a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","pdf":"/pdf/ade247befee7fc59f5293b8e372be246ef9e3cc3.pdf","TL;DR":"We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference.","paperhash":"anonymous|gaussian_process_neurons","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-IifZRW}\n}","keywords":["gaussian process neuron activation function stochastic transfer function learning variational bayes probabilistic"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper936/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222823140,"tcdate":1511804222006,"number":1,"cdate":1511804222006,"id":"H1IrTpFxz","invitation":"ICLR.cc/2018/Conference/-/Paper936/Official_Review","forum":"By-IifZRW","replyto":"By-IifZRW","signatures":["ICLR.cc/2018/Conference/Paper936/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Placing Gaussian process priors on the form of the activation functions in neural nets. The work is interesting but still very preliminary ","rating":"5: Marginally below acceptance threshold","review":"The paper addresses the problem of learning the form of the activation functions in neural networks.  The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net. This  somehow allows to non-parametrically infer from the data the \"shape\" of the activation functions needed for a specific problem.  The paper then proposes an inference framework (to approximately marginalize out all GP functions)  based on sparse GP methods that use inducing points and variational inference.  The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values (denoted by U in the paper).  Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations.   \n       \nIn general I like the idea and I believe that it can lead to a very useful model. However, I have found the current paper quite preliminary and incomplete.  The authors need to address the following:  \n\nFirst (very important): You need to show experimentally how your method compares against regular neural nets (with specific fixed forms for their activation functions such relus etc). At the moment in the last section you mention \"We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication. In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters.\" ===>  Well all this needs to be included in the same paper.  \n\nSecondly: Discuss the connection with Deep GPs (Damianou and Lawrence 2013). Your method seems to be connected with Deep GPs although there appear to be important differences as well. E.g. you place GPs on the scalar activation functions in an otherwise  heavily parametrized neural network (having interconnection weights between layers) while deep GPs model the full hidden layer mapping as a single GP (which does not require interconnection weights).  \n\nThirdly:  You need to better explain the propagation of uncertainly in section 3.2.2  and the central limit of distribution in section 3.2.1. This is the technical part of your paper which is a non-standard approximation. I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix.  \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Neurons","abstract":"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\nFirst, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\nSecond, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\nThird, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\nThe resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\nLike a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","pdf":"/pdf/ade247befee7fc59f5293b8e372be246ef9e3cc3.pdf","TL;DR":"We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference.","paperhash":"anonymous|gaussian_process_neurons","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-IifZRW}\n}","keywords":["gaussian process neuron activation function stochastic transfer function learning variational bayes probabilistic"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper936/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092385496,"tcdate":1509137241597,"number":936,"cdate":1510092362234,"id":"By-IifZRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By-IifZRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gaussian Process Neurons","abstract":"We propose a method to learn stochastic activation functions for use in probabilistic neural networks.\nFirst, we develop a framework to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks.\nSecond, we analytically derive expressions for the propagation of means and covariances in such a network, thus allowing for an efficient implementation and training without the need for sampling.\nThird, we show how to apply variational Bayesian inference to regularize and efficiently train this model.\nThe resulting model can deal with uncertain inputs and implicitly provides an estimate of the confidence of its predictions.\nLike a conventional neural network it can scale to datasets of arbitrary size and be extended with convolutional and recurrent connections, if desired.","pdf":"/pdf/ade247befee7fc59f5293b8e372be246ef9e3cc3.pdf","TL;DR":"We model the activation function of each neuron as a Gaussian Process and learn it alongside the weight with Variational Inference.","paperhash":"anonymous|gaussian_process_neurons","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Neurons},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By-IifZRW}\n}","keywords":["gaussian process neuron activation function stochastic transfer function learning variational bayes probabilistic"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper936/Authors"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}