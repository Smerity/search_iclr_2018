{"notes":[{"tddate":null,"ddate":null,"tmdate":1515094232223,"tcdate":1515094232223,"number":6,"cdate":1515094232223,"id":"HyxkZZhXM","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Comment","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper772/Authors"],"content":{"title":"Revision Summary","comment":"Dear Readers,\n\nWe have uploaded a revision. There are two main changes:\n\n1. An improvement to Lemma 4.1 that allows the embedding dimension $d$ in Theorem 4.1 to depend linearly (up to log factors) on the document length $T$. This directly addresses the concern of AnonReviewer1 that $d$ scales quadratically with $T$.\n\n2. Updates to Section 5 and the Appendix to a) clarify why basis pursuit is the natural choice for our setting and b) discuss weak sparse recovery conditions (including NSP/REP) in greater depth to see how they can help understand why BoW recovery improves when the sensing matrix consists of word embeddings. We hope these changes address the concern of AnonReviewer3 about the scope of compressed sensing considered in the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1513790646831,"tcdate":1513790646831,"number":5,"cdate":1513790646831,"id":"BkyahGuzM","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Comment","forum":"B1e5ef-C-","replyto":"rytGHUTWM","signatures":["ICLR.cc/2018/Conference/Paper772/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper772/Authors"],"content":{"title":"Response to follow up","comment":"Thank you for following up.\n\n1) “Much weaker conditions than RIP that are sufficient for sparse recovery should be explicitly mentioned. The authors say \"These embeddings cannot fit into the usual compressed sensing worldview since the matrix defined by the embeddings cannot satisfy RIP.\" Compressed sensing is not RIP.” (Paraphrase)\n\nYou are right; as discussed later in the paper an explanation should likely come from some other (weaker) condition. We chose to focus on the polytope condition but agree that discussion of others (like REC) is warranted and will include it in revision. \n\n\n2) “Rather than verify NSP (NP-hard), empirically check conditions that imply it. For example, the restricted singular/eigenvalue condition (Theorem 3.2 of Chandrasekaran et al.) implies it. Try generating random directions in the tangent cone, computing their norms, and checking the constant distribution; notice that in this case RSC is equivalent to RS/EC.” (Paraphrase)\n\nWe computed such upper bounds on RE constants** for both pretrained and random embeddings and found that they were smaller for the former (note that larger constants are better for recovery). Unfortunately this does not settle the issue about RE property for pretrained embeddings as lower bounds for random vectors are vacuous when recovery isn’t perfect (e.g. when the dimension is small enough that pretrained embeddings do better) (Banerjee et al., 2014). Note that verifying RE is also NP-hard (Dobriban & Fan, 2016). We will discuss these points in revision.\n\n\n(** We don’t know how to sample directions uniformly from tangent cone ---rejection sampling doesn’t work well as the intersection of the cone with the unit sphere is very small compared to the unit sphere--- so these bounds were computed in a greedy manner.)\n\n\n3) “LASSO/Dantzig selector are common compressive sensing algorithms (LASSO is possibly more popular than Basis Pursuit). Hence it feels weird to equate compressive sensing to BP in Appendix A. Maybe the authors can simply mention that this paper focuses on the BP algorithm, and defer the others to future work.” \n\nAppendix A represents a brief overview of only the parts of compressed sensing needed in the main paper. We didn’t include LASSO/Dantzig because the word embeddings setting has no signal/measurement noise, in which case both methods are equivalent to BP. The revision will clarify.\n\n\n4) “In the response the authors mentioned that they had tried LASSO but it didn't work as well. I'm not sure what the authors exactly did, but I guess in this case the *constrained* LASSO would perform well.” \n\nWe have indeed tried constrained LASSO and it works quite well, but BP works slightly better. This does not seem surprising to us since we are in the noiseless setting. We’ll add a note to that effect.\n\n\n5) “The connection to \"classical information theory\" is still quite vague (you refer to a paper which was inspired by the MDL and used LZ77, and I see very little connection with this paper).” \n\nWe’re happy to omit the problematic phrase; it only refers to a past work of Paskov et al. (which uses compression ideas on Bag-of-n-Grams representation). \n\n\nA. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. “Estimation with Norm Regularization.” NIPS 2014.\nE. Dobriban and J. Fan. “Regularity Properties for Sparse Regression.” Communications in Mathematics and Statistics 2016.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1513084176929,"tcdate":1513084176929,"number":4,"cdate":1513084176929,"id":"rytGHUTWM","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Comment","forum":"B1e5ef-C-","replyto":"SkhuaD3Zf","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper772/AnonReviewer3"],"content":{"title":"follow up ","comment":"Thanks for the authors' detailed response. It seems that the authors missed some of my points, hence a few further clarifications below:\n\n1) Regarding the conditions that I posed (NSP, RSC, etc.), my main point is that there exist much weaker conditions than RIP which are sufficient for sparse recovery. This should at least be explicitly mentioned, otherwise researchers in compressive sensing will have doubts to many of the statements in this paper. \n\nFor instance, in Section 5, the authors wrote \"These embeddings cannot fit into the usual compressed sensing worldview since the matrix defined by the embeddings cannot satisfy RIP.\" Compressed sensing is not RIP. Moreover, this is certainly not surprising to most high-dimensional statisticians as RIP is a very strong condition. On the other hand, they would be much more surprised if the RSC fails to hold (which is suggested by the authors in the response; I'm much more interested in this fact).\n\n2) I did not ask the authors to verify the NSP, which is of course NP-hard, but rather to check empirically the conditions that imply NSP. For instance, the famous Restricted Singular/Eigenvalue Condition does the job (see Theorem 3.2 of [Chandrasekran et al. 2012]): Generate random directions in the tangent cone, compute their norms, and check the RSP constant distribution; notice that in this case RSC is equivalent to RS/EC. Again, it is very intriguing to me that the RSC fails not hold, as suggested by the authors.\n\n3) LASSO/Dantzig selector are common compressive sensing algorithms (LASSO is possibly more popular than Basis Pursuit). Hence it feels weird to equate compressive sensing to BP in Appendix A. Maybe the authors can simply mention that this paper focuses on the BP algorithm, and defer the others to future work.\n\n\nSome minor comments:\n\n4) In the response the authors mentioned that they had tried LASSO but it didn't work as well. I'm not sure what the authors exactly did, but I guess in this case the *constrained* LASSO would perform well. Maybe the authors can take a shot.\n\n5) The connection to \"classical information theory\" is still quite vague (you refer to a paper which was inspired by the MDL and used LZ77, and I see very little connection with this paper). I personally feel that using \"classical information theory\" only damages the credibility of this paper to the experts. The simplest way to remedy this is to remove the term; otherwise, making the connection explicit is also an option."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1513024938865,"tcdate":1513024938865,"number":3,"cdate":1513024938865,"id":"BkX26PhZf","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Comment","forum":"B1e5ef-C-","replyto":"SkzuQ_dlG","signatures":["ICLR.cc/2018/Conference/Paper772/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper772/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Thank you for the positive review! We are currently preparing a revision incorporating these comments. We would also like to clarify that our paper concerns LSTM document embeddings, not word embeddings."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1513024884279,"tcdate":1513024884279,"number":2,"cdate":1513024884279,"id":"SkhuaD3Zf","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Comment","forum":"B1e5ef-C-","replyto":"H1kgYgogM","signatures":["ICLR.cc/2018/Conference/Paper772/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper772/Authors"],"content":{"title":"Response to AnonReviewer3","comment":"Thank you for the thorough review! We’ll revise incorporating your comments.\n\nMain Responses:\n\n1) “instead of using [Donoho & Tanner 2005] it should be better to use [Chandrasekaran et al. 2012]’s deterministic condition, the null space property or NSP” (paraphrase)\n\nWe knew of NSP but turned to Donoho & Tanner (2005) because NSP is difficult to work with (no obvious method to check if local NSP holds; checking global NSP is NP-hard (Tillmann & Pfetsch, 2014)). Since NSP is equivalent to exact recovery, our experiments (Fig. 1-2) strongly suggest that local NSP holds, but we did not find a way to use it to gain intuition or proofs. While closely related to NSP, the polytope condition of Donoho & Tanner (2005) implies Corollary 5.1, which suggests both a nice property of word embeddings and an efficient method to check recovery of nonnegative signals.\n\n 2): “[the claim that] certain LSTMs with random initialization are at least as good as the linear classifiers… ...is almost a direction application of the RIP of random Rademacher matrices”\n\nThis is true for the unigram (BoW) case. The proof for the n-gram case necessitated constructing a design matrix with correlated entries for which RIP is not as obvious. We agree that the bigger technical contribution is in connecting these ideas to text embeddings.\n\n\nOther Responses: \n\nRestricted Strong Convexity (RSC): “it is proved in [Chandrasekran et al. 2012]  that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery.”\n\nTo our knowledge RSC is used mostly for the case of signal/measurement noise (Negahban et al., 2010; Chandrasekaran et al., 2012), whereas we are in the noiseless setting. We know of work by Elenberg et al. (2016) using RSC to guarantee recovery with Orthogonal Matching Pursuit, but we have found that such greedy methods do not work well for pretrained embeddings (Section 5.1 paragraph 2), indicating that a sufficient RSC condition does not hold.\n\nLASSO/Dantzig Selector: “the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise.”\n\nLASSO was in fact the first approach we tried, with similar results as Basis Pursuit (we refer to it in Section 5.1 paragraph 2 as an “l_0-surrogate method”). However, as we are in the noiseless setting we do not need the robustness provided by LASSO; indeed, experiments show it performs somewhat worse for both pretrained and random vectors. Furthermore, to our knowledge guarantees for LASSO often have analogous results for Basis Pursuit, so the theoretical benefit to studying it is unclear. Although we did not try the Dantzig Selector, it can also be seen as a robust extension of Basis Pursuit and so similarly does not provide a clear advantage in our case.\n\n\nMinor Points:\n\n1. We use the phrase “classical information theory” only in connection with the scheme in Paskov et al., (2013) which is inspired by the Lempel-Ziv compression algorithm (Ziv & Lempel, 1977); 40 years old and directly inspired by Shannon’s works!\n2. In theory the regularization constant C is chosen to minimize the error bound; in practice it is chosen by cross-validation.\n3. We extend the analysis in order to handle logistic loss as it is commonly used in the NLP community and by supervised LSTMs. We do not need Theorem 4.2 to hold for all Lipschitz functions to get Theorem 4.1, but the function does need to be Lipschitz to control the error.\n4.1 This assumption is without loss of generality and is made to remove a spurious dependence on T in the error bound.\n4.2. There will sometimes be n-cooccurrences that contain a word more than once, e.g. (as, long, as), but they occur infrequently and can be removed by merging words as a preprocessing step. In the SST training corpus only 0.019% of bigrams and 0.75% of trigrams have this issue, the latter often due to words between two commas in a list.\n5-8. Will be addressed in revision.\n\n\nV. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. “The Convex Geometry of Linear Inverse Problems.” Found. of Comp. Mathematics 2012.\nD. L. Donoho and J. Tanner. “Sparse nonnegative solution of underdetermined linear equations by linear programming.” PNAS 2005.\nE. R. Elenberg, R. Khanna, A. G. Dimakis, and S. Negahban. “Restricted strong convexity implies weak submodularity.” arXiv 2016.\nS. Negahban, B. Yu, M. J. Wainwright, and P. K. Ravikumar. “A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers.” NIPS 2009.\nH. S. Paskov, R. West, J. C. Mitchell, and T. J. Hastie. “Compressive feature learning.” NIPS 2013.\nA. M. Tillmann and M. E. Pfetsch. “The computational complexity of the restricted isometry property, the nullspace property, and related concepts in compressed sensing.” IEEE Trans. on Info. Theory 2014.\nJ. Ziv and A. Lempel. “A Universal Algorithm for Sequential Data Compression.” IEEE Trans. on Info. Theory 1977."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1513024563317,"tcdate":1513024563317,"number":1,"cdate":1513024563317,"id":"BJi42DnZM","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Comment","forum":"B1e5ef-C-","replyto":"SyURgFFlG","signatures":["ICLR.cc/2018/Conference/Paper772/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper772/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Thank you for the positive review! We are currently preparing a revision incorporating these comments. \n\nComment: “the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.”\nYes the bound may scale poorly with document length. At the moment many tasks in this area use short sentences (e.g. SST has avg. length < 20), and Fig. 4 indicates convergence of DisC to BonC performance even on the IMDB task (avg. length > 250) so perhaps our bound is too pessimistic. Note that in the unigram (BoW) case the scaling is (provably) linear in T because then the design matrix is an i.i.d. Rademacher ensemble."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1515642506093,"tcdate":1511880934735,"number":3,"cdate":1511880934735,"id":"H1kgYgogM","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Review","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. ","rating":"6: Marginally above acceptance threshold","review":"My review reflects more from the compressive sensing perspective, instead that of deep learners.\n\nIn general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.\n\nThe paper studies text embeddings through the lens of compressive sensing theory. The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices. Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow.\n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory. Partial explanations are provided, again using results in compressive sensing theory. In my personal opinion, the explanations are opaque and unsatisfactory. An alternative route is suggested in my detailed review.\nFinally, extensive experiments are conducted and they are in accordance with the theory.\n\nMy most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5.\n\nSpecifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not. This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery.\n\nIn particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]. There, a simple deterministic condition (the null space property) for successful recovery is proved. It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs. Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all. While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC.\n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise.\n\nSeveral minor comments:\n\n1. Please avoid the use of “information theory”, especially “classical information theory”, in the current context. These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon. I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there’s no need to mention information theory here.\n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen.\n\n3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case. I understood the necessity only through reading proofs.\n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical?\n\n5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017].\n\n6. Page 2, first paragraph of related work, the sentence “Our method also closely related to ...” is incomplete.\n\n7. Page 2, second paragraph of related work, “Pagliardini also introduceD a linear ...”\n\n8. Page 9, conclusion, the beginning sentence of the second paragraph is erroneous.\n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, “The Convex Geometry of Linear Inverse Problems”, Foundations of Computational Mathematics, 2012.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1515642506128,"tcdate":1511784653592,"number":2,"cdate":1511784653592,"id":"SyURgFFlG","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Review","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting paper","rating":"7: Good paper, accept","review":"The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized.\n\n-In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1515642506165,"tcdate":1511715690041,"number":1,"cdate":1511715690041,"id":"SkzuQ_dlG","invitation":"ICLR.cc/2018/Conference/-/Paper772/Official_Review","forum":"B1e5ef-C-","replyto":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference/Paper772/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper applies techniques from compressed sensing to analyze the classification performance of LSTM word embeddings","rating":"7: Good paper, accept","review":"The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings. They then show that the result matrix satisfies a restricted isometry condition.  Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.\n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]}},{"tddate":null,"ddate":null,"tmdate":1515189441603,"tcdate":1509134472178,"number":772,"cdate":1509739108778,"id":"B1e5ef-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1e5ef-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs","abstract":"Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of word embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, a standard sparse recovery tool, which may explain why they lead to better representations in practice.","pdf":"/pdf/ec3476f11e52d37f23359661e72549c5487a0915.pdf","TL;DR":"We use the theory of compressed sensing to prove that LSTMs can do at least as well on linear text classification as Bag-of-n-Grams.","paperhash":"anonymous|a_compressed_sensing_view_of_unsupervised_text_embeddings_bagofngrams_and_lstms","_bibtex":"@article{\n  anonymous2018a,\n  title={A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1e5ef-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper772/Authors"],"keywords":["theory","LSTM","unsupervised learning","word embeddings","compressed sensing","sparse recovery","document representation","text classification"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}