{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222795503,"tcdate":1512033824456,"number":3,"cdate":1512033824456,"id":"Hku7RS6lf","invitation":"ICLR.cc/2018/Conference/-/Paper844/Official_Review","forum":"rye7IMbAZ","replyto":"rye7IMbAZ","signatures":["ICLR.cc/2018/Conference/Paper844/AnonReviewer1"],"readers":["everyone"],"content":{"title":"limited novelty, but consistently improving fine-tuning ","rating":"6: Marginally above acceptance threshold","review":"The paper addresses the problem of transfer learning in deep networks. A pretrained network on a large dataset exists, what is the best way to retrain the model on a new small dataset? \nIt argues that the standard regularization done in conventional fine-tuning procedures is not optimal, since it tries to get the parameters close to zero, thereby forgetting the information learnt on the larger dataset.\nIt proposes to have a regularization term that penalizes divergence from initialization (pretrained network) as opposed to from zero-vector. It tries different norms (L2, L1, group Lasso) as well as Fisher information matrix to avoid interfering with important nodes, and shows the effectiveness of these alternatives over the standard practice of “weight decay”.\n\nAlthough the novelty of the paper is limited and have been shown for transfer learning with SVM classifiers prior to resurgence of deep learning, the reviewer is unable to find a prior work doing same regularization in deep networks. Number of datasets and experiments are moderately high, results are consistently better than standard fine-tuning and fine-tuning is a very common tool for ML practitioners in various application fields, so, I think there is benefit for transfer learning audience to be exposed to the experiments of this paper.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":" Explicit Induction Bias for Transfer Learning with Convolutional Networks","abstract":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.\nWhen using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.\nHowever, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source domain.\nIn this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.\nWe eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","pdf":"/pdf/bb8ba0eddb3a867a237b35d80049dda523080a18.pdf","TL;DR":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.","paperhash":"anonymous|explicit_induction_bias_for_transfer_learning_with_convolutional_networks","_bibtex":"@article{\n  anonymous2018,\n  title={ Explicit Induction Bias for Transfer Learning with Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rye7IMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper844/Authors"],"keywords":["transfer Learning","convolutional networks","fine-tuning","regularization","induction bias"]}},{"tddate":null,"ddate":null,"tmdate":1512222795549,"tcdate":1511816334711,"number":2,"cdate":1511816334711,"id":"ryD53e9xG","invitation":"ICLR.cc/2018/Conference/-/Paper844/Official_Review","forum":"rye7IMbAZ","replyto":"rye7IMbAZ","signatures":["ICLR.cc/2018/Conference/Paper844/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A reasonably thorough study of regularization techniques for transfer learning through fine-tuning","rating":"5: Marginally below acceptance threshold","review":"This work addresses the scenario of fine-tuning a pre-trained network for new data/tasks and empirically studies various regularization techniques. Overall, the evaluation concludes with recommending that all layers of a network whose weights are directly transferred during fine-tuning should be regularized against the initial net with an L2 penalty during further training. \n\nRelationship to prior work:\nRegularizing a target model against a source model is not a new idea. The authors miss key connections to A-SVM [1] and PMT-SVM [2] -- two proposed transfer learning models applied to SVM weights, but otherwise very much the same as the proposed solution in this paper. Though the study here may offer new insights for deep nets, it is critical to mention prior work which also does analysis of these regularization techniques. \n\nSignificance:\nAs the majority of visual recognition problems are currently solved using variants of fine-tuning, if the findings reported in this paper generalize, then it could present a simple new regularization which improves the training of new models. The change is both conceptually simple and easy to implement so could be quickly integrated by many people.\n\nClarity and Questions:\nThe purpose of the paper is clear, however, some questions remain unanswered. \n1) How is the regularization weight of 0.01 chosen? This is likely a critical parameter. In an experimental paper, I would expect to see a plot of performance for at least one experiment as this regularization weighting parameter is varied. \n2) How does the use of L2 regularization on the last layer effect the regularization choice of other layers? What happens if you use no regularization on the last layer? L1 regularization?\n3) Figure 1 is difficult to read. Please at least label the test sets on each sub-graph.\n4) There seems to be some issue with the freezing experiment in Figure 2. Why does performance of L2 regularization improve as you freeze more and more layers, but is outperformed by un-freezing all. \n5) Figure 3 and the discussion of linear dependence with the original model in general seems does not add much to the paper. It is clear that regularizing against the source model weights instead of 0 should result in final weights that are more similar to the initial source weights. I would rather the authors use this space to provide a deeper analysis of why this property should help performance. \n6) Initializing with a source model offers a strong starting point so full from scratch learning isn’t necessary -- meaning fewer examples are needed for the continued learning (fine-tuning) phase. In a similar line of reasoning, does regularizing against the source further reduce the number of labeled points needed for fine-tuning? Can you recover L2 fine-tuning performance with fewer examples when you use L2-SP?\n\n[1] J. Yang, R. Yan, and A. Hauptmann. Adapting svm classifiers to data with shifted distributions. In ICDM Workshops, 2007.\n[2]  Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In Proc. ICCV, 2011.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":" Explicit Induction Bias for Transfer Learning with Convolutional Networks","abstract":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.\nWhen using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.\nHowever, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source domain.\nIn this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.\nWe eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","pdf":"/pdf/bb8ba0eddb3a867a237b35d80049dda523080a18.pdf","TL;DR":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.","paperhash":"anonymous|explicit_induction_bias_for_transfer_learning_with_convolutional_networks","_bibtex":"@article{\n  anonymous2018,\n  title={ Explicit Induction Bias for Transfer Learning with Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rye7IMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper844/Authors"],"keywords":["transfer Learning","convolutional networks","fine-tuning","regularization","induction bias"]}},{"tddate":null,"ddate":null,"tmdate":1512222795593,"tcdate":1511708763402,"number":1,"cdate":1511708763402,"id":"BJQD_I_eM","invitation":"ICLR.cc/2018/Conference/-/Paper844/Official_Review","forum":"rye7IMbAZ","replyto":"rye7IMbAZ","signatures":["ICLR.cc/2018/Conference/Paper844/AnonReviewer3"],"readers":["everyone"],"content":{"title":"well written, needs more comparisons/analysis","rating":"6: Marginally above acceptance threshold","review":"The paper proposes an analysis on different adaptive regularization techniques for deep transfer learning. \nSpecifically it focuses on the use of an L2-SP condition that constraints the new parameters to be close to the\nones previously learned when solving a source task. \n\n+ The paper is easy to read and well organized\n+ The advantage of the proposed regularization against the more standard L2 regularization is clearly visible \nfrom the experiments\n\n- The idea per se is not new: there is a list of shallow learning methods for transfer learning based \non the same L2 regularization choice\n[Cross-Domain Video Concept Detection using Adaptive SVMs, ACM Multimedia 2007]\n[Learning categories from few examples with multi model knowledge transfer, PAMI 2014]\n[From n to n+ 1: Multiclass transfer incremental learning, CVPR 2013]\nI believe this literature should be discussed in the related work section\n\n- It is true that the L2-SP-Fisher regularization was designed for life-long learning cases with a \nfixed task, however, this solution seems to work quite well in the proposed experimental settings. \nFrom my understanding L2-SP-Fisher can be considered the best competitor of L2-SP so I think\nthe paper should dedicate more space to the analysis of their difference and similarities both\nfrom the theoretical and experimental point of view. For instance:\n--  adding the L2-SP-Fisher results in table 2\n--  repeating the experiments of figure 2 and figure 3 with L2-SP-Fisher\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":" Explicit Induction Bias for Transfer Learning with Convolutional Networks","abstract":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.\nWhen using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.\nHowever, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source domain.\nIn this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.\nWe eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","pdf":"/pdf/bb8ba0eddb3a867a237b35d80049dda523080a18.pdf","TL;DR":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.","paperhash":"anonymous|explicit_induction_bias_for_transfer_learning_with_convolutional_networks","_bibtex":"@article{\n  anonymous2018,\n  title={ Explicit Induction Bias for Transfer Learning with Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rye7IMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper844/Authors"],"keywords":["transfer Learning","convolutional networks","fine-tuning","regularization","induction bias"]}},{"tddate":null,"ddate":null,"tmdate":1509739069926,"tcdate":1509135896469,"number":844,"cdate":1509739067271,"id":"rye7IMbAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rye7IMbAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":" Explicit Induction Bias for Transfer Learning with Convolutional Networks","abstract":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.\nWhen using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.\nHowever, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source domain.\nIn this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.\nWe eventually recommend a simple $L^2$ penalty using the pre-trained model as a reference, and we show that this approach behaves much better than the standard scheme using weight decay on a partially frozen network.","pdf":"/pdf/bb8ba0eddb3a867a237b35d80049dda523080a18.pdf","TL;DR":"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch.","paperhash":"anonymous|explicit_induction_bias_for_transfer_learning_with_convolutional_networks","_bibtex":"@article{\n  anonymous2018,\n  title={ Explicit Induction Bias for Transfer Learning with Convolutional Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rye7IMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper844/Authors"],"keywords":["transfer Learning","convolutional networks","fine-tuning","regularization","induction bias"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}