{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222698731,"tcdate":1511910096378,"number":3,"cdate":1511910096378,"id":"B1_R9Digf","invitation":"ICLR.cc/2018/Conference/-/Paper598/Official_Review","forum":"r1kNDlbCb","replyto":"r1kNDlbCb","signatures":["ICLR.cc/2018/Conference/Paper598/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple but useful extension of recent works, missing important experiments","rating":"6: Marginally above acceptance threshold","review":"TL;DR of paper: Generating summaries by using summaries as an intermediate representation for autoencoding the document. An encoder reads in the document to condition the generator which outputs a summary. The summary is then used to condition the decoder which is trained to output the original document. An additional GAN loss is used on the generator output to encourage the output to look like summaries -- this procedure only requires unpaired summaries. The results are that this procedure improves upon the trivial baseline  but still significantly underperforms supervised training.\n\nThis paper builds upon two recent trends:  a) cycle consistency, where f(g(x)) = x, which only requires unpaired data (i.e., CycleGAN), and (b) encoder-decoder models with a sequential latent representation (i.e., \"Language as a latent variable\" by Miao and Blunsom). A similar idea has also been explored by He et al. 2016 in \"Dual Learning for Machine Translation\". Both CycleGAN and He et al. 2016 are not cited. The key difference between this paper and He et al. 2016 is the use of GANs so only unpaired summaries are needed.\n\nThe idea is a simple but useful extension of these previous works. The problem set-up of unpaired summarization is not particularly compelling, since summaries are typically found paired with their original documents. It would be more interesting to see how well it can be used for other textual domains such as translation, where a lot of unpaired data exists (some other submissions to ICLR tackle this problem). Unsurprisingly, the proposed method requires a lot of twiddling to make it work since GANs, REINFORCE, and pretraining are necessary.\n\nA key baseline that is missing is pretraining the generator as a language model over summaries. The pretraining baseline in the paper is over predicting the next sentence / reordering, but this is an unfair comparison since the next sentence baseline never sees summaries over the course of training. Without this baseline, it is hard to tell whether GAN training is even useful. Another experiment missing is seeing whether joint supervised-GAN-reconstruction training can outperform purely supervised training. What is the performance of the joint training as the size of the supervised dataset is varied?\n\nThis paper has numerous grammatical and spelling errors throughout the paper (worse, the same errors are copy-pasted everywhere). Please spend more time editing the paper.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network","abstract":"Auto-encoders compress the input data into latent-space representation and reconstruct the original data from the representation. The latent representation cannot be easily interpreted by human. In this paper, we propose a new idea to train an auto-encoder that encodes input text into comprehensible sentences. The auto-encoder is composed of a generator and a reconstructor.  The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the input of generator from the output of generator. To make the generator output comprehensible by human, a discriminator restricts the output of the generator to look like the summaries written by human. By taking the generator output as the summary of the input text, abstractive summarization can be achieved without document-summery pairs as training data. Promising results were obtained on both English and Chinese corpora.","pdf":"/pdf/dd330b899933bf82d9022b3fefeeb1e191ebfbb4.pdf","paperhash":"anonymous|learn_to_encode_text_as_comprehensible_summary_by_generative_adversarial_network","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kNDlbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper598/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222699107,"tcdate":1511857563527,"number":2,"cdate":1511857563527,"id":"S1ms6cqxM","invitation":"ICLR.cc/2018/Conference/-/Paper598/Official_Review","forum":"r1kNDlbCb","replyto":"r1kNDlbCb","signatures":["ICLR.cc/2018/Conference/Paper598/AnonReviewer1"],"readers":["everyone"],"content":{"title":"LEARN TO ENCODE TEXT AS COMPREHENSIBLE SUMMARY BY GENERATIVE ADVERSARIAL NETWORK","rating":"4: Ok but not good enough - rejection","review":"Summary: In this work, the authors propose a text reconstructing auto encoder which takes a sentence as the input sequence and an integrated text generator generates another version of the input text while a reconstructor determines how well this generated text reconstructs the original input sequence. The input to the discriminator (as real data) is a sentence that summarizes the ground truth sentences (rather than the ground truth sentences themselves). The experiments are conducted in two datasets of English and Chinese corpora.\n\nStrengths:\nThe proposed idea of generating text using summary sentences is new.\nThe model overview in Figure 1 is informative.\nThe experiments are conducted on English and Chinese corpora, comparison with competitive baselines are provided.\n\nWeaknesses:\nThe paper is poorly written which makes it difficult to understand. The second paragraph in the introduction is quite cryptic. Even after reading the entire paper a couple of times, it is not clear how the summary text is obtained, e.g. do the authors ask annotators to read sentences and summarize them? If so, based on which criteria do the annotators summarize text, how many annotators are there? Similarly, if so this would mean that the authors use additional supervision than the compared models. Please clarify how the summary text is obtained.\n\nIn footnote 1, the authors mention “seq2seq2seq2” term which they do not explain anywhere in the text.\n\nNo experiments that generate raw text (without using summaries) are provided. It would be interesting to see if GAN learns to memorize the ground truth sentences or generates sentences with enough variation. \n\nIn the English Gigaword dataset the results consistently drop compared to WGAN. This behavior is observed for both the unsupervised setting and two versions of transfer learning settings. There are too few qualitative results: One positive qualitative result is provided in Figure 3 and one negative qualitative result is provided in Figure 4. Therefore, it is not easy for the reader to judge the behavior of the model well. \n\nThe choice of the evaluation metric is not well motivated. The standard measures in the literature also include METEOR, CIDER and SPICE. It would be interesting to see how the proposed model performs in these additional criteria. Moreover, the results are not sufficiently discussed. \n\nAs a general remark, although the idea presented in this paper is interesting, both in terms of writing and evaluation, this paper has not yet reached the maturity expected from an ICLR paper. Regarding writing, the definite and indefinite articles are sometimes missing and sometimes overused, similarly most of the times there is a singular/plural mismatch. This makes the paper very difficult to read. Often the reader needs to guess what is actually meant. Regarding the experiments, presenting results with multiple evaluation criteria and showing more qualitative results would improve the exposition.\n\nMinor comments:\nPage 5: real or false —> real or fake (true or false)\n\t     the lower loss it get —> ?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network","abstract":"Auto-encoders compress the input data into latent-space representation and reconstruct the original data from the representation. The latent representation cannot be easily interpreted by human. In this paper, we propose a new idea to train an auto-encoder that encodes input text into comprehensible sentences. The auto-encoder is composed of a generator and a reconstructor.  The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the input of generator from the output of generator. To make the generator output comprehensible by human, a discriminator restricts the output of the generator to look like the summaries written by human. By taking the generator output as the summary of the input text, abstractive summarization can be achieved without document-summery pairs as training data. Promising results were obtained on both English and Chinese corpora.","pdf":"/pdf/dd330b899933bf82d9022b3fefeeb1e191ebfbb4.pdf","paperhash":"anonymous|learn_to_encode_text_as_comprehensible_summary_by_generative_adversarial_network","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kNDlbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper598/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222699148,"tcdate":1511832508566,"number":1,"cdate":1511832508566,"id":"HkSpi4cgz","invitation":"ICLR.cc/2018/Conference/-/Paper598/Official_Review","forum":"r1kNDlbCb","replyto":"r1kNDlbCb","signatures":["ICLR.cc/2018/Conference/Paper598/AnonReviewer2"],"readers":["everyone"],"content":{"title":"GANs for text, with text latent variables","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a model for generating long text strings given shorter text strings, and for inferring suitable short text strings given longer strings. Intuitively, the inference step acts as a sort of abstractive summarization. The general gist of this paper is to take the idea from \"Language as a Latent Variable\" by Miao et al., and then change it from a VAE to an adversarial autoencoder. The authors should cite \"Adversarial Autoencoders\" by Makzhani et al. (ICLR 2016).\n\nThe experiment details are a bit murky, and seem to involve many ad-hoc decisions regarding preprocessing and dataset management. The vocabulary is surprisingly small. The reconstruction cost is not precisely explained, though I assume it's a teacher-forced conditional log-likelihood (conditioned on the \"summary\" sequence). The description of baselines for REINFORCE is a bit strange -- e.g., annealing a constant in the baseline may affect variance of the gradient estimator, but the estimator is still unbiased and shouldn't significantly impact exploration. Similar issues are present in the \"Self-critical...\" paper by Rennie et al. though, so this point isn't a big deal.\n\nThe results look decent, but I would be more impressed if the authors could show some benefit relative to the supervised model, e.g. in a reasonable semisupervised setting. Overall, the paper covers an interesting topic but could use extra editing to clarify details of the model and training procedure, and could use some redesign of the experiments to minimize the number of arbitrary (or arbitrary-seeming) decisions.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network","abstract":"Auto-encoders compress the input data into latent-space representation and reconstruct the original data from the representation. The latent representation cannot be easily interpreted by human. In this paper, we propose a new idea to train an auto-encoder that encodes input text into comprehensible sentences. The auto-encoder is composed of a generator and a reconstructor.  The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the input of generator from the output of generator. To make the generator output comprehensible by human, a discriminator restricts the output of the generator to look like the summaries written by human. By taking the generator output as the summary of the input text, abstractive summarization can be achieved without document-summery pairs as training data. Promising results were obtained on both English and Chinese corpora.","pdf":"/pdf/dd330b899933bf82d9022b3fefeeb1e191ebfbb4.pdf","paperhash":"anonymous|learn_to_encode_text_as_comprehensible_summary_by_generative_adversarial_network","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kNDlbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper598/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739209864,"tcdate":1509127975270,"number":598,"cdate":1509739207204,"id":"r1kNDlbCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1kNDlbCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network","abstract":"Auto-encoders compress the input data into latent-space representation and reconstruct the original data from the representation. The latent representation cannot be easily interpreted by human. In this paper, we propose a new idea to train an auto-encoder that encodes input text into comprehensible sentences. The auto-encoder is composed of a generator and a reconstructor.  The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the input of generator from the output of generator. To make the generator output comprehensible by human, a discriminator restricts the output of the generator to look like the summaries written by human. By taking the generator output as the summary of the input text, abstractive summarization can be achieved without document-summery pairs as training data. Promising results were obtained on both English and Chinese corpora.","pdf":"/pdf/dd330b899933bf82d9022b3fefeeb1e191ebfbb4.pdf","paperhash":"anonymous|learn_to_encode_text_as_comprehensible_summary_by_generative_adversarial_network","_bibtex":"@article{\n  anonymous2018learn,\n  title={Learn to Encode Text as Comprehensible Summary by Generative Adversarial Network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kNDlbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper598/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}