{"notes":[{"tddate":null,"ddate":null,"tmdate":1515182161535,"tcdate":1515182161535,"number":6,"cdate":1515182161535,"id":"BJFUdLTQM","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Comment","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper822/Authors"],"content":{"title":"A revised version of paper is uploaded.","comment":"We made the following changes in the revised version:\n\n- We did additional proofreading.\n\n- We adjusted resolution of figures for better presentation.\n\n- We added results of VGG variations for additional data set (SVHN) in Section 5 and Appendix A.\n\n- We explained why we use simple method and what other methods we tried in Section 2.3 and Appendix B.\n\n- We fixed a typo in proximal operator function in Section 2.3.\n\n- We explained how our method is different from using slightly weaker regularization strength in Section 2.3.\n\n- We added explanation of Figure 5.\n\n- We added p-values for improvements.\n\n- We removed unsubstantiated speculations in Section 3.\n\n- We further explained why we did not use ImageNet data set in Section 5. \n\n- We explained that our method could not be applied to ResNet without normalization in Section 5.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1514947221348,"tcdate":1514947221348,"number":5,"cdate":1514947221348,"id":"SJp9GptXM","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Comment","forum":"Bys_NzbC-","replyto":"BJk4jatgM","signatures":["ICLR.cc/2018/Conference/Paper822/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper822/Authors"],"content":{"title":"The sudden failure in performance found by our analysis is novel. ","comment":"We thank the reviewer for asking important questions.\n\n1. Novelty\n\nWe agree with the referee that the phenomenon that the performance first increases and then falls down as the regularization parameter increases is well known for constrained optimization problems. However, we observe a different phenomenon that the performance first increases and then \"suddenly\" fails at some point as the regularization parameter increases in deep networks. The sudden failure (as opposed to gradual falling) in performance found by our analysis is novel.  \n\nAlso, in order to claim that the sudden failure in performance also follows from vanishing gradients problem in deep networks, we would at least need to know that the gradients would SUDDENLY vanish as the regularization parameter increases. To the best of our knowledge, there is no such an analysis, be it theoretical or empirical. In contrast, we conduct our analysis in an opposite direction — first of all, we empirically demonstrated that the early-stage weights would diminish suddenly, when the regularization parameter is increased right above a certain threshold. This together with the aid of the derivation in Section 2.2 lead us to conclude that the gradients would suddenly vanish as the regularization parameter increases.\n\nIn other words, knowing the empirically found diminishing weights, we have explained the sudden failure in performance by sudden vanishing gradients through the derivation in Section 2.2. Indeed, this intuition has also guided us to introduce the Delayed Strong Regularization in this paper. (Please see the difference between Delayed Strong Regularization and Reducing Regularization Parameter clarified below.)\n\n\n\n1a. Delayed Strong Regularization vs. Reducing Regularization Parameter\n\nThey are not equivalent. Reducing the regularization parameter means that we enforce weaker regularization in each training step. This is different from our approach (Delayed Strong Regularization) where we enforce the same strong regularization in each training step after five (\\gamma) epochs. In fact, by skipping regularization for the first five epochs out of 300 epochs, the total reduced amount by regularization throughout training is decreased. However, the decreased amount is negligible. Indeed, our approach does not fail in learning with regularization parameter that is two orders of magnitude greater than the highest regularization parameter the baseline can adopt without a fail.\n\nIn case the reviewer meant reducing the regularization strength by \"gradually\" reducing regularization strength throughout the training, we also performed a simple experiment with VGG-16 on CIFAR-100. We set the initial regularization parameter \\lambda=2*10^-3 and 6*10^-5 for L2 and L1 regularization, respectively, which are just above the \"tolerance level\". Then, we continuously reduced \\lambda to zero throughout the training session. The trained models didn't show any improvement over \"random guess\", which means that they were not able to learn. \n\n\n\n1b. Why should we keep increasing the regularization constant beyond a limit?\n\nOften, deep neural networks need strong regularization especially when they are too complex while training data is small. Although data is key in deep learning, it is often very expensive to obtain, so it is difficult to secure enough data set for the networks in practice. When the model is overfitted, one possible solution is to keep increasing the regularization strength, and strong regularization may boost the accuracy of the networks. \n\nAlthough many models are still over-fitted, stronger regularization cannot be achieved due to the vanishing gradient problem in the deep networks, as described in the paper (especially Section 2.2). In the beginning of the training, where gradient is small for stochastic gradient descent method, we find that learning fails if strong regularization is enforced. However, we find that we can overcome this by waiting for the model to reach an \"active learning\" phase, where the gradients' magnitudes are significant, and then enforcing strong regularization. Delayed Strong Regularization enables us to obtain the superior performance that is otherwise hidden by learning failure in deep networks.\n\nStrong regularization provides not only a better accuracy for over-fitted models but also more model compression. We show that we can achieve 2 to 4 times more compression compared to the baseline. The model compression can be done by other approaches such as pruning and quantization, but compression by regularization is also effective especially for removing neurons in groups with group sparsity. Certainly, there are recent efforts on this direction (Wen et al., 2016; Scardapane et al., 2017; Yoon & Hwang, 2017). Although our approach is not applied to group sparsity regularization in this paper, our approach has no limit on it."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1514947164311,"tcdate":1514947164311,"number":4,"cdate":1514947164311,"id":"HJNPfatQf","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Comment","forum":"Bys_NzbC-","replyto":"BJk4jatgM","signatures":["ICLR.cc/2018/Conference/Paper822/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper822/Authors"],"content":{"title":"We described why we do not employ the state-of-the-art architectures in the paper. Here is further clarification.","comment":"2.  Clarification\n\n- State-of-the-art architectures\nAs explained in Section 2.2 and 3, we do not employ architectures that contain normalization techniques. Please see the Normalization paragraph of Section 2.2 for details. Unfortunately, most recent architectures contain normalization techniques. In order to apply our approach to recent architectures such as Residual Networks, we actually tried to intentionally excluded normalization part from them. However, we could not control the exploding gradients caused by the exclusion of normalization.\n\n- More data sets\nAs described in Section 5, we did not experiment on ImageNet only because it requires much time to train each model although we need to train many models. We need to fine-tune the models with different regularization parameters, and we also need multiple training sessions of each model to obtain confidence interval. For example, the experiment results in Figure 3 and 4 include 750 training sessions in total. This is something we cannot afford with ImageNet data set, which requires several weeks of training for EACH session (unless you have GPU clusters). However, we instead performed more experiments on another data set. Specifically, we will add results of different VGG architectures on the SVHN data set, in order to see the difference in the tolerance level that is caused by a different number of hidden layers. We will add these results in the revised version.\n\n- Explanation of Figure 5\nHere is the detailed explanation of Figure 5. Through the grace period where the regularization parameter is zero, we expect the model to reach an \"active learning\" phase with an elevated gradient amount (e.g., green and blue lines in Figure 2b reach there in a couple of epochs). We hypothesize that once the model reaches there, it does not suffer from vanishing gradients any more even when strong regularization is enforced. We empirically show that the hypothesis is valid in Figure 5a, where the gradient amount does not decrease when the strong regularization is enforced (at epoch=5).\n\n In Figure 5b, although the same strong regularization is enforced since epoch 5, the magnitude of weights in our model stops decreasing around epoch 20, while that in baseline keeps decreasing towards zero. This means that our model can cope with strong regularization, and it maintains its equilibrium between gradients from L and those from regularization. We will change the Figure 5b and its description to make it more clear.\n\n- p-value\nWe did not compute p-values since we only ran three training sessions for each model. However, as suggested by the reviewer, we computed the p-value and found that most improvements are statistically significant (p < 0.05). We will include the exact p-values in the revised version.\n\nThank you again for the comments, and we will make them clear in the next version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1514945176506,"tcdate":1514945176506,"number":2,"cdate":1514945176506,"id":"B1ej93KXz","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Comment","forum":"Bys_NzbC-","replyto":"rk43lpulz","signatures":["ICLR.cc/2018/Conference/Paper822/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper822/Authors"],"content":{"title":"Thank you for the interesting suggestion. Here's our answer and experiment result for the suggestion.","comment":"We thank the reviewer for the interesting suggestion. \n\nIn our paper, we proposed to adopt strong regularization for two main goals. One goal is to improve the model's accuracy, and the other goal is to compress the model while the accuracy is kept at the same level. Your suggestion meets especially the latter one. However, we think that it will be very difficult for your suggested approach to perform well with deep neural networks. Once the strong regularization is enforced in the beginning, the magnitudes of weights decrease quickly. This in turn drives the magnitudes of gradients to diminish exponentially in deep neural networks as explained in Section 2.2, and thus, the model loses its ability to learn after a short period of strong regularization. Even if we reduce the strength of the regularization after the strong regularization, it will be difficult for the model to recover its learning ability because the gradients are proportional to the product of the weights at later layers.\n\nIn order to actually check if your suggested method works, we performed a simple experiment with VGG-16 on CIFAR-100. We set the initial regularization parameter \\lambda=2*10^-3 and 6*10^-5 for L2 and L1 regularization, respectively, which are just above the \"tolerance level\". Then, we continuously reduced \\lambda_t to zero throughout the training session. The trained models didn't show any improvement over \"random guess\", which means that they were not able to learn. \n\nWe could not perform experiments on ImageNet for the following reason (as answered to other reviewers).\n\nAs described in Section 5, we did not experiment on ImageNet only because it requires much time to train each model although we need to train many models. We need to fine-tune the models with different regularization parameters, and we also need multiple training sessions of each model to obtain confidence interval. For example, the experiment results in Figure 3 and 4 include 750 training sessions in total. This is something we cannot afford with ImageNet data set, which requires several weeks of training for EACH session (unless we have GPU clusters). \n\nHowever, we instead performed additional experiments on another data set. Specifically, we will add results of different VGG architectures on the SVHN data set, in order to see the difference in the tolerance level that is caused by a different number of hidden layers. We will add these results in the revised version.\n\nWe will make these points clear in the revised draft."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1514944478413,"tcdate":1514944478413,"number":1,"cdate":1514944478413,"id":"rJLJ_3KXz","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Comment","forum":"Bys_NzbC-","replyto":"H1U0Hpcef","signatures":["ICLR.cc/2018/Conference/Paper822/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper822/Authors"],"content":{"title":"We tried other approaches, but ours achieved the best accuracy while it is the simplest.","comment":"We thank the reviewer for the insightful comments.\n\nWe agree that our proposed approach is very simple. The reason we employed the simple method is that it is effective while it is simple to implement for readers. The only additional hyper-parameter, which is the number of initial epochs to skip regularization, is also not difficult to set. We think that the proposed method is very close to the traditional regularization method so that it inherits the traditional one's good performance for non-strong regularization while it also achieves strong regularization. \n\nWe actually tried a couple more approaches other than the proposed one in our preliminary experiments. We found that the proposed one shows the best accuracy among the approaches we tried while it is the simplest. For example, we tried an approach that can be regarded as a warm-start strategy. It starts with the regularization parameter \\lambda_t=0, and then it gradually increases \\lambda_t to \\lambda for \\gamma epochs, where \\gamma >= 0 and it is empirically set. We found that it can achieve strong regularization, but its best accuracy is slightly lower than that of our proposed approach. We think that this is because our model can explore the search space more freely without regularization while the warm-start model enforces some regularization during the warm-up stage. \n\nWe also tried a method that is similar to Ivanov regularization. In this method, the regularization term is applied only when the L1 norm of the weights is greater than a certain threshold. To enforce strong regularization, we set the \\lambda just above the tolerance level that is found by the baseline method. However, this method did not accomplish any learning. The reason is that, to reach the level of L1 norm that is low enough, the model needs to go through the strong regularization for the first few epochs, and the neurons already lose its learning ability during this period like the baseline method. If we set the lambda below the tolerance level, it cannot reach the desired L1 norm without strong regularization, and thus the performance is inferior to our proposed method. \n\nWe did not extend these preliminary experiments to full experiments because the required number of training sessions is overwhelming, and the preliminary results were not promising. As mentioned in the answers to the other reviewers, the number of training sessions needed for the results in Figure 3 and 4 was 750, which takes quite much time. We will add this discussion to the paper in the new version to make it clear. We will also add more experiment results on another data set to convince readers of our proposed method's superiority. Specifically, we will add results of different VGG architectures on the SVHN data set, in order to see the difference in the tolerance level that is caused by a different number of hidden layers.\n\nWe also thank the detailed comments at the end of the review. We agree with the reviewer, and we will revise the paper accordingly."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1515642516926,"tcdate":1511867854539,"number":3,"cdate":1511867854539,"id":"H1U0Hpcef","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Review","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting diagnosis but a rudimentary cure","rating":"5: Marginally below acceptance threshold","review":"The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization. When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning. The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \"strong\" penalty parameter. In their experimental results, the phase transition is not observed anymore with their protocol. This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol.\n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked. The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem. I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions. I think that the problem would be better handled that way than with the proposed strategy, which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases.\n\nTo summarize, I believe that the paper addresses an important point, but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere.\n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9) \n- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper \n- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1515642516963,"tcdate":1511803687302,"number":2,"cdate":1511803687302,"id":"BJk4jatgM","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Review","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors propose a modification on regularization weights in an attempt to impose very strong regularization on backpropataion. There are several issues with the proposal as well as its efficacy in practice. The current statues of the paper needs more work.","rating":"4: Ok but not good enough - rejection","review":"The paper is well motivated and written. However, there are several issues.\n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems. Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks. The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel. Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)? \n1b. Why should we keep increasing the regularization constant beyond a limit? Is this for compressing the networks (for which there are alternate procedures), or anything else. In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers? Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)? \n2. The proposed experiments are not very conclusive. Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks. Secondly, more datasets including imagenet needs to be tested. Unless these two are done, we cannot assertively say that the proposal seems to do interesting things. Thirdly, it is not clear what Figure 5 means in terms of goodness of learning. And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1515642517000,"tcdate":1511735467754,"number":1,"cdate":1511735467754,"id":"rk43lpulz","invitation":"ICLR.cc/2018/Conference/-/Paper822/Official_Review","forum":"Bys_NzbC-","replyto":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference/Paper822/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors proposed to apply L1/L2 regularization in training of deep neural networks after 5-10 epochs. ","rating":"6: Marginally above acceptance threshold","review":"The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks. Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16. The results seem to show that a delayed application of the regularization parameter leads to improved classification performance.\n\nThe proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning. In the latter case, a stronger parameter is applied, followed by reduced regularization parameter. One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization. It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on.\n\nOne limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]}},{"tddate":null,"ddate":null,"tmdate":1515181399033,"tcdate":1509135475125,"number":822,"cdate":1509739079484,"id":"Bys_NzbC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bys_NzbC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Achieving Strong Regularization for Deep Neural Networks","abstract":"L1 and L2 regularizers are critical tools in machine learning due to their ability to simplify solutions. However, imposing strong L1 or L2 regularization with gradient descent method easily fails, and this limits the generalization ability of the underlying neural networks. To understand this phenomenon, we investigate how and why training fails for strong regularization. Specifically, we examine how gradients change over time for different regularization strengths and provide an analysis why the gradients diminish so fast. We find that there exists a tolerance level of regularization strength, where the learning completely fails if the regularization strength goes beyond it. We propose a simple but novel method, Delayed Strong Regularization, in order to moderate the tolerance level. Experiment results show that our proposed approach indeed achieves strong regularization for both L1 and L2 regularizers and improves both accuracy and sparsity on public data sets. Our source code is published.","pdf":"/pdf/8f2e3811f5803ca52ebdfec30fe4ed6019e025c2.pdf","TL;DR":"We investigate how and why strong L1/L2 regularization fails and propose a method than can achieve strong regularization.","paperhash":"anonymous|achieving_strong_regularization_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving Strong Regularization for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bys_NzbC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper822/Authors"],"keywords":["deep learning","regularization"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}