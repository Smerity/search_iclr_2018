{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222659406,"tcdate":1511996296474,"number":3,"cdate":1511996296474,"id":"Hkg9o32gG","invitation":"ICLR.cc/2018/Conference/-/Paper468/Official_Review","forum":"H1dh6Ax0Z","replyto":"H1dh6Ax0Z","signatures":["ICLR.cc/2018/Conference/Paper468/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"\n This was an interesting read.  \n\nI feel that there is a mismatch between intuition of what a model could do (based on the structure of the architecture) versus what a model does. Just because the transition function is shared and the model could learn to construct a tree, when trained end-to-end the system is not sufficiently constrained to learn this specific behaviour. More to a point. I think the search tree perspective is interesting, but isn’t this just a deeper model with shared weights? And a max operation? It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state, right? Is there any specific attempt to visualize or understand the embeddings inside the tree? The same regarding the rewards. If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards, why would the model use those free latent variables to encode rewards? I think this is a pitfall that many deep network papers fall, where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution (where the latent have prescribed semantics). I would argue that is rarely the case. When the system is learned end-to-end, the structure does not impose the behaviour of the model, and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree. And this is not by showing final performance on a game. If indeed the model does anything similar to search, than all intermediary representations should correspond to what semantically they should. \nIgnoring my verbose comment, another view is that the baseline are disadvantaged to the treeQN, because they have less parameters (and are less deep which has a huge impact on the learnability and expressivity of the deep network). \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222659447,"tcdate":1511821911511,"number":2,"cdate":1511821911511,"id":"ry1vffqeM","invitation":"ICLR.cc/2018/Conference/-/Paper468/Official_Review","forum":"H1dh6Ax0Z","replyto":"H1dh6Ax0Z","signatures":["ICLR.cc/2018/Conference/Paper468/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"# Summary\nThis paper proposes TreeQN and ATreeC which perform look-ahead planning using neural networks. TreeQN simulates the future by predicting rewards/values of the future states and performs tree backup to construct Q-values. ATreeC is an actor-critic architecture that uses a softmax over TreeQN. The architecture is trained through n-step Q-learning with reward prediction loss. The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games.\n\n[Pros]\n- The paper is easy to follow.\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea was proposed by [O'Donoghue et al., Schulman et al.].\n\n[Cons]\n- The proposed method has a technical issue.\n- The proposed idea (TreeQN) and underlying motivation are almost same as those of VPN [Oh et al.], but there is no in-depth discussion that shows why TreeQN is potentially better than VPN. \n- Comparison to VPN on Atari is not much convincing. \n\n# Novelty and Significance\n- The underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (n-step Q-learning with reward prediction loss) are same as those of VPN. But, the paper does not provide in-depth discussion on this. The following is the differences that I found from this paper, so it would be important to discuss why such differences are important.\n\n1) The paper emphasizes the \"fully-differentiable tree planning\" aspect in contrast to VPN that back-propagates only through \"non-branching\" trajectories during training. However, differentiating TreeQN also amounts to back-propagating through a \"single\" trajectory in the tree that gives the maximum Q-value. Thus, the only difference between TreeQN and VPN is that TreeQN follows the best (estimated) action sequence, while VPN follows the chosen action sequence in retrospect during back-propagation. Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation (see Technical Soundness section for discussion)?\n\n2) TreeQN only sets targets for the final Q-value after tree backup, whereas VPN sets targets for all intermediate value predictions in the tree. Why is TreeQN's approach better than VPN's approach? \n\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea of combining Q-learning with policy gradient was proposed by [O'Donoghue et al.] and [Schulman et al.].\n\n# Technical Soundness\n- The proposed idea of setting targets for the final Q-value after tree backup can potentially make the temporal credit assignment difficult, because the best estimated actions during tree planning does not necessarily match with the chosen actions. Suppose that TreeQN estimated \"up-right-right\" as the best future action sequence the during 3-step tree planning, while the agent actually ended up with choosing \"up-left-left\" (this is possible because the agent re-plans at every step and follows epsilon-greedy policy). Following n-step Q-learning procedure, we end up with setting target Q-value based on on-policy action sequence \"up-left-left\", while back-propagating through \"up-right-right\" action sequence in the TreeQN's plan. This causes a wrong temporal credit assignment, because TreeQN can potentially increase/decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen actions. So, it is unclear why the proposed algorithm is technically correct or better than VPN's approach (i.e., back-propagating through the chosen actions in the search tree).\n \n# Quality\n- Comparison to VPN on Atari is not convincing because TreeQN-1 is actually (almost) equivalent to VPN-1, but the results show that TreeQN-1 performs much better than VPN on many games. Since the authors took the numbers from [Oh et al.] rather than replicating VPN, it is possible that the gap comes from implementation details (e.g., hyperparameter). \n\n# Clarity\n- The paper is overall easy to follow and the description of the proposed method is clear.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222659488,"tcdate":1511809682175,"number":1,"cdate":1511809682175,"id":"r1cczyqef","invitation":"ICLR.cc/2018/Conference/-/Paper468/Official_Review","forum":"H1dh6Ax0Z","replyto":"H1dh6Ax0Z","signatures":["ICLR.cc/2018/Conference/Paper468/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting proposal, well-written, some short-comings in the results.","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors propose a new network architecture for RL that contains some relevant inductive biases about planning. This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task. The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end). This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes. The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax.\n\nI thought the paper was clear and well-motivated. The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility. \n\nExperimental results seem promising but I wasn’t fully convinced of its conclusions. In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn’t clear to me that this is the right baseline. Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top. Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity? Same comment in Atari, but there it’s not really obvious that the proposed architecture is helping. Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc.\n\nPage 5, the auxiliary loss on reward prediction seems appropriate, but it’s not clear from the text and experiments whether it actually was necessary. Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?  \n\nDespite some shortcomings in the result section, I believe this is good work and worth communicating as is.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1511433116823,"tcdate":1511433116823,"number":2,"cdate":1511433116823,"id":"r1SimQEgf","invitation":"ICLR.cc/2018/Conference/-/Paper468/Public_Comment","forum":"H1dh6Ax0Z","replyto":"rJVUBbXxG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Although the inductive bias cannot learn the exact models of the environment, I appreciate this method very much.","comment":"I understand this method now. However, if add the comments below in the original paper, readers can understand it more easily. Overall, this method is very interesting and insightful, I appreciate it very much.\n\n ''There is no guarantee that the trained sub-networks learn a faithful model of the environment''\n''However, this flexibility is intentional because, at the end of the day, we only care about predicting accurate state-action values. Consequently, we want to make our architecture flexible enough to learn an abstract representation of the environment and a transition model that, when used together inside TreeQN, are effective at predicting those state-action values even if the resulting architecture does not correspond to rigid definitions of model, state, or plan.''\n\n\nThe sub-networks are not CNNs indeed. I want to express that the sub-networks can mimic the planning, while CNN can extract local features, and both of them are universal modules although designed for special purposes."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1511359819785,"tcdate":1511359819785,"number":1,"cdate":1511359819785,"id":"rJVUBbXxG","invitation":"ICLR.cc/2018/Conference/-/Paper468/Official_Comment","forum":"H1dh6Ax0Z","replyto":"HJAkFD-xM","signatures":["ICLR.cc/2018/Conference/Paper468/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper468/Authors"],"content":{"title":"An inductive bias that encourages planning","comment":"Thank you for your comments! It is true that the whole network is trained end-to-end based on the TD-error of n-step Q-learning and so there is no guarantee that the trained sub-networks learn a faithful model of the environment (i.e., the internal state representations are not guaranteed or necessarily expected to contain the information needed to accurately predict observations in future time-steps).\n\nHowever, this flexibility is intentional because, at the end of the day, we only care about predicting accurate state-action values. Consequently, we want to make our architecture flexible enough to learn an abstract representation of the environment and a transition model that, when used together inside TreeQN, are effective at predicting those state-action values even if the resulting architecture does not correspond to rigid definitions of model, state, or plan. Instead, we incorporate an inductive bias through the recursive application of a learned transition function that is shared across the tree. This inductive bias is introduced through the network’s structure and does indeed encourage planning.\n\nNote that there is a weak grounding of predicted states via our reward prediction auxiliary loss. Furthermore, our results show that the model does produce interpretable trees in some situations, which demonstrates that grounded planning is performed when useful, even though the model is not limited to it. We experimented with ways of further grounding the transition function (and thus the states) but found that it only hurt performance.  Finding ways to encourage stronger grounding without hurting performance is an interesting research direction, but not in the scope of this paper.\n\nThe sub-networks (transition, reward and value function) are not CNNs (see Equations 6 to 8).\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1511254245919,"tcdate":1511254245919,"number":1,"cdate":1511254245919,"id":"HJAkFD-xM","invitation":"ICLR.cc/2018/Conference/-/Paper468/Public_Comment","forum":"H1dh6Ax0Z","replyto":"H1dh6Ax0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The structure of TreeQN and how to train the model functions","comment":"The idea that integrate model planning into the Q-function or policy is interesting. \n\nHowever, I wander how the model functions (transition function, reward function and value function) are trained. From the description of the paper, they may be special designed sub-networks, the whole network is trained based on the TD-error of n-step Q-learning. If it is, how can we know the sub-networks are planning indeed?\n\nIn my opinion, TreeQN is a complex network with a fixed planning step (i.e., the tree depth). And each planning step is a special designed sub-network. The sub-network is similar as CNN, which can extract useful feature from the state representation z.\n\nDo I understand this correctly?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1509739285576,"tcdate":1509121456546,"number":468,"cdate":1509739282916,"id":"H1dh6Ax0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1dh6Ax0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning","abstract":"Combining deep model-free reinforcement learning with on-line planning is a\npromising approach to building on the successes of deep RL. On-line planning\nwith look-ahead trees has proven successful in environments where transition\nmodels are known a priori. However, in complex environments where transition\nmodels need to be learned from data, the deficiencies of learned models have\nlimited their utility for planning. To address these challenges, we propose\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\ndrop-in replacement for any value function network in deep RL with discrete\nactions. TreeQN dynamically constructs a tree by recursively applying a\ntransition model in a learned abstract state space and then aggregating\npredicted rewards and state-values using a tree backup to estimate Q-values. We\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a softmax\nlayer to form a stochastic policy network.  Both approaches are trained\nend-to-end, such that the learned model is optimised for its actual use in the\nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\nal., 2017) on multiple Atari games, with deeper trees often outperforming\nshallower ones. We also present a qualitative analysis that sheds light on the\ntrees learned by TreeQN.","pdf":"/pdf/1f71bbe2cf336d03014329a3c054a0e492d2a6eb.pdf","TL;DR":"We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.","paperhash":"anonymous|treeqn_and_atreec_differentiable_tree_planning_for_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018treeqn,\n  title={TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1dh6Ax0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper468/Authors"],"keywords":["reinforcement learning","deep learning","planning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}