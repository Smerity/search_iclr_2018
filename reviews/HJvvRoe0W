{"notes":[{"tddate":null,"ddate":null,"tmdate":1512342553228,"tcdate":1512341833426,"number":2,"cdate":1512341833426,"id":"r1bLW-MWz","invitation":"ICLR.cc/2018/Conference/-/Paper377/Public_Comment","forum":"HJvvRoe0W","replyto":"HJvvRoe0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Validation of method","comment":"Hi authors,\n\nThe usage of a Hilbert space filling curve for chromatin structure classification is not clearly motivated to me. If I understand the method correctly, the goal is to perform some deterministic mapping from the 1D structure to the 2D space filling curve in order to leverage the spatial structure of popular CNN methods. However, since the Hilbert space filling curve does not leverage any biologically motivated information to construct the 2D image, it is unclear how this would help. The Hilbert space filling curve has nice properties such as (continuity, clustering, etc), but the original 1D sequence also possesses all of these properties. It seems simply using a larger filter size on the 1D sequence should achieve similar results since the 1D sequence itself should be able to achieve the continuity, clustering, etc properties of the Hilbert space filling curve. \n\nIn addition, as the below reviewers have mentioned, the success of CNNs are not specific to 2D images, and there is not really a theoretical argument for converting a 1D structure to a 2D structure. Am I missing something here regarding the problem of chromatin structure classification or the representational power of Hilbert space filling curves?\n\nFurthermore, the results section has not fully convinced me that the Hilbert-CNN is more powerful than applying a direct 1D CNN on the sequence. The method compared against in Nguyen et al (2016) only uses a 7 x 1 filter size in the first layer while the Hilbert-CNN uses a 7x7 filter size in the first layer. Perhaps a more convincing comparison would be comparing against a method that uses the same number of parameters (i.e. a 1D method that uses a filter size of 49 x 1 in the first layer) to demonstrate the utility of converting a 1D sequence to a 2D image.\n\nI would be very interested if the Hilbert space filling curve was indeed a superior representation than its 1D counterpart."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolving DNA using two-dimensional Hilbert curve representations","abstract":"The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that predicts key determinants of chromatin structure from primary DNA sequence. Key in our approach is the transformation of the input data from a sequence to an image using the Hilbert curve, which has several desirable properties from a biological point of view. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.","pdf":"/pdf/aa10771c8662dbd4b6b09f9f483eef6f722b8316.pdf","TL;DR":"A method to transform DNA sequences into 2D images using space-filling Hilbert Curves to enhance the strengths of CNNs","paperhash":"anonymous|convolving_dna_using_twodimensional_hilbert_curve_representations","_bibtex":"@article{\n  anonymous2018convolving,\n  title={Convolving DNA using two-dimensional Hilbert curve representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJvvRoe0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper377/Authors"],"keywords":["DNA sequences","Hilbert curves","Convolutional neural networks","chromatin structure"]}},{"tddate":null,"ddate":null,"tmdate":1512222627779,"tcdate":1511619537662,"number":3,"cdate":1511619537662,"id":"r19CoevgM","invitation":"ICLR.cc/2018/Conference/-/Paper377/Official_Review","forum":"HJvvRoe0W","replyto":"HJvvRoe0W","signatures":["ICLR.cc/2018/Conference/Paper377/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The idea of converting k-mer representations to 2D images using Hilbert curves is novel in application.","rating":"7: Good paper, accept","review":"The authors of this manuscript transformed the k-mer representation of DNA fragments to a 2D image representation using the space-filling Hilbert curves for the classification of chromatin occupancy. In generally, this paper is easy to read. The components of the proposed model mainly include Hilbert curve theory and CNN which are existing technologies. But the authors make their combination useful in applications. Some specific comments are:\n\n1. In page 5, I could not understand the formula d_kink < d_out. d_link ;\n2. There may exist some new histone modification data that were captured by the next-generation sequencing (e.g. ChIP-seq) and are more accurate;  \n3. It seems that the authors treat it as a two-class problem for each data set. It would be more useful in real applications if all the data sets are combined to form a multi-class problem.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolving DNA using two-dimensional Hilbert curve representations","abstract":"The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that predicts key determinants of chromatin structure from primary DNA sequence. Key in our approach is the transformation of the input data from a sequence to an image using the Hilbert curve, which has several desirable properties from a biological point of view. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.","pdf":"/pdf/aa10771c8662dbd4b6b09f9f483eef6f722b8316.pdf","TL;DR":"A method to transform DNA sequences into 2D images using space-filling Hilbert Curves to enhance the strengths of CNNs","paperhash":"anonymous|convolving_dna_using_twodimensional_hilbert_curve_representations","_bibtex":"@article{\n  anonymous2018convolving,\n  title={Convolving DNA using two-dimensional Hilbert curve representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJvvRoe0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper377/Authors"],"keywords":["DNA sequences","Hilbert curves","Convolutional neural networks","chromatin structure"]}},{"tddate":null,"ddate":null,"tmdate":1512222627817,"tcdate":1511375054113,"number":2,"cdate":1511375054113,"id":"r18RxrXlG","invitation":"ICLR.cc/2018/Conference/-/Paper377/Official_Review","forum":"HJvvRoe0W","replyto":"HJvvRoe0W","signatures":["ICLR.cc/2018/Conference/Paper377/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Innovative but insufficiently evaluated","rating":"6: Marginally above acceptance threshold","review":"The authors present Hilbert-CNN, a convolutional neural network for DNA sequence classification. Unlike existing methods, their model does not use the raw one-dimensional (1D) DNA sequence as input, but two-dimensional (2D) images obtained by mapping sequences to images using spacing-filling Hilbert-Curves. They further present a model (Hilbert-CNN) that is explicitly designed for Hilbert-transformed DNA sequences. The authors show that their approach can increase classification accuracy and decrease training time when applied to predicting histone-modification marks and splice junctions. \n\nMajor comments\n=============\n1. The motivation of transforming sequences into images is unclear and claimed benefits are not sufficiently supported by experiments. The essence of deep neural networks is to learn a hierarchy of features from the raw data instead of engineering features manually. Using space filling methods such as Hilbert-curves to transform (DNA) sequences into images can be considered as unnecessary feature-engineering. \n\nThe authors claim that ‘CNNs have proven to be most powerful when operating on multi-dimensional input, such as in image classification’, which is wrong. Sequence-based convolutional and recurrent models have been successfully applied for modeling natural languages (translation, sentiment classification, …), acoustic signals (speech recognition, audio generation), or biological sequences (e.g. predicting various epigenetic marks from DNA as reviewed in Angermueller et al).    \n\nThey further claim that their method can ‘better take the spatial features of DNA sequences into account’ and  can better model ‘long-term interactions’ between distant regions. This is not obvious since Hilbert-curves map adjacent sequence characters to pixels that are close to each other as described by the authors, but distant characters to distant pixels. Hence, 2D CNN must be deep enough for modeling interactions between distant image features, in the same way as a 1D CNN.\n\nTransforming sequences to images has several drawbacks. 1) Since the resulting images have a small width and height but many channels, existing 2D CNNs such as ResNet or Inception can not be applied, which also required the authors to design a specific model (Hilbert-CNN). 2) Hilbert-CNN requires more memory due to empty image regions. 3) Due to the high number of channels, convolutional filters have more parameters. 4) The sequence-to-image transformation makes model-interpretability hard, which is in particular important in biology. For example, motifs of the first convolutional layers can not be interpreted as sequence motifs (as described in Angermueller et al) and it is unclear how to analyze the influence of sequence characters using attention or gradient-based methods.\n\nThe authors should more clearly motivate their model in the introduction, tone-down the benefit of sequence-to-image transformations, and discuss drawbacks of their model. This requires major changes of introduction and discussion.\n\n2. The authors should more clearly describe which and how they optimized hyper-parameters. The authors should optimize the most important hyper-parameters of their model (learning rate, batch size, weight decay, max vs. average pooling, ELU vs. ReLU, …) and baseline models on a holdout validation set. The authors should also report the validation accuracy for different sequence lengths, k-mer sizes, and space filling functions. Can their model be applied to longer sequences (>= 1kbp) which had been shown to improve performance (e.g. 10.1101/gr.200535.115)? Does Figure 4 show the performance on the training, validation, or test set?\n\n3. It is unclear if the performance gain is due the proposed sequence-to-image transformation, or due to the proposed network architecture (Hilbert-CNN). It is also unclear if Hilbert-CNNs are applicable to DNA sequence classification tasks beyond predicting chromatin states and splice junctions. To address these points, the authors should compare Hilbert-CNN to models of the same capacity (number of parameters) and optimize hyper-parameters (k-mer size, convolutional filter size, learning rate, …) in the same way as they did for Hilbert-CNN. The authors should report the number of parameters of all models (Hilbert-CNN, Seq-CNN, 1D-sequence-CNN (Table 5), and LSTM (Table 6), …) in an additional table. The authors should also compare Hilbert-CNN to the DanQ architecture on predicting epigenetic markers using the same dataset as reported in the DanQ publication (DOI: 10.1093/nar/gkw226). The authors should also compare Hilbert-CNNs to gapped-kmer SVM, a shallow model that had been successfully applied for genomic prediction tasks.\n\n4. The authors should report the AUC and area under precision-recall curve (APR) in additional to accuracy (ACC) in Table 3.\n\n5. It is unclear how training time was measured for baseline models (Seq-CNN, LSTM, …). The authors should use the same early stopping criterion as they used for training Hilber-CNNs. The authors should also report the training time of SVM and gkm-SVM (see comment 3) in Table 3.\n\n\nMinor comments\n=============\n1. The authors should avoid uninformative adjectives and clutter throughout the manuscript, for example ‘DNA is often perceived’, ‘Chromatin can assume’,  ‘enlightening’, ‘very’, ‘we first have to realize’, ‘do not mean much individually’, ‘very much like the tensor’, ‘full swing’, ‘in tight communication’, ‘two methods available in the literature’.\n\nThe authors should point out in section two that k-mers can be overlapping.\n\n2. Section 2.1: One-hot vectors is not the only way for embedding words. The authors should also mention Glove and word2vec. Similar approaches had been applied to protein sequences (DOI: 10.1371/journal.pone.0141287)\n\n3. The authors should more clearly describe how Hilbert-curves map sequences to images and how images are cropped. What does ‘that is constructed in a recursive manner’ mean? Simply cropping the upper half of Figure 1c would lead to two disjoint sequences. What is the order of Figure 1e?\n\n4. The authors should consistently use ‘channels’ instead of ‘full vector of length’ to denote the dimensionality of image pixels.\n\n5. The authors should use ‘Batch norm’ instead of ‘BN’ in Figure 2 for clarification.\n\n6. Hilber-CNN is similar to ResNet (DOI: 10.1371/journal.pone.0141287), which consists of multiple ‘residual blocks’, where each block is a sequence of ‘residual units’. A ‘computational block’ in Hilbert-CNN contains two parallel ‘residual blocks’ (Figure 3) instead of a sequence of ‘residual units’. The authors should use ‘residual block’ instead of ‘computational block’, and ‘residual units’ as in the original ResNet publication. The authors should also motivate why two residual units/blocks are applied parallely instead of sequentially.\n\n7. Caption table 1: the authors should clarify if ‘Output size’ is ‘height, width, channels’, and explain the notation in ‘Description’ (or refer to the text.)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolving DNA using two-dimensional Hilbert curve representations","abstract":"The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that predicts key determinants of chromatin structure from primary DNA sequence. Key in our approach is the transformation of the input data from a sequence to an image using the Hilbert curve, which has several desirable properties from a biological point of view. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.","pdf":"/pdf/aa10771c8662dbd4b6b09f9f483eef6f722b8316.pdf","TL;DR":"A method to transform DNA sequences into 2D images using space-filling Hilbert Curves to enhance the strengths of CNNs","paperhash":"anonymous|convolving_dna_using_twodimensional_hilbert_curve_representations","_bibtex":"@article{\n  anonymous2018convolving,\n  title={Convolving DNA using two-dimensional Hilbert curve representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJvvRoe0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper377/Authors"],"keywords":["DNA sequences","Hilbert curves","Convolutional neural networks","chromatin structure"]}},{"tddate":null,"ddate":null,"tmdate":1512222627858,"tcdate":1511351833112,"number":1,"cdate":1511351833112,"id":"BkZQ81QlG","invitation":"ICLR.cc/2018/Conference/-/Paper377/Official_Review","forum":"HJvvRoe0W","replyto":"HJvvRoe0W","signatures":["ICLR.cc/2018/Conference/Paper377/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting method","rating":"7: Good paper, accept","review":"There are functional elements attached on the DNA sequence, such as transcription factors and different kinds of histones as stated in this ms. A hidden assumption is that the binding sites of these functional elements over the genome share some common features. It is therefore biologically interesting to predict if a new DNA sequence could be a binding site. Naturally this is is classification problem where the input is the DNA sequence and the output is whether the give sequence is a binding site.\n\nThis ms makes a novel way to transform the DNA sequence into a 3-dimensional tensor which could be easily utilised by CNN for images. The DNA sequence is first made into a a list of 4-mers. Then then each 4-mer is coded as a 4^4=256 dimensional vector. The order of the 4-mers is then coded into a image using Hilbert curve which presumably has nice properties to keep spatial information.\n\nI am not familiar with neural networks and do not comment on the methods but rather from the application point of view. \n\nFirst to my best knowledge, it is still controversial if the binding sites of different histones carries special features. I mean it could be possible that the assumption I mentioned in the beginning may not hold for this special application, especially for human data. I feel this method is more suitable for transcription factor motif data. see https://www.nature.com/articles/nbt.3300\n\nSecond, the experiments data in 2005 is measured using microarray, which uses probes of 500bp long. But the whole binding site for a nucleosome (or histone complex) is 147bp, which is much shorter than the probe. Nowadays we have more accurate sequencing data for nucleosome (check https://www.ncbi.nlm.nih.gov/pubmed/26411474). I am not sure whether this result will generalised to some other similar dataset. \n\nThird, the results only list the accuracy, it will be interesting to see the proportion of false negatives.\n\nIn general I feel the transformation is quite useful, it nicely reserves the spatial information, also can be seen from the improved results over all datasets. The result, in my opinion, is not sufficient to support the assumption that we could predict the DNA structures solely base on the sequence.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Convolving DNA using two-dimensional Hilbert curve representations","abstract":"The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that predicts key determinants of chromatin structure from primary DNA sequence. Key in our approach is the transformation of the input data from a sequence to an image using the Hilbert curve, which has several desirable properties from a biological point of view. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.","pdf":"/pdf/aa10771c8662dbd4b6b09f9f483eef6f722b8316.pdf","TL;DR":"A method to transform DNA sequences into 2D images using space-filling Hilbert Curves to enhance the strengths of CNNs","paperhash":"anonymous|convolving_dna_using_twodimensional_hilbert_curve_representations","_bibtex":"@article{\n  anonymous2018convolving,\n  title={Convolving DNA using two-dimensional Hilbert curve representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJvvRoe0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper377/Authors"],"keywords":["DNA sequences","Hilbert curves","Convolutional neural networks","chromatin structure"]}},{"tddate":null,"ddate":null,"tmdate":1509739336253,"tcdate":1509109342712,"number":377,"cdate":1509739333600,"id":"HJvvRoe0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJvvRoe0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Convolving DNA using two-dimensional Hilbert curve representations","abstract":"The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that predicts key determinants of chromatin structure from primary DNA sequence. Key in our approach is the transformation of the input data from a sequence to an image using the Hilbert curve, which has several desirable properties from a biological point of view. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.","pdf":"/pdf/aa10771c8662dbd4b6b09f9f483eef6f722b8316.pdf","TL;DR":"A method to transform DNA sequences into 2D images using space-filling Hilbert Curves to enhance the strengths of CNNs","paperhash":"anonymous|convolving_dna_using_twodimensional_hilbert_curve_representations","_bibtex":"@article{\n  anonymous2018convolving,\n  title={Convolving DNA using two-dimensional Hilbert curve representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJvvRoe0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper377/Authors"],"keywords":["DNA sequences","Hilbert curves","Convolutional neural networks","chromatin structure"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}