{"notes":[{"tddate":null,"ddate":null,"tmdate":1515079716589,"tcdate":1515079716589,"number":7,"cdate":1515079716589,"id":"B16XOpsXf","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Paper updated","comment":"The paper is updated. We added part of the discussions raised by AnonReviewer3 and enhanced the clarity suggested by AnonReviewer2 and AnonReviewer1. We greatly appreciate the contribution of reviewers."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514752514469,"tcdate":1514752514469,"number":5,"cdate":1514752514469,"id":"Sk5-cTImG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"Byum0OYlG","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Rebuttal","comment":"Thank you for the detailed comments. We would like to respond to the cons you mentioned.\n\n1. First, we discuss several points about the cross domain transfer experiment. We separate them in the following:\n\n(1) Thank you so much for referring us to Bousmalis’s CVPR work. It is a great work and we added the citation in our updated version. However, that paper doesn’t include the experiments on Office-31 nor SVHN-MNIST. Could you please share the mentioned results with us?\n\n(2) Sener’s paper is another great work. However, to make the numbers be comparable, the backbone networks (e.g., AlexNet, VGG, ResNet…), training configurations, and image preprocessing must be the same. Our table 2 for Office-31 makes sure the setup is comparable. Sener’s work has a different setup and has not released the code yet. Therefore a fair comparison on Office-31 dataset is not viable at this date. For the SVHN-to-MNIST experiment, we use another way to compare the results. In table 10, it shows the relative performance gain against the baselines (source-only) of each original paper. In such way we don’t need the code of the referred approaches be available to make the absolute numbers comparable.\n\n(3) Based on above explanation, our statement about the performance achievement is still valid.\n\n(4) We would like to heavily emphasize that in the context of domain adaptation, our work should be considered as a complementary strategy to current mainstream domain adaptation approaches since the proposed LCO does not minimize the domain discrepancy. Instead of beating the benchmark, the main purpose of our paper is showing that transferring more information is an effective strategy for transfer learning across both tasks and domains. In fact, we show improved results using one of the domain adaptation methods (DANN) but we anticipate that incorporating later advances in domain adaptation using discrepancy losses will improve this. Furthermore, the performance of the framework is largely dependent on the accuracy of the learned similarity prediction. We use a naive implementation of a similarity prediction network (SPN) to obtain the presented numbers. The performance can be improved further when a more powerful similarity learning algorithm is available.\n\n2. The inference procedure for cross task transfer applies forward propagation on the network in figure 4 and uses the outputs after the cluster assignment layer. For cross domain transfer, the network is in figure 5 and the inference outputs is from the classification layer. We included the description in the updated version. Thank you so much for the feedback to improve the clarity.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514752207066,"tcdate":1514752207066,"number":4,"cdate":1514752207066,"id":"B1D0OaIQz","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"BJbSJYcgG","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Great suggestion","comment":"We greatly appreciate your constructive suggestions for enhancing the clarity. The suggestions make sense and can be done with minor refinement. We adopt them in our updated version. Thank you so much for your contribution.\n\nThe experimental settings you mentioned are also very interesting to us. Although we believe the current experimental settings serve well the purpose of supporting the main claim that transferring predictive similarity is an effective and generic way of transfer learning, the settings you described will help exploring other dimensions of the framework. For example, learning the similarity with only limited semantic categories, i.e., only use T’ instead of the large A. We would like to include the aspects you mentioned in a future work.\n\nFor the discussion about the Office-31 experiment, we are pleased to explain why the ImageNet data (or A) doesn’t need to be presented during the training with T. The community uses the weights pretrained with ImageNet as a generic initialization due to its training on a large number of categories. We leverage a similar idea and argue that a semantic similarity learned with ImageNet-scale probably generalize well to unseen classes. Our cross-task ImageNet experiment shows support to this idea. Thereby we augment the transferring scheme from only transferring the weights to transferring both weights and the learned similarity prediction. In other words, once the similarity prediction network is learned on ImageNet, it can be applied directly to other image classification datasets without access to ImageNet dataset, just like how we use the pre-trained weights (features).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1514751811039,"tcdate":1514751811039,"number":3,"cdate":1514751811039,"id":"B1oBwTIXM","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"BJZ2B0agf","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Discussion","comment":"Thank you for your comments and for recognizing the work. We are pleased to have a discussion of the limitations here and we added part of it into the paper.\n\nAs you pointed out, the performance of the similarity prediction is crucial, and the amount of performance gain on the target task is proportional to the accuracy of the similarity. One idea for enhancement is applying domain adaptation to learn the similarity prediction network (SPN). Any existing adaptation method can be used to train the SPN; as long as that adaptation method can deal with the high discrepancy between source and target, our framework will directly benefit from improvements in the learned SPN. With our proposed learning framework, the meta-knowledge carried by the SPN can then be transferred across either tasks or domains.\n\nAn extreme case involving the density of pairs is when there is a large number of categories in the target dataset. If there are 100 categories and we only randomly sample 32 instances for a mini-batch, there could be no similar pairs in a mini-batch since the instances are all from different categories. The LCO might not work well in this case since it has a form of contrastive loss. There are two ways to address this. The first is enlarge the mini-batch size, so that the number of sampled similar pairs increases. This method is limited by the memory size. The second way is to obtain dense similarity predictions offline. Then a mini-batch is sampled based on the pre-calculated dense similarity to ensure similar pairs are presented. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1513931063293,"tcdate":1513931063293,"number":2,"cdate":1513931063293,"id":"BJkB-Sczf","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"ryA6vo2gf","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Performance is dependent on G","comment":"Yes, figure 5 is trained end-to-end, except G.\nThe applicability of LCO is decided by its learnable part (G). In our experiment (appendix D), if G can perform better than a random guess, the clustering could benefit from it. Therefore the limitation would be how to learn a G, so its prediction of similarity is better than a random guess. If the datasets have high domain discrepancy, learn the G with domain adaptation strategy will be a good idea."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1515642421371,"tcdate":1512068521436,"number":3,"cdate":1512068521436,"id":"BJZ2B0agf","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Review","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors propose a novel framework for task-/domain- transfer in an unsupervised setting (no labels for target data). The idea of defining the similarity based-clustering within domain adaptation /transfer learning framework is novel. Experiments are thorough and show clear benefits of the proposed learning strategy.  ","rating":"9: Top 15% of accepted papers, strong accept","review":"pros:\nThis is a great paper - I enjoyed reading it. The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion. The paper is clearly written and easy to understand. Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework.  Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cases. \n\ncons/suggestions: \n- the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail. How to deal with these cases, or better, how to detect these before deploying this method?\n- the pair-wise similarity prediction network can become very dense: how to deal with extreme cases?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1511991238316,"tcdate":1511991238316,"number":2,"cdate":1511991238316,"id":"ryA6vo2gf","invitation":"ICLR.cc/2018/Conference/-/Paper26/Public_Comment","forum":"ByRWCqvT-","replyto":"HktVdZdlG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"About cross-domain transfer, Figure 5","comment":"Did you train the model depicted in Figure 5 end-to-end including backbone with classification model (except for G)?\nWhat are your thoughts on the applicability of LCO for cross-domain transfer in fields other than vision and language modelling? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1516003242303,"tcdate":1511849784887,"number":2,"cdate":1511849784887,"id":"BJbSJYcgG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Review","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/AnonReviewer2"],"readers":["everyone"],"content":{"title":"New method with a rough presentation","rating":"7: Good paper, accept","review":"The authors propose a method for performing transfer learning and domain adaptation via a clustering approach. The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class. Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide \"soft labels\" for whether or not pairs of target data belong to the same class. A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points. \n\nThe proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest. The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task.\n\nThe way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on. To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters. However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model. Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear. Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO. Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works.\n\nThe experimental evaluation is very thorough, spanning a variety of tasks and settings. Strong results in multiple settings indicate that the proposed method is effective and generalizable. Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results. It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works. I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup.\n\nAt times, the experimental details are a little unclear. Consistent use of the A, T, and S' dataset abbreviations would help. Also, the results section seems to switch off between calling the method CCN and LCO interchangeably. Finally, a few of the experimental settings differ from their baselines in nontrivial ways. For the Office experiment, the LCO appears to be trained on ImageNet data. While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data. Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting. In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature.\n\nOverall, the paper seems to have both a novel contribution and strong technical merit. However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained. I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.\n\n=========================\nUpdate after author rebuttal:\n=========================\nI have read the author's response and have looked at the changes to the manuscript. I am satisfied with the improvements to the paper and have changed my review to 'accept'. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1515642421461,"tcdate":1511783968220,"number":1,"cdate":1511783968220,"id":"Byum0OYlG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Review","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference/Paper26/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"(Summary)\nThis paper tackles the cross-task and cross-domain transfer and adaptation problems. The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence). What's similar vs dissimilar is trained with a binary classifier.\n\n(Pros)\n1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning.\n2. Learning to output the k class membership probability and the loss in eqn 5 seems novel.\n\n(Cons)\n1. The authors overclaim to be state of the art. For example, table 2 doesn't compare against two recent methods which report results exactly on the same dataset. I checked the numbers in table 2 and the numbers aren't on par with the recent methods. 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16. Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2.\n2. There are some typos in the related works section and the inferece procedure isn't clearly explained. Perhaps the authors can clear this up in the text after sec 4.3.\n\n(Assessment)\nBorderline. Refer to the Cons section above.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1511688240735,"tcdate":1511688240735,"number":1,"cdate":1511688240735,"id":"HktVdZdlG","invitation":"ICLR.cc/2018/Conference/-/Paper26/Official_Comment","forum":"ByRWCqvT-","replyto":"rkVJuH4ez","signatures":["ICLR.cc/2018/Conference/Paper26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper26/Authors"],"content":{"title":"Question Answering","comment":"Thank you very much for your interest. The responses of each point are in below:\na) It dependents on how hard the dataset is. In our experiments, the backbone networks and G are randomly initialized for the experiments on Omniglot and MNIST, and they perform well. For the experiments on Office-31 and the subsets of ImageNet, we do initialize the backbones with pre-training. When dealing with real-world photos, it is a common practice of pre-training, especially when the target dataset is small, e.g., Office-31. The general suggestion is: If the dataset needs a pre-trained network to help it performs well on classification, then it would be better also to use a pre-trained network in our approach.\nb) We believe the fixed value 2 is sufficient for most of the case. Our experiments involve the datasets of different complexity (e.g. MNIST vs ImageNet), unbalanced dataset (e.g. Office-31), and the varied number of categories (e.g. Omniglot alphabets). It shows the same setting performs well on the diverse conditions. In practice, we do see sometimes the performance could be improved by setting a larger margin (e.g. 2~5), but that extra gain seems dataset-dependent. Therefore we use the fixed value 2 as a conservative but universal setting.\nc) As you mentioned, it is for making the distance metric symmetric. Using only one part will introduce a hard question: Which one should be chosen? I have no clear answer for this. But from the implementation aspect, the symmetric form has an efficient vectorization thus it adds neglectable computational time compared to using only one part."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1511442847834,"tcdate":1511442395634,"number":1,"cdate":1511442395634,"id":"rkVJuH4ez","invitation":"ICLR.cc/2018/Conference/-/Paper26/Public_Comment","forum":"ByRWCqvT-","replyto":"ByRWCqvT-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Alternatives for Backbone network","comment":"Great work! Thank you for your contribution and I have three questions.\na) Do you have any suggestion for situations when a pre-trained backbone network is not available, it seems very important for getting good results. As far as I understand, training backbone end-to-end in the proposed solution would not be easy due to G's dependence over it. \nb) What range of values for sigma do you recommend? In paper you used fixed value i.e. 2 for all experiments. \nc) In actual implementation of equation 1 and 3, do you add terms D_KL(P* || Q) + D_KL(Q* || P) (or 2 x D_KL(P* || Q) as they are symmetric or is it fine to just optimize one e.g. D_KL(P* || Q)?\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]}},{"tddate":null,"ddate":null,"tmdate":1515077849013,"tcdate":1508515333691,"number":26,"cdate":1509739521602,"id":"ByRWCqvT-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByRWCqvT-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to cluster in order to transfer across domains and tasks","abstract":"This paper introduces a novel method to perform transfer learning across domains and tasks, formulating it as a problem of learning to cluster. The key insight is that, in addition to features, we can transfer similarity information and this is sufficient to learn a similarity function and clustering network to perform both domain adaptation and cross-task transfer learning. We begin by reducing categorical information to pairwise constraints, which only considers whether two instances belong to the same class or not (pairwise semantic similarity). This similarity is category-agnostic and can be learned from data in the source domain using a similarity network. We then present two novel approaches for performing transfer learning using this similarity function. First, for unsupervised domain adaptation, we design a new loss function to regularize classification with a constrained clustering loss, hence learning a clustering network with the transferred similarity metric generating the training inputs. Second, for cross-task learning (i.e., unsupervised clustering with unseen categories), we propose a framework to reconstruct and estimate the number of semantic clusters, again using the clustering network. Since the similarity network is noisy, the key is to use a robust clustering algorithm, and we show that our formulation is more robust than the alternative constrained and unconstrained clustering approaches. Using this method, we first show state of the art results for the challenging cross-task problem, applied on Omniglot and ImageNet. Our results show that we can reconstruct semantic clusters with high accuracy. We then evaluate the performance of cross-domain transfer using images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both datasets.  Our approach doesn't explicitly deal with domain discrepancy. If we combine with a domain adaptation loss, it shows further improvement.","pdf":"/pdf/8f4ac472d6059bfa385a9ebb2182732d2bb36a47.pdf","TL;DR":"A learnable clustering objective to facilitate transfer learning across domains and tasks","paperhash":"anonymous|learning_to_cluster_in_order_to_transfer_across_domains_and_tasks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to cluster in order to Transfer across domains and tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByRWCqvT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper26/Authors"],"keywords":["transfer learning","similarity prediction","clustering","domain adaptation","unsupervised learning","computer vision","deep learning","constrained clustering"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}