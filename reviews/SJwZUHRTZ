{"notes":[{"tddate":null,"ddate":null,"tmdate":1515734681635,"tcdate":1515734681635,"number":14,"cdate":1515734681635,"id":"HyzjLprVz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"r155u3nQM","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Responses to the revised review","comment":"Hi, \n\nThank you so much for the new comments and appreciation of our revisions. A few responses to the new comments. \n\n1) We agree that the way we used the new dataset is not optimal, and further efforts will be put into both making the dataset easier to use and coming up with a more optimal way utilizing the current structure of the dataset. \n\n2) The reason why the number of neighbors in the kNN experiment for \"song classification\" (Figure 5) jumps up a scale is because for there is 56 performances for each \"song\", while there is only 4 performances for each \"singer\". \n\nThanks again for your input and time!!! \n\n\n\nBest,\n\nCheng-i Wang\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515141266214,"tcdate":1515141266214,"number":10,"cdate":1515141266214,"id":"r155u3nQM","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"HyJwxAiQf","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Description clarified for the added dataset","comment":"Hello, \n\nWe just uploaded a new version where the descriptions of the added dataset in section 3.1 are cleaned. Thank you so much! \n\n\n\nBest,\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515096051875,"tcdate":1515096051875,"number":9,"cdate":1515096051875,"id":"ByneuW37f","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"HkTlmkn7z","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Standard Deviation for classification accuracies added","comment":"Hi, \n\nThank you for your quick response. The standard deviations were just added and a new version was uploaded just now. Thanks again. \n\n\n\nBest,"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515086581186,"tcdate":1515086581186,"number":8,"cdate":1515086581186,"id":"HkTlmkn7z","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"rkxB0Comz","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/AnonReviewer1"],"content":{"title":"Re: Responses to comments","comment":"> We are not sure about the significance analysis, since the results shown are already averaged results from 10-fold experiments over each song for every singer in DAMP (we are aware of the fact that it` it not exhaustive combinations of all possible test song permutations). \n\nCan you at least report standard deviation in addition to the mean?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515085367671,"tcdate":1515085367671,"number":7,"cdate":1515085367671,"id":"rkxB0Comz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"H1iUNk4gz","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Responses to comments","comment":"Hello, \n\nFirst of all, thank you so much for taking your time reviewing the paper, the comments are all helpful and constructive. The general comments on the novelty and generality of the submission are humbly accepted by the authors since it is nearly impossible to address those comments without altering the scope and topic of the submission. But we would like to advocate for ourselves by saying that the the submission is a report on exercising existing methods, that have been proven to be successful, on new problems and different domain (in deep learning for audio/music domain) and new datasets, which we consider the report to be valuable for both machine learning and audio informatics communities. \n\nFor the specific comments:\n\n1) Wordings, citations and typos are fixed. \n\n2) The benefits of embedding are added in Section 1.1 paragraph 4 (p.2). The suggested future works are re-organized and added in Discussion and Conclusions. K-nearest neighbor classification are using the embedding are conducted and added in Section 3.5 (p.8) and Figure 5. \n\n3) Convolution formulas between vanila and ResNeXt version are added in Section 2.1 paragraph 1 (p.3). A detailed realization of one of the network configurations is added in Appendix B to show specific feature dimension changes and operation flow chart of the experiments.  \n\n4) We replaced PCA with t-SNE for the performed song embedding visualization. We also added a plot in Appendix B that visualizes the 5-second clip embedding with t-SNE which should visually be more convincing. The k-nearest neighbor classifications on singers and performed songs should provide enough quantitative indications about the singer clustering learned by the embedding and the \"song\" effect with baseline features. \n\n5) We are not sure about the significance analysis, since the results shown are already averaged results from 10-fold experiments over each song for every singer in DAMP (we are aware of the fact that it` it not exhaustive combinations of all possible test song permutations). And we did not do the label permutation significant analysis since we are prohibited by our budget and computing power. \n\nThank you again for you comments and sorry for the late response. Please let us know any quick comments or questions!!! \n\n\n\nBest,\n\n\n "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515081815317,"tcdate":1515081815317,"number":6,"cdate":1515081815317,"id":"HyJwxAiQf","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"Skjx8WqxG","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Revision summarization ","comment":"Hello, \n\nThank you so much for taking your time reviewing our paper, your suggestions helped shaping up the paper a lot. Here are the summaries and responses to your comments. \n\n1) About the dataset. The DAMP-balanced is a separate dataset that has the same label format as the original DAMP dataset. The source of the dataset is the same as the DAMP dataset, which is from the SING! app database. The 3462 singers in the original DAMP are not necessarily included in the DAMP-balanced dataset (they are queried independently) but the performed audio recordings are guaranteed to have no overlaps between the two datasets. The dataset is balanced when the first 10 songs are split into any \"6/4\" combinations. Meaning for any \"6/4\" combinations of the first 10 songs, the two group of singers in the \"6/4\" split will have performed the 6 and 4 songs according to the \"6/4\" split once for each song. But since for different \"6/4\" splits, the number of singers included varies, thus leads to the fact that the number of performances for each song differs. These texts are included in Appendix A. \n\nThe reason why we only used a small subset of the entire dataset is that since one point of the experiments is to explore the singer and song effects, which required us to have the \"balanced\" dataset, and the split we used was the one having the most ideal train/valid/test ratio split. To utilize the rest of the dataset, we would use other splits and \"re-train\" the model (not continuing training it after different split) to see if the trained results are similar to models trained on different splits. The reason we want to avoid training on the \"whole\" dataset is that the \"whole\" dataset is not balanced as you have pointed out. This avoidance is due to the experiment settings we have now, but we do want others to explore the dataset differently with different experiment settings or assumptions. The other reason for not being able to do different train/valid split is mainly due to budget and computing power constraints. \n\n2) For the description of the CNN building blocks, we have corrected the typo in Figure 2 where the number of channels do not match. We also added a specific realization of the network in Appendix B to show detailed feature map dimensions and operation flow chart. We also added some convolutional formulas in Section 2.1. Since for the ResNeXt configurations, there is no innovation around the cited work, we were trying to keep the description minimal to save space. Now with the added plots and texts, we hope that clarifies your considerations. \n\n3) We actually did experiment with CQT but it performed worth then Mel-spectrogram in all configurations, and that is why we did not include results using CQT in the first place. We added descriptions of using CQT in Section 3.2 (p.5) and explained why it worked worse. In short, since we are working with a single voice audio recording, not polyphonic audio recordings, the \"constant\" distances between pitches invariant of pitch height (CQT) is not a factor here. The single voice recording has harmonics which spaces non-linearly in CQT space. Although that is also true for Mel-spectrogram, but it is not as \"non-linearly\" as CQT. We added two representative experiment results using CQT in Table 1. \n\n4) We agree that the visualization is not convincing by visual inspection, so we added two sets of K-nearest neighbor classification results, one for singer classification to show the singer cluster property of the embedding, one for performed song to show the \"song\" effect with baseline features. The k-nearest neighbor classification with varied K is in Figure 5. Corresponding texts are also added in Section 3.5 (p.8). The visualization of the embeddings are also changed from PCA to t-SNE to have a stronger clustering visualization. For completeness, we also added a plot of the embeddings of the 5-sesond clips (clip length used during training) of the test set in Appendix B, in our opinion, the visualization of the embedded clips are visually more convincing than its summarized song version. \n\nWe were not sure about your \"enrollment\" comment (and we are sorry that we did not try to clarify that earlier) since the test sets (used in the visualization of the plots and the k-nearest neighbor experiments) are never used during training. \n\n5) All the minor comments have been taken care of. We also took out the paragraphs in Conclusion and Discussion about deeper architecture did not really help and global-aggregation might be prone to over-fitting.  \n\nThank you again for you time and sorry again that we responded so late, but please let us know if there is any quick questions or comments. \n\n\n\nBest,\n\n   "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515078413382,"tcdate":1515078413382,"number":5,"cdate":1515078413382,"id":"SkBM76iXM","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"BynhTbclf","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Revisions based on your suggestions","comment":"Hello, \n\nThank you so much for taking your time reviewing our paper and leave suggestions for us. The revised version is uploaded with modifications following your suggestions. \n\n1) Legends and labels in Figure 4 are re-calibrated to enhance readability. \n2) The link at the end of section 1 is deleted since related works are included already in the text.\n\nOther modifications following other reviewers`   comments:\n\n1) Added formulas and a detailed plot for the CNN configurations. (In Section 2.1 and Figure 6).\n\n2) Added a few CQT experiment results and explanation texts to show that in all situations, CQT performed worth than Mel-spectrogram. (In Table. 1 and Section 3.2)\n\n3) Added detailed description of the DAMP-balanced dataset in Appendix A.\n\n4) Added k-nearest neighbor classification results on the embedding experiments to show quantitative results. (In Section 3.5 and Figure 5)\n\n5) Changed to visualization of the embedding from PCA to t-SNE, also added a visualization for the performed 5-seconds clips (Figure 4 and Figure 7)\n\nThanks again for your time and sorry about the late response. \n\n\n\nBest \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1514625156493,"tcdate":1514625156493,"number":4,"cdate":1514625156493,"id":"BkhKdREmz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"HJYu1XMmz","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Number of epochs","comment":"For the training settings in our paper, it`s about 1200 ~ 2000 epochs. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1514446705331,"tcdate":1514446705331,"number":4,"cdate":1514446705331,"id":"HJYu1XMmz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Public_Comment","forum":"SJwZUHRTZ","replyto":"SyDzvK1XM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Number of epochs","comment":"Thanks for the clarification!\n\nCould you give us an estimation of the number of batches you ran for training the network for the classification task? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1514276623393,"tcdate":1514276623393,"number":3,"cdate":1514276623393,"id":"SyDzvK1XM","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"S14wrtAGG","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"The principles apply to both classification and embedding","comment":"Yes, the training is on the 6-second clips, which means the trained classifier will predict the user ID based on unseen 6-second clip input. Both classification and embedding does not take the whole song into account, which is a reasonable thing to ask for since human can distinguish (to a degree) different singers only by listening to short clips. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1514210651543,"tcdate":1514210651543,"number":3,"cdate":1514210651543,"id":"S14wrtAGG","invitation":"ICLR.cc/2018/Conference/-/Paper90/Public_Comment","forum":"SJwZUHRTZ","replyto":"Hk6Af3TMz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Referring to classification and not embeddings","comment":"Hi,\nThanks again for your help.\nIt seems that your answer is referring to the singer performance embeddings while our focus at the moment is on the singer classification problem.\nAs we understand, the input to the CNN is a 6-second clip, which means a 2-D array with dimensions of 96x256.\nThe output of the CNN is a 3-D matrix with dimensions of 128x96x256.\nBefore feeding the global aggregation module you flatten this 3-D array to a 2-D matrix and then apply aggregation over time and feed in to the fully-connected layers. This outputs a one-hot vector with the classification.\nThe part we don't seem to understand is whether each training example is just this one clip of 6-seconds, or is there a way in which the classification takes into account the entire song. If so, how is it done, since each song is composed of a different number of 6-seconds clips.\nThanks for clarifying this issue for us."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1514156756941,"tcdate":1514156756941,"number":2,"cdate":1514156756941,"id":"Hk6Af3TMz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"H1ToLBTff","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"About summarizing over varying lengths","comment":"The training was done on a \"clip-to-clip\" fashion, meaning that the siamese network is trained on the 6-seconds clips, not the entire performance, so there is no averaging before the global aggregation. \n\nThe summarization from 6-second clips to a single performance was done by averaging the \"projected\" vectors of each 6-seond clip from the same performance. So the summarization happens after the training and projection.  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1514129060564,"tcdate":1514129060564,"number":2,"cdate":1514129060564,"id":"H1ToLBTff","invitation":"ICLR.cc/2018/Conference/-/Paper90/Public_Comment","forum":"SJwZUHRTZ","replyto":"ByACmvuGM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Dealing with varying lengths","comment":"Thanks for the quick and detailed comment.\n\nFrom our understanding of your answer to 1, each song is divided into a *varying* number of 6-seconds overlapping clips. \nDoes this mean that prior to the global-aggregation module, we'd have to average all the clips in order to produce a single 2D matrix to feed to the global aggregation?\nDid you just take the average over all clips or used a weighted sum with learnable parameters (or pre-defined)?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1513808853924,"tcdate":1513808853924,"number":1,"cdate":1513808853924,"id":"ByACmvuGM","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Comment","forum":"SJwZUHRTZ","replyto":"rJFVEb_MG","signatures":["ICLR.cc/2018/Conference/Paper90/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper90/Authors"],"content":{"title":"Kernel size is 5x5 and channel size is 1 ","comment":"Thanks for trying this out and leaving feedbacks here. We are still working on have a full responses for the official reviews so sorry first about this being just short answers.\n\n1. Each single training example is just one 6-second clip against another 6-second clip, so channel size is just 1. And to compare the entire song, we just average the learned projections for all the clips from one song. \n\n2. Since the kernel size is 5x5. The calculation is 1*64*5*5+64*128*5*5+128*128*5*5 = 616000, with additional parameters from batch normalizations. \n\nPlease let us know if there are still confusions. Thanks for your feedback again and we will improve the readability of our report accordingly."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1513784368648,"tcdate":1513784368648,"number":1,"cdate":1513784368648,"id":"rJFVEb_MG","invitation":"ICLR.cc/2018/Conference/-/Paper90/Public_Comment","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Paper reproduction attempt - gaps that need to be filled","comment":"Hi,\nWe've read your work and were very impressed with your approach, thus we've chosen to try and reproduce the results in a deep learning seminar we are taking.\n\nThere are a few issues standing in our way, and we'd appreciate if you could clarify them for us, so we could achieve better results.\n\n1. How did you deal with the fact that the songs have varying lengths but the network's input should be at fixed size?\nFrom our understanding, each input channel corresponds to Mel-spectogram composed of 6 seconds of the song, which means that each song will have a different number of channels for the input layer.\n\n2. We are trying to implement the network that is composed of 3 layers of CNN (1 CNN layer and 1 CNN block).\nWe were not able to understand the calculation for the number of CNN parameters (621k).\nOur assumption is that the network architecture is as follows: \nNum of input channels (denoted as C - what is the correct number?)\nFirst CNN layer outputs 64 channels  --> C*(3x3)*64 = 576*C params\nCNN block: Input is 64 channels - output is 64 channels --> 64*(3x3)*64 = 36,864 params\n                     Input is 64 channels - output is 128 channels --> 64*(3x3)*64 = 73,728 params\nThis adds up to 576*C + 110,592 parameters which means that C should be 886 in order to reach 612k, which doesn't seem reasonable to us due to the lengths of the songs. Could you clarify this issue for us? Which of our assumptions is wrong?\n\nThanks a lot for your response and help."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515642528802,"tcdate":1511820723832,"number":3,"cdate":1511820723832,"id":"BynhTbclf","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Review","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Authors explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. They show that this last helps to improve the performance of CNNs the most.","rating":"7: Good paper, accept","review":"It is an original contribution towards  global aggregation over time. Results are significant. \n\nThe idea that the attention mechanism can be viewed as a special case of a global aggregation operation along the time axis, and has learnable parameters is interesting.\n\nThe model is well described and motivated.\n\nPlease precise fig 4, not easy to read. \nHowever other results / tables are very efficent.\n\nps : complete link end of section 1.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515724405196,"tcdate":1511818739284,"number":2,"cdate":1511818739284,"id":"Skjx8WqxG","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Review","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting dataset that is not fully utilized, could be clearer on models as well","rating":"5: Marginally below acceptance threshold","review":"This paper describes the use of convolutional neural networks for classification and similarity of solo vocal performances.  It compares vanilla CNNs, ResNets and ResNeXts on two tasks: singer classification using softmax outputs and singer clustering using Siamese networks.  It also compares using fixed pooling versus feed-forward attention to combine the output of the convoultional layers into a fixed-sized representation.  Experimental results show that the singer classification task seems to work well, while the qualitative results on singer clustering are less conclusive.\n\nBecause this is a very empirical comparison of several system variations, its conclusions rely to a very large extent on the data that it is trained and evaluated on.  It is difficult to understand from the paper what that is, exactly.  The dataset is described as an addition to the DAMP dataset containing more singers, but the actual dataset used in the experiments is a tiny subset of either this new dataset or the original DAMP dataset.  It is not clear why it is so small.  The description of the DAMP-balanced dataset is unclear as well.  Where did the extra music in the DAMP-balanced dataset come from? The same source as DAMP?  Are all 3462 singers from DAMP also in DAMP-balanced?  In appendix A, how can this dataset be balanced if some songs have more performances than others?  How can 5429 singers have all performed all of these songs if all songs have fewer than 5429 performances?\n\nThe use of the tiny subset also seems to have affected the performance of the networks.  The conclusions report that feed-forward attention \"makes learning faster but also is more probably to overfit the data\" and that \"The experiment results do not suggest that deper architecture leads to better performance...\"  Both of these could be explained by using too little data to train the models.  If there is so much data available, artificially limiting the data to be used in experiments doesn't make sense.\n\nIn addition to incomplete descriptions of the data used, the models could be defined more completely as well.  On page 3 and in figure 2, it is not clear which layers are the max pooling layers, what the relationship between cardinality and group = 32 is, or how the first layer of (b) can have 64 output dimensions and the second layer have 256 input dimensions.  \n\nIn general in terms of system design, 2D convolution for audio would make more sense if it were performed on a spectrogram with a logarithmic frequency axis, like some constant-Q transforms, in which case changes in pitch result in shifts in frequency.  The current paper uses mel spectrograms, which do not have this property, since mel frequency is linear below 1000 Hz.  In addition, the feedforward attention mechanism seems to be able to find a single pattern to pay attention to, since it only uses a single weight vector and not a matrix.  It would be interesting to explore more sophisticated attention mechanisms.\n\nFinally, in terms of experimental validation, the results in figure 4, showing PCA projections of songs colored by artist, are not very convincing.  It is not clear to me that coloring by singer looks more \"clustered\" than coloring by song.  It would be much more convincing to perform k-nearest neighbors classification with the learned embeddings for singer verification.  That way performance could be measured quantitatively.  In addition, you could use different singers in the test set with an \"enrollment\" set of examples from each one that are processed by the siamese network but not trained on it.\n\n\n\nMinor Comments\n--------------\n\np2: \"When the same song is performed by different singers, ... -- ie., the 'song effect'\" citation?\n\np2: \"songs sang by a singer\" typo: \"songs sung by a singer\"\n\np2: \"Lastly, related work is described in section ??\" typo\n\np4: Please number all equations so that readers and reviewers can refer to them!\n\np6: \"all the neural network models far exceeded it by at least 25%\" is that relative or absolute?  Either way, it looks like more than that.\n\n\nAfter revision/response\n--------------------------------\nThe authors have done a good job of updating the paper in response to the reviews, and responding in the openreview system. I appreciate their efforts, which have made the paper much clearer.\n\nNow that I know how the new dataset is constructed, it seems like it's not really the best way to go about doing it.  If it contains 30,000 recordings, but only 500 of them can be used at a time, that seems like kind of a waste. This limitation is still causing the models to be under-fit.\n\nThe kNN experiments do give better evidence that the learned embeddings are useful for representing singer identity, although I'm not sure why figure 5 jumps straight to 10 nearest neighbors for the song classification task.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515642528879,"tcdate":1511416914945,"number":1,"cdate":1511416914945,"id":"H1iUNk4gz","invitation":"ICLR.cc/2018/Conference/-/Paper90/Official_Review","forum":"SJwZUHRTZ","replyto":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference/Paper90/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting application but not much novelty","rating":"5: Marginally below acceptance threshold","review":"This paper discusses the application of modern deep convolutional network architectures to the task of singer identification.  It also compares different ways of aggregating a sequence over time to produce a single embedding.  The paper is mainly empirical.\n\nI am a bit on the fence about this paper, because on the one hand I think it's important to study the relative benefits of different convolutional models on tasks other than vision, but on the other hand I'm worried the results presented in this paper have limited applicability outside of the experiment/domain presented.  If the paper discussed applied convolutional networks to many various sequence classification tasks (document classification, speaker identification, etc), I might be more convinced that general lessons could be learned from these results.  The paper also does not propose anything novel - all of the models and techniques are taken from existing work (by contrast, imagine that instead the paper proposing some novel temporal integration method and showing that it gave amazing results compared to max-pooling, average-pooling, or feed-forward attention).  In particular, the model seems very similar to that of Raffel & Ellis 2016, except that the task is somewhat different (though still audio-related) and various different (pre-existing) convolutional models and temporal integration techniques are compared.  For these reasons I don't think it makes a lot of sense to publish this at ICLR, but I think it would be well-suited to a more (audio) application-specific venue.\n\nSome specific comments/suggestions:\n\n-  \" Ideally, it should be possible to identify 'singing style' or 'singing characteristics' by looking (and listening) to the clusters formed from the projections of audio recordings onto the embedding space.\"  Unless the embedding space is 2 (or 3) dimensional, you can't really *look* at clusters in it - you have to also project it down to 2 (or 3) dimensions.  Probably worth mentioning this.\n- \"The interfering 'song effect' is even more dominant in the singing voice case than that of the environment/pose effect in face recognition.\" Citation needed!\n- May be worth mentioning another benefit of embeddings - namely it makes it so that comparing two long sequences (spectrograms) can be done efficiently with a simple Euclidian distance operation.  You can also precompute embeddings for all of the entries in some large database of spectrograms and efficiently find the closest match to a query, etc.\n- Along those lines, I think you can actually unify singer classification and singer performance embedding by doing k-nearest-neighbors in embedding space against a database of recordings of different singers (as opposed to using a softmax to classify singer, as you propose).  This is the approach sometimes taken e.g. in speaker identification and face recognition.\n- LaTeX typo: \"related work is described insection ??.\"\n- Vague ask, but I think Figure 1 could be a lot more descriptive - maybe illustrating that the input starts as a sequence, which is then transformed to a bank of (filtered) sequences, then collapsed across time to a single vector via the temporal aggregation, etc...\n- For completeness, you should provide equations for the different convolutional blocks you are describing.\n- Suggestions for future work: 1) Try striding the convolutions instead of max-pooling; this seems like more common practice these days. 2) Your temporal pooling operations ignore temporal order (e.g. if you permuted the order of the input sequence, the output won't change).  It was shown in \"Attention is All You Need\" that if you include an encoding of the temporal position, attention mechanisms of this type can benefit.  You might try this too.\n- I think you can shorten some of the description of computing the mel spectrograms, or move some of this information into the appendix.\n- Casey et al. (2008) should be \\citep\n- Your train/test accuracies are very close in some cases - can you run a significance analysis?\n- LaTeX typo:  \"earlier in section .\"\n- It seems like you should be using a more powerful dimensionality reduction method (like t-SNE) instead of PCA for visualization, unless you have an important reason to use PCA.\n- To be honest, it is not obvious at all that the networks learned to cluster songs from the same singer - the plots look like they have a lot of overlap between the clusters corresponding to each singer.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]}},{"tddate":null,"ddate":null,"tmdate":1515141069340,"tcdate":1508951551261,"number":90,"cdate":1509739490278,"id":"SJwZUHRTZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJwZUHRTZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Audio Features for Singer Identification and Embedding","abstract":"There has been an increasing use of neural networks for music information retrieval tasks. In this paper, we empirically investigate different ways of improving the performance of convolutional neural networks (CNNs) on spectral audio features. More specifically, we explore three aspects of CNN design: depth of the network, the use of residual blocks along with the use of grouped convolution, and global aggregation over time. The application context is singer classification and singing performance embedding and we believe the conclusions extend to other types of music analysis using convolutional neural networks. The results show that global time aggregation helps to improve the performance of CNNs the most. Another contribution of this paper is the release of a singing recording dataset that can be used for training and evaluation. ","pdf":"/pdf/2b37e1c2ffd574cc5a87611590b1e19cd0cd36ee.pdf","TL;DR":"Using deep learning techniques on singing voice related tasks.","paperhash":"anonymous|learning_audio_features_for_singer_identification_and_embedding","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Audio Features for Singer Identification and Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJwZUHRTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper90/Authors"],"keywords":["convolution neural networks","attention","music information retrieval"]},"nonreaders":[],"replyCount":19,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}