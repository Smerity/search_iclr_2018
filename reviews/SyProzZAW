{"notes":[{"tddate":null,"ddate":null,"tmdate":1515128607502,"tcdate":1515128607502,"number":3,"cdate":1515128607502,"id":"HkvXDF2XM","invitation":"ICLR.cc/2018/Conference/-/Paper928/Official_Comment","forum":"SyProzZAW","replyto":"HJqsNbFez","signatures":["ICLR.cc/2018/Conference/Paper928/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper928/Authors"],"content":{"title":"Response to review","comment":"Thank you for this thoughtful feedback. To respond to the particular comments raised:\n\n- This is a very interesting question. In this work, we have supposed that connections between layers of a network are dense. In this case, the topology is given simply by the number of neurons in each layer, and this architecture is relatively versatile. Architectures of the form described in the proof of Thm. 5.1 (where the sizes of the hidden layers follow a decreasing geometric progression) should be especially flexible, able to learn a wide range of monomials and sums of monomials. Intuitively, this network architecture learns well because the initial large hidden layers capture many lower order correlations between input variables, which are then used to calculate higher-order correlations deeper within the network.\n\n- The conditions on the activation function appear to be at least largely tight. As we mention in the text, Thm. 3.4 fails for ReLU activation (where the Taylor series is not even defined), implying that all subsequent theorems also fail for ReLUs. More interestingly, it is possible to multiply d inputs with (slightly) fewer than 2^d neurons if the constant term in the Taylor series for the activation function is zero. We had previously proven that a less elegant exponential bound still holds as long as the dth Taylor coefficient itself is nonzero (without any assumptions on the other coefficients), and we have included this in our revision.\n\n- In practice, we are rarely concerned with uniform approximation for epsilon truly arbitrarily small. ReLUs can be (imperfectly) approximated by Taylor-approximable functions, and the behavior diverges as the desired epsilon decreases. In running our experiments, we observed similar behavior with ReLUs as with Taylor-approximable activation functions, even though the full power of our theoretical results is indeed not applicable."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515128277524,"tcdate":1515128277524,"number":2,"cdate":1515128277524,"id":"HyaCBK3XG","invitation":"ICLR.cc/2018/Conference/-/Paper928/Official_Comment","forum":"SyProzZAW","replyto":"S1z1Zf9xM","signatures":["ICLR.cc/2018/Conference/Paper928/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper928/Authors"],"content":{"title":"Response to review","comment":"We are very grateful for this helpful feedback, and have responded below to individual issues raised.\n\nThank you for the suggestion that we make clearer the domain under which Prop. 3.3 and Thm. 3.4 hold. We have made explicit in our revision that these results hold for any (fixed) domain (-R, R)^n, and that Taylor series are constructed around the origin.\n\nWhile it is indeed true that a ReLU can be approximated by a smooth function with a well-defined Taylor series, any particular choice of such a function would fail our strict requirement of uniform approximation for arbitrarily small \\epsilon. Since we have assumed that the choice of nonlinear function \\sigma is fixed, we cannot use progressively better approximations to ReLUs. Another way of thinking about this is to note that a neural network with ReLUs is ultimately piecewise linear. For a fixed budget of neurons, the number of linear pieces is bounded. Given a fixed number of linear pieces and a general polynomial to approximate, the approximation cannot be better than some fixed \\epsilon (depending on the polynomial), whereas we would like \\epsilon to be arbitrarily small.\n\nTheorems 4.1 and 4.2 are in fact independent, with neither implying the other. This is because it is possible for a polynomial to admit a compact uniform approximation without admitting a compact Taylor approximation. We have made this clearer in the text.\n\nWe have rephrased our discussion of prior literature to emphasize that “existence proofs” are a feature only of *some* of the prior work. There are indeed excellent papers that provide explicit constructions."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515128056341,"tcdate":1515128056341,"number":1,"cdate":1515128056341,"id":"B1ebHYnXG","invitation":"ICLR.cc/2018/Conference/-/Paper928/Official_Comment","forum":"SyProzZAW","replyto":"B1B65zqef","signatures":["ICLR.cc/2018/Conference/Paper928/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper928/Authors"],"content":{"title":"Response to review","comment":"We are very grateful for this close reading and constructive comments. Detailed responses follow:\n\n1. We believe that in addition to presenting more general results than in the literature, we also contribute techniques that are significantly stronger than those in Lin et al. In particular, tighter proof techniques are required in order to prove lower bounds on the number of neurons required for a uniform approximation. One of the more interesting methodological insights resulting from our approach is that even though uniform approximation does not imply Taylor approximation, we can still use the lack of a Taylor approximation as a significant step towards proving the lack of a uniform approximation. To the best of our knowledge, this is the first time that Taylor approximation and uniform approximation of neural networks have been rigorously linked.\n\n2. The assumptions on the activation function can be weakened somewhat at the expense of less elegant formulations. We had previously proven that an exponential bound still holds as long as the dth Taylor coefficient itself is nonzero (without any assumptions on the other coefficients), and we have included this statement and proof in our revision. As we mention in the text, Thm. 3.4 fails for ReLU activation (where the Taylor series is not even defined), implying that all subsequent theorems also fail for ReLUs. In practice, however, we are rarely concerned with uniform approximation for epsilon truly arbitrarily small. ReLUs can be (imperfectly) approximated by Taylor-approximable functions, and the behavior diverges as the desired epsilon decreases. In running our experiments, we observed similar behavior with ReLUs as with Taylor-approximable activation functions, even though the full power of our theoretical results is indeed not applicable.\n\n3. In our revision, we have rewritten the proof of prop. 3.3 to encompass all cases. Thank you for calling this to our attention."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642531863,"tcdate":1511824060832,"number":3,"cdate":1511824060832,"id":"B1B65zqef","invitation":"ICLR.cc/2018/Conference/-/Paper928/Official_Review","forum":"SyProzZAW","replyto":"SyProzZAW","signatures":["ICLR.cc/2018/Conference/Paper928/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Proves exponential improvement for expressing polynomial functions using deep NNs, generalizes Lin et al.","rating":"6: Marginally above acceptance threshold","review":"Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons. \nThe paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth.\n\n+ves:\nExplaining the power of depth in NNs is fundamental to an understanding of deep learning. The paper is very easy to follow. and the proofs are clearly written. The theorems provide exponential gaps for very simple polynomial functions.\n\n-ves:\n1. My main concern with the paper is the novelty of the contribution to the techniques. The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas. \n2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.).\n3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(δx)| <= δ^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation.\n\nDespite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642531899,"tcdate":1511821529829,"number":2,"cdate":1511821529829,"id":"S1z1Zf9xM","invitation":"ICLR.cc/2018/Conference/-/Paper928/Official_Review","forum":"SyProzZAW","replyto":"SyProzZAW","signatures":["ICLR.cc/2018/Conference/Paper928/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper explores the representation of polynomials up to a given maximum degree by deep networks, demonstrating gaps between deep and shallow architectures. ","rating":"6: Marginally above acceptance threshold","review":"The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations. It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial. \n\nBy focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question. Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question. Here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation. \n\nTheorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by Lin et al that I did not verify). The idea being to construct a superposition of Taylor approximations of the individual monomials. Here it would be good to be more specific about the domain. Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g. the origin. \n\nThe paper mentions that ``the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series''. However, a ReLU can also be approximated by a smooth function and a Taylor series. \n\nTheorem 4.1 seems to be implied by Theorem 4.2. Similarly, parts of Section 4.2 seem to follow directly from the previous discussion. \n\nIn page 1 ```existence proofs' without explicit constructions'' This is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642531945,"tcdate":1511752866065,"number":1,"cdate":1511752866065,"id":"HJqsNbFez","invitation":"ICLR.cc/2018/Conference/-/Paper928/Official_Review","forum":"SyProzZAW","replyto":"SyProzZAW","signatures":["ICLR.cc/2018/Conference/Paper928/AnonReviewer3"],"readers":["everyone"],"content":{"title":"a main theorem on polynomial approximation of deep vs shallow neural networks","rating":"7: Good paper, accept","review":"Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks. \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c. \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables. The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2). \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation). \nOverall, I like the paper.\n\nPros: new and interesting result, theoretically sound. \nCons: nothing major.\nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary? Could you comment on this problem?\n* Is the assumption that \\sigma has Taylor expansion to order d tight? (That is, are there counter examples for relaxations of this assumption?) \n* As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs, but ReLUs network perform well in practice. Could you provide some further comments on this?\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515129811253,"tcdate":1509137224799,"number":928,"cdate":1510092362343,"id":"SyProzZAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyProzZAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The power of deeper networks for expressing natural functions","abstract":"It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n^{1/k}, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.","pdf":"/pdf/b624a214aa7a152470dcf06d379fda8b257f9be9.pdf","TL;DR":"We prove that deep neural networks are exponentially more efficient than shallow ones at approximating sparse multivariate polynomials.","paperhash":"anonymous|the_power_of_deeper_networks_for_expressing_natural_functions","_bibtex":"@article{\n  anonymous2018the,\n  title={The power of deeper networks for expressing natural functions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyProzZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper928/Authors"],"keywords":["expressivity of neural networks","depth of neural networks","universal approximators","function approximation","deep learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}