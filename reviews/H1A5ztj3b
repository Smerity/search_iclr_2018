{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222699372,"tcdate":1512170500145,"number":3,"cdate":1512170500145,"id":"Hyn-NPJbz","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Review","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper shows an observation of “super-convergence” when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.","rating":"4: Ok but not good enough - rejection","review":"This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.\n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen.\n\nThe explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained  using an order of magnitude fewer iterations than is used with standard training methods, which we named ``\"super-convergence\".  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.  In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures and code to replicate this work will be made available upon publication.\n","pdf":"/pdf/c042faa833797f1bc094498e5583120934e0456a.pdf","TL;DR":"Residual networks can be trained with large learning rates using an order of magnitude fewer iterations. ","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222699416,"tcdate":1511767575385,"number":2,"cdate":1511767575385,"id":"H1yQ04YxG","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Review","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Large cyclic learning rates for fast convergence, works in very narrow conditions","rating":"4: Ok but not good enough - rejection","review":"In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR). The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets – this is called “super-convergence”. However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting. \n\n\nSome specific comments by sections:\n\n2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates. The authors mention Loshchilov & Hutter in next section but do not compare it to their work. The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf\n\n3. Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence. Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units. It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.\n\n4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods. \n5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot). Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.\n\n6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work. However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work. \n\nOverall, the work is presented as a positive result in very specific conditions but it seems more like a negative result. It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained  using an order of magnitude fewer iterations than is used with standard training methods, which we named ``\"super-convergence\".  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.  In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures and code to replicate this work will be made available upon publication.\n","pdf":"/pdf/c042faa833797f1bc094498e5583120934e0456a.pdf","TL;DR":"Residual networks can be trained with large learning rates using an order of magnitude fewer iterations. ","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222699456,"tcdate":1511275978036,"number":1,"cdate":1511275978036,"id":"rJfAp3Zef","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Review","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting Phenomenon, but no deep insights","rating":"4: Ok but not good enough - rejection","review":"The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates. Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper.\n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum. In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well. However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge.\n\nIn the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior. \n\nPersonally i would not use the term \"convergence\" in this setting at all as the runs are very short and thus we might not be close to any region of convergence. Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all. The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping.\n\nPros:\n- Many experiments which try to study the effect\nCons:\n-The described phenomenon seems to depend strongly on the problem surface and might never \nbe encountered on any problem aside of Cifar-10\n- Only single runs are shown, considering the noise on those the results might not be reproducible.\n-Experiments are not described in detail\n-Experiment design feels \"ad-hoc\" and unstructured\n-The role and value of the many LR-plots remains unclear to me.\n\nForm:\n- The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas\n- Figures are not properly described, e.g. axes in Figures 3 a) and b)\n- Explicit references to code are made which require familiarity with the used framework(if at all published).  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained  using an order of magnitude fewer iterations than is used with standard training methods, which we named ``\"super-convergence\".  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.  In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures and code to replicate this work will be made available upon publication.\n","pdf":"/pdf/c042faa833797f1bc094498e5583120934e0456a.pdf","TL;DR":"Residual networks can be trained with large learning rates using an order of magnitude fewer iterations. ","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1511194726655,"tcdate":1511194726655,"number":1,"cdate":1511194726655,"id":"rJJuxtxeG","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper6/Authors"],"content":{"title":"Two new papers (after the submission deadline) independently provide theoretical support for the super-convergence phenomenon ","comment":"Jastrz{\\k{e}}bski,  et al. [1] show that the larger the ratio of the learning rate to the batch size, the greater the noise during training and the better the network generalizes. They also demonstrate that instead of increasing the learning rate via cyclical learning rates, one obtains a similar effect by decreasing the batch size.  Independently, Chaudhari, et al. [2] show that the entropy of the steady-state distribution of the weights scales linearly with the ratio of the learning rate over two times the batch size and this ratio completely determines the strength of SGD's regularization.  Although the authors don't suggest a cycle, they do recommend that this ratio be large in practice, which coincides with our empirical results.\n\n1. Jastrzębski, Stanisław, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. \"Three Factors Influencing Minima in SGD.\" arXiv preprint arXiv:1711.04623 (2017).\n2. Chaudhari, Pratik, and Stefano Soatto. \"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.\" arXiv preprint arXiv:1710.11029 (2017).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained  using an order of magnitude fewer iterations than is used with standard training methods, which we named ``\"super-convergence\".  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.  In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures and code to replicate this work will be made available upon publication.\n","pdf":"/pdf/c042faa833797f1bc094498e5583120934e0456a.pdf","TL;DR":"Residual networks can be trained with large learning rates using an order of magnitude fewer iterations. ","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1509454917309,"tcdate":1509454862597,"number":1,"cdate":1509454862597,"id":"H1Df4xU0Z","invitation":"ICLR.cc/2018/Conference/-/Paper6/Public_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["~Dmytro_Mishkin2"],"readers":["everyone"],"writers":["~Dmytro_Mishkin2"],"content":{"title":"External validation: both for and against paper results.","comment":"I have done an additional experiement in different domain. \n\n1) Task: local patch descriptor learning. \nArchitecture: Siamese network, output is 128-channel descriptor, which is L2-normed.\nThen triplet margin loss applied to the triplet of : anchor, positive, negative.\n\n2)Networks itself is VGG-style:\n32x32 grayscale, locally normalized patch -> 32C3-32C3-64C3/2-64C3-128C3/2-128C8 - L2norm\nNo residual connections, no bottlenecks, but batch-normalization after each conv layer.\n\n3)Dataset: 5M triplets, randomly sampled from 100K patches from Brown dataset \nhttp://phototour.cs.washington.edu/patches/default.htm\n\n4) lr_rate decay is linear from max_lr to 0 , as it work better than standard \"step\" one. \n\n5) Metric is mAP two view matching on two other datasets: W1BS and HPatches. So metric really tests generatlization\n\nSo, results:\n\nLR policy                                                         | Iterations | mAP\n\nLinear, from 0.1 to 0                                       | 50K          | 0.1065\nLinear, from 50 to 0                                        |    5K          | 0.1087\n(0.9 * abs(sin)) * + 0.1) *(Linear, from 50 to 0)|    5K          | 0.1100\n\nSo I am  not sure, if it can be called \"super-convergence\" in authors sense, but large learning rate lead to improved performance in my case + \"cyclic modulation\" makes effect bigger.\n\n\nFirst, batch normalization seems necessary part, because it basically allows to have huge weights, which does not influence output. And at the end of my network there is L2norm, so everything is always fine-scaled. \n\nSecond, if the large weights are one of the responsible parts, may be recent paper on \"Feature Incay\" is relevant https://arxiv.org/pdf/1705.10284.pdf \nIn short, authors argue, that large values of the features contrary to common practice, lead to better generalization. But they don`t tell anything about convergency speed.\n\nThird, unfortunately, result of the faster converged network was _worse_ on real-world with matching. \n\nThe last, but not least, this paper contradicts recent NIPS oral \"Train longer, generalize better: closing the\ngeneralization gap in large batch training of neural\nnetworks\"  https://arxiv.org/pdf/1705.08741.pdf, where authors show that longer training is important for generalization. \n\nSolving this contradiction could lead to new interesting results.\n\n\n****\nPaper overall is good written and opens an interesting discussion. I would vote for poster acceptance"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained  using an order of magnitude fewer iterations than is used with standard training methods, which we named ``\"super-convergence\".  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.  In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures and code to replicate this work will be made available upon publication.\n","pdf":"/pdf/c042faa833797f1bc094498e5583120934e0456a.pdf","TL;DR":"Residual networks can be trained with large learning rates using an order of magnitude fewer iterations. ","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739534994,"tcdate":1507721877831,"number":6,"cdate":1509739532334,"id":"H1A5ztj3b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained  using an order of magnitude fewer iterations than is used with standard training methods, which we named ``\"super-convergence\".  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network.  In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures and code to replicate this work will be made available upon publication.\n","pdf":"/pdf/c042faa833797f1bc094498e5583120934e0456a.pdf","TL;DR":"Residual networks can be trained with large learning rates using an order of magnitude fewer iterations. ","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}