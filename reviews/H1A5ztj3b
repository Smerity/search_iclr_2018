{"notes":[{"tddate":null,"ddate":null,"tmdate":1514043625346,"tcdate":1514043625346,"number":3,"cdate":1514043625346,"id":"ByWgFx2Mf","invitation":"ICLR.cc/2018/Conference/-/Paper6/Public_Comment","forum":"H1A5ztj3b","replyto":"H1yQ04YxG","signatures":["~Dmytro_Mishkin2"],"readers":["everyone"],"writers":["~Dmytro_Mishkin2"],"content":{"title":"Residual is not necessary","comment":">It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.\n\nResNet is not needed, actually. My experiments with HardNet local descriptor (see me previous public comment)  use plain VGG-like architecture and still achieve some king of \"super-convergence\"."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1513802386432,"tcdate":1513284334738,"number":2,"cdate":1513284334738,"id":"HkvgXwlfM","invitation":"ICLR.cc/2018/Conference/-/Paper6/Public_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["~Josh_Varty1"],"readers":["everyone"],"writers":["~Josh_Varty1"],"content":{"title":"Reproduction","comment":"I have chosen to reproduce elements of this paper as part of the ICLR 2018 Reproducibility Challenge: http://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html\n\nThe key claim of this paper that I attempted to reproduce is that a Resnet-56 network can be trained to ~90% accuracy on Cifar-10 in just 10,000 steps with a Cyclical Learning Rate (CLR). I also wanted to confirm the baseline result that it would take 80,000 steps to train the same network to similar accuracy using a traditional multistep learning.\n\nThese results were presented in Figure 1A from the paper.\n\nI took two approaches when reproducings this work:\n\n1. I attempted to reproduce the work in Tensorflow using both the paper and the author's Caffe code as a guide.\n2. I attempted to reproduce the work using the author's Caffe code and GitHub instructions.\n\n\nReproducing with Tensorflow\n\nI have made my code available on GitHub at: https://github.com/JoshVarty/ReproducingSuperconvergence\n\nUsing Tensorflow I was able to weakly reproduce evidence of super-convergence.\n\nAfter 10,000 steps training with CLR, the network achieved ~85% accuracy. See: https://i.imgur.com/e9RXHl1.png\nAfter 20,000 steps training with multistep, the network achieved ~80% accuracy. See: https://i.imgur.com/PGZ9nlI.png\n\t\nAlthough these results do not quite align perfectly with those of the paper, I believe they support it. Although multistep training was run for 80,000 steps it did not  improve after the first 20,000 steps. I was also unable to achieve accuracies over 90% as shown in the paper. I believe this may be due to the fact I was only able to use a mini-batch size of 125 compared to the author's mini-batch size of 1,000.\n\n\nReproducing with Caffe\n\nUsing the provided Caffe code, I was able to partially reproduce the results presented in the paper. \n\nFor baseline multistep learning, I achieved a test accuracy of 85%. See: https://i.imgur.com/8SaqJJ3.png\nFor CLR learning, I achieved a test accuracy of 91.2%. See: https://i.imgur.com/zVds4VF.png\n\nThe overall trend looks similar to that of the author's results, but the test accuracy of CLR does not quite match the expected results presented in the paper.\n\nPotential reasons for lack of reproduction\n\t- The author trained their network using an 8-GPU machine with a mini-batch size of 1,000. I used a batch size of 125 on a single K80 GPU.\n\t- Difference in Batch Normalization parameter settings. Currently investigating this here: https://github.com/lnsmith54/super-convergence/issues/2\n\n\nCorrections\n\n\t- Appendix A claims the first Conv Layer has stride=2, but the code provided uses stride=1. \n\t\n\nUndocumented Elements\n\nSome elements of the network were undocumented in the paper making it harder to reproduce:\n\n\t- While training, images were flipped left-to-right with 50% probability\n\t- All weights before ReLUs are initialized according to \"Delving Deep into Rectifiers.\" [1]\n\t- All weights before softmax are initialized according to \"Understanding the difficulty of training deep feedforward neural networks.\" [2]\n\t- Bias variables are initialized to zero.\n\t- Learning rate scaling on weights layers was 1\n\t- Learning rate scaling on bias layers was 2\n\t\n\nConclusion\n\nThere is evidence to suggest that super-convergence reproduces in some form on Cifar-10 with a Resnet-56 architecture. On a personal note, I will be incorporating Cyclical Learning Rates into future projects of mine.\n\t\n\t\n[1] https://arxiv.org/pdf/1502.01852v1.pdf\n[2] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&rep=rep1&type=pdf"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1513018922718,"tcdate":1513018922718,"number":3,"cdate":1513018922718,"id":"SyXNU83Wf","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper6/Authors"],"content":{"title":"Replies to specific reviewer’s comments.","comment":"1. AnonReviewer1 comments and replies:\n\"Loshchilov & Hutter in next section but do not compare it to their work.\"\nWe stated that Loshchilov & Hutter's form for CLR (called SGDR) does not work for super-convergence.  The paper now states this more clearly in the Related Works Section.\n\n\"contradictory claim from NIPS 2017 (as pointed out in the public comment)\"\nWe do not consider the claims in Hoffer, et al. (2017) contradictory.  They show that a longer training is a form of regularization, which doesn't contradict the regularization effects of large learning rates any more than it contradicts the use of dropout for regularization.  From a practical perspective, training longer has the obstacle of an even larger computational burden, hence other forms of regularization are preferable.\n\n\"It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.\"\nWe don't think it is the residual nature of the networks that are relevant but how batch norm stabilizes the training in the presence of large learning rates causing gradient noise.  We discuss this more clearly now.\n\n\"Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods.\" \nHoffer, et al. (2017) implies that longer training would improve the slower convergence rate in Fig 1a.  We actually let the training for the typical training schedule go to 120,000 iterations but the test accuracy was higher at 80,000 so Fig 1a shows longer training in a better light.  \n\n\"It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.\"\nThis has been done.  For example, see \"Snapshot ensembles: Train 1, get m for free\" arXiv:1704.00109.  Most of our experiments were performed last winter, prior to this paper but we saw similar results as they described.\n\n\"Overall, the work is presented as a positive result in very specific conditions but it seems more like a negative result.\"\nThe super-convergence paper presents empirical evidence of a new phenomenon that is not yet adequately explained by the literature on SGD, as such it is a positive result.  The Discussion Section should make the impact of this work clearer.\n\nThank you for your comments and the opportunity to address your concerns.\n\n\n2. AnonReviewer2 comments and replies:\n\"the work cannot be reproduced from the paper.\"\nArchitectures and code will be available on github.com.\n\n\"convergence in training loss, which is not shown at all\"\nThe training loss is shown in Figure 4.  Furthermore, we examined the training loss for all of the figures but did not include them in most of the figures for readability and it did not provide any additional insights.  \n\n\"-The described phenomenon seems to depend strongly on the problem surface and might never be encountered on any problem aside of Cifar-10\"\n\"- Only single runs are shown, considering the noise on those the results might not be reproducible.\"\nIf the purpose of the paper was to demonstrate another new technique to obtain a half a percent improvement in results, we would have averaged over 10 runs to show that the half-percent improvement. Also, the limitation of the effect to only Cifar would heavily detract from the practical significance of this paper. However, that is tangential to the primary purpose of this paper. Instead, this super-convergence paper presents empirical evidence of a new phenomenon that is not yet adequately explained by the literature on SGD and regularization.\n\n\"-Experiments are not described in detail.\"\n\"-Experiment design feels \"ad-hoc\" and unstructured\"\n\"-The role and value of the many LR-plots remains unclear to me.\"\n\"- The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas\"\nArchitectures and code will be available on github.com.\n\n\"- Figures are not properly described, e.g. axes in Figures 3 a) and b)\"\nThe caption for Figure 3 was amended.  This figure was borrowed with permission from \"Qualitatively characterizing neural network optimization problems.\" arXiv:1412.6544 (2014) and a full description is available in that paper.\n\n\"- Explicit references to code are made which require familiarity with the used framework(if at all published).\"  \nArchitectures and code will be available on github.com.\n\n3. AnonReviewer3 comments and replies:\n\"I don't understand what is offered beyond the original papers.\"\n\"I am concerned if it is of general interest for deep learning models.\"\n\"Also, the authors do not give a conclusive analysis under what condition it may happen.\"\n\"a lack of solid analysis or discussion behind these observations.\"\n\nWe believe the significance of this paper and how it is intertwined with recent discussions in the literature on SGD and generalization is made clearer by the Discussions Section."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1513018246655,"tcdate":1513018246655,"number":2,"cdate":1513018246655,"id":"r1J5783bz","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper6/Authors"],"content":{"title":"General reply to the Reviewers comments.","comment":"Thank you to all the reviewers for your time and effort in reading our paper.\n\nAlthough many papers in the deep learning literature suggest new techniques for training deep networks, we did not intend for this paper to be of this kind.  Instead, this super-convergence paper presents empirical evidence of a new phenomenon that is not yet adequately explained by the literature on SGD.  While super-convergence might be of some practical value, the primary purpose of this paper is to provide empirical support and theoretical insights to the active discussions in the literature on SGD and understanding generalization.  Based on the reviewers' comments, it is apparent that the relevance of super-convergence to ongoing discussions in the literature is unclear.  We have rewritten the Discussion Section and revised various other parts of the paper to more explicitly show how our results are relevant to ongoing discussions in the literature on SGD and generalizations.  We hope the response to super-convergence is similar to the reaction to the initial report of network memorization, which sparked an active discussion within the deep learning research community on better ways of understanding the factors in SGD leading to solutions that generalize well. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642477082,"tcdate":1512170500145,"number":3,"cdate":1512170500145,"id":"Hyn-NPJbz","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Review","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper shows an observation of “super-convergence” when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.","rating":"4: Ok but not good enough - rejection","review":"This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.\n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen.\n\nThe explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642477119,"tcdate":1511767575385,"number":2,"cdate":1511767575385,"id":"H1yQ04YxG","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Review","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Large cyclic learning rates for fast convergence, works in very narrow conditions","rating":"4: Ok but not good enough - rejection","review":"In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR). The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets – this is called “super-convergence”. However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting. \n\n\nSome specific comments by sections:\n\n2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates. The authors mention Loshchilov & Hutter in next section but do not compare it to their work. The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf\n\n3. Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence. Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units. It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.\n\n4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods. \n5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot). Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.\n\n6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work. However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work. \n\nOverall, the work is presented as a positive result in very specific conditions but it seems more like a negative result. It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642477157,"tcdate":1511275978036,"number":1,"cdate":1511275978036,"id":"rJfAp3Zef","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Review","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting Phenomenon, but no deep insights","rating":"4: Ok but not good enough - rejection","review":"The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates. Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper.\n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum. In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well. However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge.\n\nIn the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior. \n\nPersonally i would not use the term \"convergence\" in this setting at all as the runs are very short and thus we might not be close to any region of convergence. Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all. The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping.\n\nPros:\n- Many experiments which try to study the effect\nCons:\n-The described phenomenon seems to depend strongly on the problem surface and might never \nbe encountered on any problem aside of Cifar-10\n- Only single runs are shown, considering the noise on those the results might not be reproducible.\n-Experiments are not described in detail\n-Experiment design feels \"ad-hoc\" and unstructured\n-The role and value of the many LR-plots remains unclear to me.\n\nForm:\n- The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas\n- Figures are not properly described, e.g. axes in Figures 3 a) and b)\n- Explicit references to code are made which require familiarity with the used framework(if at all published).  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1511194726655,"tcdate":1511194726655,"number":1,"cdate":1511194726655,"id":"rJJuxtxeG","invitation":"ICLR.cc/2018/Conference/-/Paper6/Official_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference/Paper6/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper6/Authors"],"content":{"title":"Two new papers (after the submission deadline) independently provide theoretical support for the super-convergence phenomenon ","comment":"Jastrz{\\k{e}}bski,  et al. [1] show that the larger the ratio of the learning rate to the batch size, the greater the noise during training and the better the network generalizes. They also demonstrate that instead of increasing the learning rate via cyclical learning rates, one obtains a similar effect by decreasing the batch size.  Independently, Chaudhari, et al. [2] show that the entropy of the steady-state distribution of the weights scales linearly with the ratio of the learning rate over two times the batch size and this ratio completely determines the strength of SGD's regularization.  Although the authors don't suggest a cycle, they do recommend that this ratio be large in practice, which coincides with our empirical results.\n\n1. Jastrzębski, Stanisław, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. \"Three Factors Influencing Minima in SGD.\" arXiv preprint arXiv:1711.04623 (2017).\n2. Chaudhari, Pratik, and Stefano Soatto. \"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.\" arXiv preprint arXiv:1710.11029 (2017).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1509454917309,"tcdate":1509454862597,"number":1,"cdate":1509454862597,"id":"H1Df4xU0Z","invitation":"ICLR.cc/2018/Conference/-/Paper6/Public_Comment","forum":"H1A5ztj3b","replyto":"H1A5ztj3b","signatures":["~Dmytro_Mishkin2"],"readers":["everyone"],"writers":["~Dmytro_Mishkin2"],"content":{"title":"External validation: both for and against paper results.","comment":"I have done an additional experiement in different domain. \n\n1) Task: local patch descriptor learning. \nArchitecture: Siamese network, output is 128-channel descriptor, which is L2-normed.\nThen triplet margin loss applied to the triplet of : anchor, positive, negative.\n\n2)Networks itself is VGG-style:\n32x32 grayscale, locally normalized patch -> 32C3-32C3-64C3/2-64C3-128C3/2-128C8 - L2norm\nNo residual connections, no bottlenecks, but batch-normalization after each conv layer.\n\n3)Dataset: 5M triplets, randomly sampled from 100K patches from Brown dataset \nhttp://phototour.cs.washington.edu/patches/default.htm\n\n4) lr_rate decay is linear from max_lr to 0 , as it work better than standard \"step\" one. \n\n5) Metric is mAP two view matching on two other datasets: W1BS and HPatches. So metric really tests generatlization\n\nSo, results:\n\nLR policy                                                         | Iterations | mAP\n\nLinear, from 0.1 to 0                                       | 50K          | 0.1065\nLinear, from 50 to 0                                        |    5K          | 0.1087\n(0.9 * abs(sin)) * + 0.1) *(Linear, from 50 to 0)|    5K          | 0.1100\n\nSo I am  not sure, if it can be called \"super-convergence\" in authors sense, but large learning rate lead to improved performance in my case + \"cyclic modulation\" makes effect bigger.\n\n\nFirst, batch normalization seems necessary part, because it basically allows to have huge weights, which does not influence output. And at the end of my network there is L2norm, so everything is always fine-scaled. \n\nSecond, if the large weights are one of the responsible parts, may be recent paper on \"Feature Incay\" is relevant https://arxiv.org/pdf/1705.10284.pdf \nIn short, authors argue, that large values of the features contrary to common practice, lead to better generalization. But they don`t tell anything about convergency speed.\n\nThird, unfortunately, result of the faster converged network was _worse_ on real-world with matching. \n\nThe last, but not least, this paper contradicts recent NIPS oral \"Train longer, generalize better: closing the\ngeneralization gap in large batch training of neural\nnetworks\"  https://arxiv.org/pdf/1705.08741.pdf, where authors show that longer training is important for generalization. \n\nSolving this contradiction could lead to new interesting results.\n\n\n****\nPaper overall is good written and opens an interesting discussion. I would vote for poster acceptance"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]}},{"tddate":null,"ddate":null,"tmdate":1513017310220,"tcdate":1507721877831,"number":6,"cdate":1509739532334,"id":"H1A5ztj3b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1A5ztj3b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates","abstract":"In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n","pdf":"/pdf/c2a72fc08292cb19265508ac2153034dda56d405.pdf","TL;DR":"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.","paperhash":"anonymous|superconvergence_very_fast_training_of_residual_networks_using_large_learning_rates","_bibtex":"@article{\n  anonymous2018super-convergence:,\n  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1A5ztj3b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper6/Authors"],"keywords":["Deep Learning","machine learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}