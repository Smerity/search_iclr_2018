{"notes":[{"tddate":null,"ddate":null,"tmdate":1512357760281,"tcdate":1512357760281,"number":3,"cdate":1512357760281,"id":"HJ_YkBz-G","invitation":"ICLR.cc/2018/Conference/-/Paper12/Official_Review","forum":"rkhCSO4T-","replyto":"rkhCSO4T-","signatures":["ICLR.cc/2018/Conference/Paper12/AnonReviewer4"],"readers":["everyone"],"content":{"title":"a specific architecture for action recognition, tested on one dataset -> limited contribution","rating":"3: Clear rejection","review":"Summary: the paper considers an architecture combining neural networks and Gaussian processes to classify actions in a video stream for *one* dataset. The neural network part employs inception networks and residual networks. Upon pretraining these networks on RGB and optical flow data, the features at the final layer are used as inputs to a GP classifier. To sidestep the intractability, a model using a product of independent GP experts is used, each expert using a small subset of data and the Laplace approximation for inference and learning.\n\nAs it stands, I think the contributions of this paper is limited:\n\n* the paper considers a very specific architecture for a specific task (classifying actions in video streams) and a specific dataset (the HMDB-51 dataset). There is no new theoretical development.\n\n* the elements of the neural network architecture are not new/novel and, as cited in the paper, they have been used for action classification in Wang et al (2016), Ma et al (2017) and Sengupta and Qian (2017). I could not tell if there is any novelty on this part of the paper and it seems that the only difference between this paper and Sengupta and Qian (2017) is that Sengupta and Qian used SVM with multi-kernel learning and this paper uses GPs.\n\n* the paper considers a product of independent GP experts on the neural net features. It seems that combining predictions provided by the GPs helps. It is, however, not clear from the paper how the original dataset was divided into subsets.\n\n* it seems that the paper was written in a rush and many extensions and comparisons are only discussed briefly and left as future work, for example: using the Bayesian committee machine or modern sparse GP approximation techniques, end-to-end training and training with fewer training points.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributed non-parametric deep and wide networks","abstract":"In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural networks (CNN) alleviate the need to augment a neural network with hand-crafted features. In contrast to prior work, we treat each deep neural convolutional network as an expert wherein the individual predictions (and their respective uncertainties) are combined into a Product of Experts (PoE) framework.","pdf":"/pdf/08321e325d02e9617112f64e90a4d4f5bc91f5a5.pdf","paperhash":"anonymous|distributed_nonparametric_deep_and_wide_networks","_bibtex":"@article{\n  anonymous2018distributed,\n  title={Distributed non-parametric deep and wide networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhCSO4T-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper12/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222564841,"tcdate":1511873940431,"number":2,"cdate":1511873940431,"id":"SJ2q6Rqgf","invitation":"ICLR.cc/2018/Conference/-/Paper12/Official_Review","forum":"rkhCSO4T-","replyto":"rkhCSO4T-","signatures":["ICLR.cc/2018/Conference/Paper12/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Official Reviewer 2","rating":"3: Clear rejection","review":"This paper, although titled \"Distributed Non-Parametric Deep and Wide Networks\", is mostly about fusion of existing models for action recognition on the HMDB51 dataset. The fusion is performed with Gaussian Processes, where each of the i=1,..,4 inspected models (TSN-Inception RGB, TSN-Inception Flow, ResNet-LSTM RGB, ResNet-LSTM Flow) returns a (\\mu_i, \\sigma_i), which are then combined in a product of experts formulation, optimized w.r.t. maximum likelihood.\n\nAt its current form this paper is unfit for submission. First, the novelty of the paper is not clear. It is stated that a framework is introduced for independent deep neural networks. However, this framework, the Gaussian Processes, already exists. Also, it is stated that the method can classify video snippets that have heterogeneity regarding camera angle, video quality, pose, etc. This is something characterizes also all other methods that report similar results on the same dataset. The third claim is that deep networks are combined with non-parameteric Bayesian models. That is a good claim, which is also shared between papers at http://bayesiandeeplearning.org/. The last claim is that model averaging taking into account uncertainty is shown to be useful. That is not true, the only result are the final accuracies per GP model, there is no experiment that directly reports any results regarding uncertainty and its contribution to the final accuracy.\n\nSecond, it is not clear that the proposed method is the one responsible for the reported improvements in the experiments. Currently, the training set is split into 7 sets, and each set is used to train 4 models, totalling 28 GP experts. It is unclear what new is learned by the 7 GP expert models for the 7 splits. Why is this better than training a single model on the whole dataset? Also, why is difference bigger between ResNet Fusion-1 and Resnet SVM-SingleKernel?\n\nThird, the method reports results only on a single dataset, HMDB51, which is also rather small. Deriving conclusions from results on a single dataset is suboptimal. Other datasets that can be considered are (mini) Kinetics or Charades.\n\nForth,  the paper does not have the structure of a scientific publication. It rather looks like an unofficial technical report. There is no related work. The methodology section reads more like a tutorial of existing methods. And the discussion section is larger than any other section in the paper.\n\nAll in all, there might be some interesting ideas in the paper, specifically how to integrate GPs with deep nets. However, at the current stage the submission is not ready for publication.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributed non-parametric deep and wide networks","abstract":"In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural networks (CNN) alleviate the need to augment a neural network with hand-crafted features. In contrast to prior work, we treat each deep neural convolutional network as an expert wherein the individual predictions (and their respective uncertainties) are combined into a Product of Experts (PoE) framework.","pdf":"/pdf/08321e325d02e9617112f64e90a4d4f5bc91f5a5.pdf","paperhash":"anonymous|distributed_nonparametric_deep_and_wide_networks","_bibtex":"@article{\n  anonymous2018distributed,\n  title={Distributed non-parametric deep and wide networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhCSO4T-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper12/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222564883,"tcdate":1511809961572,"number":1,"cdate":1511809961572,"id":"rkG37k9xM","invitation":"ICLR.cc/2018/Conference/-/Paper12/Official_Review","forum":"rkhCSO4T-","replyto":"rkhCSO4T-","signatures":["ICLR.cc/2018/Conference/Paper12/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper proposes a classification system, that trains Gaussian Process based classifiers on a top of a number of neural networks, trained on two modalities (RGB and optical flow), and combines them  using a product of experts formulation. There is very low novelty and it is an applications paper which combines well known methods.","rating":"3: Clear rejection","review":"- The paper is fairly written and it is clear what is being done\n- There is not much novelty in the paper; it combines known techniques and is a systems paper, so I \n  would judge the contributions mainly in terms of the empirical results and messsage conveyed (see\n  third point)\n- The paper builds on a  previous paper (ICCV Workshops, https://arxiv.org/pdf/1707.06923.pdf),\n  however, there is non-trivial overlap between the two papers, e.g. Fig. 1 seems to be almost the\n  same figure from that paper, Sec 2.1 from the previous paper is largely copied     \n- The message from the empirical validation is also not novel, in the ICCVW paper it was shown that\n  the combination of different modalities etc. using a multiple kernel learning framework improved\n  results (73.3 on HMDB51), while in the current paper the same message comes across with another\n  kind of (known) method for combining different classifiers and modality (without iDT their best\n  results are 73.6 for CNN+GP-PoE) ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributed non-parametric deep and wide networks","abstract":"In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural networks (CNN) alleviate the need to augment a neural network with hand-crafted features. In contrast to prior work, we treat each deep neural convolutional network as an expert wherein the individual predictions (and their respective uncertainties) are combined into a Product of Experts (PoE) framework.","pdf":"/pdf/08321e325d02e9617112f64e90a4d4f5bc91f5a5.pdf","paperhash":"anonymous|distributed_nonparametric_deep_and_wide_networks","_bibtex":"@article{\n  anonymous2018distributed,\n  title={Distributed non-parametric deep and wide networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhCSO4T-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper12/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739530819,"tcdate":1508308436304,"number":12,"cdate":1509739528170,"id":"rkhCSO4T-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkhCSO4T-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Distributed non-parametric deep and wide networks","abstract":"In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural networks (CNN) alleviate the need to augment a neural network with hand-crafted features. In contrast to prior work, we treat each deep neural convolutional network as an expert wherein the individual predictions (and their respective uncertainties) are combined into a Product of Experts (PoE) framework.","pdf":"/pdf/08321e325d02e9617112f64e90a4d4f5bc91f5a5.pdf","paperhash":"anonymous|distributed_nonparametric_deep_and_wide_networks","_bibtex":"@article{\n  anonymous2018distributed,\n  title={Distributed non-parametric deep and wide networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkhCSO4T-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper12/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}