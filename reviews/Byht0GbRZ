{"notes":[{"tddate":null,"ddate":null,"tmdate":1512413455394,"tcdate":1512413414035,"number":3,"cdate":1512413414035,"id":"S1zkYGm-G","invitation":"ICLR.cc/2018/Conference/-/Paper1053/Official_Review","forum":"Byht0GbRZ","replyto":"Byht0GbRZ","signatures":["ICLR.cc/2018/Conference/Paper1053/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An intriguing idea, a few weaknesses however.","rating":"5: Marginally below acceptance threshold","review":"This paper describes the use of latent context-free derivations, using\na CRF-style neural model, as a latent level of representation in neural\nattention models that consider pairs of sentences. The model implicitly\nlearns a distribution over derivations, and uses marginals under this\ndistribution to bias attention distributions over spans in one sentence\ngiven a span in another sentence.\n\nThis is an intriguing idea. I had a couple of reservations however:\n\n* The empirical improvements from the method seem pretty marginal, to the\npoint that it's difficult to know what is really helping the model. I would\nliked to have seen more explanation of what the model has learned, and\nmore comparisons to other baselines that make use of attention over spans.\nFor example, what happens if every span is considered as an independent random\nvariable, with no use of a tree structure or the CKY chart?\n\n* The use of the \\alpha^0 vs. \\alpha^1 variables is not entirely clear. Once they\nhave been calculated in Algorithm 1, how are they used? Do the \\rho values\nsomewhere treat these two quantities differently?\n\n* I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately.\nI think something much more extensive would be interesting here. As one\nexample, the PP attachment example with \"at a large venue\" is highly suspect;\nthere's a 50/50 chance that any attachment like this will be correct, there's\nabsolutely no way of knowing if the model is doing something interesting/correct\nor performing at a chance level, given a single example. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"STRUCTURED ALIGNMENT NETWORKS","abstract":" Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches possible spans in the first sentence to possible spans in the second sentence, simultaneously discovering the tree structure of each sentence and performing a comparison, in a model that is fully differentiable and is trained only on the comparison objective. We evaluate this model on two sentence comparison tasks: the Stanford natural language inference dataset and the TREC-QA dataset. We find that comparing spans results in superior performance to comparing words individually, and that the learned trees are consistent with actual linguistic structures.","pdf":"/pdf/7cc678302888436c96fed050dfb89b10e20932d2.pdf","TL;DR":"Matching sentences by learning the latent constituency tree structures with a variant of the inside-outside algorithm embedded as a neural network layer.","paperhash":"anonymous|structured_alignment_networks","_bibtex":"@article{\n  anonymous2018structured,\n  title={STRUCTURED ALIGNMENT NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byht0GbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1053/Authors"],"keywords":["structured attention","sentence matching"]}},{"tddate":null,"ddate":null,"tmdate":1512222545108,"tcdate":1511827433013,"number":2,"cdate":1511827433013,"id":"Sybe_7qlG","invitation":"ICLR.cc/2018/Conference/-/Paper1053/Official_Review","forum":"Byht0GbRZ","replyto":"Byht0GbRZ","signatures":["ICLR.cc/2018/Conference/Paper1053/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Structured alignment networks","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a model of \"structured alignments\" between sentences as a means of comparing two sentences by matching their latent structures. Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention.\n\nIn section 3.1, the formula for p(c|x) looks wrong: c_{ijk} are indicator variables. but where are the scores for each span? I think it should be c_{ijk} * \\delta_{ijk} under the summations instead.\n\nIn the same section, the expression for \\alpha_{ij} seems to assume that \\delta_{ijk} = \\dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions). This seems rather limiting, can you comment on that?\n\nIn the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing.\n\nThe plots in Fig 2 with the marginals on CKY charts are not very enlightening. How do this marginals help solving the NLI task?\n\nMinor comments:\n- Sec. 3: \"Language is inherently tree structured\" -- this is debatable...\n- page 8: (laf, 2008): bad formatted reference","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"STRUCTURED ALIGNMENT NETWORKS","abstract":" Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches possible spans in the first sentence to possible spans in the second sentence, simultaneously discovering the tree structure of each sentence and performing a comparison, in a model that is fully differentiable and is trained only on the comparison objective. We evaluate this model on two sentence comparison tasks: the Stanford natural language inference dataset and the TREC-QA dataset. We find that comparing spans results in superior performance to comparing words individually, and that the learned trees are consistent with actual linguistic structures.","pdf":"/pdf/7cc678302888436c96fed050dfb89b10e20932d2.pdf","TL;DR":"Matching sentences by learning the latent constituency tree structures with a variant of the inside-outside algorithm embedded as a neural network layer.","paperhash":"anonymous|structured_alignment_networks","_bibtex":"@article{\n  anonymous2018structured,\n  title={STRUCTURED ALIGNMENT NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byht0GbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1053/Authors"],"keywords":["structured attention","sentence matching"]}},{"tddate":null,"ddate":null,"tmdate":1512222545150,"tcdate":1511770647993,"number":1,"cdate":1511770647993,"id":"HJem9rYlf","invitation":"ICLR.cc/2018/Conference/-/Paper1053/Official_Review","forum":"Byht0GbRZ","replyto":"Byht0GbRZ","signatures":["ICLR.cc/2018/Conference/Paper1053/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences. The span representations are weighted by the spans marginal scores given by the inside-outside algorithm. Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016).\n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting. \n\nWeaknesses:\nThe paper is 8.5 pages long.\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017)\n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive. Am I missing something about the algorithm?\n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax. The paper claimed “the model is able to recover tree structures that very closely mimic syntax”, but it’s hard to draw this conclusion from the two examples in Figure 2.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"STRUCTURED ALIGNMENT NETWORKS","abstract":" Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches possible spans in the first sentence to possible spans in the second sentence, simultaneously discovering the tree structure of each sentence and performing a comparison, in a model that is fully differentiable and is trained only on the comparison objective. We evaluate this model on two sentence comparison tasks: the Stanford natural language inference dataset and the TREC-QA dataset. We find that comparing spans results in superior performance to comparing words individually, and that the learned trees are consistent with actual linguistic structures.","pdf":"/pdf/7cc678302888436c96fed050dfb89b10e20932d2.pdf","TL;DR":"Matching sentences by learning the latent constituency tree structures with a variant of the inside-outside algorithm embedded as a neural network layer.","paperhash":"anonymous|structured_alignment_networks","_bibtex":"@article{\n  anonymous2018structured,\n  title={STRUCTURED ALIGNMENT NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byht0GbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1053/Authors"],"keywords":["structured attention","sentence matching"]}},{"tddate":null,"ddate":null,"tmdate":1510092381428,"tcdate":1509138072434,"number":1053,"cdate":1510092360253,"id":"Byht0GbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Byht0GbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"STRUCTURED ALIGNMENT NETWORKS","abstract":" Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches possible spans in the first sentence to possible spans in the second sentence, simultaneously discovering the tree structure of each sentence and performing a comparison, in a model that is fully differentiable and is trained only on the comparison objective. We evaluate this model on two sentence comparison tasks: the Stanford natural language inference dataset and the TREC-QA dataset. We find that comparing spans results in superior performance to comparing words individually, and that the learned trees are consistent with actual linguistic structures.","pdf":"/pdf/7cc678302888436c96fed050dfb89b10e20932d2.pdf","TL;DR":"Matching sentences by learning the latent constituency tree structures with a variant of the inside-outside algorithm embedded as a neural network layer.","paperhash":"anonymous|structured_alignment_networks","_bibtex":"@article{\n  anonymous2018structured,\n  title={STRUCTURED ALIGNMENT NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byht0GbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1053/Authors"],"keywords":["structured attention","sentence matching"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}