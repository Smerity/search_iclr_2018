{"notes":[{"tddate":null,"ddate":null,"tmdate":1512339089871,"tcdate":1512339089871,"number":3,"cdate":1512339089871,"id":"HyI9Lxf-z","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Review","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference/Paper1003/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper proposes ShakeDrop regularization, which is essentially a combination of the PyramidDrop and Shake-Shake regularization. The procedure consists of essentially weighting the residual branch with a random weight, in the style of Shake-Shake, where the weight is sampled from a mixture of uniform distribution in [-1, 1] and delta at 1, such that the mixture of those two distributions varies linearly with layer depth, in the style of PyramidDrop. In the style of Shake-Shake, a different random weight (in [0, 1]) is used for the backward pass. The most surprising part is that that the forward weight can be negative thus inverting the output of a convolution. Apparently the goal is to \"disturb\" the training, and the procedure yields state-of-the-art results on CIFAR-10/100.\n\nPositives:\n\n- Results: state-of-the-art on CIFAR-10/100\n\nNegatives:\n\n1. No real motivation on why should this work. I guess the motivation is the mixture of PyramidDrop and Shake-Shake motivations, but the main surprising part (forward weight can be negative) is not motivated at all. There is a tiny bit of discussion at the very end, section 4.4, where authors examine the training loss (showing it's non-zero so less overfitting) and mean/variance of gradients (increased). However, this doesn't really satisfy me - it is clear that more disturbance will cause this behaviour, but that doesn't mean any disturbance is good, e.g. if I always apply the negative weight and make my model weights go in the wrong direction, I'm pretty sure training loss and gradients will be even larger, but it's a bad idea to do.\n\n2. I'm concerned with the \"weird trick that happens to work on CIFAR\" line of work (not saying that this paper is the only offender) - are these methods actually useful and generalizable to other problems, or are we overfitting on CIFAR and creating MNIST v2.0 ? It would be nice to demonstrate that this regularization works in at least one more problem, maybe ImageNet, though maybe regularization is not needed there but just find one more dataset that needs regularization and test this on that.\n\n3. The paper doesn't explain well what is the problem with Shake-Shake and memory. I see that the author of Shake-Shake has made a comment on this and that makes a lot of sense, i.e. there is no memory issue, just because there are 2x branches doesn't mean shake-shake needs 2x memory as it can use less capacity=memory to achieve the same performance. So it seems the main premise of the paper - \"let's apply Shake-Shake to deeper models but we need to come up with a modified method because Shake-Shake cannot be applied due to memory problems\" - seems wrong.\n\n4. The writing quality is quite bad, it is very hard to understand what authors mean in parts of the text. E.g. at two places \"it has almost the same residual block as Eqn. (1)\" - how is it \"almost\"? Below equation 5, it is never specified that alpha and beta are sampled uniformly(?) from those ranges, one could think that alpha and beta are fixed constants that take a specific value that is in that range. There are also various grammatical errors such as \"is expected to be powerful but slight memory overhead\" or \"which is introduced essence\", etc.\n\nSmaller comments:\n- Isn't it surprising that alpha in [-1, 1] and beta in [0, 1] works well, but alpha in [0, 1] and beta in [-1, 1] works much worse? The two important cases, (alpha negative, beta positive) and (alpha positive, beta negative), seem to me like they are conceptually very similar.\n- End of section 4.1, should it be b_l as p_L is a constant and b_l is what is sampled?\n- I don't like that exactly the same text is repeated 3 times (abstract, end of intro, end of 1.1) and in very short distance from each other - repeating the same words 3 times doesn't make the reader understand it better, slight rephrasing is much more beneficial.\n\nOverall:\nGood to know that this method sets the new state of the art on CIFAR-10/100, so as such it should be of interest to the community to be available online (arXiv). But with fairly little novelty (is a combination of 2 methods), very little insights of why this should work at all (especially the negative scaling coefficient which is the only extra thing that one learns from this paper, since the rest is a combination of PyramidDrop and Shake-Shake), no idea on whether the method would work outside of the CIFAR-world, and bad quality of the text - I don't think the manuscript is sufficiently good for ICLR.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512253635847,"tcdate":1512253635847,"number":3,"cdate":1512253635847,"id":"Sy3adjgWz","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Public_Comment","forum":"S1NHaMW0b","replyto":"r1F6F5Ygf","signatures":["~Xavier_Gastaldi1"],"readers":["everyone"],"writers":["~Xavier_Gastaldi1"],"content":{"title":"There is no memory issue","comment":"If I understand correctly, the authors assume that, for Shake-Shake regularization to bring an improvement, you have to keep the same number of filters, add another branch and then apply Shake-Shake regularization. \n\nWhile I can understand why the authors would assume that, the tests below paint a different story. The models are the same as in the Shake-Shake regularization paper. They were run once and Shake-Shake regularization was not applied  (i.e. Even-Even-Batch in the paper):\n\nA. 26 layers, 1 residual banch, 32 filters, 1.47M params: 4.69% test error\nB. 26 layers, 2 residual banches, 22 filters, 1.37M params: 4.65% test error\nC. 26 layers, 1 residual banch, 22 filters, 0.696M params: 5.35% test error\nD. 26 layers, 2 residual banches, 16 filters, 0.736M params: 5.11% test error\nE. 26 layers, 1 residual banch, 16 filters, 0.369M params: 5.98% test error\nF. 26 layers, 2 residual banches, 12 filters, 0.416M params: 5.59% test error\nG. 26 layers, 1 residual banch, 12 filters, 0.209M params: 7.12% test error\nH. 26 layers, 2 residual banches, 8 filters, 0.186M params: 7.24% test error\n\n[B,C],[D,E],[F,G] have the same number of filters per residual branch.\n[A,B],[C,D],[E,F],[G,H] have roughly the same capacity.\n\nIf the author's claim was correct then we would observe the same error rates for [B,C],[D,E],[F,G]. What we see in practice is that [A,B],[C,D],[E,F],[G,H] have roughly the same error rates. This means that what is important is the total capacity of the model not the number of filters per residual branch. \n\nTo apply Shake-Shake regularization correctly, you should add a second branch, reduce the number of filters to get back to the capacity of your initial 1 branch model and then apply Shake-Shake regularization. Following this procedure does not lead to a memory issue."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222536630,"tcdate":1512073173625,"number":2,"cdate":1512073173625,"id":"HkAAvk0gf","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Review","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference/Paper1003/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a new form of regularization that is an extension of \"Shake-Shake\" regularization (Gastaldi, 2017). The original \"shake-shake\" proposes using two residual paths adding to the same output (so x + F_1(x) + F_2(x)), and during training, considering different randomly selected convex combinations of the two paths (while using an equally weighted combination at test time). However, this paper contends that this requires additional memory, and attempt to achieve similar regularization with a single path. To do so, they train a network with a single residual path, where the residual is included without attenuation in some cases with some fixed probability, and  attenuated randomly (or even inverted) in others. The paper contends that this achieves superior performance than choosing simply a random attenuation for every sample (although, this can be seen as choosing an attenuation under a distribution with some fixed probability mass at 1). Experiments show improved generalization on CIFAR-10 and CIFAR-100.\n\nI don't think the paper contains sufficiently novel elements to be accepted as a conference track paper at ICLR. While it is interesting that this works well (especially the \"negative\" weight on the residual), the proposed method is fundamentally a combination of prior work: dropout and \"shake-shake\" regularization. Moreover, the evaluation is somewhat limited---essentially, I feel there isn't conclusive proof that \"shake-drop\" is a generically useful regularization technique. For one, the method is evaluated only on small toy-datasets: CIFAR-10 and CIFAR-100. I think at the very least, evaluation on Imagenet is necessary. The proposed regularization is applied only to the \"PyramidNet\" architecture---which begs the question of whether the proposed regularization is useful only for this specific network architecture. It would have been more useful to see results with and without \"shake-drop\" on different architectures (the point being to show a consistent improvement with this regularization, rather than achieving 'state of the art' on CIFAR-10). Moreover, it would be interesting to see if the hyperparameter comparison shown in Tables 1 and 2 remained consistent across architectures.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511791041135,"tcdate":1511791041135,"number":2,"cdate":1511791041135,"id":"r1F6F5Ygf","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Comment","forum":"S1NHaMW0b","replyto":"S1rHOM-gz","signatures":["ICLR.cc/2018/Conference/Paper1003/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1003/Authors"],"content":{"title":"Reply to \"Memory issue clarification\"","comment":"Roughly speaking, Shake-Shake requires as twice the amount of memory as ResNet on a residual block due to twice the number of residual branches. ShakeDrop can solve the issue by using a single residual branch (this corresponds to PyramidShake). Since PyramidShake is unstable in learning, we combined it with ResDrop to stabilize it."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222536678,"tcdate":1511499645061,"number":1,"cdate":1511499645061,"id":"r1HFPmSgG","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Review","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference/Paper1003/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Compelling experimental results, analysis not totally clear","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a regularization technique for deep residual networks.  It is inspired by regularization techniques which disturb the training by applying multiplicative factors to the convolutional layer outputs e.g  Shake-Shake (Gastaldi '17) and PyramidDrop (Yamada '16).  The proposed approach samples a Bernoulli variable randomly to either follow the standard variant of Pyramid net, or applies a variant of shake-shake to pyramid net.\n\n+ Experimental results on CIFAR-10 and CIFAR-100 well-exceed exceed the existing \"vanilla\" techniques + regularizers.  \n- Clarity: some statements are not clear / not substantiated e.g. how does the proposed method overcome the memory problem that shake-shake has?  There are some minor issues wrt presentation, e.g. grammatical correctness of sentences, consistent usage of references, which can be fixed with more careful proofreading.\n- Quality: even though the experimental results are compelling, the paper lacks thorough analysis in understanding the effects of the regularizer.  The two experiments looks at (1) the training error, which the paper openly states does not explain why the proposed regularization works and (2) variance of the gradients throughout learning; the larger variance of gradients is speculated to be the cause, but this is almost expected, given that the method is designed to allow larger fluctuations and perturbations during training.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511233864300,"tcdate":1511233597375,"number":2,"cdate":1511233597375,"id":"S1rHOM-gz","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Public_Comment","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["~Yauheni_Selivonchyk1"],"readers":["everyone"],"writers":["~Yauheni_Selivonchyk1"],"content":{"title":"Memory issue clarification","comment":"Can you please explain in a few more words what is the memory issue with Shake-Shake networks? Namely, in which way Shake-Shake network is different in memory consumption from, say, a ResNet. And how does Shake-Drop addresses/solves this problem. Thank you."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510244341508,"tcdate":1510244341508,"number":1,"cdate":1510244341508,"id":"B16xxZzJf","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Comment","forum":"S1NHaMW0b","replyto":"SyrG-ql1f","signatures":["ICLR.cc/2018/Conference/Paper1003/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1003/Authors"],"content":{"title":"Reply to \"Implementation Clarification\"","comment":"Thank you very much for your inquiry. \nThe answer of your question is the former.\nThat is, we sample b_l on the forward pass and reuse it on the backward pass.\n\nRegarding Fig. 2(d), yes, what you have pointed out is correct.\nWe will revise it on a later version.\nThank you very much for pointing out the mistake. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510150630958,"tcdate":1510150412611,"number":1,"cdate":1510150412611,"id":"SyrG-ql1f","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Public_Comment","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Implementation Clarification","comment":"Quick question, do you sample one bernoulli variable b_l during the forward pass and then save it and use it again on the backward pass, or are the bernoulli variables independently sampled on both the forward and backward passes? Thanks.\n\nAdditionally, there would appear to be an error in Figure 2(d) for the backward pass, where it has the equation as (b_l + \\beta - b_l), instead of (b_l + \\beta - (b_l * \\beta))."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092382568,"tcdate":1509137735831,"number":1003,"cdate":1510092360728,"id":"S1NHaMW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}