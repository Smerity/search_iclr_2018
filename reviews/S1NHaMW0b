{"notes":[{"tddate":null,"ddate":null,"tmdate":1512253635847,"tcdate":1512253635847,"number":3,"cdate":1512253635847,"id":"Sy3adjgWz","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Public_Comment","forum":"S1NHaMW0b","replyto":"r1F6F5Ygf","signatures":["~Xavier_Gastaldi1"],"readers":["everyone"],"writers":["~Xavier_Gastaldi1"],"content":{"title":"There is no memory issue","comment":"If I understand correctly, the authors assume that, for Shake-Shake regularization to bring an improvement, you have to keep the same number of filters, add another branch and then apply Shake-Shake regularization. \n\nWhile I can understand why the authors would assume that, the tests below paint a different story. The models are the same as in the Shake-Shake regularization paper. They were run once and Shake-Shake regularization was not applied  (i.e. Even-Even-Batch in the paper):\n\nA. 26 layers, 1 residual banch, 32 filters, 1.47M params: 4.69% test error\nB. 26 layers, 2 residual banches, 22 filters, 1.37M params: 4.65% test error\nC. 26 layers, 1 residual banch, 22 filters, 0.696M params: 5.35% test error\nD. 26 layers, 2 residual banches, 16 filters, 0.736M params: 5.11% test error\nE. 26 layers, 1 residual banch, 16 filters, 0.369M params: 5.98% test error\nF. 26 layers, 2 residual banches, 12 filters, 0.416M params: 5.59% test error\nG. 26 layers, 1 residual banch, 12 filters, 0.209M params: 7.12% test error\nH. 26 layers, 2 residual banches, 8 filters, 0.186M params: 7.24% test error\n\n[B,C],[D,E],[F,G] have the same number of filters per residual branch.\n[A,B],[C,D],[E,F],[G,H] have roughly the same capacity.\n\nIf the author's claim was correct then we would observe the same error rates for [B,C],[D,E],[F,G]. What we see in practice is that [A,B],[C,D],[E,F],[G,H] have roughly the same error rates. This means that what is important is the total capacity of the model not the number of filters per residual branch. \n\nTo apply Shake-Shake regularization correctly, you should add a second branch, reduce the number of filters to get back to the capacity of your initial 1 branch model and then apply Shake-Shake regularization. Following this procedure does not lead to a memory issue."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222536630,"tcdate":1512073173625,"number":2,"cdate":1512073173625,"id":"HkAAvk0gf","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Review","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference/Paper1003/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a new form of regularization that is an extension of \"Shake-Shake\" regularization (Gastaldi, 2017). The original \"shake-shake\" proposes using two residual paths adding to the same output (so x + F_1(x) + F_2(x)), and during training, considering different randomly selected convex combinations of the two paths (while using an equally weighted combination at test time). However, this paper contends that this requires additional memory, and attempt to achieve similar regularization with a single path. To do so, they train a network with a single residual path, where the residual is included without attenuation in some cases with some fixed probability, and  attenuated randomly (or even inverted) in others. The paper contends that this achieves superior performance than choosing simply a random attenuation for every sample (although, this can be seen as choosing an attenuation under a distribution with some fixed probability mass at 1). Experiments show improved generalization on CIFAR-10 and CIFAR-100.\n\nI don't think the paper contains sufficiently novel elements to be accepted as a conference track paper at ICLR. While it is interesting that this works well (especially the \"negative\" weight on the residual), the proposed method is fundamentally a combination of prior work: dropout and \"shake-shake\" regularization. Moreover, the evaluation is somewhat limited---essentially, I feel there isn't conclusive proof that \"shake-drop\" is a generically useful regularization technique. For one, the method is evaluated only on small toy-datasets: CIFAR-10 and CIFAR-100. I think at the very least, evaluation on Imagenet is necessary. The proposed regularization is applied only to the \"PyramidNet\" architecture---which begs the question of whether the proposed regularization is useful only for this specific network architecture. It would have been more useful to see results with and without \"shake-drop\" on different architectures (the point being to show a consistent improvement with this regularization, rather than achieving 'state of the art' on CIFAR-10). Moreover, it would be interesting to see if the hyperparameter comparison shown in Tables 1 and 2 remained consistent across architectures.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511791041135,"tcdate":1511791041135,"number":2,"cdate":1511791041135,"id":"r1F6F5Ygf","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Comment","forum":"S1NHaMW0b","replyto":"S1rHOM-gz","signatures":["ICLR.cc/2018/Conference/Paper1003/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1003/Authors"],"content":{"title":"Reply to \"Memory issue clarification\"","comment":"Roughly speaking, Shake-Shake requires as twice the amount of memory as ResNet on a residual block due to twice the number of residual branches. ShakeDrop can solve the issue by using a single residual branch (this corresponds to PyramidShake). Since PyramidShake is unstable in learning, we combined it with ResDrop to stabilize it."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222536678,"tcdate":1511499645061,"number":1,"cdate":1511499645061,"id":"r1HFPmSgG","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Review","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference/Paper1003/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Compelling experimental results, analysis not totally clear","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a regularization technique for deep residual networks.  It is inspired by regularization techniques which disturb the training by applying multiplicative factors to the convolutional layer outputs e.g  Shake-Shake (Gastaldi '17) and PyramidDrop (Yamada '16).  The proposed approach samples a Bernoulli variable randomly to either follow the standard variant of Pyramid net, or applies a variant of shake-shake to pyramid net.\n\n+ Experimental results on CIFAR-10 and CIFAR-100 well-exceed exceed the existing \"vanilla\" techniques + regularizers.  \n- Clarity: some statements are not clear / not substantiated e.g. how does the proposed method overcome the memory problem that shake-shake has?  There are some minor issues wrt presentation, e.g. grammatical correctness of sentences, consistent usage of references, which can be fixed with more careful proofreading.\n- Quality: even though the experimental results are compelling, the paper lacks thorough analysis in understanding the effects of the regularizer.  The two experiments looks at (1) the training error, which the paper openly states does not explain why the proposed regularization works and (2) variance of the gradients throughout learning; the larger variance of gradients is speculated to be the cause, but this is almost expected, given that the method is designed to allow larger fluctuations and perturbations during training.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511233864300,"tcdate":1511233597375,"number":2,"cdate":1511233597375,"id":"S1rHOM-gz","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Public_Comment","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["~Yauheni_Selivonchyk1"],"readers":["everyone"],"writers":["~Yauheni_Selivonchyk1"],"content":{"title":"Memory issue clarification","comment":"Can you please explain in a few more words what is the memory issue with Shake-Shake networks? Namely, in which way Shake-Shake network is different in memory consumption from, say, a ResNet. And how does Shake-Drop addresses/solves this problem. Thank you."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510244341508,"tcdate":1510244341508,"number":1,"cdate":1510244341508,"id":"B16xxZzJf","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Official_Comment","forum":"S1NHaMW0b","replyto":"SyrG-ql1f","signatures":["ICLR.cc/2018/Conference/Paper1003/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1003/Authors"],"content":{"title":"Reply to \"Implementation Clarification\"","comment":"Thank you very much for your inquiry. \nThe answer of your question is the former.\nThat is, we sample b_l on the forward pass and reuse it on the backward pass.\n\nRegarding Fig. 2(d), yes, what you have pointed out is correct.\nWe will revise it on a later version.\nThank you very much for pointing out the mistake. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510150630958,"tcdate":1510150412611,"number":1,"cdate":1510150412611,"id":"SyrG-ql1f","invitation":"ICLR.cc/2018/Conference/-/Paper1003/Public_Comment","forum":"S1NHaMW0b","replyto":"S1NHaMW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Implementation Clarification","comment":"Quick question, do you sample one bernoulli variable b_l during the forward pass and then save it and use it again on the backward pass, or are the bernoulli variables independently sampled on both the forward and backward passes? Thanks.\n\nAdditionally, there would appear to be an error in Figure 2(d) for the backward pass, where it has the equation as (b_l + \\beta - b_l), instead of (b_l + \\beta - (b_l * \\beta))."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092382568,"tcdate":1509137735831,"number":1003,"cdate":1510092360728,"id":"S1NHaMW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1NHaMW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"ShakeDrop regularization","abstract":"This paper proposes a powerful regularization method named ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning. Important and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying\neven a negative factor to the output of a convolutional layer in the forward training pass. In addition, in the backward training pass, a different factor from the forward pass is multiplied. As a byproduct, however, learning process gets unstable. Hence, the learning process is stabilized by employing the essence of the Stochastic Depth (ResDrop). Combined with existing techniques (longer training and image preprocessing), ShakeDrop achieved error rates of 2.31\\% (better by 0.25\\%) on the CIFAR-10 dataset and 12.19\\% (better by 2.85\\%) on the CIFAR-100 dataset.","pdf":"/pdf/04315033857a1806baf59b2871c7375c263aa65d.pdf","paperhash":"anonymous|shakedrop_regularization","_bibtex":"@article{\n  anonymous2018shakedrop,\n  title={ShakeDrop regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1NHaMW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1003/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}