{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222722634,"tcdate":1511899365147,"number":3,"cdate":1511899365147,"id":"HkTkZHjlM","invitation":"ICLR.cc/2018/Conference/-/Paper706/Official_Review","forum":"r1TA9ZbA-","replyto":"r1TA9ZbA-","signatures":["ICLR.cc/2018/Conference/Paper706/AnonReviewer2"],"readers":["everyone"],"content":{"title":"nice paper, flawed experiments","rating":"5: Marginally below acceptance threshold","review":"This paper designs a deep learning architecture that mimics the structure of the well-known MCTS algorithm. From gold standard state-action pairs, it learns each component of this architecture in order to predict similar actions.\n\nI enjoyed reading this paper. The presentation is very clear, the design of the architecture is beautiful, and I was especially impressed with the related work discussion that went back to identify other game search and RL work that attempts to learn parts of the search algorithm. Nice job overall.\n\nThe main flaw of the paper is in its experiments. If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close-to-optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). That makes for a staggering 15 billion rollouts of prior data that goes into the MCTSNet model. This is compared to 25 rollouts of MCTS that make the decision for the baseline. I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. I'm sure I'm misinterpreting some detail here, but how is this a fair comparison?\n\nWould it be fair to have a baseline that learns the MCTS coefficient on the training data? Or one that uses the value function that was learned with classic search? I find it difficult to understand the details of the experimental setup, and maybe some of these experiments are reported. Please clarify. Also: colors are not distinguishable in grey print.\n\nHow would the technique scale with more MCTS iterations? I suspect that the O(N^2) complexity is very prohibitive and will not allow this to scale up?\n\nI'm a bit worried about the idea of learning to trade off exploration and exploitation. In the end you'll just allow for the minimal amount of exploration to solve the games you've already seen. This seems risky, and I suspect that UCB and more statistically principled approaches would be more robust in this regard?\n\nAre these Sokoban puzzles easy for classical AI techniques? I know that many of them can be solved by A* search with a decent heuristic. It would be fair to discuss this.\n\nThe last sentence of conclusions is too far reaching; there is really no evidence for that claim.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to search with MCTSnets","abstract":"Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines. ","pdf":"/pdf/d2282eb6f31aa78ed3cd5fdc4362d1d125cdb62a.pdf","paperhash":"anonymous|learning_to_search_with_mctsnets","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to search with MCTSnets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1TA9ZbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper706/Authors"],"keywords":["Monte-Carlo Tree Search","search","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222722673,"tcdate":1511886485560,"number":2,"cdate":1511886485560,"id":"HyC90Zoez","invitation":"ICLR.cc/2018/Conference/-/Paper706/Official_Review","forum":"r1TA9ZbA-","replyto":"r1TA9ZbA-","signatures":["ICLR.cc/2018/Conference/Paper706/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Some major concerns","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a framework for learning to search, MCTSNet. The paper proposes an idea to integrate simulation-based planning into a neural network. By this integration, solving planning problems can be end-to-end training. The idea is to represent all operators such as backup, action selection and node initialisation by neural networks. The authors propose to train this using policy gradient in which data of optimal state-action pairs is generated by a standard MCTS with a large number of simulations.\n\nIn overall, it is a nice idea to use DNN to represent all update operators in MCTS. However the paper writing has many unclear points. In my point of view, the efficiency of the proposed framework is also questionable.\n\n\nHere, I have many major concerns about the proposed idea.\n\n- It looks like after training MCTSnet with a massive amount of data from another MCTS, MTCSnet algorithm as in Algorithm 2 will not do very much more planning yet. More simulations (M increases) will not make the statistics in 4(a) improved. Is it the reason why in experiments M is always small and increasing it does not make a huge improvement?. This is different from standard planning when a backup is handled with more simulations, the Q value function will have better statistics, and then get smaller regrets (see 4(b) in Algorithm 1). This supervising step is similar to one previous work [1] and not mentioned in the paper.\n\n\n- MCTSnet has not dealt well with large/continuous state spaces. Each generated $s$ will amount to one tree node, with its own statistics. If M is large, the tree maintained is huge too. It might be not correct, then I am curious how this technical aspect is handled by MCTSnet.\n\n- Other questions:\n\n + how the value network used in the MCTS in section 3.5 is constructed?\n\n + what does p(a|s,{\\mathbf z}), p({\\mathbf s}|{\\mathbf z}) mean?\n\n + is R_1 similar to R^1\n\n + is z_m in section 3.5 and z^m in section 3.6 different?\n\n + is the action distribution from the root memory p_{\\theta}(a|s)? \n\n- Other related work: \n\n\n[1] Xiaoxiao Guo et. al. Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning, NIPS 2014\n\n[2] Guez et. al. Bayes-Adaptive Simulation-based Search with Value Function Approximation, NIPS 2014\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to search with MCTSnets","abstract":"Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines. ","pdf":"/pdf/d2282eb6f31aa78ed3cd5fdc4362d1d125cdb62a.pdf","paperhash":"anonymous|learning_to_search_with_mctsnets","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to search with MCTSnets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1TA9ZbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper706/Authors"],"keywords":["Monte-Carlo Tree Search","search","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222722715,"tcdate":1511843757850,"number":1,"cdate":1511843757850,"id":"ByL3DP9gf","invitation":"ICLR.cc/2018/Conference/-/Paper706/Official_Review","forum":"r1TA9ZbA-","replyto":"r1TA9ZbA-","signatures":["ICLR.cc/2018/Conference/Paper706/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A solid, well-explained paper","rating":"7: Good paper, accept","review":"The authors introduce an approach for adding learning to search capability to Monte Carlo tree search. The proposed method incorporates simulation-based search inside a neural network by expanding, evaluating and backing-up a vector-embedding. The key is to represent the internal state of the search by a memory vector at each node. The computation of the network proceeds just like a simulation of MCTS, but using a simulation policy based on the memory vector to initialize the memory vector at the leaf. The proposed method allows each component of MCTS to be rich and learnable, and allows the joint training of the evaluation network, backup network, and simulation policy in optimizing the MCTS network. The paper is thorough and well-explained. My only complaint is the evaluation is only done on one domain, Sokoban. More evaluations on diverse domains are called for.  ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to search with MCTSnets","abstract":"Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines. ","pdf":"/pdf/d2282eb6f31aa78ed3cd5fdc4362d1d125cdb62a.pdf","paperhash":"anonymous|learning_to_search_with_mctsnets","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to search with MCTSnets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1TA9ZbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper706/Authors"],"keywords":["Monte-Carlo Tree Search","search","planning"]}},{"tddate":null,"ddate":null,"tmdate":1509739149986,"tcdate":1509133012721,"number":706,"cdate":1509739147328,"id":"r1TA9ZbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1TA9ZbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to search with MCTSnets","abstract":"Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines. ","pdf":"/pdf/d2282eb6f31aa78ed3cd5fdc4362d1d125cdb62a.pdf","paperhash":"anonymous|learning_to_search_with_mctsnets","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to search with MCTSnets},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1TA9ZbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper706/Authors"],"keywords":["Monte-Carlo Tree Search","search","planning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}