{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222813332,"tcdate":1511821235728,"number":3,"cdate":1511821235728,"id":"HJh2yfcgz","invitation":"ICLR.cc/2018/Conference/-/Paper912/Official_Review","forum":"H1LAqMbRW","replyto":"H1LAqMbRW","signatures":["ICLR.cc/2018/Conference/Paper912/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach to learning a model, but underperforms model-free methods","rating":"4: Ok but not good enough - rejection","review":"Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS). The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information. They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%).\n\nPros:\n- Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory.\n- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available.\n\nCons:\n- The model-based approach is disappointing compared to the model-free approach.\n\nQuality and Clarity:\n\nThe paper in general is well-written and easy to follow and seems technically correct, though I found some of the figures and definitions confusing, specifically:\n\n- The terms for different forward models are not defined (e.g. MatchPi, MatchA, etc.). I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly.\n- In Figure 3b, it is not clear to me what the difference between the red and blue curves is.\n- In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI.\n- The caption in Figure 8 is malformatted.\n- In Figure 7, the baseline of \\hat{h_t}=h_{t-2} seems strange---I would find it more useful for Figure 7 to compare to the performance if the model were not used (i.e. if \\hat{h_t}=h_t) to see how much performance suffers as a result of model error.\n\nOriginality:\n\nI am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain. However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features. The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below).\n\nSignificance:\n\nI feel the paper overstates the results in saying that the learned forward model is usable in MCTS. The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms. The baseline used in the paper is a random baseline, which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice, such as the model-free approach.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent forward model for Real-time Strategy game planning with incomplete information","abstract":"Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.","pdf":"/pdf/1ab4ab0a8c425e3c3dfbab7708bec527a4bc0029.pdf","TL;DR":"The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.","paperhash":"anonymous|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent forward model for Real-time Strategy game planning with incomplete information},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1LAqMbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper912/Authors"],"keywords":["Real time strategy","latent space","forward model","monte carlo tree search","reinforcement learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222813372,"tcdate":1511754737742,"number":2,"cdate":1511754737742,"id":"B1qenWKxM","invitation":"ICLR.cc/2018/Conference/-/Paper912/Official_Review","forum":"H1LAqMbRW","replyto":"H1LAqMbRW","signatures":["ICLR.cc/2018/Conference/Paper912/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting direction of research, but analysis is not complete and exposition is unclear.","rating":"4: Ok but not good enough - rejection","review":"Summary:\n\nThis paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games. The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS.\n\nForecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning.\n\nThe paper:\n1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.\n2. Evaluates how informative the latent states are via state reconstruction.\n3. trains variatns of a forward model f on the hidden states of the various learned agents.\n4. evaluates different f within MCTS for MiniRTS.\n\nPro:\n- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods.\n- The experimental setting is very non-trivial and novel.\n\nCon:\n- The manuscript is unclear in many parts -- this should be greatly improved.\n1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?). Which forward model is trained from which model-free agent?\n2. How is the forward model / value function used in MCTS? I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together.\n\n- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X. Yet it is unclear how this informs us about the quality of the learned forward models f. It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix.\n\n- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models. Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8.\n\n- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent. Is this because of the MCTS approach? Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. \n\nOverall:\nI think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis.\n\nDetailed:\n- What are the right prediction tasks that ensure the latent space captures enough of the forward model?\n- What is the error of the raw h-predictions? Only the state-reconstruction error is shown now.\n- Figure 6 / sect 4.2: which model-free agent is used? Also fig 6 misses captions.\n- Figure 8: scrambled caption.\n- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent forward model for Real-time Strategy game planning with incomplete information","abstract":"Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.","pdf":"/pdf/1ab4ab0a8c425e3c3dfbab7708bec527a4bc0029.pdf","TL;DR":"The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.","paperhash":"anonymous|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent forward model for Real-time Strategy game planning with incomplete information},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1LAqMbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper912/Authors"],"keywords":["Real time strategy","latent space","forward model","monte carlo tree search","reinforcement learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1512222813416,"tcdate":1511701672798,"number":1,"cdate":1511701672798,"id":"BJ-32VOxf","invitation":"ICLR.cc/2018/Conference/-/Paper912/Official_Review","forum":"H1LAqMbRW","replyto":"H1LAqMbRW","signatures":["ICLR.cc/2018/Conference/Paper912/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting way of re-using pre-trained agents with a lot of room for improvement","rating":"5: Marginally below acceptance threshold","review":"The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning.\nThe idea of re-using a pretrained agent has both pros and cons. On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment. On the other hand, the usefulness of the learned representation for planning is unclear. A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions.\nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation. In addition to that, one could fine-tune the representation during forward model training. \nIt would be interesting to see if this can improve the results.\n\nI personally miss a more technical and detailed exposition of the ideas. For example, it is not described anywhere what loss is used for learning the model. MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained. \nAgain, due to lack of equations, I don’t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way.\nI also could find the details on how figure 1 was produced. As I understand, MCTS was not used in this experiment. If so, how would one play with just a forward model?\n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability. Is mini-RTS a deterministic environment? \nWould it be possible to include a non-deterministic baseline in the experimental comparison?\n\nExperimentally, the results are rather weak compared to pure model-free agents. Somewhat unsatisfying, longer-term prediction results into weaker game play. Doesn’t this support the argument about need in stochastic prediction? \n\nTo me, the paper in it’s current form is not written well and does not contain strong enough empirical results, so that I can’t recommend acceptance. \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names\n* Figure 1 that introduces them contains typos. \n* Formatting of figure 8 needs to be fixed. This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent forward model for Real-time Strategy game planning with incomplete information","abstract":"Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.","pdf":"/pdf/1ab4ab0a8c425e3c3dfbab7708bec527a4bc0029.pdf","TL;DR":"The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.","paperhash":"anonymous|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent forward model for Real-time Strategy game planning with incomplete information},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1LAqMbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper912/Authors"],"keywords":["Real time strategy","latent space","forward model","monte carlo tree search","reinforcement learning","planning"]}},{"tddate":null,"ddate":null,"tmdate":1509739033804,"tcdate":1509137101842,"number":912,"cdate":1509739031145,"id":"H1LAqMbRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1LAqMbRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Latent forward model for Real-time Strategy game planning with incomplete information","abstract":"Model-free deep reinforcement learning approaches have shown superhuman performance in simulated environments (e.g., Atari games, Go, etc). During training, these approaches often implicitly construct a latent space that contains key information for decision making. In this paper, we learn a forward model on this latent space and apply it to model-based planning in miniature Real-time Strategy game with incomplete information (MiniRTS). We first show that the latent space constructed from existing actor-critic models contains relevant information of the game, and design training procedure to learn forward models. We also show that our learned forward model can predict meaningful future state and is usable for latent space Monte-Carlo Tree Search (MCTS), in terms of win rates against rule-based agents.","pdf":"/pdf/1ab4ab0a8c425e3c3dfbab7708bec527a4bc0029.pdf","TL;DR":"The paper analyzes the latent space learned by model-free approaches in a miniature incomplete information game, trains a forward model in the latent space and apply it to Monte-Carlo Tree Search, yielding positive performance.","paperhash":"anonymous|latent_forward_model_for_realtime_strategy_game_planning_with_incomplete_information","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent forward model for Real-time Strategy game planning with incomplete information},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1LAqMbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper912/Authors"],"keywords":["Real time strategy","latent space","forward model","monte carlo tree search","reinforcement learning","planning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}