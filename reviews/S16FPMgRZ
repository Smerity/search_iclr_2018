{"notes":[{"tddate":null,"ddate":null,"tmdate":1511836856551,"tcdate":1511836856551,"number":1,"cdate":1511836856551,"id":"SyZphSqeM","invitation":"ICLR.cc/2018/Conference/-/Paper225/Public_Comment","forum":"S16FPMgRZ","replyto":"S16FPMgRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Related Work","comment":"In what way is the proposed tensor contraction layer different from Tucker Decomposition for Feature Fusion in 'Attribute-Enhanced Face Recognition with Neural Tensor Fusion Networks'[1]? How does the tensor regression layer compare with the nonlinear activations in 'Factorized Bilinear Models for Image Recognition'[2] and 'Non-linear Convolution Filters for CNN-based Learning'[3]?\n\n[1] http://www.research.ed.ac.uk/portal/en/publications/attributeenhanced-face-recognition-with-neural-tensor-fusion-networks(b5f001b1-21c5-44e0-ad6f-21481e83590e).html\n\n[2] https://arxiv.org/abs/1611.05709\n\n[3] https://arxiv.org/abs/1708.07038"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor Contraction & Regression Networks","abstract":"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks. Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \nWe present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation. By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","pdf":"/pdf/59a3afb908781e389556b3a560280695c27bcf1a.pdf","TL;DR":"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance.","paperhash":"anonymous|tensor_contraction_regression_networks","_bibtex":"@article{\n  anonymous2018tensor,\n  title={Tensor Contraction & Regression Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S16FPMgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper225/Authors"],"keywords":["tensor contraction","tensor regression","network compression","deep neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222591507,"tcdate":1511626442392,"number":3,"cdate":1511626442392,"id":"Byz0IGvgz","invitation":"ICLR.cc/2018/Conference/-/Paper225/Official_Review","forum":"S16FPMgRZ","replyto":"S16FPMgRZ","signatures":["ICLR.cc/2018/Conference/Paper225/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Contribution seems less.","rating":"4: Ok but not good enough - rejection","review":"This paper combines the tensor contraction method and the tensor regression method and applies them to CNN. This paper is well written and easy to read. \n\nHowever, I cannot find a strong or unique contribution from this paper. Both of the methods (tensor contraction and tensor decomposition) are well developed in the existing studies, and combining these ideas does not seem non-trivial.\n\n--Main question\n\nWhy authors focus on the combination of the methods? Both of the two methods can perform independently. Is there a special synergy effect?\n\n--Minor question\n\nThe performance of the tensor contraction method depends on a size of tensors. Is there any effective way to determine the size of tensors?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor Contraction & Regression Networks","abstract":"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks. Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \nWe present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation. By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","pdf":"/pdf/59a3afb908781e389556b3a560280695c27bcf1a.pdf","TL;DR":"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance.","paperhash":"anonymous|tensor_contraction_regression_networks","_bibtex":"@article{\n  anonymous2018tensor,\n  title={Tensor Contraction & Regression Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S16FPMgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper225/Authors"],"keywords":["tensor contraction","tensor regression","network compression","deep neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222591547,"tcdate":1511573842607,"number":2,"cdate":1511573842607,"id":"rysUKSIxM","invitation":"ICLR.cc/2018/Conference/-/Paper225/Official_Review","forum":"S16FPMgRZ","replyto":"S16FPMgRZ","signatures":["ICLR.cc/2018/Conference/Paper225/AnonReviewer3"],"readers":["everyone"],"content":{"title":"see detailed review below","rating":"6: Marginally above acceptance threshold","review":"This paper incorporates tensor decomposition and tensor regression into CNN by replacing its flattening operations and fully-connected layers with a new tensor regression layer. \n\nPros:\n\nThe low-rank representation of tensors is able to reduce the model complexity in the original CNN without sacrificing much prediction accuracy. This is promising as it enables the implementation of complex deep learning algorithms on mobile devices due to its huge space saving performance.  Overall, this paper is easy to follow. \n\nCons: \n\nQ1: Can the authors discuss the computational time of the proposed tensor regression layers and compare it to that of the baseline CNN? The tensor regression layer is computationally more expensive than the flattening operations in original CNN. Usually, it also involves expensive model selection procedure to choose the tuning parameters (N+1 ranks and a L2 norm sparsity parameter). In the experiments, the authors simply tried a few ranks without serious tuning. \n\nQ2: The authors reported the space saving in Table 1 but not in Table 2. Since spacing saving is a major contribution of the proposed method, can authors add the space saving percentage in Table 2?\n\nQ3: There are a few typos in the current paper. I would suggest the authors to take a careful proofreading. For example,\n\n(1) In the “Related work“ paragraph on page 2, “Lebedev et al. (2014) proposes…” should be “Lebedev et al. (2014) propose…”. Many other references have the same issue. \n\n(2) In Figure 1, the letter $X$ should be $\\tilde{\\cal X}$.\n\n(3) In expression (5) on page 3, the core tensor is denoted by $\\tilde{\\cal G}$. Is this the same as $\\tilde{\\cal X}^{‘}$ in Figure 1?\n\n(4) In expression (5) on page 3, the core tensor $\\tilde{\\cal G}$ is of dimension $(D_0, R_1, \\ldots, R_N)$. However, in expression (8) on page 5, $\\tilde{\\cal G}$ is of dimension $(R_0, R_1, \\ldots, R_N, R_{N+1})$.\n\n(5) Use \\cite{} and \\citep{} correctly. For example, in the “Related work“ paragraph on page 2,\n\n“Several prior papers address the power of tensor regression to preserve natural multi-modal structure and learn compact predictive models Guo et al. (2012); Rabusseau & Kadri (2016); Zhou et al. (2013); Yu & Liu (2016).”\n\nshould be\n\n“Several prior papers address the power of tensor regression to preserve natural multi-modal structure and learn compact predictive models (Guo et al., 2012; Rabusseau & Kadri, 2016; Zhou et al., 2013; Yu & Liu, 2016).”\n\n\n\n\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor Contraction & Regression Networks","abstract":"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks. Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \nWe present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation. By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","pdf":"/pdf/59a3afb908781e389556b3a560280695c27bcf1a.pdf","TL;DR":"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance.","paperhash":"anonymous|tensor_contraction_regression_networks","_bibtex":"@article{\n  anonymous2018tensor,\n  title={Tensor Contraction & Regression Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S16FPMgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper225/Authors"],"keywords":["tensor contraction","tensor regression","network compression","deep neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222591584,"tcdate":1511515364385,"number":1,"cdate":1511515364385,"id":"SJ3JSwBlG","invitation":"ICLR.cc/2018/Conference/-/Paper225/Official_Review","forum":"S16FPMgRZ","replyto":"S16FPMgRZ","signatures":["ICLR.cc/2018/Conference/Paper225/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting but contributions are not enough","rating":"4: Ok but not good enough - rejection","review":"In this paper, new layer architectures of neural networks using a low-rank representation of tensors are proposed. The main idea is assuming Tucker-type low-rank assumption for both a weight and an input. The performance is evaluated with toy data and Imagenet.\n\n[Clarity]\nThe paper is well written and easy to follow.\n\n[Originality]\nI mainly concern about the originality. Applying low-rank tensor decomposition in a network architecture has a lot of past studies and I feel this paper fails to clarify what is really distinguished from the other studies. For example, I found at least two papers [1,2] that are relevant. ([2] appears at the reference but it is not referred to.) How is the proposed method different from them?\n\nAlso, the \"end-to-end\" feature is repeatedly emphasized in the paper, but I don't understand its benefit. \n\n[1] Tai, Cheng, et al. \"Convolutional neural networks with low-rank regularization.\" arXiv preprint arXiv:1511.06067 (2015).\n[2] Lebedev, Vadim, et al. \"Speeding-up convolutional neural networks using fine-tuned cp-decomposition.\" arXiv preprint arXiv:1412.6553 (2014).\n\n[Significance]\nIn the experiments, the proposed method is compared with the vanilla model (i.e., the model having no low-rank structure) but with no other baseline using different compression techniques such as Novikov et al., 2015. So I cannot judge whether this method is better in terms of compression-accuracy tradeoff.\n\n\nPros:\n- The proposed model (layer architecture) is simple and easy to implement\n\nCons:\n- The novelty is low\n- No competitive baseline in experiments\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor Contraction & Regression Networks","abstract":"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks. Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \nWe present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation. By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","pdf":"/pdf/59a3afb908781e389556b3a560280695c27bcf1a.pdf","TL;DR":"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance.","paperhash":"anonymous|tensor_contraction_regression_networks","_bibtex":"@article{\n  anonymous2018tensor,\n  title={Tensor Contraction & Regression Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S16FPMgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper225/Authors"],"keywords":["tensor contraction","tensor regression","network compression","deep neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739419335,"tcdate":1509070725449,"number":225,"cdate":1509739416681,"id":"S16FPMgRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S16FPMgRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Tensor Contraction & Regression Networks","abstract":"Convolution neural networks typically consist of many convolutional layers followed by several fully-connected layers.  While convolutional layers map between high-order activation tensors, the fully-connected layers operate on flattened activation vectors.  Despite its success, this approach has notable drawbacks. Flattening discards the multi-dimensional structure of the activations, and the fully-connected layers require a large number of parameters. \nWe present two new techniques to address these problems.  First, we introduce tensor contraction layers which can replace the ordinary fully-connected layers in a neural network. Second, we introduce tensor regression layers, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer.  Both the contraction and regression weights are learned end-to-end by backpropagation. By imposing low rank on both, we use significantly fewer parameters.  Experiments on the ImageNet dataset show that applied to the popular VGG and ResNet architectures, our methods significantly reduce the number of parameters in the fully connected layers (about 65% space savings) while negligibly impacting accuracy.","pdf":"/pdf/59a3afb908781e389556b3a560280695c27bcf1a.pdf","TL;DR":"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance.","paperhash":"anonymous|tensor_contraction_regression_networks","_bibtex":"@article{\n  anonymous2018tensor,\n  title={Tensor Contraction & Regression Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S16FPMgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper225/Authors"],"keywords":["tensor contraction","tensor regression","network compression","deep neural networks"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}