{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222767361,"tcdate":1511826825191,"number":3,"cdate":1511826825191,"id":"SkW9BQ9lG","invitation":"ICLR.cc/2018/Conference/-/Paper788/Official_Review","forum":"HJcSzz-CZ","replyto":"HJcSzz-CZ","signatures":["ICLR.cc/2018/Conference/Paper788/AnonReviewer2"],"readers":["everyone"],"content":{"title":"extension of the Prototypical Network to semi-supervised setting","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible \nstrategies. One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the \nassigned pseudo-labels. Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to\nany of the known categories. In practice this second solution is analogous to the first, but a general 'distractor' class\nis added. Finally the third technique learns to weight the samples according to their distance to the original prototypes.\n\nThese strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained \non some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting\nmultiple times a large dataset), then they are used on a final target task with again few labeled data and large \nunlabeled samples but beloning to a different set of categories.\n\n+ the paper is well written, well organized and overall easy to read\n+/-  this work builds largely on previous work. It introduces only some small technical novelty inspired by soft-k-means\nclustering that anyway seems to be effective.\n+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of\nsemantic relatedness between the source and the target sets\n\nFew notes and questions\n1) why for the omniglot experiment the table reports the error results? It would be better to present accuracy as for the other tables/experiments\n2) I would suggest to use source and target instead of train and test -- these two last terms are confusing because\nactually there is a training phase also at test time.\n3) although the paper indicate that there are different other few-shot methods that could be applicable here, \nno other approach is considered besides the prothotipical network and its variants. An further external reference \ncould be used to give an idea of what would be the experimental result at least in the supervised case.\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning for Semi-Supervised Few-Shot Classification","abstract":"In few-shot classification, we are interested in learning algorithms that train a\nclassifier from only a handful of labeled examples. Recent progress in few-shot\nclassification has featured meta-learning, in which a parameterized model for a\nlearning algorithm is defined and trained on episodes representing different clas-\nsification problems, each with a small labeled training set and its corresponding\ntest set. In this work, we advance this few-shot classification paradigm towards\na scenario where unlabeled examples are also available within each episode. We\nconsider two situations: one where all unlabeled examples are assumed to belong\nto the same set of classes as the labeled examples of the episode, as well as the\nmore challenging situation where examples from other distractor classes are also\nprovided. To address this paradigm, we propose novel extensions of Prototypical\nNetworks (Snell et al., 2017) that are augmented with the ability to use unlabeled\nexamples when producing prototypes. These models are trained in an end-to-end\nway on episodes, to learn to leverage the unlabeled examples successfully. We\nevaluate these methods on versions of the Omniglot and miniImageNet bench-\nmarks, adapted to this new framework augmented with unlabeled examples. We\nalso propose a new split of ImageNet, consisting of a large set of classes, with a\nhierarchical structure. Our experiments confirm that our Prototypical Networks\ncan learn to improve their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would.","pdf":"/pdf/67386ebe7d9f7a702909ecc18545398b9cbe9a2c.pdf","paperhash":"anonymous|metalearning_for_semisupervised_fewshot_classification","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning for Semi-Supervised Few-Shot Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcSzz-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper788/Authors"],"keywords":["Few-shot learning","semi-supervised learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222767401,"tcdate":1511633176069,"number":2,"cdate":1511633176069,"id":"Hyx7bEPez","invitation":"ICLR.cc/2018/Conference/-/Paper788/Official_Review","forum":"HJcSzz-CZ","replyto":"HJcSzz-CZ","signatures":["ICLR.cc/2018/Conference/Paper788/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The studied problem is interesting, and the paper is well-written. While the proposed method is a natural extension of the existing works. ","rating":"6: Marginally above acceptance threshold","review":"In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes.  The studied problem is interesting, and the paper is well-written. Extensive experiments are performed to demonstrate the effectiveness of the proposed methods.  While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning).On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers. Overall, I would like to vote for a weakly acceptance regarding this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning for Semi-Supervised Few-Shot Classification","abstract":"In few-shot classification, we are interested in learning algorithms that train a\nclassifier from only a handful of labeled examples. Recent progress in few-shot\nclassification has featured meta-learning, in which a parameterized model for a\nlearning algorithm is defined and trained on episodes representing different clas-\nsification problems, each with a small labeled training set and its corresponding\ntest set. In this work, we advance this few-shot classification paradigm towards\na scenario where unlabeled examples are also available within each episode. We\nconsider two situations: one where all unlabeled examples are assumed to belong\nto the same set of classes as the labeled examples of the episode, as well as the\nmore challenging situation where examples from other distractor classes are also\nprovided. To address this paradigm, we propose novel extensions of Prototypical\nNetworks (Snell et al., 2017) that are augmented with the ability to use unlabeled\nexamples when producing prototypes. These models are trained in an end-to-end\nway on episodes, to learn to leverage the unlabeled examples successfully. We\nevaluate these methods on versions of the Omniglot and miniImageNet bench-\nmarks, adapted to this new framework augmented with unlabeled examples. We\nalso propose a new split of ImageNet, consisting of a large set of classes, with a\nhierarchical structure. Our experiments confirm that our Prototypical Networks\ncan learn to improve their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would.","pdf":"/pdf/67386ebe7d9f7a702909ecc18545398b9cbe9a2c.pdf","paperhash":"anonymous|metalearning_for_semisupervised_fewshot_classification","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning for Semi-Supervised Few-Shot Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcSzz-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper788/Authors"],"keywords":["Few-shot learning","semi-supervised learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222767444,"tcdate":1511628170266,"number":1,"cdate":1511628170266,"id":"rJzcaGvgf","invitation":"ICLR.cc/2018/Conference/-/Paper788/Official_Review","forum":"HJcSzz-CZ","replyto":"HJcSzz-CZ","signatures":["ICLR.cc/2018/Conference/Paper788/AnonReviewer3"],"readers":["everyone"],"content":{"title":"limited novelty","rating":"5: Marginally below acceptance threshold","review":"This paper is an extension of the “prototypical network” which will be published in NIPS 2017. The classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode. The paper solves a new semi-supervised situation, which is more close to the setting of the real world, with an extension of the prototype network.  Sufficient implementation detail and analysis on results.\n\nHowever, this is definitely not the first work on semi-supervised formed few-shot learning. There are plenty of works on this topic [R1, R2, R3]. The authors are advised to do a thorough survey of the relevant works in Multimedia and computer vision community. \n \nAnother concern is that the novelty. This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data. \n\nThe experiments are also not enough. Not only some other works such as [R1, R2, R3]; but also the other naïve baselines should also be compared, such as directly nearest neighbor classifier, logistic regression, and neural network in traditional supervised learning. Additionally, in the 5-shot non-distractor setting on tiered ImageNet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset?\n\n[R1] “Videostory: A new multimedia embedding for few-example recognition and translation of events,” in ACM MM, 2014\n\n[R2] “Transductive Multi-View Zero-Shot Learning”, IEEE TPAMI 2015\n\n[R3] “Video2vec embeddings recognize events when examples are scarce,” IEEE TPAMI 2014\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning for Semi-Supervised Few-Shot Classification","abstract":"In few-shot classification, we are interested in learning algorithms that train a\nclassifier from only a handful of labeled examples. Recent progress in few-shot\nclassification has featured meta-learning, in which a parameterized model for a\nlearning algorithm is defined and trained on episodes representing different clas-\nsification problems, each with a small labeled training set and its corresponding\ntest set. In this work, we advance this few-shot classification paradigm towards\na scenario where unlabeled examples are also available within each episode. We\nconsider two situations: one where all unlabeled examples are assumed to belong\nto the same set of classes as the labeled examples of the episode, as well as the\nmore challenging situation where examples from other distractor classes are also\nprovided. To address this paradigm, we propose novel extensions of Prototypical\nNetworks (Snell et al., 2017) that are augmented with the ability to use unlabeled\nexamples when producing prototypes. These models are trained in an end-to-end\nway on episodes, to learn to leverage the unlabeled examples successfully. We\nevaluate these methods on versions of the Omniglot and miniImageNet bench-\nmarks, adapted to this new framework augmented with unlabeled examples. We\nalso propose a new split of ImageNet, consisting of a large set of classes, with a\nhierarchical structure. Our experiments confirm that our Prototypical Networks\ncan learn to improve their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would.","pdf":"/pdf/67386ebe7d9f7a702909ecc18545398b9cbe9a2c.pdf","paperhash":"anonymous|metalearning_for_semisupervised_fewshot_classification","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning for Semi-Supervised Few-Shot Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcSzz-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper788/Authors"],"keywords":["Few-shot learning","semi-supervised learning","meta-learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739102268,"tcdate":1509134914222,"number":788,"cdate":1509739099607,"id":"HJcSzz-CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJcSzz-CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Meta-Learning for Semi-Supervised Few-Shot Classification","abstract":"In few-shot classification, we are interested in learning algorithms that train a\nclassifier from only a handful of labeled examples. Recent progress in few-shot\nclassification has featured meta-learning, in which a parameterized model for a\nlearning algorithm is defined and trained on episodes representing different clas-\nsification problems, each with a small labeled training set and its corresponding\ntest set. In this work, we advance this few-shot classification paradigm towards\na scenario where unlabeled examples are also available within each episode. We\nconsider two situations: one where all unlabeled examples are assumed to belong\nto the same set of classes as the labeled examples of the episode, as well as the\nmore challenging situation where examples from other distractor classes are also\nprovided. To address this paradigm, we propose novel extensions of Prototypical\nNetworks (Snell et al., 2017) that are augmented with the ability to use unlabeled\nexamples when producing prototypes. These models are trained in an end-to-end\nway on episodes, to learn to leverage the unlabeled examples successfully. We\nevaluate these methods on versions of the Omniglot and miniImageNet bench-\nmarks, adapted to this new framework augmented with unlabeled examples. We\nalso propose a new split of ImageNet, consisting of a large set of classes, with a\nhierarchical structure. Our experiments confirm that our Prototypical Networks\ncan learn to improve their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would.","pdf":"/pdf/67386ebe7d9f7a702909ecc18545398b9cbe9a2c.pdf","paperhash":"anonymous|metalearning_for_semisupervised_fewshot_classification","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning for Semi-Supervised Few-Shot Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcSzz-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper788/Authors"],"keywords":["Few-shot learning","semi-supervised learning","meta-learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}