{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222604312,"tcdate":1511767181941,"number":3,"cdate":1511767181941,"id":"S1Lqh4YxG","invitation":"ICLR.cc/2018/Conference/-/Paper263/Official_Review","forum":"r11Q2SlRW","replyto":"r11Q2SlRW","signatures":["ICLR.cc/2018/Conference/Paper263/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well written paper with somewhat limited novelty but state-of-the-art results","rating":"6: Marginally above acceptance threshold","review":"Paper presents an approach for conditional human (skeleton) motion generation using a form of the LSTM, called auto-conditioned LSTM (acLSTM). The key difference of acLSTM is that in it parts of the generated sequences, at regular intervals, are conditioned on generated data (as opposed to just ground truth data). In this way, it is claimed that acLSTM can anticipate and correct wrong predictions better than traditional LSTM models that only condition generation on ground truth when training. It is shown that trained models are more accurate at long-term prediction (while being a bit less accurate in short-term prediction). \n\nGenerally the idea is very sensible. The novelty is somewhat small, given the fact that a number of other methods have been proposed to address the explored challenge in other domains. The cited paper by Bengio et al., 2015 is among such, but by no means the only one. For example, “Professor Forcing: A New Algorithm  for Training Recurrent Nets” by Goyal et al. is a more recent variant that does away with the bias that the scheduled sampling of Bengio et al., 2015 would introduce. The lack of comparison to these different methods of training RNNs/LSTMs with generated or mixture of ground truth and generated data is the biggest shortcoming of the paper. That said, the results appear to be quite good in practice, as compared to other state-of-the-art methods that do not use such methods to train. \n\nOther comments and corrections:\n\n- The discussion about the issues addressed not arising in NLP is in fact wrong. These issues are prevalent in training of any RNN/LSTM model. In particular, similar approaches have been used in the latest image captioning literature.\n\n- In the text, when describing Figure 1, unrolling of u=v=1 is mentioned. This is incorrect; u=v=4 in the figure.\n\n- Daniel Holden reference should not contain et. al. (page 9)","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis","abstract":"We present a real-time method for synthesizing highly complex human motions using a novel LSTM network training regime we call the auto-conditioned LSTM (acLSTM). Recently, researchers have attempted to synthesize new motions by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or running. In contrast, our approach can synthesize arbitrary motions with highly complex styles, including dances or martial arts in addition to locomotion. The acLSTM is able to accomplish this by explicitly accommodating for autoregressive noise accumulation during training. Furthermore, the structure of the acLSTM is modular and compatible with any other recurrent network architecture, and is usable for tasks other than motion. Our work is the first to our knowledge that demonstrates the ability to generate over 18,000 continuous frames (300 seconds) of new complex human motion w.r.t. different styles. ","pdf":"/pdf/10b70081ec0d939fcf0801bcecfc07670d5b514b.pdf","TL;DR":"Synthesize complex and extended human motions using an auto-conditioned LSTM network","paperhash":"anonymous|autoconditioned_lstm_network_for_extended_complex_human_motion_synthesis","_bibtex":"@article{\n  anonymous2018auto-conditioned,\n  title={Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r11Q2SlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper263/Authors"],"keywords":["motion synthesis","motion prediction","human pose","human motion","recurrent networks","lstm"]}},{"tddate":null,"ddate":null,"tmdate":1512222604361,"tcdate":1511734795836,"number":2,"cdate":1511734795836,"id":"r1NGC2dlf","invitation":"ICLR.cc/2018/Conference/-/Paper263/Official_Review","forum":"r11Q2SlRW","replyto":"r11Q2SlRW","signatures":["ICLR.cc/2018/Conference/Paper263/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review:   generally positive; some reservations","rating":"7: Good paper, accept","review":"The problem of learning auto-regressive (data-driven) human motion models that have long-term stability\nis of ongoing interest. Steady progress is being made on this problem, and this paper adds to that.\nThe paper is clearly written. The specific form of training (a fixed number of self-conditioned predictions,\nfollowed by a fixed number of ground-truth conditioned steps) is interesting for simplicity and its efficacy.\nThe biggest open question for me is how it would compare to the equally simple stochastic version proposed\nby the scheduled sampling approach of [Bengio et al. 2015].\n\nPROS:  The paper provides a simple solution to a problem of interest to many.\nCONS:  It is not clear if it improves over something like scheduled sampling, which is a stochastic predecessor\n       of the main idea introduced here. The \"duration of stability\" is a less interesting goal than\n       actually matching the distribution of the input data.\n\nThe need to pay attention to the distribution-mismatch problem for sequence prediction problems\nhas been known for a while. In particular, the DAGGER (see below) and scheduled sampling algorithms (already cited) \ntarget this issue, in addition to the addition of progressively increasing amounts of noise during training\n(Fragkiadaki et al). Also see papers below on Professor Forcing, as well as \"Learning Human Motion Models\nfor Long-term Predictions\" (concurrent work?), which uses annealing over dropout rates to achieve stable long-term predictions.\n\n  DAGGER algorithm (2011):  http://www.jmlr.org/proceedings/papers/v15/ross11a/ross11a.pdf\n  \"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\"\n\n  Professor Forcing (NIPS 2016)\n  http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf\n\n  Learning Human Motion Models for Long-term Predictions (2017)\n  https://arxiv.org/abs/1704.02827\n  https://www.youtube.com/watch?v=PgJ2kZR9V5w\n  \nWhile the motions do not freeze, do the synthesized motion distributions match the actual data distributions?\nThis is not clear, and would be relatively simple to evaluate.  Is the motion generation fully deterministic?\nIt would be useful to have probabilistic transition distributions that match those seen in the data.\nAn interesting open issue (in motion, but also of course NLP domains) is that of how to best evaulate\nsequence-prediction models.  The duration of \"stable prediction\" does not directly capture the motion quality. \n\nFigure 1:  Suggest to make u != v for the purposes of clarity, so that they can be more easily distinguished.\n\nData representation:\nWhy not factor out the facing angle, i.e., rotation about the vertical axis, as done by Holden et al, and in a variety of\nprevious work in general?\nThe representation is already made translation invariant. Relatedly, in the Training section,\ndata augmentation includes translating the sequence: \"rotate and translate the sequence randomly\".\nWhy bother with the translation if the representation itself is already translation invariant?\n\nThe video illustrates motions with and without \"foot alignment\".\nHowever, no motivation or description of \"foot alignment\" is given in the paper.\n\nThe following comment need not be given much weight in terms of evaluation of the paper, given that the\ncurrent paper does not use simulation-based methods. However, it is included for completeness.\nThe survey of simulation-based methods for modeling human motions is not representative of the body of work in this area\nover the past 25 years.  It may be more useful to reference a survey, such as \n\"Interactive Character Animation Using Simulated Physics: A State‐of‐the‐Art Review\" (2012)\nAn example of recent SOTA work for modeling dynamic motions from motion capture, including many\nhighly dynamic motions, is \"Guided Learning of Control Graphs for Physics-Based Characters\" (2016)\nMore recent work includes \"Learning human behaviors from motion capture by adversarial imitation\", \n\"Robust Imitation of Diverse Behaviors\", and \"Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning\", all of which demonstrate imitation of various motion styles to various degrees.\n\nIt is worthwhile acknowledging that the synthesized motions are still low quality, particular when rendered with more human-like looking models, and readily distinguishable from the original motions.  In this sense, they are not comparable to the quality of results demonstrated in recent works by Holden et al. or some other recent works.  However, the authors should be given credit for including some results with fully rendered characters, which much more readily exposes motion flaws.\n\nThe followup work on [Lee et al 2010 \"Motion Fields\"] is quite relevant:\n\"Continuous character control with low-dimensional embeddings\"\nIn terms of usefulness, being able to provide some control over the motion output is a more interesting problem than\nbeing able to generate long uncontrolled sequences.  A caveat is that the methods are not applied to large datasets.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis","abstract":"We present a real-time method for synthesizing highly complex human motions using a novel LSTM network training regime we call the auto-conditioned LSTM (acLSTM). Recently, researchers have attempted to synthesize new motions by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or running. In contrast, our approach can synthesize arbitrary motions with highly complex styles, including dances or martial arts in addition to locomotion. The acLSTM is able to accomplish this by explicitly accommodating for autoregressive noise accumulation during training. Furthermore, the structure of the acLSTM is modular and compatible with any other recurrent network architecture, and is usable for tasks other than motion. Our work is the first to our knowledge that demonstrates the ability to generate over 18,000 continuous frames (300 seconds) of new complex human motion w.r.t. different styles. ","pdf":"/pdf/10b70081ec0d939fcf0801bcecfc07670d5b514b.pdf","TL;DR":"Synthesize complex and extended human motions using an auto-conditioned LSTM network","paperhash":"anonymous|autoconditioned_lstm_network_for_extended_complex_human_motion_synthesis","_bibtex":"@article{\n  anonymous2018auto-conditioned,\n  title={Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r11Q2SlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper263/Authors"],"keywords":["motion synthesis","motion prediction","human pose","human motion","recurrent networks","lstm"]}},{"tddate":null,"ddate":null,"tmdate":1512222604405,"tcdate":1511639320781,"number":1,"cdate":1511639320781,"id":"H1b7FSwgM","invitation":"ICLR.cc/2018/Conference/-/Paper263/Official_Review","forum":"r11Q2SlRW","replyto":"r11Q2SlRW","signatures":["ICLR.cc/2018/Conference/Paper263/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A useful technique to avoid prediction error accumulation","rating":"7: Good paper, accept","review":"This paper proposes acLSTM to synthesize long sequences of human motion. It tackles the challenge of error accumulation of traditional techniques to predict long sequences step by step. The key idea is to combine prediction and ground truth in training. It is impressive that this architecture can predict hundreds of frames without major artifacts.\n\nThe exposition is mostly clear. My only suggestion is to use either time (seconds) or frame number consistently. In the text, the paper sometimes use time, and other time uses frame index (e.g. figure 7 and its caption). It confuses me a bit since it is not immediate clear what the frame rate is.\n\nIn evaluation, I think that it is important to analyze the effect of condition length in the main text, not in the Appendix. To me, this is the most important quantitive evaluation that give me the insight of acLSTM. It also gives a practical guidance to readers how to tune the condition length. As indicated in Appendix B, \"Further experiments need to be conducted to say anything meaningful.\" I really hope that in the next version of this paper, a detailed analysis about condition length could be added. \n\nIn summary, I like the method proposed in the paper. The result is impressive. I have not seen an LSTM based architecture predicting a complex motion sequence for that long. However, more detailed analysis about condition length is needed to make this paper complete and more valuable.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis","abstract":"We present a real-time method for synthesizing highly complex human motions using a novel LSTM network training regime we call the auto-conditioned LSTM (acLSTM). Recently, researchers have attempted to synthesize new motions by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or running. In contrast, our approach can synthesize arbitrary motions with highly complex styles, including dances or martial arts in addition to locomotion. The acLSTM is able to accomplish this by explicitly accommodating for autoregressive noise accumulation during training. Furthermore, the structure of the acLSTM is modular and compatible with any other recurrent network architecture, and is usable for tasks other than motion. Our work is the first to our knowledge that demonstrates the ability to generate over 18,000 continuous frames (300 seconds) of new complex human motion w.r.t. different styles. ","pdf":"/pdf/10b70081ec0d939fcf0801bcecfc07670d5b514b.pdf","TL;DR":"Synthesize complex and extended human motions using an auto-conditioned LSTM network","paperhash":"anonymous|autoconditioned_lstm_network_for_extended_complex_human_motion_synthesis","_bibtex":"@article{\n  anonymous2018auto-conditioned,\n  title={Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r11Q2SlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper263/Authors"],"keywords":["motion synthesis","motion prediction","human pose","human motion","recurrent networks","lstm"]}},{"tddate":null,"ddate":null,"tmdate":1509739397578,"tcdate":1509084183453,"number":263,"cdate":1509739394915,"id":"r11Q2SlRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r11Q2SlRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis","abstract":"We present a real-time method for synthesizing highly complex human motions using a novel LSTM network training regime we call the auto-conditioned LSTM (acLSTM). Recently, researchers have attempted to synthesize new motions by using autoregressive techniques, but existing methods tend to freeze or diverge after a couple of seconds due to an accumulation of errors that are fed back into the network. Furthermore, such methods have only been shown to be reliable for relatively simple human motions, such as walking or running. In contrast, our approach can synthesize arbitrary motions with highly complex styles, including dances or martial arts in addition to locomotion. The acLSTM is able to accomplish this by explicitly accommodating for autoregressive noise accumulation during training. Furthermore, the structure of the acLSTM is modular and compatible with any other recurrent network architecture, and is usable for tasks other than motion. Our work is the first to our knowledge that demonstrates the ability to generate over 18,000 continuous frames (300 seconds) of new complex human motion w.r.t. different styles. ","pdf":"/pdf/10b70081ec0d939fcf0801bcecfc07670d5b514b.pdf","TL;DR":"Synthesize complex and extended human motions using an auto-conditioned LSTM network","paperhash":"anonymous|autoconditioned_lstm_network_for_extended_complex_human_motion_synthesis","_bibtex":"@article{\n  anonymous2018auto-conditioned,\n  title={Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r11Q2SlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper263/Authors"],"keywords":["motion synthesis","motion prediction","human pose","human motion","recurrent networks","lstm"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}