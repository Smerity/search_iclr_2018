{"notes":[{"tddate":null,"ddate":null,"tmdate":1515803860836,"tcdate":1515803860836,"number":13,"cdate":1515803860836,"id":"Hk6R4RIEM","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Comment","forum":"SyYe6k-CW","replyto":"BkRk8K_Mz","signatures":["ICLR.cc/2018/Conference/Paper524/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper524/AnonReviewer3"],"content":{"title":"The refinements made it a stronger submission.","comment":"The refinements made it a stronger submission. The authors also promised to release the code for reproducibility as Reviewer 1 recommended. I'm happy to change the score from 4 to 5."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1515610028232,"tcdate":1515610028232,"number":6,"cdate":1515610028232,"id":"rkN31yEVM","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Comment","forum":"SyYe6k-CW","replyto":"SyYe6k-CW","signatures":["ICLR.cc/2018/Conference/Paper524/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper524/Authors"],"content":{"title":"Main changes in the new version of the paper.","comment":"A new version of the paper is now available. We updated the initial submission based on the reviews and the feedback provided in the additional comments.\n\nThe main changes to the initial version are the following:\n\n- Implemented and tested an expectation-propagation algorithm (black-box alpha-divergence).\n- Implemented and tested the sparse GP algorithm.\n- Extended the algorithm description of variational inference methods, dropout, and sparse GPs. Also, we added the description of expectation-propagation methods.\n- Extended the explanation of priors used by linear models.\n- Extended the explanation of the Wheel bandit, and added explanatory plots to the main text.\n- Extended the example that compares BBB with linear methods versus PrecisionDiag, and added two outcome plots to the main text.\n- Updated and extended the experimental framework description, mainly metrics, regret, and hyper-parameter tuning.\n- Updated and extended the discussion section, putting more focus on linking the statements to the empirical results in the tables.\n\n- Added table that links names to specific algorithm configurations.\n- Added table with the running time required by each algorithm and dataset.\n- Added ranking column to cumulative regret table, where the mean ranking of each algorithm across datasets is shown. This way it is easier to parse the connection between the final big-picture conclusions and the empirical results.\n\n- Removed cumulative and simple regret plots, given their low information content."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513817574243,"tcdate":1513817574243,"number":5,"cdate":1513817574243,"id":"BkRk8K_Mz","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Comment","forum":"SyYe6k-CW","replyto":"SJniHtdMf","signatures":["ICLR.cc/2018/Conference/Paper524/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper524/Authors"],"content":{"title":"Continued from above (due to max character count)","comment":"While collecting real-world datasets for a benchmark is challenging, the ones that we use are diverse. Some of them are not learnable or solvable (like Jester), while still of interest due to their practical applications (recommendation systems, in this case). For most datasets, we set the horizon to be the full size of the dataset, so it cannot be increased.  The regret appears linear because these are simply hard problems. Some dataset-dependent conclusions can be drawn: the Gaussian process does well on small datasets where it can handle a large proportion of the data, whereas constant-SGD performs much better on larger data.\n\n[1] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable Bayesian optimization using deep neural networks. In International Conference on Machine Learning, 2015."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513817507562,"tcdate":1513817507562,"number":4,"cdate":1513817507562,"id":"SJniHtdMf","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Comment","forum":"SyYe6k-CW","replyto":"rkI9YHhlz","signatures":["ICLR.cc/2018/Conference/Paper524/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper524/Authors"],"content":{"title":"RE: A large-scale comparison on some posterior estimation methods for Thompson sampling without much insight","comment":"We thank the reviewer for their feedback. The reviewer raises several important concerns, which we address below.\n\nOverall, the main concerns were a lack of insightful conclusions/practical guidelines and that the paper relies too heavily on the appendix. Unfortunately, due to poor organization and writing, the insights we gained from the empirical benchmark were not made clear. We plan to significantly revise the paper for clarity. We briefly summarize our contributions and the insights we derived from the empirical results:\n\nSeveral recent papers claim to innovate on exploration with deep neural networks (e.g., two concurrent ICLR submissions: https://openreview.net/forum?id=ByBAl2eAZ, https://openreview.net/forum?id=rywHCPkAW). We argue that such innovations should be benchmarked against existing literature and baselines on simple decision making tasks (if the methods don’t improve on contextual bandits, how could they hope to improve in RL?). Our major contribution is this empirical comparison - a series of reproducible benchmarks with baseline implementations (all of which will be open sourced). We hope that the reviewer agrees that this empirical benchmark is a scientifically useful contribution.\n \nFrom the empirical benchmark, we find that:\n\n1) Variational approaches to estimate uncertainty in neural networks are an active area of research, however, to the best of our knowledge, there is no study that systematically benchmarks variational approaches in decision-making scenarios against other state-of-the-art approaches.\n\nFrom our evaluation, surprisingly, we find that Bayes by Backprop (BBB) underperforms even with a linear model. We demonstrate that because the method is simultaneously learning the representation and the uncertainty level, when faced with a limited optimization budget (for online learning), slow convergence becomes a serious concern. In particular, when the fitted model is linear, we evaluate the performance of a mean field model which we we can solve in closed form for the variational objective. We find that as we increase number of training iterations for BBB, it slowly converges to the performance of this exact method (Fig 25). We also see that the difference can be much larger than the degradation due to using a mean field approximation. We plan to move this experiment to the main text and expand upon the details.\n\nThis is not a problem in the supervised learning setting, where we can train until convergence. Unfortunately, in the online learning setting, this is problematic, as we cannot train for an unreasonable number of iterations at each step, so poor uncertainty estimates lead to bad decisions. Additionally, tricks to speed up convergence of BBB, such as initializing the variance parameters to a small value, distort uncertainty estimates and thus are not applicable in the online decision making setting.\n\nWe believe that these insights into the problems with variational approaches are of value to the community, and highlight the need for new ways to estimate uncertainty for online scenarios (i.e., without requiring great computational power). \n\n2) We study an algorithm, which we call NeuralLinear, that is remarkably simple, and combines two classic ideas (NNs and Bayesian linear regression). A very similar algorithm was used before in Bayesian optimization [1] and an independent ICLR submission (https://openreview.net/forum?id=Bk6qQGWRb) proposes nearly the same algorithm for RL. In our evaluation, NeuralLinear performs well across datasets. Our insight is that, once the learned representation is of decent quality, being able to exactly compute the posterior in closed form with something as simple as a linear model already leads to better decisions than most of the other methods. We believe this simple argument is novel and encourages further development of this promising approach.\n\n3) More generally, an interesting observation is that in many cases the stochasticity induced by stochastic gradient descent is enough to perform an implicit Thompson sampling. The greedy approach sometimes suffices (or conversely is equally bad as approximate inference). However, we also proposed the wheel problem, where the need for exploration is smoothly parameterized. In this case, we see that all greedy approaches fail."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513806604411,"tcdate":1513806604411,"number":3,"cdate":1513806604411,"id":"B14GjL_GG","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Comment","forum":"SyYe6k-CW","replyto":"HyxcSZ9lG","signatures":["ICLR.cc/2018/Conference/Paper524/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper524/Authors"],"content":{"title":"RE: An interesting comparative study of deep neural bandits (but with rather limited results analysis)","comment":"First, we would like to thank the reviewer for their feedback.\n\nWe acknowledge that the submitted version of the paper does not clearly connect the numerical results and our conclusions and claims. For the revision, we are focused on improving clarity. We plan to expand the discussion of the results and to add tables that summarize the relative ranking among algorithms across datasets to make comparison simpler.\n\nMoreover, we plan to extend the sections corresponding to algorithm descriptions and experimental setup. We also now include a table that explains the abbreviated algorithm names and hyperparameter settings (e.g., difference between RMS2 and RMS3, etc.).\n\nRegret is computed based on the best expected reward (as is standard). For some real datasets, the rewards were deterministic, in which case, both definitions of regret agree. We reshuffle the order of the contexts, and rerun the experiment a number of times to obtain the cumulative regret distribution and report its statistics. We now clarify this procedure in the experimental setup section.\n\nWe agree that the wheel bandit protocol was not clearly explained, and we have expanded the description. \n\nWe agree that expectation propagation methods are relevant to this study, so we have implemented the black-box alpha-divergence algorithm [1] and will add it to the study. \n\nNeuralLinear is based on a standard deep neural network. However, decisions are made according to a Bayesian linear regression applied to the features at the last layer of the network. Note that the last hidden layer representation determines the final output of the network via a linear function, so we can expect a representation that explains the expected value of an action with a linear model. For all the training contexts, their deep representation is computed, and then uncertainty estimates on linear parameters for each action are derived via standard formulas. Thompson sampling will sample from this distribution, say \\beta_t,i at time t for action i, and the next context will be pushed through the network until the last layer, leading to its representation c_t. Then, the sampled beta’s will predict an expected value, and the action with the highest prediction will be taken. Importantly, the algorithm does not use any uncertainty estimates on the representation itself (as opposed to variational methods, for example). On the other hand, the way the algorithm handles uncertainty conditional on the representation and the linear assumption is exact, which seems to be key to its success.\n\nWe will add a comment explaining the assumed prior for linear methods.\n\n[1] Hernández-Lobato, J. M., Li, Y., Rowland, M., Hernández-Lobato, D., Bui, T., and Turner, R. E. (2016). Black-box α-divergence minimization. In International Conference on Machine Learning."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513800921282,"tcdate":1513800921282,"number":3,"cdate":1513800921282,"id":"ByW1BBdMf","invitation":"ICLR.cc/2018/Conference/-/Paper524/Public_Comment","forum":"SyYe6k-CW","replyto":"r1VQYmOMG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for your quick clarification","comment":"Personally, I agree with reviewer1, and believe this work could be very good contribution to the community for benchmarking the algorithms, if the implementation is of quality, reproducible and public available. Good luck.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513793819903,"tcdate":1513793819903,"number":2,"cdate":1513793819903,"id":"r1VQYmOMG","invitation":"ICLR.cc/2018/Conference/-/Paper524/Public_Comment","forum":"SyYe6k-CW","replyto":"BynWicwMz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for your feedback","comment":"We appreciate your interest and feedback on this paper.  Given the number of algorithms we compare, we unfortunately could not give a complete treatment of the background on each method.   \n\n-\"why (Neal, 1994) is cited for SGLD?\"\nNeal is cited for SGLD because in his thesis he proposes Langevin dynamics (and HMC) for neural networks.  He experiments in Section 3.5.1 (page 103) with what he refers to as \"partial gradients\", which are gradient updates computed from single examples.  While he doesn't refer to it directly as stochastic gradient Langevin dynamics, we think it's fair to consider this the seed of the basic idea.  Teh and Welling also cite that work in the SGLD paper.  \n\n- \"SGLD is the mini-batch version of Langevin dynamics (1st order), while SGHMC is the mini-batch version of HMC (2nd order).\"\nThis seems like splitting hairs...  I appeal to Neal on the connection to Langevin dynamics.  From Neal, (Section 5.2 of https://arxiv.org/pdf/1206.1901.pdf): \"The Langevin method: A special case of Hamiltonian Monte Carlo arises when the trajectory used to propose a new state consists of only a single leapfrog step.\"  Nevertheless, we will try to make our wording more precise.  A mention of SG-HMC seems warranted - however we didn't consider the methods because of the higher order terms involved.\n\n- \"It seems the correct reference is  \"Li, C., Chen, C., Carlson, D.E. and Carin, L. Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks,  AAAI 2016\", NOT \"Li, Ke, Swersly, Kevin, Ryan, and Zemel, Richard S. Efficient feature learning using perturb-and-map. NIPS Workshop on Perturbations, Optimization, and Statistics, 2013.\"\nThanks for finding that!  Yes, that should certainly be the other Li et al.\n\n\n- \"2. Connection between SGLD (in paragraph \"Monte Carlo\") and injecting noise on model parameters (in paragraph \"Direct Noise Injection\"). In terms of update rule, these two algorithms seem very similar: the update quantity in both consists of two parts: the gradient term and Gaussian noise term\" \nThere are subtle (and tremendously interesting) connections between many of the methods presented here.  Parameter noise is also related to variational inference (if the posterior is assumed to be a diagonal unit variance Gaussian).  Yes, one can see how parameter noise (adding noise to the weights) can be thought of as related to SGLD (adding noise to the gradient updates).  It is e.g. easy to formalize the distinction between these two with a linear model  with squared error loss.  You will see that the scale of the noise added is very different and this is compounded with non-linearities.  However, the major difference is that SGLD is running a Markov chain while each sample from parameter noise is a draw from a diagonal Gaussian centered at the mean.  Thus SGLD can be shown to sample from the posterior (albeit under some strong assumptions) while parameter noise draws samples from a Gaussian approximation centered around the MAP estimate (the variance of which is left as a hyperparameter).\n\n\"Could the authors clarify the connections and compare them empirically if they are different?\"\nAs stated above, they are rather different.  The paper is essentially an empirical comparison of the different methods and they behave tremendously differently empirically."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513793046290,"tcdate":1513793046290,"number":2,"cdate":1513793046290,"id":"SyCf87dGG","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Comment","forum":"SyYe6k-CW","replyto":"H11if2uxf","signatures":["ICLR.cc/2018/Conference/Paper524/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper524/Authors"],"content":{"title":"Re: Benchmark most useful if accompanying code is well engineered ","comment":"We thank the reviewer for carefully reading the manuscript and for their thoughtful feedback.\n\nTo address the primary concerns:\n\n1 - The code is written in Python and Tensorflow, and will be committed to a well-known Anonymized open source library. Currently, the code is going through third party code review within our organization and is subject to a high quality standard. We designed the implementation so that adding new algorithms and rerunning the benchmark is straightforward for an external contributor.\n\n2 - We agree that making the hyperparameter selection reproducible is essential. To this end, we will re-run the experiments doing the following: 1) we will choose two representative datasets and apply Bayesian optimization to find parameters for each algorithm based on the results from the training datasets. Then, we will freeze these parameters for the remaining datasets and report numbers (and parameters) on these heldout datasets. We will update this post when we have revised the manuscript with the new numbers.\n\nFinally, we have fixed the typos and improved the figures' legends. We added a table mapping algorithm names to their meaning and parameters. We agree that a table showing wall clock time for each algorithm is highly informative, and we plan to add that to the revised manuscript.\n\nWe confirm that the authors reimplemented all of the algorithms.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1513757443633,"tcdate":1513757443633,"number":1,"cdate":1513757443633,"id":"BynWicwMz","invitation":"ICLR.cc/2018/Conference/-/Paper524/Public_Comment","forum":"SyYe6k-CW","replyto":"SyYe6k-CW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The description and connection to SG-MCMC literature might be improved","comment":"Two comments:\n\n1.  The description on the literature on Stochastic Gradient MCMC seems not accurate: \n\n\"A variety of methods have been developed to approximate HMC using mini-batch\nstochastic gradients, These Stochastic Gradient Langevin Dynamics (SGLD) methods (Neal, 1994;\nWelling & Teh, 2011) add Gaussian noise...\"\n\nSGLD is the mini-batch version of Langevin dynamics (1st order), while SGHMC is the mini-batch version of HMC (2nd order). Also, why (Neal, 1994) is cited for SGLD?\n\n\"Li et al. (2013) show that a preconditioner based on the RMSprop algorithm performs well\non deep neural networks.\"\n\nIt seems the correct reference is  \"Li, C., Chen, C., Carlson, D.E. and Carin, L. Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks,  AAAI 2016\", NOT \"Li, Ke, Swersly, Kevin, Ryan, and Zemel, Richard S. Efficient feature learning using perturb-and-map. NIPS Workshop on Perturbations, Optimization, and Statistics, 2013.\"\n\n2. Connection between SGLD (in paragraph \"Monte Carlo\") and injecting noise on model parameters (in paragraph \"Direct Noise Injection\")\n\nIn terms of update rule, these two algorithms seem very similar: the update quantity in both consists of two parts: the gradient term and Gaussian noise term. Could the authors clarify the connections and compare them empirically if they are different?\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1515803917934,"tcdate":1511967117911,"number":3,"cdate":1511967117911,"id":"rkI9YHhlz","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Review","forum":"SyYe6k-CW","replyto":"SyYe6k-CW","signatures":["ICLR.cc/2018/Conference/Paper524/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A large-scale comparison on some posterior estimation methods for Thompson sampling without much insight","rating":"5: Marginally below acceptance threshold","review":"This paper presents the comparison of a list of algorithms for contextual bandit with Thompson sampling subroutine. The authors compared different methods for posterior estimation for Thompson sampling. Experimental comparisons on contextual bandit settings have been performed on a simple simulation and quite a few real datasets.\n\nThe main paper + appendix are clearly written and easy to understand. The main paper itself is very incomplete. The experimental results should be summarized and presented in the main context. There is a lack of novelty of this study. Simple comparisons of different posterior estimating methods do not provide insights or guidelines for contextual bandit problem. \n\nWhat's the new information provided by running such methods on different datasets? What are the newly observed advantages and disadvantages of them? What could be the fundamental reasons for the variety of behaviors on different datasets? No significant conclusions are made in this work.\n\nExperimental results are not very convincing. There are lots of plots show linear cumulative regrets within the whole time horizon. Linear regrets represent either trivial methods or not long enough time horizon.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1515642462054,"tcdate":1511818631755,"number":2,"cdate":1511818631755,"id":"HyxcSZ9lG","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Review","forum":"SyYe6k-CW","replyto":"SyYe6k-CW","signatures":["ICLR.cc/2018/Conference/Paper524/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting comparative study of deep neural bandits (but with rather limited results analysis)  ","rating":"6: Marginally above acceptance threshold","review":"The paper \"DEEP BAYESIAN BANDITS SHOWDOWN\" proposes a comparative study about bandit approaches using deep neural networks. \n\nWhile I find that such a study is a good idea, and that I was really interested by the listing of the different possibilities in the algorithms section, I regret that the experimental results given and their analysis do not allow the reader to well understand the advantages and issues of the approaches. The given discussion is not enough connected to the presented results from my point of view and it is difficult to figure out what is the basis of some conclusion.\n\nAlso, the considered algorithms are not enough described to allow the reader to have enough insights to fully understand the proposed arguments. Maybe authors should have focused on less algorithms but with more implementation details. Also, what does not help is that it is very hard to conect the names in the result table with the corresponding approaches (some abbreviations are not defined at all - BBBN or RMS for instances).\n\nAt last, the experimental protocol should be better described. For instance it is not clear on how the regret is computed : is it based on the best expectation (as done in most os classical studies) or on the best actual score of actions? The wheel bandit protocol is also rather hard to follow (and where is the results analysis?).\n\nOther remarks:\n   - It is a pitty that expectation propagation approaches have been left aside since they correspond to an important counterpart to variational ones. It would have been nice to get a comparaison of both; \n   - Variational inference decsription in section algorithms is not enough developped w.r.t. the importance of this family of approaches\n   - Neural Linear is strange to me. Uncertainty does not consider the neural representation of inputs ? How does it work then ?\n   - That is strange that \\Lambda_0 and \\mu_0 do not belong to the stated asumptions in the linear methods part (ok they correspond to some  prior but it should be clearly stated)\n   - Figure 1 is referenced very late (after figure 2)\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1515642462095,"tcdate":1511731863491,"number":1,"cdate":1511731863491,"id":"H11if2uxf","invitation":"ICLR.cc/2018/Conference/-/Paper524/Official_Review","forum":"SyYe6k-CW","replyto":"SyYe6k-CW","signatures":["ICLR.cc/2018/Conference/Paper524/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Benchmark most useful if accompanying code is well engineered","rating":"7: Good paper, accept","review":"If two major questions below are answered affirmatively, I believe this article could be very good contribution to the field and deserve publication in ICLR.\n\nIn this article the authors provide a service to the community by comparing the current most used algorithms for Thompson Sampling-based contextual (parametric) bandits on clear empirical benchmark. They reimplement the key algorithms, investing time to make up for the lack of published source code for some. \n\nAfter a clear exposure of the reasons why Thompson Sampling is attractive, they overview concisely the key ideas behind 7 different families of algorithms, with proper literature review. They highlight some of the subtleties of benchmarking bandit problems (or any active learning algorithms for that matter): the lack of counterfactual and hence the difference in observed datasets. They explain their benchmark framework and datasets, then briefly summarise the results for each class of algorithms. Most of the actual measures from the benchmark are provided in a lengthy appendix 12 pages appendix choke-full of graphs and tables.\n\nIt is refreshing to see an article that does not boast to offer the new \"bestest-ever\" algorithm in town, overcrowding a landscape, but instead tries to prune the tree of possibilities and wading through other people's inflated claims. To the authors: thank you! It is too easy to dismiss these articles as \"pedestrian non-innovative groundwork\": if there were more like it, our field would certainly be more readable and less novelty-prone.\n\nOf course, there is no perfect benchmark, and like every benchmark, the choices made by the authors could be debated to no end. At least, the authors try to explain them, and the tradeoffs they faced, as clearly as possible (except for two points mentioned below), which again is too rare in our field. \n\nMajor clarifications needed:\n\nMy two key questions are:\n* Is the code of good quality, with exact  reproducibility and good potential extension in a standard language (e.g. Python)? This benchmark only gets its full interest if the code is publicised and well engineered. The open-sourcing is planned, according to footnote 1, is planned -- but this should be made clearer in the main text. There is no discussion of the engineering quality, not even of the language used, and this is quite important if the authors want the community to build upon this work. The code was not submitted for review, and as such its accessibility to new contributors is unknown to this reviewer. That could be a make or break feature of this work. \n* Is the hyper parameter tuning reproducible? Hyperparameter tuning should be discussed much more clearly (in the Appendix): while I appreciate the discussion page 8 of how they were frozen across datasets, \"they were chosen through careful tuning\" is way too short. What kind of tuning? Was it  manual, and hence not reproducible? Or was it a clear, reproducible grid search or optimiser? I thoroughly hope for the later, otherwise an unreproducible benchmark would be very \n\nIf the answers to the two questions above is \"YES\", then brilliant article, I am ready to increase my score. However, if either is a \"NO\", I am afraid that would limit to how much this benchmark will serve as a reference (as opposed to \"just one interesting datapoint\").\n\n\nMinor improvements:\n* Please proofread some obvious typos: \n  - page 4 \"suggesed\" -> \"suggested\",  \n  - page 8 runaway math environment wreaking the end of the sentence.\n  - reference \"Meire Fortunato (2017)\" should be  \"Fortunato et al. (2017)\", throughout.\n* Improve readability of figures' legends, e.g. Figure 2.(b) key is un-readable. \n* A simple table mapping the name of the algorithm to the corresponding article is missing. Not everyone knows what BBB and BBBN stands for.\n* A measure of wall time would be needed: while computational cost is often mentioned (especially as a drawback to getting proper performance out of variational inference), it is nowhere plotted. Of course that would partly depend on the quality of the implementation, but this is somewhat mitigated if all the algorithms have been reimplemented by the authors (is that the case? please clarify).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]}},{"tddate":null,"ddate":null,"tmdate":1515190615050,"tcdate":1509125361402,"number":524,"cdate":1509739253048,"id":"SyYe6k-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyYe6k-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","abstract":"Recent interest in decision making with deep neural networks has led to a wide development of practical methods that trade off exploration and exploitation. Bayesian approaches to deep learning are especially appealing for this purpose as they can provide accurate uncertainty estimates as input for reinforcement learning algorithms. However, these methods are rarely compared on benchmarks that evaluate the impact of their approximations in terms of decision-making performance, and their empirical effectiveness seems poorly understood. In this paper, we compare a variety of well-established and recent methods under the lens of Thompson Sampling over a series of contextual bandit problems.","pdf":"/pdf/9aa2802d104ff764c29f3c7b901ecbc2f6bb134c.pdf","TL;DR":"An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling","paperhash":"anonymous|deep_bayesian_bandits_showdown_an_empirical_comparison_of_bayesian_deep_networks_for_thompson_sampling","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Bayesian Bandits Showdown:  An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyYe6k-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper524/Authors"],"keywords":["exploration","Thompson Sampling","Bayesian neural networks","bandits","reinforcement learning","variational inference","Monte Carlo"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}