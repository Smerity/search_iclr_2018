{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222654978,"tcdate":1512145916942,"number":3,"cdate":1512145916942,"id":"HJH-EWkWM","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Review","forum":"B1QRgziT-","replyto":"B1QRgziT-","signatures":["ICLR.cc/2018/Conference/Paper45/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Paper review: the methodology is neat, but presentation can be improved","rating":"7: Good paper, accept","review":"The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator. The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \"spectrally normalized\" objective.\n\nI think the methodology presented in this paper is neat and the experimental results are encouraging. However, I do have some comments on the presentation of the paper:\n\n1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato). For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2. I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients. Please clarify this.\n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator. Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222655021,"tcdate":1512055256034,"number":2,"cdate":1512055256034,"id":"H1xyfspez","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Review","forum":"B1QRgziT-","replyto":"B1QRgziT-","signatures":["ICLR.cc/2018/Conference/Paper45/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice step forward in improving training of GANs","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes \"spectral normalization\" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function. The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient. Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.\n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach. The experimental results seem solid and seem to support the authors' claims. I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer. Like the anonymous commenter, I also initially thought that the proposed \"spectral normalization \" is basically the same as \"spectral norm regularization\", but given the authors' feedback on this I think the differences should be made more explicit in the paper.\n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication.\n\nSmall Nits: \n\nSection 4: \"In order to evaluate the efficacy of our experiment\": I think you mean \"approach\".\n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \"As we prophesied ...\", and in the paragraph below \n * \"... is a tad slower ...\".","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511980237354,"tcdate":1511980237354,"number":9,"cdate":1511980237354,"id":"rJrC3dhlz","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"S1g_eb9gz","signatures":["~Ian_Goodfellow1"],"readers":["everyone"],"writers":["~Ian_Goodfellow1"],"content":{"title":"I don't know of one","comment":"Because people don't usually release negative results, it's hard to know whether people have tried WGAN-GP on high-res ImageNet and it didn't work or whether no one has seriously tried it.\n\nI tried WGAN with weight clipping, not GP, on 128x128 ImageNet, by modifying the openai/improved-gan implementation of Minibatch (NS)GAN for the same dataset. WGAN with weight clipping didn't work very well for me on that task.\n\nThis recent work suggests that WGAN-GP would probably perform comparably to NS-GAN: https://arxiv.org/abs/1711.10337 "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512233857329,"tcdate":1511974554351,"number":4,"cdate":1511974554351,"id":"B1zjIDhef","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Comment","forum":"B1QRgziT-","replyto":"r1skYxEgG","signatures":["ICLR.cc/2018/Conference/Paper45/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper45/Authors"],"content":{"title":"Thanks for your comments!","comment":"Thanks for your comments, and thank you very much for letting us know about the closely related work.  Our stance on this matter is that, mathematically, spectral normalization and orthonormality constraints are quite different, because the orthonormality constraint destroys the information about the spectrum by setting all the singular values to one.  On the other hand, spectral normalization only scales the spectrum so that its maximum will be one.  Therefore it is probably hard to say which method is more superior than the other, and their utility depends on the task and the dataset.  \n\nFor an additional set of comparative study, we implemented GANs with orthonormality constraint. For CIFAR10, spectral normalization outperformed the orthonormality constraint in terms of the inception score with most of the hyper-parameter settings.  On STL, our method seems to be losing with many hyper-parameter settings.  We summarize the inception scores achieved by the orthonormality method on CIFAR10 and STL. The values inside the parenthesis are the inception scores achieved by spectral normalization. \n(A-F corresponds to the different settings of the hyper-parameters introduced on the paper, Table 1)\n-CIFAR10\nA 6.37 (7.03) B 6.13 (7.20) C 6.71 (7.42) D 7.40 (7.29) E 6.68(7.16) F 6.30 (6.43)\n-STL10\nA 8.04 (6.37) B 8.39 (8.29) C 8.56 (8.14) D 7.99 (7.87) E 2.24(7.80) F 6.55 (5.73)\nThe experiment on the ImageNet is still running, and we will be getting results in a week or two.  \n\n>Besides, spectral normalization (SN) is proposed by (https://arxiv.org/abs/1705.10941).\nWe would like to emphasize that our method is different from spectral norm regularization.  Unlike our method, their method penalizes the spectral norm by additional regularization term (Eq.(1), https://arxiv.org/abs/1705.10941). Their method is fundamentally different from our method in that they do not make an attempt to ‘set’ the spectral norm to a designated value.  Also, as we mention in Eq.(11) and (12), when we reorganize the derivative of our normalized cost function, we can see that our method is in fact practically imposing a “sample data dependent” regularization on the cost function.  \nSpectral norm regularization, on the other hand, imposes “sample data independent ” regularization on the cost function,  just like L2 regularization and Lasso. \nIn fact, when we trained GANs with 'spectral norm regularization', each layer’s spectral norm quickly shrunk to 0 and the training practically stopped. \n\nWe will momentarily add all these discussions and results to our revised version. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511969085701,"tcdate":1511969060983,"number":3,"cdate":1511969060983,"id":"SyTXZU2xz","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Comment","forum":"B1QRgziT-","replyto":"ryjuZSQlG","signatures":["ICLR.cc/2018/Conference/Paper45/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper45/Authors"],"content":{"title":"Thanks for the comments!","comment":"\n>I don't see where the square comes in.  If you flatten $W$, it should be of shape $d_{out}d_{in}hw$, right?  I am also interested to hear more about the semantics of the spectral norm of this object (flattened filterbank), which Ian asked about below.\n\nYes, it's a typo. We meant to write 2-D, not square.  \nAs for the spectral norm of convolutional operator, please take a look at our response to Ian’s comment. \n\n>Relatedly, I think there is a typo in the caption of Table 6:\n\"we replaced the usual batch normalization layer in the ResBlock of the with the conditional batch normalization layer\"\n\nWe are sorry for the confusion, and you are correct about our typo in the caption of Table 6. We meant to write \n“we replaced the usual batch normalization layer in the ResBlock of the '''generator''' with the conditional batch normalization layer\". \nWe introduced the conditional batch normalization layer to the generators of ALL the GANs. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511974322093,"tcdate":1511968418414,"number":2,"cdate":1511968418414,"id":"Hkci0r3lM","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Comment","forum":"B1QRgziT-","replyto":"Hkgbu7Qgz","signatures":["ICLR.cc/2018/Conference/Paper45/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper45/Authors"],"content":{"title":"backprop, the spectral norm of the convoluation operation","comment":"1)\nIndeed, u and v are both functions of W, and we technically have to backprop through these vectors as well.  However,  in our implementation, we ignored the dependency of u and v on W for the sake of computational efficiency, and we were still able to maintain the Lipschitz constraint.\nIn fact, to be on the safe side, we ran experiments with backprop on u and v as a separate experiment.  We were not able to observe any notable improvement.  \n\n2)\nTo make the long story short,  sigma(W) and sigma(B) may and may not differ depending on the padding and stride size.  We briefly discuss this matter on the second footnote in page 5.  Let us elaborate on this a little further.  For the sake of argument, let us assume that the input image is infinite dimensional in both directions. If the stride size is 1,  the value on each output pixel will be computed from the outputs of exactly same number (say, m) of filter blocks.  The same holds also when the stride size divides the dimension of the filter block. In such cases, sigma(W) and sigma(B) will be off by the root m, and the dominant vectors will be exactly same. \nWhen the stride size does not divide the dimension of the dimension of the filter block, however, there will be some output pixels that are computed from the outputs of more filter blocks than other. In such cases, the relationship between sigma(W) and sigma(B) appears complex; at least so complex that we decided not to elaborate further on our paper. \nFor our experiment, we made sure that the stride size divides the dimension of the filter block so that, even after taking the padding size into consideration, the dominant direction will not be too much off from what we mathematically intended. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222655058,"tcdate":1511838059062,"number":1,"cdate":1511838059062,"id":"SkQdbLclM","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Review","forum":"B1QRgziT-","replyto":"B1QRgziT-","signatures":["ICLR.cc/2018/Conference/Paper45/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Standard idea, great results ","rating":"7: Good paper, accept","review":"This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator. This Lipschitz property has already been proposed by recent methods and has showed some success. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator. This is demonstrated in comparison to weight normalization in Figure 4. The experimental results are very good and give strong support for the proposed normalization.\n\n\nWhile the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. I am recommending acceptance, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art. More details in the comments below.\n\nComments:\n1. One concern about this paper is that it doesn’t fully answer the reasons why this normalization works better. I found the discussion about rank to be very intuitive, however this intuition is not fully tested.  Figure 4 reports layer spectra for SN and WN. The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency. I would like to see the same spectra included. \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge? One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments. What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?\n3. Section 4 needs some careful editing for language and grammar.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511817320356,"tcdate":1511817320356,"number":8,"cdate":1511817320356,"id":"S1g_eb9gz","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"Hkdca0ZlG","signatures":["~R_Devon_Hjelm1"],"readers":["everyone"],"writers":["~R_Devon_Hjelm1"],"content":{"title":"ILSVRC2012 collapse","comment":"Have there been any solid attempts at training Imagenet with WGAP-GP? This was not done in Gulrajani et al 2017, nor have I seen many recent attempts in the literature, even in the conditional setting. In my own experience I've found that the gradient norm regularization (https://arxiv.org/abs/1705.09367) seems to train reasonably well (no mode collapse, good diversity, some semblance of realistic samples, good quality as far as the usual features) with ResNets that usually fail with GANs, though I have to admit I'm in the middle of such an experiment and the samples still look quite strange at 20 epochs (like Dali paintings, this is taking weeks to train on one GPU)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511375219366,"tcdate":1511375219366,"number":6,"cdate":1511375219366,"id":"ryjuZSQlG","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"B1QRgziT-","signatures":["~Colin_Raffel1"],"readers":["everyone"],"writers":["~Colin_Raffel1"],"content":{"title":"Possible typo when describing convolution filterbank flattening?","comment":"Hi, thanks for the paper, impressive results!\n\nI am confused about how you describe flattening the convolutional filterbank for computing the spectral norm.  You wrote\n\"Also, for the evaluation of the spectral norm for the convolutional weight $W \\in R^{d_{out} \\times d_{in} \\times h \\times w}$, we treated the operator as a square matrix of dimension $d_{out} \\times (d_{in}hw)^2$.\"\nI don't see where the square comes in.  If you flatten $W$, it should be of shape $d_{out}d_{in}hw$, right?  I am also interested to hear more about the semantics of the spectral norm of this object (flattened filterbank), which Ian asked about below.\n\nAlso, a separate question - for your imagenet experiments, did all of the GAN variants you report (no normalization, layer normalization, spectral normalization) have conditional batch norm?  Or just the SN-GAN?  Relatedly, I think there is a typo in the caption of Table 6:\n\"we replaced the usual batch normalization layer in the ResBlock of the with the conditional batch normalization layer\"\n\nThanks for any responses."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511368697380,"tcdate":1511368697380,"number":5,"cdate":1511368697380,"id":"Hkgbu7Qgz","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"SJmRwz7xG","signatures":["~Ian_Goodfellow1"],"readers":["everyone"],"writers":["~Ian_Goodfellow1"],"content":{"title":"Thanks!","comment":"Thanks! Yes, I see from Appendix C1 that you're finding a good approximation of the spectral norm. I was asking these questions so I can re-implement it successfully myself, not because I doubt you're finding the spectral norm.\n\nTwo follow up questions:\n1)\nIf the spectral norm\nsigma(W) = u^T W v\nthen to estimate the derivatives of sigma(W) with respect to W, don't you need to backprop through u and v too? u and v are both functions of W.\nOr is the estimate still useful somehow when u and v are constant?\n(Either way, you successfully maintain the spectral norm constraint, but learning would be faster if you get the gradient of sigma(W) correct because this means your gradient of the cost function will be tangent to the constraint region and prevent you from wasting time moving in forbidden directions)\n\n2)\nFor convolution, you have a kernel K that can be reshaped to a matrix W but convolving the input actually uses a different matrix B. B is a big doubly block circulant matrix where the number of rows is equal to the number of pixels in the input image.\n\nsigma(B) is max_{image, subject to l2_norm(image)=1} l2_norm(conv(image, K)).\n\nsigma(W) is max_{x, subject to l2_norm(x)=1} l2_norm(Wx).\n\nDo you know if sigma(B) and sigma(W) are approximately the same  as each other?\nI haven't thought it through and don't actually know the answer.\n\nTo bound the Lipschitz constant of the neural net, you want to constrain sigma(B), but in your experiments you constrained sigma(W).\n\nI'm guessing that maybe sigma(B) and sigma(W) are related by a constant factor, so you probably are still constraining the Lipschitz constant of the whole net, but maybe constraining it to a different value than you thought.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511366207759,"tcdate":1511364555183,"number":1,"cdate":1511364555183,"id":"SJmRwz7xG","invitation":"ICLR.cc/2018/Conference/-/Paper45/Official_Comment","forum":"B1QRgziT-","replyto":"HJdKxkGxf","signatures":["ICLR.cc/2018/Conference/Paper45/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper45/Authors"],"content":{"title":"Thanks for the comments and remarks!","comment":"Thanks for the comments and remarks!\nLet me try to resolve the concerns one by one. \n\n>I'm not sure which time step I'm meant to take u and v from when computing the spectral norm. Here I chose to use the *new* value of both u and v, so that get u^T W v for free when I compute the normalizing constant for the new value of u.\n\nI am not sure if I am understanding the question clearly, but at each forward propagation, we prepare new u and v from the same set of u.     \nBy the way, we would like to note that we didn't propagate gradients thorough new_u and new_v .\nIf we write our code in Tensorflow, our implementation is like:\n       new_v = tf.nn.l2_normalize(tf.matmul(self.u, W), 1)\n       new_u = tf.nn.l2_normalize(tf.matmul(new_v, tf.transpose(W)), 1)\n       new_u = tf.stop_gradient(new_u)\n       new_v = tf.stop_gradient(new_v)\n       spectral_norm = tf.reduce_sum(new_u * tf.transpose(tf.matmul(W, tf.transpos e(new_v))), 1)\n       power_method_update =  tf.assign(self.u, new_u)\n\n>For convolution, I *think* I'm meant to use convolution and convolution transpose on a 4-D tensor, based on the comment in the paper about the sparse matrix, but I wasn't totally sure if I should do this or reshape the kernels into a matrix and use matrix-vector products.\n\nIn our implementations, we reshaped the 4D convolutional kernel into a 2-D matrix for the computation of the spectral norm. So, to be honest, our “spectral norm” does not include the parameters like padding and stride size.  We did away with these parameters just for the ease of computation.   So far however, this way is yielding satisfactory results.\n\nYour implementation is mathematically more faithful to our theoretical statement in that it is approximating the honest-to-goodness operator norm of the convolutional operator that includes these parameters.  We cannot say for 100% sure, but your speculate your way of computation shall work just fine.\n\n>I'm not 100% sure when I'm meant to run the `power_method_update` op. Should I just run this once per gradient step or do I need to run it several times to get u close to optimal before I start running SGD?\n\nIn our experiment, we applied the power method update operation only one time per gradient step. It turned out that one power iteration was enough. \nTo check how good we are doing with one application of the power method,  we used SVD to compute the spectral norm of the convolution kernel normalized with our method (AppendixC.1)  Note that our method is doing just fine with  one power method. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511284863666,"tcdate":1511284863666,"number":4,"cdate":1511284863666,"id":"HJdKxkGxf","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"B1QRgziT-","signatures":["~Ian_Goodfellow1"],"readers":["everyone"],"writers":["~Ian_Goodfellow1"],"content":{"title":"Clarifications: implementing the power method","comment":"I want to check that I understand how to implement the convolutional version of your spectral norm approximation correctly.\n\nI make u be a convolutional input tensor that contains only one example:\n\nsingle_input_shape = [1, rows, cols, input_channels]\nself.single_input_shape = single_input_shape\ninit_u = tf.random_normal(single_input_shape, dtype=tf.float32)\ninit_u = init_u / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(init_u)))\nself.u = tf.Variable(init_u, trainable=False)\n\n\nThen on every iteration of SGD I do these updates:\n\n    new_v = self.conv(self.u, self.kernels)\n    new_v = new_v / tf.sqrt(1e-7 + tf.reduce_sum(tf.square(new_v)))\n\n    new_u = self.conv_t(new_v, self.kernels)\n    # u^T W v = (W v / l2_norm(Wv))^T Wv = l2_norm(Wv) = l2_norm(new_u)\n    spectral_norm = tf.sqrt(1e-7 + tf.reduce_sum(tf.square(new_u)))\n    new_u = new_u / spectral_norm\n\n   power_method_update =  tf.assign(self.u, new_u)\n\n\nI ask because there are a few subtle things:\n- I'm not sure which time step I'm meant to take u and v from when computing the spectral norm. Here I chose to use the *new* value of both u and v, so that get u^T W v for free when I compute the normalizing constant for the new value of u.\n- For convolution, I *think* I'm meant to use convolution and convolution transpose on a 4-D tensor, based on the comment in the paper about the sparse matrix, but I wasn't totally sure if I should do this or reshape the kernels into a matrix and use matrix-vector products.\n- I'm not 100% sure when I'm meant to run the `power_method_update` op. Should I just run this once per gradient step or do I need to run it several times to get u close to optimal before I start running SGD?\n\nThanks, and sorry if this is in the paper and I've missed it."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511284112491,"tcdate":1511284112491,"number":3,"cdate":1511284112491,"id":"Hkdca0ZlG","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"B1w_vpZef","signatures":["~Ian_Goodfellow1"],"readers":["everyone"],"writers":["~Ian_Goodfellow1"],"content":{"title":"it's mode collapse","comment":"The main *symptom* is mode collapse.\n\nWhen I was working on this paper ( https://arxiv.org/abs/1606.03498 ) I was able to get GANs to draw dogs occasionally. Sometimes I could get them to draw a different class but I never got them to draw more than one class at a time.\n\nAugustus did some good experiments while working on AC-GAN, where he uses MS-SSIM to measure mode collapse. He found that increasing the number of classes causes collapse. https://arxiv.org/abs/1610.09585\n\nOf course, these are *symptoms*. We don't know a lot about the *cause*. There has been a lot of work over the past few years suggesting that the cause is optimizing the wrong loss (f-GAN, WGAN, both kinds of LS-GAN, etc. have proposed new losses). There has also been a lot of work suggesting the cause is that the learning algorithm fails to equilibrate the game or does so extremely inefficiently ( https://arxiv.org/abs/1412.6515 https://arxiv.org/abs/1701.00160 https://arxiv.org/abs/1706.04156 https://arxiv.org/abs/1706.08500 etc). Finally, at the MILA summer school this year, I said that I think the model family could bias the learning algorithm toward mode collapse. The success of SN-GAN in this submission seems to be evidence in favor of the 2nd or 3rd hypothesis about the cause."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511278447221,"tcdate":1511278447221,"number":2,"cdate":1511278447221,"id":"B1w_vpZef","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"r12BeAgeM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The reason why ILSVRC2012 is fundamentally hard","comment":"This may be a naive question, but can someone explain to me why scaling to ILSVRC2012 dataset is more than a computation problem? Is it because of the instability so that few realistic images will be generated or training progresses real slow? Or is it because mode collapsing so that less diverse set of realistic images will be generated? Or something else?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511215172505,"tcdate":1511215172505,"number":1,"cdate":1511215172505,"id":"r12BeAgeM","invitation":"ICLR.cc/2018/Conference/-/Paper45/Public_Comment","forum":"B1QRgziT-","replyto":"B1QRgziT-","signatures":["~Ian_Goodfellow1"],"readers":["everyone"],"writers":["~Ian_Goodfellow1"],"content":{"title":"This is a great paper!","comment":"This is a great paper! I don't think this paper explains the importance of its results nearly enough and I'm concerned that it may not be obvious what a breakthrough it is just from skimming the abstract.\n\n\"We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques\" is a major understatement. This paper represents an extraordinary advance on the ILSVRC2012 dataset.\n\nBefore this paper, there was only one GAN that worked very well at all on ILSVRC2012: AC-GAN. AC-GAN was sort of cheating because it divided ImageNet into 100 smaller datasets that each contained only 10 classes. The new SN-GAN is the first GAN to ever fit all 1000 ImageNet classes in one GAN.\n\nScaling GANs to a high amount of classes has been a major open challenge and this paper has achieved an amazing 10X leap forward."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739514228,"tcdate":1508741322944,"number":45,"cdate":1509739511567,"id":"B1QRgziT-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1QRgziT-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Spectral Normalization for Generative Adversarial Networks","abstract":"One of the challenges in the study of generative adversarial networks is the instability of its training. \nIn this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.\nOur new normalization technique is computationally light and easy to incorporate into existing implementations. \nWe tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. ","pdf":"/pdf/84cd298e215c0afbf3fa2f40d42f7265ec7c02ef.pdf","TL;DR":"We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.","paperhash":"anonymous|spectral_normalization_for_generative_adversarial_networks","_bibtex":"@article{\n  anonymous2018spectral,\n  title={Spectral Normalization for Generative Adversarial Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1QRgziT-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper45/Authors"],"keywords":["Generative Adversarial Networks","Deep Generative Models","Unsupervised Learning"]},"nonreaders":[],"replyCount":17,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}