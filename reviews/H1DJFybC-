{"notes":[{"tddate":null,"ddate":null,"tmdate":1516242590141,"tcdate":1516242590141,"number":4,"cdate":1516242590141,"id":"ryUoLK6VG","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Comment","forum":"H1DJFybC-","replyto":"ryAfyd-fz","signatures":["ICLR.cc/2018/Conference/Paper498/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper498/AnonReviewer1"],"content":{"title":"Thank you for the revision. Became better, but still needs more work.","comment":"I think the paper became better. However, it still needs more work.\n\nOverall, it is not very clear what to be solved in the paper -- if they want to verify the trace hypothesis, or they want to show that the combination of the proposed components is important to build a system for the problem, or the improvement of each component to deal with difficult challenges is important, or all of these.\n\nFor example, in Section 1, the authors summarize the contributions of the paper. However, right after that, they mention challenges of the paper which do not correspond to the contributions. The contributions are made not by addressing the challenges.\n\nI would suggest a major revision to the paper. I think the concept is good and it can be potentially a great paper. \n\nMinor comments:\n\nExperiment 1 is not very clear to me. Neural network with SMC is explained in Section 2, but other methods are not well explained. Figure 4 implies all methods use \"particle\" in some ways. I do not know how \"particle\" is used in beach search for example.\n\nShould Section 6 be \"Conclusion\"?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1513353238496,"tcdate":1513353238496,"number":3,"cdate":1513353238496,"id":"B1Rzx_Zzf","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Comment","forum":"H1DJFybC-","replyto":"Sk-ZlwcgG","signatures":["ICLR.cc/2018/Conference/Paper498/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper498/Authors"],"content":{"title":"Response to anonymous reviewer 3","comment":"Thank you for your helpful review.\n\nWe agree that this paper tries to pack a lot of content into one manuscript and would be much improved by an editing pass. Our posted revision is much shorter (9.25 pages, excluding references) and more clearly outlines the content.\n\nWe believe one of the reasons why the initial manuscript was difficult to read is because it did not clearly delineate the domain-specific design choices (like the neural architecture and learned distance metric) from the domain-general ideas (the trace hypothesis and the learned search policy). In the posted revision we draw attention to this distinction in the introduction and outline where in the paper each model component is explained.\n\n\"The trace hypothesis\" is a hypothesis in the sense that it is a claim about how to architect certain AI systems, and the word \"hypothesis\" is sometimes used for claims like this (for example, \"the strong story hypothesis\" and the \"directed perception hypothesis\": see [1]). But we agree that this might be confusing and we are open to renaming it to something like the trace set architecture/framing.\n\nRegarding \"How is the edit distance defined?\": We treat the drawing commands as a set, and define the edit distance as the size of the symmetric difference between the ground truth set and the set produced by the model.\n\nIt is correct and that we evaluate our model on only 100 real hand drawings. These 100 drawings are best thought of as an out-of-sample test set.\n\nRegarding the (+) operator in Figure 3: This is the direct sum operator, which in here takes 2 single-channel images and stacks them to make a single 2-channel image.\n\nReferences:\n[1] Winston, Patrick Henry. \"The Strong Story Hypothesis and the Directed Perception Hypothesis.\" AAAI Fall Symposium: Advances in Cognitive Systems. 2011."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1513353087676,"tcdate":1513353087676,"number":2,"cdate":1513353087676,"id":"HyOFkdZMG","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Comment","forum":"H1DJFybC-","replyto":"B1Te809gM","signatures":["ICLR.cc/2018/Conference/Paper498/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper498/Authors"],"content":{"title":"Response to anonymous reviewer 2","comment":"Thank you for the thoughtful review.\n\nRegarding \"The potential limitation is generalization to non-synthetic data\": We wish to clarify that, even though the neural network is trained exclusively on synthetic data, we apply it to real hand drawings. We have prominently clarified this in the posted revision.\n\nThank you for suggesting framing the proposal training as amortized inference. This is a correct and insightful way of communicating the purpose of the neural network, and we have used this framing in the revision.\n\nRegarding \"There are many ad-hoc choices made in the paper\": Although many of engineering decisions are specific to our domain, we believe that the core generalizable idea is the trace hypothesis, which factors the problem into two independent pieces that can be tackled separately, rather than trying to go straight from raw input to a program. One could also consider using an amortized inference approach that does not use the trace set as an intermediate steppingstone, like RobustFill [1], or other ICLR papers currently under review [2]. There would be two problems with this hypothetical alternative perception->program approach:\n1. We would need a large data set of (image, program) pairs, where the programs are drawn from the actual distribution that real-world diagrams are drawn from. \n2. As shown experimentally in DeepCoder [3], neural approaches to program synthesis that attempt to go directly from the problem specification to the program tend to not work as well in practice as those that also leverage symbolic approaches to program synthesis.\nIn the posted revision, we have clarified the boundary between the domain-specific design choices and what we believe to be the domain-general ideas.\n\nThe reason we need to learn a surrogate likelihood function L_{learned} is not because of the mismatch between the distribution of the synthetic data and the actual hand drawings. Instead, it is because we need to wrap a stochastic search procedure (SMC) around the neurally-guided proposals, and the SMC sampler needs some way of measuring how well a particle explains an image that is robust to variations in the exact details of how something was drawn, which means we can't use pixel-wise distance. L_{learned} generalizes to real data because it is trained on LaTeX TikZ output rendered with the \"pencildraw\" package, which causes LaTeX output to look like it was drawn with a pencil.\n\nThank you for pointing out the fact that we did not define the \"Intersection over Union\" (IoU). The IoU for two sets A and B is $|A\\cap B|/|A\\cup B|$. We use IoU to measure the system's accuracy at recovering trace sets. Here the sets are sets of primitive drawing commands.\n\n$t(\\sigma | T)$ is the length of time it takes the program synthesizer to find the minimum cost program in $\\sigma$ such that that program evaluates to the trace set $T$. Our revision now clarifies this point of confusion.\n\nBias optimality buys us three important things. First, it guarantees that the policy will always eventually find the minimum cost program. Second, it explicitly takes into account the cost of searching, in contrast to e.g. DeepCoder [3]. Lastly it gives us a differentiable loss function for the policy parameters. The posted revision now discusses these points.\n\nYou are correct to notice that the program space is already limited to programs with a maximum depth of 3 - meaning that we can have loops within loops, but not loops within loops within loops. Sketch does not support unbounded program spaces. Most of our graphics programs have depth 2-3.\n\nReferences:\n[1] Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.R. and Kohli, P., RobustFill: Neural Program Learning under Noisy I/O. 2017.\n[2] Neural Program Search: Solving Data Processing Tasks from Description and Examples. Under review at ICLR 2018.\n[3] Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., & Tarlow, D. Deepcoder: Learning to write programs. ICLR 2017.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1513352982087,"tcdate":1513352982087,"number":1,"cdate":1513352982087,"id":"ryAfyd-fz","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Comment","forum":"H1DJFybC-","replyto":"HJR0yoJ-z","signatures":["ICLR.cc/2018/Conference/Paper498/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper498/Authors"],"content":{"title":"Response to anonymous reviewer 1","comment":"Thank you for your thoughtful reviewing. We agree that this paper tries to pack a lot of content into one manuscript and would be much improved by an editing pass. Our posted revision is much shorter (9.25 pages, excluding references) and more clearly outlines the content."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642457194,"tcdate":1512185816034,"number":3,"cdate":1512185816034,"id":"HJR0yoJ-z","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Review","forum":"H1DJFybC-","replyto":"H1DJFybC-","signatures":["ICLR.cc/2018/Conference/Paper498/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting algorithms and results, but too many contents in a single paper","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a method to infer lines of code that produces a given image. The method consists of two components. One is to generate traces, which are primitive commands of a graphic program, given an image. The other is to infer lines of code given traces. The first component uses a deep neural network for the conversion and a novel architecture is used for the network. The second component uses a learnt search polity to speed up the inference. Experimental results on a small dataset show that the proposed method can generate lines of code of a graphics program for the images reasonably well. It also discusses possible applications of the method.\n\nOverall, the paper is interesting and the proposed method seems reasonable. Also, it is well contrasted with related work. However, the paper contains too many contents and it is hard to understand the important details without reading supplement and the references. It might be even worth considering to split the paper into two ones and each paper proposes one idea (component) at a time with more details.\n\nThat said, I understood the basic ideas of the paper and I liked them. My concern is only around how to write.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642457233,"tcdate":1511871988665,"number":2,"cdate":1511871988665,"id":"B1Te809gM","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Review","forum":"H1DJFybC-","replyto":"H1DJFybC-","signatures":["ICLR.cc/2018/Conference/Paper498/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice experiments but a lot of ad-hoc choices","rating":"6: Marginally above acceptance threshold","review":"Summary of paper:\n\nThis paper tackles the problem of inferring graphics programs from hand-drawn images by splitting it into two separate tasks:\n(1) inferring trace sets (functions to use in the program) and\n(2) program synthesis, using the results from (1).\nThe usefulness of this split is referred to as the trace hypothesis.\n\n(1) is done by training a neural network on data [input = rendered image; output = trace sets] which is generated synthetically. During test time, a trace set is generated using a population-based method which samples and assigns weights to the guesses made by the neural network based on a similarity metric. Generalization to hand-drawn images is ensured by by learning the similarity metric.\n\n(2) is done by feeding the trace set into a program synthesis tool of Solar Lezama. Since this is too slow, the authors design a search policy which proposes a restriction on the program search space, making it faster. The final loss for (2) in equation 3 takes into consideration the time taken to synthesize images in a search space. \n\n---\n\nQuality: The experiments are thorough and it seems to work. The potential limitation is generalization to non-synthetic data.\nClarity: The high level idea is clear however some of the details are not clear.\nOriginality: This work is one of the first that tackles the problem described.\nSignificance: There are many ad-hoc choices made in the paper, making it hard to extract an underlying insight that makes things work. Is it the trace hypothesis? Or is it just that trying enough things made this work?\n\n---\n\nSome questions/comments:\n- Regarding the trace set inference, the loss function during training and the subsequent use of SMC during test time is pretty unconventional. The use of the likelihood P_{\\theta}[T | I] as a proposal, as the paper also acknowledges, is also unconventional. One way to look at this which could make it less unconventional is to pose the training phase as learning the proposal distribution in an amortized way (instead of maximizing likelihood) as, for example, in [1, 2].\n- In section 2.1., the paper talks about learning the surrogate likelihood function L_{learned} in order to work well for actual hand drawings. This presumably stems from the problem of mismatch between the distribution of the synthetic data used for training and the actual hand drawings. But then L_{learned} is also learned from synthetic data. What makes this translate to non-synthetic data? Does this translate to non-synthetic data?\n- What does \"Intersection over Union\" in Figure 8 mean?\n- The details for 3.1 are not clear. In particular, what does t(\\sigma | T) in equation 3 refer to? Time to synthesize all images in \\sigma? Why is the concept of Bias-optimality important?\n- It seems from Table 4 that by design, the learned policy for the program search space already limits the search space to programs with maximum depth of the abstract syntax tree of 3. What is the usual depth of an AST when using Sketch?\n\n---\n\nMinor Comments:\n- In page 4, section 2.1: \"But pixel-wise distance fares poorly... match the model's renders.\" and \"Pixel-wise distance metrics are sensitive... search space over traces.\" seem to be saying the same thing\n- End of page 5: \\citep Polozov & Gulwani (2015)\n- Page 6: \\citep Solar Lezama (2008)\n\n---\n\nReferences\n\n[1] Paige, B., & Wood, F. (2016). Inference Networks for Sequential Monte Carlo in Graphical Models. In Proceedings of the 33rd International Conference on Machine Learning, JMLR W&CP 48: 3040-3049.\n[2] Le, T. A., Baydin, A. G., & Wood, F. (2017). Inference Compilation and Universal Probabilistic Programming. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (Vol. 54, pp. 1338–1348). Fort Lauderdale, FL, USA: PMLR.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642457275,"tcdate":1511841785132,"number":1,"cdate":1511841785132,"id":"Sk-ZlwcgG","invitation":"ICLR.cc/2018/Conference/-/Paper498/Official_Review","forum":"H1DJFybC-","replyto":"H1DJFybC-","signatures":["ICLR.cc/2018/Conference/Paper498/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A great paper that needs an editing pass","rating":"4: Ok but not good enough - rejection","review":"I think the idea of inferring programmatic descriptions of handwritten diagrams is really cool, and that the combination of SMC-based inference with constraint-based synthesis is nice. I also think the application is clearly useful – one could imagine that this type of technology would eventually become part of drawing / note-taking applications.\n\nThat said, based on the current state of the manuscript, I find it difficult to recommend acceptance. I understand that the ICLR does not strictly have a page limit, but I think submitting a manuscript of over 11 pages is taking things a bit too far. The manuscript would greatly benefit from a thorough editing pass and some judicious reconsideration of space allocated to figures. Moreover, despite its relative verbosity, or perhaps because of it, I found it surprisingly difficult to extract simple implementation details from the text (for example I had to dig up the size of the synthetic training corpus from the 44-page appendix). \n\nPresentation issues aside, I think this is great work. There is a lot here, and I am sympathetic to the challenges of explaining everything clearly in a single (short) paper. That said, I do think that the authors need to take another stab at this to get the manuscript to a point where it can be impactful. \n\nMinor Comments \n\n- I don't understand what the \"hypothesis\" is in the trace hypothesis. Breaking down the problem into an AIR-style sequential detection task and a program induction is certainly a reasonable thing to do. However, the word \"hypothesis\" is generally used to refer to a testable explanation of a phenomenon, which is not really applicable here. \n\n- How is the edit distance defined? In particular, are we treating the drawing commands as a set or a sequence when we calculate \"the number of drawing commands by which two trace sets differ\"?\n\n- I took me a while to understand that the authors first consider the case of SMC for synthetic images with a pixel-based likelihood, and then move on to SMC with and edit-distance based surrogate likelihood for hand-drawn pictures. The text seems to suggest that only 100 of such hand drawn images were actually used, is that correct?\n \n- What does the (+) operator do in Figure 3?\n\n- I am not sure that \"correcting errors made by the neural network\" is the most accurate way to describe a reranking of the top-k samples returned by the SMC sweep.\n\n- Table 3 is very nice, but does not need to be a full page. \n\n- I would recommend that the authors consolidate wrap-around figures into full-width figures. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1513352825384,"tcdate":1509124318848,"number":498,"cdate":1509739267444,"id":"H1DJFybC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1DJFybC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Infer Graphics Programs from Hand-Drawn Images","abstract":"  We introduce a model that learns to convert simple hand drawings\n  into graphics programs written in a subset of \\LaTeX.~The model\n  combines techniques from deep learning and program synthesis.  We\n  learn a convolutional neural network that proposes plausible drawing\n  primitives that explain an image. These drawing primitives are like\n  a trace of the set of primitive commands issued by a graphics\n  program. We learn a model that uses program synthesis techniques to\n  recover a graphics program from that trace. These programs have\n  constructs like variable bindings, iterative loops, or simple kinds\n  of conditionals. With a graphics program in hand, we can correct\n  errors made by the deep network and extrapolate drawings.  Taken\n  together these results are a step towards agents that induce useful,\n  human-readable programs from perceptual input.","pdf":"/pdf/40a05da6d518dae6d3f36ec4b4ea232e06443cd3.pdf","TL;DR":"Learn to convert a hand drawn sketch into a high-level program","paperhash":"anonymous|learning_to_infer_graphics_programs_from_handdrawn_images","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer Graphics Programs from Hand-Drawn Images},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1DJFybC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper498/Authors"],"keywords":["program induction","HCI","deep learning"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}