{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642436086,"tcdate":1511901748626,"number":3,"cdate":1511901748626,"id":"SJ6V5roeG","invitation":"ICLR.cc/2018/Conference/-/Paper342/Official_Review","forum":"HJDUjKeA-","replyto":"HJDUjKeA-","signatures":["ICLR.cc/2018/Conference/Paper342/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting ideas but lack of experimentation","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a neural architecture to map video streams to a discrete collection of objects, without human annotations, using an unsupervised pixel reconstruction loss. The paper uses such object representation to inform state representation for reinforcement learning. Each object is described by a position, appearance feature and confidence of existence (presence). The proposed network predicts a 2D mask image, where local maxima  correspond to object locations, and values of the maxima correspond to presence values. The paper uses a hard decision on the top-k objects (there can be at most k objects) in the final object list, based on the soft object presence values (I have not understood if these top k are sampled based on the noisy presence values or are thresholded, if the authors could kindly clarify).  \n The final presence values though are sampled using Gumbell-softmax.\n\nObjects are matched across consecutive frames using non parametric (not learnable) deterministic matching functions, that takes into account the size and appearance of the objects. \n\nFor the unsupervised reconstruction loss, a static background is populated with objects, one at a time, each passing its state and feature through deconvolution layers to generate RGB object content.\n\nThen a policy network is trained with deep Q learning whose architecture takes into account the objects in the scene, in an order agnostic way, and pairwise features are captured between pairs of objects, using similar layers as visual interaction nets.\n\nPros\nThe paper presents interesting ideas regarding unsupervised object discovery\n\nCons:\nThe paper shows no results. The objects discovered could be discovered with mosaicing (since the background is static) and background subtraction. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning objects from pixels","abstract":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.\n\nMore precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.\n\nApplying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion. During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.\n\nThis gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","pdf":"/pdf/299e2039114bd76fce1b0316448b7d22a424e079.pdf","TL;DR":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.","paperhash":"anonymous|learning_objects_from_pixels","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning objects from pixels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDUjKeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper342/Authors"],"keywords":["objects","unsupervised","reinforcement learning","atari"]}},{"tddate":null,"ddate":null,"tmdate":1515642436125,"tcdate":1511808837102,"number":2,"cdate":1511808837102,"id":"BJTS11qlz","invitation":"ICLR.cc/2018/Conference/-/Paper342/Official_Review","forum":"HJDUjKeA-","replyto":"HJDUjKeA-","signatures":["ICLR.cc/2018/Conference/Paper342/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A method for learning object representations from pixels for doing reinforcement learning. Very preliminary work. Previous work on computer vision is completely ignored ","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a method  for learning object representations from pixels and then use such representations for doing reinforcement learning.  This method is based on convnets that map raw pixels to a mask and feature map. The mask contains information about the presence/absence of objects in different pixel locations and the feature map contains information about object appearance. \n\nI believe that the current method can only learn and track simple objects in a constant background, a problem which is  well-solved in computer vision. Specifically, a simple method such as \"background subtraction\" can easily infer the mask (the outlying pixels which correspond to moving objects)  while simple tracking methods (see a huge literature over decades on computer vision) can allow to track these objects across frames.  The authors completely ignore all this previous work and their \"related work\" section  starts citing papers from 2016 and onwards!  Is it any benefit of learning objects with the current (very expensive) method compared to simple methods such as  \"background subtraction\"? \n\nFurthermore, the paper is very badly written since it keeps postponing the actual explanations to later sections (while these  sections eventually refer to the appendices).  This makes reading the paper very hard. For example, during the early sections you  keep referring to a loss function which will allow for learning the objects, but you never really give the form of this loss (which you should as soon as  you mentioning it) and the reader needs to search into the appendices to find out what is happening.  \n\nAlso, experimental results are very preliminary and not properly analyzed.  For example the results in Figure 3 are unclear and need to be discussed in detail in the main text. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning objects from pixels","abstract":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.\n\nMore precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.\n\nApplying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion. During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.\n\nThis gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","pdf":"/pdf/299e2039114bd76fce1b0316448b7d22a424e079.pdf","TL;DR":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.","paperhash":"anonymous|learning_objects_from_pixels","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning objects from pixels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDUjKeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper342/Authors"],"keywords":["objects","unsupervised","reinforcement learning","atari"]}},{"tddate":null,"ddate":null,"tmdate":1515642436166,"tcdate":1511522065819,"number":1,"cdate":1511522065819,"id":"Sk9f1tBlz","invitation":"ICLR.cc/2018/Conference/-/Paper342/Official_Review","forum":"HJDUjKeA-","replyto":"HJDUjKeA-","signatures":["ICLR.cc/2018/Conference/Paper342/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper learns to construct masks and feature representations from an input image, in order to represent objects. This is applied to the relatively simple domain of Atari games video input (compared to natural images). The paper is inadequate in that it fails to acknowledge and compare with previous methods for doing these tasks.","rating":"3: Clear rejection","review":"This paper learns to construct masks and feature representations from an input image, in order to represent objects. This is applied to the relatively simple domain of Atari games video input (compared to natural images). The paper is completely inadequate in respect to related work; it re-invents known techniques like non-maximum suppression and matching for tracking; fails to learn convincing objects according to visual inspection; and fails to compare with earlier methods for these tasks. (The comment above about re-invention is the most charitable intepretation -- the worst case would be using these ideas without citation.)\n\n\n1) The related work section is outrageous, containing no references before 2016.  Do the authors think researchers never tried to do this task before then? This is the bad side of the recent deep nets hype, and ICLR is particularly susceptible to this. Examples include\n\n@article{wang-adelson-94,\n  author        = \"Wang,  J. Y. A. and Adelson, E. H.\",\n  title         = {{Representing Moving Images with Layers}},\n  journal       = {{IEEE Transactions on Image Processing}},\n  year          = \"1994\",\n  volume        = \"3(5)\",\n  pages         = {625-638}\n}\nsee http://persci.mit.edu/pub_pdfs/wang_tr279.pdf\n\nand\n\n@article{frey-jojic-03,\n   author    = {Frey, B. J. and Jojic, N.},\n   title     = {{Transformation Invariant Clustering Using the EM Algorithm}},\n   journal   = {IEEE Trans Pattern Analysis and Machine Intelligence},\n   year      = {2003},\n   volume    = {25(1)},\n   pages     = {1-17}\n}\nwhere mask and appearances for each object of interest are learned. There is a literature which follows on from the F&J paper.  The methods used in Frey & Jojic are different from what is proposed in the paper, but there needs to be comparisons.\n\nThe AIR paper also contains references to relevant previous work.\n\n2) p 3 center -- this seems to be reinventing non-maximum suppression\n\n3) p 4 eq 3 and sec 3.2 -- please justify *why* it makes sense to use\nthe concrete transform.  Can you explain better (e.g. in the supp mat)\nthe effect of this for different values of q_i?\n\n4) Sec 3.5 Matching objects in successive frames using the Hungarian \nalgorithm is also well known, e.g. it is in the matlab function\nassignDetectionsToTracks .\n\n5) Overall: in this paper the authors come up with a method for learning objects from Atari games video input. This is a greatly restricted setting compared to real images. The objects learned as shown in Appendix A are quite unconvincing, e.g. on p 9. For example for Boxing why are the black and white objects broken up into 3 pieces, and why do they appear coloured in col 4?\n\nAlso the paper lacks comparisons to other methods (including ones from before 2016) which have tackled this problem.\n\nIt may be that the methods in this paper can outperform previous ones -- that would be interesting, but it would need a lot of work to address the issues raised above.\n\nText corrections:\n\np 2 \"we are more precise\" -> \"we give more details\"\n\np 3 and p 2 -- local maximum (not maxima) for a single maximum.  [occurs many times]\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning objects from pixels","abstract":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.\n\nMore precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.\n\nApplying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion. During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.\n\nThis gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","pdf":"/pdf/299e2039114bd76fce1b0316448b7d22a424e079.pdf","TL;DR":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.","paperhash":"anonymous|learning_objects_from_pixels","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning objects from pixels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDUjKeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper342/Authors"],"keywords":["objects","unsupervised","reinforcement learning","atari"]}},{"tddate":null,"ddate":null,"tmdate":1509739354402,"tcdate":1509100367352,"number":342,"cdate":1509739351743,"id":"HJDUjKeA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJDUjKeA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning objects from pixels","abstract":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.\n\nMore precisely, we construct a differentiable mapping from an image to a discrete tabular list of objects, where each object consists of a differentiable position, feature vector, and scalar presence value that allows the representation to be learnt using an attention mechanism.\n\nApplying this mapping to Atari games, together with an interaction net-style architecture for calculating quantities from objects, we construct agents that can play Atari games using objects learnt in an unsupervised fashion. During training, many natural objects emerge, such as the ball and paddles in Pong, and the submarine and fish in Seaquest.\n\nThis gives the first reinforcement learning agent for Atari with an interpretable object representation, and opens the avenue for agents that can conduct object-based exploration and generalization.","pdf":"/pdf/299e2039114bd76fce1b0316448b7d22a424e079.pdf","TL;DR":"We show how discrete objects can be learnt in an unsupervised fashion from pixels, and how to perform reinforcement learning using this object representation.","paperhash":"anonymous|learning_objects_from_pixels","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning objects from pixels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDUjKeA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper342/Authors"],"keywords":["objects","unsupervised","reinforcement learning","atari"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}