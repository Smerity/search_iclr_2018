{"notes":[{"tddate":null,"ddate":null,"tmdate":1512405011427,"tcdate":1512405011427,"number":3,"cdate":1512405011427,"id":"H1sMueXZM","invitation":"ICLR.cc/2018/Conference/-/Paper459/Official_Review","forum":"rk98KCgRW","replyto":"rk98KCgRW","signatures":["ICLR.cc/2018/Conference/Paper459/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Plausible motivation w/o enough justification; Experimental results need to be stronger ","rating":"3: Clear rejection","review":"This is an emergency review, as the replacement of an overdue review. \n\n------------------------------------------------------------------------\n\nThis paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit. This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor. \n\nHowever, the motivation above is not justified well. No theoretic results are present to support this design. Figure 4 shows the only experimental results to “support” the motivation. However, it is a bit weird that 1) 100% tuned results are not shown, and 2) the learned activation goes up as the input goes negative, which is not the shape of ELU. As a result, the motivation does not seem clear.\n\nThe shift variables seem only useful when they are not shared for different pixels. Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used). The question is if it is worth adding so many pixel-wise parameters. Moreover, the proposed formulation does not seem useful for the fully connected layer at any time. \n\nThe experiments are limited. Only the basic LeNet and another network are considered on Cifar-100. The results are not as good as the state-of-the-art. More importantly, the proposed activation functions reduce the errors only a bit (<0.5%). Stronger results on more datasets are necessary to justify the usefulness of the proposed method.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)","abstract":"The Exponential Linear Unit (ELU) has been proven to speed up learning and improve the classification performance over activation functions such as ReLU and Leaky ReLU for convolutional neural networks. The reasons behind the improved behavior are that ELU reduces the bias shift, it saturates for large negative inputs and it is continuously differentiable. However, it remains open whether ELU has the optimal shape and we address the quest for a superior activation function.\nWe use a new formulation to tune a piecewise linear activation function during training, to investigate the above question, and learn the shape of the locally optimal activation function. With this tuned activation function, the classification performance is improved and the resulting, learned activation function shows to be ELU-shaped irrespective if it is initialized as a RELU, LReLU or ELU. Interestingly, the learned activation function does not exactly pass through the origin indicating that a shifted ELU-shaped activation function is preferable. This observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation function.\nExperiments on Cifar-100 show that the classification performance is further improved when using the ShELU activation function in comparison with ELU. The improvement is achieved when learning an individual bias shift for each neuron.","pdf":"/pdf/0abadb72835ed9e679e3c478e9f16537620b65f8.pdf","paperhash":"anonymous|improved_learning_in_convolutional_neural_networks_with_shifted_exponential_linear_units_shelus","_bibtex":"@article{\n  anonymous2018improved,\n  title={Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk98KCgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper459/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222658091,"tcdate":1511927337900,"number":2,"cdate":1511927337900,"id":"ryGNAooef","invitation":"ICLR.cc/2018/Conference/-/Paper459/Official_Review","forum":"rk98KCgRW","replyto":"rk98KCgRW","signatures":["ICLR.cc/2018/Conference/Paper459/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a piecewise linear activation function that is build on ELU. In general it was an OK paper and there are many to be improved.\n\n+ Novelty seems minor. In my sense, the authors do not provide any evidence theoretically or analysis on why the shifted version of ELU (which does not pass the origin) is more favorable. The idea proposed in the paper is just a stack of \"better\" experiments. Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape?\n\nSection 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated. In  section 3, the shifted version, \\delta, is abruptly proposed only based on \"results presented in 4.1\" could improve learning. This is not a professional ML paper looks like. \n\n+ Experiment not strong to support the idea. Experiments are only conducted in CIFAR100. This is obviously not enough. In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins). And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%.\n\nAlso, the paper needs to be re-organized in a better way (eg., state clearly the difference from previous methods, how to formulate the story, etc.) For now, I don't think it is ready to ICLR. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)","abstract":"The Exponential Linear Unit (ELU) has been proven to speed up learning and improve the classification performance over activation functions such as ReLU and Leaky ReLU for convolutional neural networks. The reasons behind the improved behavior are that ELU reduces the bias shift, it saturates for large negative inputs and it is continuously differentiable. However, it remains open whether ELU has the optimal shape and we address the quest for a superior activation function.\nWe use a new formulation to tune a piecewise linear activation function during training, to investigate the above question, and learn the shape of the locally optimal activation function. With this tuned activation function, the classification performance is improved and the resulting, learned activation function shows to be ELU-shaped irrespective if it is initialized as a RELU, LReLU or ELU. Interestingly, the learned activation function does not exactly pass through the origin indicating that a shifted ELU-shaped activation function is preferable. This observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation function.\nExperiments on Cifar-100 show that the classification performance is further improved when using the ShELU activation function in comparison with ELU. The improvement is achieved when learning an individual bias shift for each neuron.","pdf":"/pdf/0abadb72835ed9e679e3c478e9f16537620b65f8.pdf","paperhash":"anonymous|improved_learning_in_convolutional_neural_networks_with_shifted_exponential_linear_units_shelus","_bibtex":"@article{\n  anonymous2018improved,\n  title={Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk98KCgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper459/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222658130,"tcdate":1511779045676,"number":1,"cdate":1511779045676,"id":"H1RkovYeG","invitation":"ICLR.cc/2018/Conference/-/Paper459/Official_Review","forum":"rk98KCgRW","replyto":"rk98KCgRW","signatures":["ICLR.cc/2018/Conference/Paper459/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Little explanation on why proposed ShElU should work better compared to others. ","rating":"1: Trivial or wrong","review":"The authors propose using piecewise linear activation functions with contraints to make it continous. They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article. Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron.\n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required. Authors present no clear explanation on why the shift should result in improved performance.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)","abstract":"The Exponential Linear Unit (ELU) has been proven to speed up learning and improve the classification performance over activation functions such as ReLU and Leaky ReLU for convolutional neural networks. The reasons behind the improved behavior are that ELU reduces the bias shift, it saturates for large negative inputs and it is continuously differentiable. However, it remains open whether ELU has the optimal shape and we address the quest for a superior activation function.\nWe use a new formulation to tune a piecewise linear activation function during training, to investigate the above question, and learn the shape of the locally optimal activation function. With this tuned activation function, the classification performance is improved and the resulting, learned activation function shows to be ELU-shaped irrespective if it is initialized as a RELU, LReLU or ELU. Interestingly, the learned activation function does not exactly pass through the origin indicating that a shifted ELU-shaped activation function is preferable. This observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation function.\nExperiments on Cifar-100 show that the classification performance is further improved when using the ShELU activation function in comparison with ELU. The improvement is achieved when learning an individual bias shift for each neuron.","pdf":"/pdf/0abadb72835ed9e679e3c478e9f16537620b65f8.pdf","paperhash":"anonymous|improved_learning_in_convolutional_neural_networks_with_shifted_exponential_linear_units_shelus","_bibtex":"@article{\n  anonymous2018improved,\n  title={Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk98KCgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper459/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739293087,"tcdate":1509120337834,"number":459,"cdate":1509739290434,"id":"rk98KCgRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk98KCgRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)","abstract":"The Exponential Linear Unit (ELU) has been proven to speed up learning and improve the classification performance over activation functions such as ReLU and Leaky ReLU for convolutional neural networks. The reasons behind the improved behavior are that ELU reduces the bias shift, it saturates for large negative inputs and it is continuously differentiable. However, it remains open whether ELU has the optimal shape and we address the quest for a superior activation function.\nWe use a new formulation to tune a piecewise linear activation function during training, to investigate the above question, and learn the shape of the locally optimal activation function. With this tuned activation function, the classification performance is improved and the resulting, learned activation function shows to be ELU-shaped irrespective if it is initialized as a RELU, LReLU or ELU. Interestingly, the learned activation function does not exactly pass through the origin indicating that a shifted ELU-shaped activation function is preferable. This observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation function.\nExperiments on Cifar-100 show that the classification performance is further improved when using the ShELU activation function in comparison with ELU. The improvement is achieved when learning an individual bias shift for each neuron.","pdf":"/pdf/0abadb72835ed9e679e3c478e9f16537620b65f8.pdf","paperhash":"anonymous|improved_learning_in_convolutional_neural_networks_with_shifted_exponential_linear_units_shelus","_bibtex":"@article{\n  anonymous2018improved,\n  title={Improved Learning in Convolutional Neural Networks with Shifted Exponential Linear Units (ShELUs)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk98KCgRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper459/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}