{"notes":[{"tddate":null,"ddate":null,"tmdate":1515777045291,"tcdate":1515777045291,"number":6,"cdate":1515777045291,"id":"Syaz3vINf","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Comment","forum":"H1q-TM-AW","replyto":"rJMJD0n7z","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper989/AnonReviewer3"],"content":{"title":"A satisfied mind","comment":"I'm truly satisfied by the new experiments, which demonstrate the benefit of \"virtual adversarial training\" in the whole process. \n\nUnfortunately, I did not take the time to explore the connection of \"Probabilistic Lipschitzness\" with the current work, but it's certainly an interesting thing to look at.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515149051015,"tcdate":1515149017623,"number":4,"cdate":1515149017623,"id":"rJMJD0n7z","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Comment","forum":"H1q-TM-AW","replyto":"Hk19swKeM","signatures":["ICLR.cc/2018/Conference/Paper989/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper989/Authors"],"content":{"title":"Response to Reviewer 3","comment":"Thank you for the review! To improve the quality of the paper, we have made several adjustments to our paper in accordance with your review.\n\n“Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7).”\n\nThank you for the suggestion. We have included an extensive ablation study of the role of the locally-Lipschitz regularization term in Section 6.3.1. Our results show that while conditional entropy minimization alone is sufficient to instantiate the cluster assumption and improve over DANN, the additional incorporation of the locally-Lipschitz regularization term does indeed offer additional performance improvement.\n\n“Also, I wonder why this term is tuned by an hyperparameter (λ_s) for the source, while a single hyperparamer (λ_t) is used for the sum of the two target quantity”\n\nThe choice to use λ_t for the sum of the two target quantities is purely for simplicity. Since the official implementation of VAT (https://github.com/takerum/vat_tf) used the same weighting for the conditional entropy and virtual adversarial training, we opted to do that as well in the target domain.\n\n“It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010)”\n\nThank you for the suggestion. We have added the connection in Appendix E. In particular, we can show that, if the embedding function has infinite-capacity, the H\\DeltaH-divergence achieves the maximum value of 2 even when the feature distribution matching constraint is satisfied. This results in Theorem 1 becoming a vacuous upper bound.\n\n“The notion of ‘Probabilistic Lipschitzness’, which is a relaxation of the ‘cluster assumption’ seems very related to the actual work.”\n\nThank you for this insight. We have incorporated a brief mention of probabilistic Lipschitzness in Section 2. It seems that a stronger connection can be made and we appreciate any additional suggestions you may have on how to better address probabilistic Lipschitzness in our paper.\n\n“Equation 14: In the first term (target loss), theta should have an index t (I think).” and “Bottom of page 6: ‘... and that as our validation set’ (missing word).”\n\nFixed. Thanks! "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515148732044,"tcdate":1515148732044,"number":3,"cdate":1515148732044,"id":"ryETB02XG","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Comment","forum":"H1q-TM-AW","replyto":"BkhjhvQ-G","signatures":["ICLR.cc/2018/Conference/Paper989/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper989/Authors"],"content":{"title":"Response to Reviewer 2","comment":"Thank you for the review! To improve the quality of the paper, we have made several adjustments to our paper in accordance with your review.\n\n“The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well.”\n\nWe have added additional experiments that may be of interest to you. In Section 6.3.1, we perform an extensive ablation study demonstrate the relative contribution of virtual adversarial training. In Section 6.2, we apply VADA/DIRT-T to a non-visual domain adaptation task.\n\n“For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don’t think this is the correct way to write this.”\n\nWe agree the the use of the max operator is informal. To account for the possibility that the maximum is not achievable, using the supremum is more appropriate. We have updated the paper to reflect this. Our choice of presentation is now in keeping with that in GAIL (Eq. (14), Ho & Ermon (2016)) and WGAN (Eq. (2), Arjovsky et al. (2017)).\n\n“(5) should discuss the min-max objective. This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it’s mentioned on p.6 that they replaced GRL with the traditional GAN objective. This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?”\n\nThank you for pointing this out. We have added a footnote next to (5) and modified Appendix C to reflect the following opinion:\n\nWe believe that, at a high level, it is not of particular importance which optimization procedure is used to approximate the mini-max optimization problem. Our decision to switch from the symmetric to asymmetric training is motivated by\n\n1. The extensive GAN literature which advocates the asymmetric optimization approach.\n\n2. Our initial experiments on MNIST → MNIST-M which suggest that the asymmetric optimization approach is more stable.\n\nWe are not committed to either optimization strategy and encourage practitioners to try both when applying VADA. In case the reviewer is interested in the performance of pure domain adversarial training using the asymmetric optimization approach, we have included it in Section 6.3.1.\n\n“The literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.”\n\nThank you for the suggestion. We have incorporated Bousmalis’s paper into our comparison.\n\nReferences\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565–4573, 2016.\n\nMartin Arjovsky, Soumith Chintala, and Le ́on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515148563663,"tcdate":1515148563663,"number":2,"cdate":1515148563663,"id":"SknfSAnQf","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Comment","forum":"H1q-TM-AW","replyto":"rknCb19ef","signatures":["ICLR.cc/2018/Conference/Paper989/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper989/Authors"],"content":{"title":"Response to Reviewer 1","comment":"Thank you for the review! To improve the quality of the paper, we have made several adjustments to our paper in accordance with your review.\n\n“Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers”\n\nThe purpose of Table 1 is to offer a holistic evaluation of the entire set-up. As such, it demonstrates that there exists a training objective + architecture + optimization configuration such significant improvements over previous methods/implementations are possible. We provided such a table in part because doing so seems to be standard practice in semi-supervised learning and domain adaptation papers (Miyato et al., 2017; Laine & Aila, 2016; Tarvainen & Valpola, 2017; Saito et al., 2017; French et al., 2017).\n\nTo offer a fairer comparison, we made the following modifications to the paper:\n\n1. We explicitly mention Table 2 in the main body of the paper, in the section Model Evaluation/Overall\n\n2. We added an ablation study to Section 6.3.1 to demonstrate the relative contribution of virtual adversarial training in comparison to our base implementation of domain adversarial training.\n\n“The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger”\n\nWe agree that the submission would be stronger by performing experiments in other modalities. To that end, we added an example of applying VADA and DIRT-T to a non-visual data in Section 6.2. We chose to apply our model to a Wi-Fi activity recognition dataset. Our results show that VADA significantly improves upon DANN. Unfortunately, due to the small target domain training set size, DIRT-T does not improve upon VADA. We provide additional experiments in Appendix F which suggest that VADA already achieves strong clustering on the Wi-Fi dataset, and therefore DIRT-T is not expected to improve performance in this situation.\n\nWe leave as future work the study of applying VADA/DIRT-T (and the general application of the cluster assumption) to text classification domain adaptation tasks. Given the success of VAT on text classification (Miyato et al., 2016), we are optimistic that this line of work is promising.\n\nReferences\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial train- ing: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.\n\nTakeru Miyato, Andrew M Dai, and Ian Goodfellow. Virtual adversarial training for semi-supervised text classification. stat, 1050:25, 2016.\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis- tency targets improve semi-supervised deep learning results. 2017.\n\nGeoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for domain adaptation. arXiv preprint arXiv:1706.05208, 2017.\n\nKuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, 2017."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515148394935,"tcdate":1515148394935,"number":1,"cdate":1515148394935,"id":"S17uEA2QG","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Comment","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper989/Authors"],"content":{"title":"Revisions to paper","comment":"To improve the quality of the paper, we have made several adjustments to our paper. In addition to minor edits (e.g. fixing typos, improving clarity), we made the following large changes:\n\n1. We added a non-visual domain adaptation task (Wi-Fi activity recognition) to Section 6.2 and Appendix F.\n\n2. We added an additional ablation experiment testing the contribution of virtual adversarial training to Section 6.3.\n\n3. We improved the presentation of Proposition 1 in Appendix E and added a subsection connecting Proposition 1 to Ben-David’s domain adaptation upper bound (Theorem 1).\n\nThank you for taking the time to review this paper!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642538240,"tcdate":1512434852122,"number":3,"cdate":1512434852122,"id":"BkhjhvQ-G","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Review","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good contribution to unsupervised domain adaptation","rating":"7: Good paper, accept","review":"The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption. The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well. \n\nI found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective. This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective. This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?\n\nThe literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515940389266,"tcdate":1511809492119,"number":2,"cdate":1511809492119,"id":"rknCb19ef","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Review","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T). The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions. VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution. Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved. The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly.    \n\nPros:\n+ The paper is written clearly and easy to read\n+ The idea to keep the decision boundary in the low-density region of the target domain makes sense\n+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks\n+ The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks\n\nCons:\n- Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers\n- The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger\n\nOverall, I think it is a good paper and deserves to be accepted to the conference. I’m especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance.\n\nPost-rebuttal revision:\nAfter reading the authors' response to my review, I decided to leave the score as is.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642538314,"tcdate":1511779207138,"number":1,"cdate":1511779207138,"id":"Hk19swKeM","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Review","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A sound approach to mix two complementary strategies for domain adaptation","rating":"7: Good paper, accept","review":"As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed. The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption.\n\nThe experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms. Moreover, we also see the learned model is consistently improved using the proposed \"Decision-boundary Iterative Refinement Training with a Teacher\" (DIRT-T) approach.\n\nThe proposed methodology relies on multiple choices that could sometimes be better studied and/or explained. Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7). Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity.\n \nOn the theoretical side, the discussion could be improved. Namely, Section 3 about \"limitation of domain adversarial training\" correctly explained that \"domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity\". It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based. The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014). In the latter, the notion of \"Probabilistic Lipschitzness\", which is a relaxation of the \"cluster assumption\" seems very related to the actual work.\n\nReference:\nBen-David and Urner. Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell., 2014\n\nPros:\n- Propose a sound approach to mix two complementary strategies for domain adaptation.\n- Great empirical results.\n\nCons:\n- Some choices leading to the optimization problem are not sufficiently explained.\n- The theoretical discussion could be improved.\n\nTypos:\n- Equation 14: In the first term (target loss), theta should have an index t (I think).\n- Bottom of page 6: \"... and that as our validation set\" (missing word).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1515149217884,"tcdate":1509137681920,"number":989,"cdate":1510092360838,"id":"H1q-TM-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/15565223b63c8b7e2e4ca3c970cb4fbc8d1643be.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}