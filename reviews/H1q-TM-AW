{"notes":[{"tddate":null,"ddate":null,"tmdate":1512434852122,"tcdate":1512434852122,"number":3,"cdate":1512434852122,"id":"BkhjhvQ-G","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Review","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good contribution to unsupervised domain adaptation","rating":"7: Good paper, accept","review":"The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption. The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well. \n\nI found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective. This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective. This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?\n\nThe literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of how to leverage labels in one source domain to boost up learning performance in a new target domain where labels are scarcely available or completely unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, feature distribution matching is under-constrained, 2) in the non-conservative domain adaptation setting, where no single classifier can perform well jointly in both the source and target domains, domain adversarial training is over-constrained. In this paper, we address these issues through the lense of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: (1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; (2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)1 model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/5d214b5eddd3ea75309ac7e2b2c203a4d01fd636.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222834481,"tcdate":1511809492119,"number":2,"cdate":1511809492119,"id":"rknCb19ef","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Review","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T). The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions. VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution. Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved. The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly.    \n\nPros:\n+ The paper is written clearly and easy to read\n+ The idea to keep the decision boundary in the low-density region of the target domain makes sense\n+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks\n+ The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks\n\nCons:\n- Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers\n- The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger\n\nOverall, I think it is a good paper and deserves to be accepted to the conference. Iâ€™m especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of how to leverage labels in one source domain to boost up learning performance in a new target domain where labels are scarcely available or completely unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, feature distribution matching is under-constrained, 2) in the non-conservative domain adaptation setting, where no single classifier can perform well jointly in both the source and target domains, domain adversarial training is over-constrained. In this paper, we address these issues through the lense of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: (1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; (2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)1 model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/5d214b5eddd3ea75309ac7e2b2c203a4d01fd636.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222834518,"tcdate":1511779207138,"number":1,"cdate":1511779207138,"id":"Hk19swKeM","invitation":"ICLR.cc/2018/Conference/-/Paper989/Official_Review","forum":"H1q-TM-AW","replyto":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference/Paper989/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A sound approach to mix two complementary strategies for domain adaptation","rating":"7: Good paper, accept","review":"As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed. The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption.\n\nThe experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms. Moreover, we also see the learned model is consistently improved using the proposed \"Decision-boundary Iterative Refinement Training with a Teacher\" (DIRT-T) approach.\n\nThe proposed methodology relies on multiple choices that could sometimes be better studied and/or explained. Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7). Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity.\n \nOn the theoretical side, the discussion could be improved. Namely, Section 3 about \"limitation of domain adversarial training\" correctly explained that \"domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity\". It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based. The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014). In the latter, the notion of \"Probabilistic Lipschitzness\", which is a relaxation of the \"cluster assumption\" seems very related to the actual work.\n\nReference:\nBen-David and Urner. Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell., 2014\n\nPros:\n- Propose a sound approach to mix two complementary strategies for domain adaptation.\n- Great empirical results.\n\nCons:\n- Some choices leading to the optimization problem are not sufficiently explained.\n- The theoretical discussion could be improved.\n\nTypos:\n- Equation 14: In the first term (target loss), theta should have an index t (I think).\n- Bottom of page 6: \"... and that as our validation set\" (missing word).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of how to leverage labels in one source domain to boost up learning performance in a new target domain where labels are scarcely available or completely unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, feature distribution matching is under-constrained, 2) in the non-conservative domain adaptation setting, where no single classifier can perform well jointly in both the source and target domains, domain adversarial training is over-constrained. In this paper, we address these issues through the lense of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: (1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; (2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)1 model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/5d214b5eddd3ea75309ac7e2b2c203a4d01fd636.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092382773,"tcdate":1509137681920,"number":989,"cdate":1510092360838,"id":"H1q-TM-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1q-TM-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A DIRT-T Approach to Unsupervised Domain Adaptation","abstract":"Domain adaptation refers to the problem of how to leverage labels in one source domain to boost up learning performance in a new target domain where labels are scarcely available or completely unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, feature distribution matching is under-constrained, 2) in the non-conservative domain adaptation setting, where no single classifier can perform well jointly in both the source and target domains, domain adversarial training is over-constrained. In this paper, we address these issues through the lense of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: (1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; (2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)1 model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.","pdf":"/pdf/5d214b5eddd3ea75309ac7e2b2c203a4d01fd636.pdf","TL;DR":"SOTA on unsupervised domain adaptation by leveraging the cluster assumption.","paperhash":"anonymous|a_dirtt_approach_to_unsupervised_domain_adaptation","_bibtex":"@article{\n  anonymous2018a,\n  title={A DIRT-T Approach to Unsupervised Domain Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1q-TM-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper989/Authors"],"keywords":["domain adaptation","unsupervised learning","semi-supervised learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}