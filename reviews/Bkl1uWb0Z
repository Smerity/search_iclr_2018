{"notes":[{"tddate":null,"ddate":null,"tmdate":1515433424610,"tcdate":1515433424610,"number":7,"cdate":1515433424610,"id":"B1KCT7WNz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Comment","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper691/Authors"],"content":{"title":"Updated paper","comment":"Dear reviewers,\nWe would like to let you know that we've updated our paper based on your valuable comments. Again, We thank you for your feedback!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1513335571432,"tcdate":1513335571432,"number":5,"cdate":1513335571432,"id":"rJtGoXWzz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Comment","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper691/Authors"],"content":{"title":"Comments on structured Attention and sharing attention","comment":"One of the aspects of this work we don’t feel we made sufficiently clear were the roles of structured and shared attention.  \nIn this work, we explore structured attention (SA) as an explicit way to encode hierarchical structure of the source sentence. SA takes the global structure of a sentence into account as it needs to compute the partition function. For completeness, we also compare SA against self-attention (Flat attention) which only consider local dependencies between a word and its head. We evaluate the quality of induced tree on Universal Dependencies dataset. We observe that overall, SA models obtain better DA/UA scores, and SA are highly sensitive with the choice of architectures (sharing attention). Our original motivation for sharing attention is to help biasing SA to capture syntactic dependencies on the source side. This is reflected in both BLEU scores and DA/UA scores. Without sharing attention, DA/UA scores are considerably worse while the models still on par or outperform the NMT baselines. We find this result is exciting. It suggests that there should a better way to exploit SA for improving NMT as well as grammar induction.  Further, the fact that the choice of the target slide language changes these values hints at the different agreement syntax of languages so combining models may lead to further gains and syntactic learning in the future.\nFinally, we also note that, for grammar induction, we are particularly interested in DE as long distance dependencies are more common in DE. The results show that structured attention are indeed superior to FA.  We leave these insights to future work to explore.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1513335462626,"tcdate":1513335462626,"number":4,"cdate":1513335462626,"id":"r1Jhq7-Gz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Comment","forum":"Bkl1uWb0Z","replyto":"HJwRq_ilz","signatures":["ICLR.cc/2018/Conference/Paper691/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper691/Authors"],"content":{"title":"visualizing trees","comment":"Thank you for your interest and encouragement!  We have been working to recompute numbers on UD for reviews (see below) so now that we have those we can try and build out better visualizations to share more standard dependency graphs.  We are sorry for the delay."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1513298701170,"tcdate":1513298701170,"number":3,"cdate":1513298701170,"id":"SkrGscezf","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Comment","forum":"Bkl1uWb0Z","replyto":"SyOq7nXxf","signatures":["ICLR.cc/2018/Conference/Paper691/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper691/Authors"],"content":{"title":"clarification","comment":"Thank you for your feedback, we are sorry that the semantic attention wasn’t explained clearly in the text. We indeed mean semantic attention as standard attention as you’ve guessed. By semantics, we meant word translation semantics (word f is translated to word e). Our assumption is based on the insights from (Koehn and Knowles, 2017) in which they computed match score between the most attended source word and the aligned word (produced by fast-align) and reported the match scores are higher than 70% for English->German, English<->Russian. \n\nBy syntactic attention, we meant that when the model decided to translate a word f in the source side, we want to model also look at syntactic relations of the word f in an explicit way, such as the head word of f. We hope that this approach would project richer structured information from the source to the target.\n\nWe have now included statistical significant test in the 1st revision. You are right that the gain of SA-NMT is not statistically significant when compared to flat (shared)-attention models. We also included significant test when compared against the flat (no-shared)-attention models. The updated results in Table (2) and (3) shows that sharing attention is beneficial for both NMT and grammar induction. Our results also suggest that there are two possible ways to get more structural information from the source side: using Structured Attention and sharing attention. The Flat attention well behaved in our experiments perhaps because the restriction of sharing attention makes it biases further to syntactic information, or dependency head in this case.\n\nWe are sorry that the idea of sharing attention wasn’t well explained in our paper. We are working on the clarification and we will update it soon.\n\nIn equation 3, we meant S\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1513298589966,"tcdate":1513298589966,"number":2,"cdate":1513298589966,"id":"Sk8occlMG","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Comment","forum":"Bkl1uWb0Z","replyto":"Hyg5qAtgf","signatures":["ICLR.cc/2018/Conference/Paper691/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper691/Authors"],"content":{"title":"thanks for detailed comments","comment":"We would like to thank the reviewer for useful comments, and apologize for disfluencies in the original text.  We will absolutely prioritize clarity as we rework the writing in the final version of the paper.\n\nWe agree with (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017) that NMT handles morphology better than phrase-based MT.  Isabelle et al does show that NMT can capture more morphology for French and English.  In our work, we choose German as a target language where long distance dependencies commonly occur.  We see that branching baselines perform measurably worse on the basic dependents of nouns and verbs in Table 4. We agree that it is reasonable to believe that existing NMT can handle the syntactic dependencies in an implicit manner, in our experiment (Figure 5), we show that if that information is available, the decoder prefers to use them, especially when predicting German verb. Additionally, when we designed our architectures, with the explicit goal of extracting interpretable structures, in part to compare the representations to prior linguistic knowledge of language. Both structured attention and gating norm to certain extent allow us to perform analysis on the task instead of training an additional classifier to probe the ability of the models in capturing linguistic phenomena.\n\nWe have now included statistical significance tests in the 1st revision and we will make sure they are more clearly explained and pronounced in the final version. We have now also included significant tests when comparing against the flat (no-shared)-attention models. We truly appreciate that the reviewer noticed the shared attention mechanism we proposed even though we didn’t explain it well, something we are remedying by getting more eyes on our paper and isolating sources of confusion. We will try our best to make it more accessible in the next revision.\n\nYou are correct in your understanding of the dependency marginals in Equation (11).  We have elaborated on this in the revision but are open to further suggestions.\n\nRegarding “production percentages” – While aggregate attachment numbers give a score to how well syntax is induced generally, they don’t give us insight into the grammar.  As a proxy for which grammatical rules the system has learned, we choose to analyze the frequency with which specific “head → child rules” were used by our model vs how often that rule exists in the grammar of the language.  For example, the three most common verb constructions are verb chains (VERB→ VERB), verb subj/obj (VERB→ NOUN) and verb’s being modified by an adverb (VERB→ ADV).  The gold column indicates how common these constructions are in the true data and the remaining columns show how often our systems believe these constructions exist.  We will need to spend more time in the next revision clarifying this demonstration.\n\nIn equation 3, we meant S. We are sorry for the typo.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1513298392012,"tcdate":1513298392012,"number":1,"cdate":1513298392012,"id":"Bkl159lfG","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Comment","forum":"Bkl1uWb0Z","replyto":"rylV679xz","signatures":["ICLR.cc/2018/Conference/Paper691/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper691/Authors"],"content":{"title":"updated grammar induction evaluation","comment":"Thank you for your insightful comments, we failed to properly convey the intention and novelty of the work.  The field of NLP has long prioritized structured model representations under the assumption that there are a fundamental property of language and therefore often a prerequisite for language tasks.  A naive reading of recent research in NMT seems to direct contradict this age old belief. Our goal here was to investigate if the success and seeming necessity of attention mechanisms is related to their ability to capture these structural/hierarchical properties of language.  The gating mechanisms and gains in BLEU score exist in service of this exploration, so so we agree are in and of themselves perhaps more minor contributions.  For example, you're absolutely correct that some of the basic components of our model are shared with previous work (Kim, 17 and Liu 17), but the shared attention and gating syntax are crucial in our models and their resulting analysis.  We will try to make this clearer in the final version.\n\nWe originally ran our evaluation on Spacy’s outputs due to discrepancies between the MT tokens and tokenization and treebanks, but we have remedied this difference and updated all of our numbers to be evaluated against the Universal Dependencies treebanks.  The new results are in Table 3.  \n\nThere are two main results from Table 3: 1. Sharing attention appears to almost exclusively increase the model’s ability to capture syntax and 2. That structure attention generally outperforms flat attention if viewed through the same lense.  Almost secondary to these results is the fact that shared structured attention also benefits translation BLEU scores (updated with statistical significance).  This result does however hint that better modeling or inducing linguistic structure might further benefit translation performance.\n\nFinally, you are correct to question the inclusion of hard-attention.   While it is harmful for translation it appears to help grammar induction.  We hope that understanding this discrepancy and the possible (de-)coupling of the two metrics may lead to new results in future-work.  Maybe multiple syntactic analyses should be used as references instead of a single formalism?  In our experiments, hard-attention is deterministically computing by taking the max instead of sampling.\n\nWe will rework our example and fix typos like z_t which you are correct should be u_t.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1511914191288,"tcdate":1511914191288,"number":1,"cdate":1511914191288,"id":"HJwRq_ilz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Public_Comment","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Neat!","comment":"Do you have any examples of the structures learned with hard attention beside the tricky-to-read example in Figure 4?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1515642492873,"tcdate":1511828775829,"number":3,"cdate":1511828775829,"id":"rylV679xz","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Review","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Inducing grammars with and for NMT","rating":"5: Marginally below acceptance threshold","review":"This paper induces latent dependency syntax in the source side for NMT. Experiments are made in En-De and En-Ru.\n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017). In light of this, I see very little novelty in this paper. The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word. Seems thin for a ICLR paper.\n\nCaption of Fig 1: \"subject/object\" are syntactic functions, not semantic roles.\n\nI don't see how the German verb \"orders\" inflects with gender... Can you post the gold German sentence?\n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.\n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper.\n\nWhy is hard attention (sec 3.3) necessary? It's not differentiable and requires sampling for training. This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).\n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing.\n\nIn table 3, it would be very helpful to display the English source.\n\nTable 4 is confusing. The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?\n\nComparison with predicted parses by Spacy are by no means \"gold\" parses...\n\nMinor comments:\n- Sec 1: \"... optimization techniques like Adam, Attention, ...\" -> Attention is not an optimization technique, but part of a model\n- Sec 1: \"abilities not its representation\" -> comma before \"not\"\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1515642492913,"tcdate":1511807624162,"number":2,"cdate":1511807624162,"id":"Hyg5qAtgf","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Review","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Worthwhile ideas give useful insight into latent graph structures for NMT; somewhat weak results.","rating":"6: Marginally above acceptance threshold","review":"This paper describes a method to induce source-side dependency structures in service to neural machine translation. The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited). This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word’s soft head (called Shared Attention in the paper); and (3) they gate when attention is used. I feel that the first two ideas are particularly interesting. Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU. However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared=No in Table 2), there would be almost no gains at all. Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system’s latent dependency decisions.\n\nThis paper has some very good ideas, and asks questions that are very much worth asking. In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile. Unfortunately, this is mostly a negative result, with gains over “flat attention” being relatively small. I also like the “Shared Attention” - it makes a lot of sense to say that if the “semantic” attention mechanism has picked a particular word, one should also attend to that word’s head; it is not something I would have thought of on my own. The paper is also marred by somewhat weak writing, with a number of disfluencies and awkward phrasings making it somewhat difficult to follow.\n\nIn terms of specific criticisms:\n\nI found the motivation section to be somewhat weak. We need a better reason than morphology to want to do source-side dependency parsing. All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems.\n\nThe paper mentions “significant improvements” in only two places: the introduction and the conclusion. With BLEU score differences being so low, the authors should specify how statistical significance is measured; ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011).\nEquation (3): I couldn’t find the definition for H anywhere.\n\nSentence before Equation (5): I believe there is a typo here, “f takes z_i” should be “f takes u_t”.\n\nFirst section of Section 3: please cite the previous work you are talking about in this sentence.\n\nMy understanding was that the dependency marginals in p(z_{i,j}=1|x,\\phi) in Equation (11) are directly used as \\beta_{i,j}. If I’m correct, that’s probably worth spelling out explicitly in Equation (11): \\beta_{i,j} = p(z_{i,j}=1|x,\\phi) = …\n\nI don’t don’t feel like the clause between equations (17) and (18), “when sharing attention weights from the decoder with the encoder” is a good description of your clever “shared attention” idea. In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow.\n\nSection 4.4: It’s very very good that you compared to “flat attention”, but it’s too bad for everyone cheering for linguistically-informed syntax that the results weren’t better.\n\nTable 5: I had a hard time understanding Table 5 and the corresponding discussion. What are “production percentages”?\n\nFinally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1515642492956,"tcdate":1511404431694,"number":1,"cdate":1511404431694,"id":"SyOq7nXxf","invitation":"ICLR.cc/2018/Conference/-/Paper691/Official_Review","forum":"Bkl1uWb0Z","replyto":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference/Paper691/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Official review","rating":"3: Clear rejection","review":"This paper adds source side dependency syntax trees to an NMT model without explicit supervision. Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results.\n\nThe paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads. Please define what semantic attention is. You just introduce this concept without any explanation. I believe you mean standard attention, if so, please explain why standard attention is semantic.\n\nClarity. What is shared attention exactly? Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more. Also the example in Figure 3 is not very clear and did not help me in understanding this concept.\n\nResults. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure. Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms. The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations). It looks as if the improvement comes from adding additional capacity to the model. \n\nEquation 3: please define H.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]}},{"tddate":null,"ddate":null,"tmdate":1515167249288,"tcdate":1509132247949,"number":691,"cdate":1509739154518,"id":"Bkl1uWb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bkl1uWb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Inducing Grammars with and for Neural Machine Translation","abstract":"Previous work has demonstrated the benefits of incorporating additional linguistic annotations such as syntactic trees into neural machine translation. However the cost of obtaining those syntactic annotations is expensive for many languages and the quality of unsupervised learning linguistic structures is too poor to be helpful. In this work, we aim to improve neural machine translation via source side dependency syntax but without explicit annotation. We propose a set of models that learn to induce dependency trees on the source side and learn to use that information on the target side. Importantly, we also show that our dependency trees capture important syntactic features of language and improve translation quality on two language pairs En-De and En-Ru.","pdf":"/pdf/49c0ad913733a7171065db27c434a8cd07361668.pdf","TL;DR":"improve NMT with latent trees","paperhash":"anonymous|inducing_grammars_with_and_for_neural_machine_translation","_bibtex":"@article{\n  anonymous2018inducing,\n  title={Inducing Grammars with and for Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkl1uWb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper691/Authors"],"keywords":["structured attention","neural machine translation","grammar induction"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}