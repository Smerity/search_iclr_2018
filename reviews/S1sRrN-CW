{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222827221,"tcdate":1512077102815,"number":3,"cdate":1512077102815,"id":"BkwNvgRgf","invitation":"ICLR.cc/2018/Conference/-/Paper951/Official_Review","forum":"S1sRrN-CW","replyto":"S1sRrN-CW","signatures":["ICLR.cc/2018/Conference/Paper951/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"The paper proposes a new method to train knowledge base embeddings using a least-squares loss. For this purpose, the paper introduces a reweighting scheme of the entries in the original adjacency tensor. The reweighting is derived from an analysis of the cross-entropy loss. In addition, the paper discusses the connections of the margin and cross-entropy loss and evaluates the proposed method on WN18 and FB15k.\n\n The paper tackles an interesting problem, as learning from knowledge bases via embedding methods has become increasingly important for tasks such as question answering. Providing additional insight into current methods can be an important contribution to advance the state-of-the-art.\n\nHowever, I'm concerned about several aspects in the current form of the paper. For instance, the derivation in Section 4 is unclear to me, as eq.4 suddenly introduces a weighted sum over expectations using the degrees of nodes. The derivation also seems to rely on a very specific negative sampling assumption (uniform sampling without checking whether the corrupted triple is a true negative). This sampling method isn't used consistently across models and also brings its own problems, e.g., see the LCWA discussion in [4]\n\nIn addition, the semantics that are introduced by the weighting scheme are not clear to me either. Using the proposed method, the probability of edges between high-degree nodes are down-weighted, since the ground-truth labels are divided by the node degrees. Since these weighted labels are then fitted using a least-squares loss, this implies that links between high-degree nodes should be less likely, which seems the opposite of what the scores should look like.\n\nWith regard to the significance of the contributions: Using a least-squares loss in combination with tensor methods is attractive because it enables ALS algorithms with closed-form updates that can be computed very fast. However, the proposed method still relies on SGD optimization. In this context, it is not clear to me why a tensor framework/least-squares loss would be preferable.\n\nFurther comments:\n- The paper seems to equate \"tensor method\" with using a least squares loss. However, this doesn't have to be the case. For instance see [1,2] which propose Logistic and Poisson tensor factorizations, respectively.\n- The distinction between tensor factorization and neural methods is unclear. Tensor factorization can be interpreted just as a particular scoring function. For instance, see [5] for a detailed discussion.\n- The margin based ranking loss has been proposed earlier than in (Collobert et al, 2011). For instance see [3]\n- p1: corrupted triples are not described entirely correct, typically only one of s or o is corrputed. \n- Closed-form tensor in Table 1: This should be least-squares loss of f(s,p,o) and log(...)?\n- p6: Adding the constant to the tensor as proposed in (Levy & Goldberg, 2014) can done while gathering the minibatch and is therefore equivalent to the proposed approach.\n\n[1] Nickel et al: Logistic Tensor Factorization for Multi-Relational Data, 2013.\n[2] Chi et al: \"On tensors, sparsity, and nonnegative factorizations\", 2012\n[3] Collobert et al: A unified architecture for natural language processing, 2008\n[4] Dong et al: Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion, 2014\n[5] Nickel et al: A Review of Relational Machine Learning for Knowledge Graphs, 2016.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222827272,"tcdate":1512047591811,"number":2,"cdate":1512047591811,"id":"SyeeEtTef","invitation":"ICLR.cc/2018/Conference/-/Paper951/Official_Review","forum":"S1sRrN-CW","replyto":"S1sRrN-CW","signatures":["ICLR.cc/2018/Conference/Paper951/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"This paper deals with the problem of representation learning from knowledge bases (KB), given in form of subject-relationship-object triplets. The paper has two main contributions: (1) Showing that two commonly used loss functions, margin-based and negative sampling-based, are closely related to each other; and (2) many of the KB embedding approaches can be reduced to a tensor decomposition problem where the entries in the tensor are a certain transformation of the original triplets values. \n\nContribution (1) related to the connection between margin-based and negative sampling-based loss functions is sort of obvious in hindsight and I am not sure if it has been not recognized in prior work (I'm not very well-versed in this area). Regardless, even though this connection  is moderately interesting, I am not sure of its practical usefulness. I would like the authors to comment on this aspect.\n\nContribution (2) that shows that KB embedding approaches based on some of the popularly used loss functions such as margin-based or negative sampling can be cast as tensor factorization of a certain transformation of the original data is also interesting. However, similar connections have been studied for word-embedding methods. For example, prior work has shown that word embedding methods that optimize loss functions such as negative sampling can be seen as doing implicit matrix factorization of a transformed version of the word-counts. Therefore contribution (2) seems similar in spirit to this line of work.\n\nOverall, the paper does have some interesting insights but it is unclear if these insights are non-trivial/surprising, and are of that much practical utility. I would like to authors to respond to these concerns.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222827312,"tcdate":1511895056605,"number":1,"cdate":1511895056605,"id":"BytzlNjez","invitation":"ICLR.cc/2018/Conference/-/Paper951/Official_Review","forum":"S1sRrN-CW","replyto":"S1sRrN-CW","signatures":["ICLR.cc/2018/Conference/Paper951/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"The paper proposes a unified view of multiple methods for learning knowledge base embeddings.\n\nThe paper's motivations are interesting but the execution does fit standard for a publication at ICLR.\nMain reasons:\n* Section 3 does not bring much value. It is a rewriting trick that many knew but never thought of publishing\n* Section 4.1 is either incorrect or clearly misleading. What happens to the summation terms related to the negative samples (o~=o' and s!=s') between the last equation and the 2 before that (on the expectations) at the bottom of page 4? They vanished while they are depending on the single triple (s, r, o), no?\n* The independence assumption at the top of page 5 is indeed clearly too strong in the case of multi-relational graphs, where triples are all interconnected.\n* In 4.2, writing that both RESCAL and KBTD explain a RDF triple through a similar latent form is not an observation that could explain intrinsic similarities between the methods but the direct consequence of the deliberate choice made for f(.) at the line before.\n* The experiments are hard to use to validate the model because they are based on really outdated baselines. Most methods in Table 4 and 5 are performing well under their best known performance.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511963280603,"tcdate":1511833004700,"number":4,"cdate":1511833004700,"id":"r1S26N5lG","invitation":"ICLR.cc/2018/Conference/-/Paper951/Public_Comment","forum":"S1sRrN-CW","replyto":"HJhXGX8AW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"You chose to use old models which maybe easy to beat","comment":"Your baseline models are old and maybe easy to beat. So I do not know your approach is good or not when comparing with strong baselines."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511831950424,"tcdate":1511831950424,"number":3,"cdate":1511831950424,"id":"BkU9F4qlf","invitation":"ICLR.cc/2018/Conference/-/Paper951/Public_Comment","forum":"S1sRrN-CW","replyto":"HkjsLOd0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"On FB15k-237 and WN18RR","comment":"Toutanova et al. [1] firstly showed the problem, but the authors of ConvE firstly showed SOTA results on FB15k and WN18 by using a simple reversal rule. It's fine if we don't know this. Otherwise, the question is why we use the models that are outperformed by this simple reversal rule on FB15k and WN18???\nFB15k-237 and WN18RR should become the main datasets for the link prediction task."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509619363323,"tcdate":1509619363323,"number":2,"cdate":1509619363323,"id":"HkjsLOd0W","invitation":"ICLR.cc/2018/Conference/-/Paper951/Public_Comment","forum":"S1sRrN-CW","replyto":"Hkw2SfERW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"On FB15k and WN18","comment":"I'm not an author but felt like responding to the first comment. I agree with the statement that the paper is missing SOTA results. With some minor tuning of TransE and DistMult one can achieve *much* better numbers than the one reported in the paper. For instance, on FB15k it is possible to get up to 90 hits@10 with DistMult and 76.x with TransE. That's clearly a weak point of the paper irrespective of the theoretical analysis which might or might not be insightful. I haven't looked at this part of the paper at all.\n\nI disagree with the comment on FB15k and WN18. First, the \"problems\" with FB15k and WN18 weren't first mentioned by the ConvE authors but much earlier by Toutanova et al [1]. The identified \"problems\" relate to the existence of reverse relations that allow simple baselines to perform better than most (at the time) existing KB embedding methods. For example, the existence of (A, parentOf, B) predicts with high probability the relation (B, childOf, A). However, I disagree with the conclusion that these two data sets should not be used anymore. There are still plenty of challenging completion queries in these data sets. Also, FB15k is a data set derived from a human-designed and populated knowledge base. It is somewhat absurd to now exclusively use artificially created KBs to evaluate KB completion methods. FB15k-237 and WN18RR are data sets that should be included in addition to FB15k and WN18.\n\n\n[1] Observed Versus Latent Features for Knowledge Base and Text Inference. Kristina Toutanova and Danqi Chen."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092431411,"tcdate":1509466660293,"number":1,"cdate":1509466660293,"id":"HJhXGX8AW","invitation":"ICLR.cc/2018/Conference/-/Paper951/Official_Comment","forum":"S1sRrN-CW","replyto":"Hkw2SfERW","signatures":["ICLR.cc/2018/Conference/Paper951/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper951/Authors"],"content":{"title":"The purpose of this paper is not to beat SOTA results","comment":"Dear Readers:\n\nWe really appreciate your comments.\n\nFor your first suggestion, we certainly noticed there are many other methods that generated superior performance on WN18 and FB15k, and we also mentioned the ProjE paper in our related work. However, the purpose of this paper is not to design the state-of-the-art methods, and we did not propose any new scoring functions. Instead, we provided the theoretical analysis on a few popular methods, and revealed that all the mentioned methods could be framed into a tensor decomposition framework. For the experiments, we are still using the same scoring functions as proposed in the mentioned papers. The purpose of the experiments is to achieve comparable results under this tensor decomposition framework. Our framework is flexible about various scoring functions. \n\nFor your second suggestion, yes, we also noticed that there is FB15k-237 dataset, and thank you for pointing us the WN18RR dataset. The reason we stick with the original FB15k and WN18 datasets is for fair comparison since most of the above methods used these two datasets in their experiments. Again, we do not aim to beat one certain score function in a certain dataset. Instead, we want to generalize the existing KB embedding models, and further help the research community understand these models. We are open to work on more datasets in our future experiments though.\n\nHope the response above clarified your questions.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509332967592,"tcdate":1509332399531,"number":1,"cdate":1509332399531,"id":"Hkw2SfERW","invitation":"ICLR.cc/2018/Conference/-/Paper951/Public_Comment","forum":"S1sRrN-CW","replyto":"S1sRrN-CW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Not mention other SOTA results","comment":"As shown in papers such as ProjE: Embedding Projection for Knowledge Graph Completion, Knowledge Base Completion: Baselines Strike Back and Convolutional 2D Knowledge Graph Embeddings, your results are not state-of-the-art results on WN18 and FB15k. You should mention other high published results in your paper.\nThe authors in the paper Convolutional 2D Knowledge Graph Embeddings analyzed and concluded that future research on knowledge base completion should not use WN18 and FB15k anymore. You should do experiments on WN18RR and FB15k-237 datasets."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]}},{"tddate":null,"replyto":null,"ddate":null,"tmdate":1510092378698,"tcdate":1509144019374,"number":951,"cdate":1510092359004,"id":"S1sRrN-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1sRrN-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Revisiting Knowledge Base Embedding as Tensor Decomposition","abstract":"We study the problem of knowledge base (KB) embedding, which is usually addressed through two frameworks---neural KB embedding and tensor decomposition. In this work, we theoretically analyze the neural embedding framework and subsequently connect it with tensor based embedding. Specifically, we show that in neural KB embedding the two commonly adopted optimization solutions---margin-based and negative sampling losses---are closely related to each other. We also reach the closed-form tensor that is implicitly approximated by popular neural KB approaches, revealing the underlying connection between neural and tensor based KB embedding models. Grounded in the theoretical results, we further present a tensor decomposition based framework KBTD to directly approximate the derived closed form tensor. Under this framework, the neural KB embedding models, such as NTN, TransE, Bilinear, and DISTMULT, are unified into a general tensor optimization architecture. Finally, we conduct experiments on the link prediction task in WordNet and Freebase, empirically demonstrating the effectiveness of the KBTD framework. \n","pdf":"/pdf/4e9e3d851b60e8aa75b53c344e0ed3988c5300fa.pdf","paperhash":"anonymous|revisiting_knowledge_base_embedding_as_tensor_decomposition","_bibtex":"@article{\n  anonymous2018revisiting,\n  title={Revisiting Knowledge Base Embedding as Tensor Decomposition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1sRrN-CW}\n}","keywords":["Knowledge base embedding"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper951/Authors"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"OpenReview.net"}],"limit":2000,"offset":0}