{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222700061,"tcdate":1511901212961,"number":3,"cdate":1511901212961,"id":"rkHX_Bjlf","invitation":"ICLR.cc/2018/Conference/-/Paper603/Official_Review","forum":"SyqAPeWAZ","replyto":"SyqAPeWAZ","signatures":["ICLR.cc/2018/Conference/Paper603/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Official review","rating":"4: Ok but not good enough - rejection","review":"The method proposes a new architecture for solving image super-resolution task. They provide an analysis that connects aims to establish a connection between how CNNs for solving super resolution and solving sparse regularized inverse problems.\n\nThe writing of the paper needs improvement. I was not able to understand the proposed connection, as notation is inconsistent and it is difficult to figure out what the authors are stating. I am willing to reconsider my evaluation if the authors provide clarifications.\n\nThe paper does not refer to recent advances in the problem, which are (as far as I know), the state of the art in the problem in terms of quality of the solutions. This references should be added and the authors should put their work into context.\n\n1) Arguably, the state of the art in super resolution are techniques that go beyond L2 fitting. Specifically, methods using perceptual losses such as:\n\nJohnson, J. et al \"Perceptual losses for real-time style transfer and super-resolution.\" European Conference on Computer Vision. Springer International Publishing, 2016.\n\nLedig, Christian, et al. \"Photo-realistic single image super-resolution using a generative adversarial network.\" arXiv preprint arXiv:1609.04802 (2016).\n\nPSNR is known to not be directly related to image quality, as it favors blurred solutions. This should be discussed.\n\n2) The overall notation of the paper should be improved. For instance, in (1), g represents the observation (the LR image), whereas later in the text, g is the HR image. \n\n3) The description of Section 2.1 is quite confusing in my view. In equation (1), y is the signal to be recovered and K is just the downsampling plus blurring. So assuming an L1 regularization in this equation assumes that the signal itself is sparse. Equation (2) changes notation referring y as f. \n\n4) Equation (2) seems wrong. The term multiplying K^T is not the norm (should be parenthesis).\n\n5) The first statement of Section 2.2. seems wrong. DL methods do state the super resolution problem as an inverse problem. Instead of using a pre-defined basis function they learn an over-complete dictionary from the data, assuming that natural images can be sparsely represented. Also, this section does not explain how DL is used for super resolution. The cited work by Yang et al learns a two coupled dictionaries (one for LR and HL), such that for a given patch, the same sparse coefficients can reconstruct both HR and LR patches. The authors just state the sparse coding problem.\n\n6) Equation (10) should not contain the \\leq \\epsilon.\n\n7) In the second paragraph of Section 3, the authors mention that the LR image has to be larger than the HR image to prevent border effects. This makes sense. However, with the size of the network (20 layers), the change in size seems to be quite large. Could you please provide the sizes? When measuring PSNR, is this taken into account? \n\n8) It would be very helpful to include an image explaining the procedure described in the second paragraph of Section 3.\n\n9) I find the description in Section 3 quite confusing. The authors relate the training of a single filter (or neuron) to equation (7), but they define D, that is not used in all of Section 2.1. And K does not show in any of the analysis given in the last paragraph of page 4. However, D and K seem two different things (it is not just one for the other), see bellow.\n\n10) I cannot understand the derivation that the authors do in the last paragraph of page 4 (and beginning of page 5). What is phi_l here? K in equation (7) seems to match to D here, but D here is a collection of patches and in (7) is a blurring and downsampling operator. I cannot review this section. I will wait for the author's response clarifications.\n\n11) The authors describe a change in roles between the representations and atoms in the training and testing phase respectively. I do not understand this. If I understand correctly, the final algorithm, the authors train a CNN mapping LR to HR images. The network is used in the same way at training and testing.\n\n12) It would be useful to provide more details about the training of the network. Please describe the training set used by Kim et al. Are the two networks trained independently? One could think of fine-tuning them jointly (including the aggregation).\n\n13) The authors show the advantage of separating networks on a single image, Barbara. It would be good to quantify this better (maybe in terms of PSNR?). This observation might be true only because the training loss, say than the works cited above. Please comment on this.\n\n14) In figures 3 and 4, the learned filters are those on the top (above the yellow arrow). It is not obvious to me that the reflect the predominant structure in the data. (maybe due to the low resolution).\n\n15) This work is related to (though clearly different)  that of LISTA (Learned ISTA) type of networks, proposed in:\n\nGregor, K., & LeCun, Y. (2010). Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML) \n\nWhich connect the network architecture with the optimization algorithm used for solving the sparse coding problem. Follow up works have used these ideas for solving inverse problems as well.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CNNs as Inverse Problem Solvers and Double Network Superresolution","abstract":"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR). In this paper we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. We prove a single neuron is able to provide the optimum solution for inverse problems. Intoducing a new concept called  representation Dictionary Duality we show that CNN layers act as sparse representation solvers. In the light of theoretical work we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and prove that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","pdf":"/pdf/44aeab521d58fcfa64a224897d63425a44a5622a.pdf","TL;DR":"After proving that a neuron acts as an inverse problem solver for superresolution and a network of neurons is guarantied to provide a solution, we proposed a double network architecture that performs faster than state-of-the-art.","paperhash":"anonymous|cnns_as_inverse_problem_solvers_and_double_network_superresolution","_bibtex":"@article{\n  anonymous2018cnns,\n  title={CNNs as Inverse Problem Solvers and Double Network Superresolution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqAPeWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper603/Authors"],"keywords":["superresolution","convolutional neural network","sparse representation","inverse problem"]}},{"tddate":null,"ddate":null,"tmdate":1512222700102,"tcdate":1511747655773,"number":2,"cdate":1511747655773,"id":"rke8ggtxG","invitation":"ICLR.cc/2018/Conference/-/Paper603/Official_Review","forum":"SyqAPeWAZ","replyto":"SyqAPeWAZ","signatures":["ICLR.cc/2018/Conference/Paper603/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of: CNNs as Inverse Problem Solvers and Double Network Superresolution","rating":"3: Clear rejection","review":"This paper discusses using neural networks for super-resolution. The positive aspects of this work is that the use of two neural networks in tandem for this task may be interesting, and the authors attempt to discuss the network's behavior by drawing relations to successful sparsity-based super-resolution. Unfortunately I cannot see any novelty in the relationship the authors draw to LASSO style super-resolution and dictionary learning beyond what is already in the literature (see references below), including in one reference that the authors cite. In addition, there are a number of sloppy mistakes (e.g. Equation 10 as a clear copy-paste error) in the manuscript. Given that much of the main result seems to already be known, I feel that this work is not novel enough at this time. \n\nSome other minor points for the authors to consider for future iterations of this work:\n\n- The authors mention the computational burden of solving L1-regularized optimizations. A lat of work has been done to create fast, efficient solvers in many settings (e.g. homotopy, message passing etc.). Are these methods still insufficient in some applications? If so, which applications of interest are the authors considering?\n\n- In figure 1, it seems that under \"superresolution problem\": 'f' should be 'High res data' and 'g' should be 'Low res data' instead of what is there. I'm also not sure how this figure adds to the information already in the text.\n\n- In the results, the authors mention how some network features represented by certain neurons resemble the training data. This seems like over-training and not a good quality for generalization. The authors should clarify if, and why, this might be a good thing for their application. \n\n- Overall a heavy editing pass is needed to fix a number of typos throughout.\n\nReferences:\n\n[1] K. Gregor and Y. LeCun , “Learning fast approximations of sparse coding,” in Proc. Int. Conf. Mach. Learn., 2010, pp. 399–406.\n[2] P. Sprechmann, P. Bronstein, and G. Sapiro, “Learning efficient structured sparse models,” in Proc. Int. Conf. Mach. Learn., 2012, pp. 615–622.\n[3] M. Borgerding, P. Schniter, and S. Rangan, ``AMP-Inspired Deep Networks for Sparse Linear Inverse Problems [pdf] [arxiv],\" IEEE Transactions on Signal Processing, vol. 65, no. 16, pp. 4293-4308, Aug. 2017.\n[4] V. Papyan*, Y. Romano* and M. Elad, Convolutional Neural Networks Analyzed via Convolutional Sparse Coding, accepted to Journal of Machine Learning Research, 2016. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CNNs as Inverse Problem Solvers and Double Network Superresolution","abstract":"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR). In this paper we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. We prove a single neuron is able to provide the optimum solution for inverse problems. Intoducing a new concept called  representation Dictionary Duality we show that CNN layers act as sparse representation solvers. In the light of theoretical work we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and prove that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","pdf":"/pdf/44aeab521d58fcfa64a224897d63425a44a5622a.pdf","TL;DR":"After proving that a neuron acts as an inverse problem solver for superresolution and a network of neurons is guarantied to provide a solution, we proposed a double network architecture that performs faster than state-of-the-art.","paperhash":"anonymous|cnns_as_inverse_problem_solvers_and_double_network_superresolution","_bibtex":"@article{\n  anonymous2018cnns,\n  title={CNNs as Inverse Problem Solvers and Double Network Superresolution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqAPeWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper603/Authors"],"keywords":["superresolution","convolutional neural network","sparse representation","inverse problem"]}},{"tddate":null,"ddate":null,"tmdate":1512222700147,"tcdate":1511636094877,"number":1,"cdate":1511636094877,"id":"rkDK2NwgG","invitation":"ICLR.cc/2018/Conference/-/Paper603/Official_Review","forum":"SyqAPeWAZ","replyto":"SyqAPeWAZ","signatures":["ICLR.cc/2018/Conference/Paper603/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting paper bringing up different domains. It could be written more reader friendly.","rating":"6: Marginally above acceptance threshold","review":"The paper proposes an understanding of the relation between inverse problems, CNNs and sparse representations. Using the ground work for each proposes a new competitive super resolution technique using CNNs. Overall I liked authors' endeavors bringing together different fields of research addressing similar issues. However, I have significant concerns regarding how the paper is written and final section of the proposed algorithm/experiments etc.  \n\nIntroduction/literature review-> I think paper significantly lacks literature review and locating itself where the proposed approach at the end stands in the given recent SR literature (particularly deep learning based methods) --similarities to other techniques, differences from other techniques etc. There have been several different ways of using CNNs for super resolution, how does this paper’s architecture differs from those? Recent GAN based methods are very promising and how does the proposed technique compares to them? \n\nNotation/readability -> I do respect the author’s mentioning different research field’s notations and understand the complication of building a single framework. However I still think that notations could be a lot more simplified—to make them look in the same page. It is very confusing for readers even if you know the mentioned sub-fields and their notations. Figure 1 was very useful to alleviate this problem. More visuals like figure 1 could be used for this problem. For example different network architecture figures (training/testing for CNNs) could be used to explain in a compact way instead of plain text. \n\nSection 3-> I liked the way authors try to use the more generalized Daubechies et. al. However I do not understand lots of pieces still. For example using the low resolution image patches as a basis—more below. In the original solution Daubechies et. al. maps data to the orthonormal Hilbert space, but authors map to the D (formed by LR patches). How does this affect the provability? \n\nRepresentation-dictionary duality concept -> I think this is a very fundamental piece for the paper and don’t understand why it is in the appendix. Using images as D in training and using filters as D in scoring/testing, is very unintuitive to me. Even after reading second time. This requires better discussion and examples. Comparison/discussion to other CNN/deep learning usage for super-resolution methods is required exactly right here.\n\nFinal proposed algorithm -> Splitting the data for high and low coherence makes sense however coherence is a continues variable. Why to keep the quantization at binary? Why not 4,8 or more? Could this be modeled in the network?\n\nResults -> I understand the numerical results and comparisons to the Kim et. Al—and don’t mind at all if they are on-par or slightly better or worse. However in super-resolution paper I do expect a lot more visual comparisons. There has been only Figure 5. Authors could use appendix for this purpose. Also I would love to understand why the proposed solution is significantly faster. This is particularly critical in super-resolution as to apply the algorithms to videos and reconstruction time is vital.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"CNNs as Inverse Problem Solvers and Double Network Superresolution","abstract":"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR). In this paper we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. We prove a single neuron is able to provide the optimum solution for inverse problems. Intoducing a new concept called  representation Dictionary Duality we show that CNN layers act as sparse representation solvers. In the light of theoretical work we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and prove that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","pdf":"/pdf/44aeab521d58fcfa64a224897d63425a44a5622a.pdf","TL;DR":"After proving that a neuron acts as an inverse problem solver for superresolution and a network of neurons is guarantied to provide a solution, we proposed a double network architecture that performs faster than state-of-the-art.","paperhash":"anonymous|cnns_as_inverse_problem_solvers_and_double_network_superresolution","_bibtex":"@article{\n  anonymous2018cnns,\n  title={CNNs as Inverse Problem Solvers and Double Network Superresolution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqAPeWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper603/Authors"],"keywords":["superresolution","convolutional neural network","sparse representation","inverse problem"]}},{"tddate":null,"ddate":null,"tmdate":1509739207158,"tcdate":1509128145722,"number":603,"cdate":1509739204492,"id":"SyqAPeWAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyqAPeWAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"CNNs as Inverse Problem Solvers and Double Network Superresolution","abstract":"In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR). In this paper we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. We prove a single neuron is able to provide the optimum solution for inverse problems. Intoducing a new concept called  representation Dictionary Duality we show that CNN layers act as sparse representation solvers. In the light of theoretical work we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and prove that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.","pdf":"/pdf/44aeab521d58fcfa64a224897d63425a44a5622a.pdf","TL;DR":"After proving that a neuron acts as an inverse problem solver for superresolution and a network of neurons is guarantied to provide a solution, we proposed a double network architecture that performs faster than state-of-the-art.","paperhash":"anonymous|cnns_as_inverse_problem_solvers_and_double_network_superresolution","_bibtex":"@article{\n  anonymous2018cnns,\n  title={CNNs as Inverse Problem Solvers and Double Network Superresolution},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyqAPeWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper603/Authors"],"keywords":["superresolution","convolutional neural network","sparse representation","inverse problem"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}