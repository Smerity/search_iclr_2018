{"notes":[{"tddate":null,"ddate":null,"tmdate":1516450578519,"tcdate":1516450025058,"number":7,"cdate":1516450025058,"id":"Hkblb3erG","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Comment","forum":"Sy33KapTb","replyto":"ByQd-bhJz","signatures":["ICLR.cc/2018/Conference/Paper79/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper79/Authors"],"content":{"title":"Thank you for your careful readings and valuable comments","comment":"We thank you for your careful readings and valuable comments. Your comments are valuable for improving our paper and research. \n\nFirst, we are sorry for sloppy mathematical presentation. For the ease of understanding, in the new manuscript, section 3, paragraph 1, we illustrate the notations in detail. In the new manuscript, all revised texts are highlighted in red. Then, we address the major concerns below:\nQ1: $\\theta$ looks to be scalar in line 1, but you earlier said it was a vector; also should $\\theta$ be indexed by $d$?\nA1: We are sorry for describing it unclear. $\\theta$ should be indexed by $d$ and is a K-dimension vector. We have revised it.\n\nQ2: from 2(b) I presume $s_n$ is a vector but appears to be a scalar in 2(a)\nA2: We are sorry for describing it unclear. The word code of word $n$ in the document $d$ should be indexed by $d, n$. $s_{d, n}$ is also a K-dimension vector. We have revised the error.\n\nQ3: what is a supergaussian, and what are the 2 parameters for it? my only thought is that it is a scalar distribution e^{-|x|^c} for 1<c<2 and x>0, but how does $\\theta$ fit in?\nA3: In order to achieve sparse word codes, STC defines $p(s_{d, n}|\\theta_d)$ as a product of two component distributions p(s_{d, n}|\\theta_d, \\gamma)p(s_{d, n}|\\rho), where p(s_{d, n}|\\theta_d, \\gamma) is an isotropic Gaussian distribution $N(\\theta, \\gamma^{-1})$, p(s_{d,n}|\\rho) is a Laplace distribution Laplace(0, \\rho^{-1}). The composite distribution is super-Gaussian, p(s_{d, n}|\\theta_d) \\propto exp(-\\gamma||s_{d, n}-\\theta_d||_2^2-\\rho|s|). We have illustrate this in the new manuscript, section 3.1.\n\nQ4: if $s_n$ is scalar, the whole distribution is too trivial to be a topic model\nA4: We are sorry for describing it unclear. The word code $s_{d, n}$ is a K-dimension vector.\n\nQ5: $\\beta$ is a topic dictionary, I presume a matrix, what then is $\\beta_n$ ... I'm guessing a $K$ dim vector.\nA5: We are sorry for describing it unclear. $\\beta$ is a $N * K$ matrix, and $\\beta_n$ is a $K$ dim vector.\n\nQ6: is $n$ the $n$-th word in the document or the word with dictionary index $n$ ... very different!\nA6: $n$ is the $n$-th word in the document.\n\nQ7: I gather when you say $s_n \\sim U(....)$ you really mean $s_{d,k} \\sim U(....)$, since $s_n$ has to be $K$ dimensional. Generally, a lot more care needed with definition of distributions.\nA7: Thank you for reminding it. We have corrected this.\n\nQ8: Note STC was a UAI paper. You have it listed as CoRR.\nA8: Thank you for reminding it. We have corrected this.\n\nQ9: Line 4 you present how to collapse the doc code. I'm lost by this. Doesn't that make $\\theta_d$ a scalar? Shouldn't it be a vector? Or did you mean $\\theta_k$? What is $s_{d,nk}$ mean? What is $nk$? Presume you meant $s_{d,k}$. Regardless, I don't see how the super-Gaussian and the Poisson (2a) and 2b)) can be collapsed to give a simple summation estimate for $\\theta$. Basically, I don't believe Bai et als, formula is strictly correct, though it’s probably a good summary statistic.\nA9: The document code is removed from our model and view the document code as a natural by-product, as the document code can be generated via a simple aggregation of the individual codes of all its words on each topic, which not only has been proven by Bai et als, but also illustrated in the STC paper. With the removing of the document code, the word codes are no longer sampled from the composite distribution.\n\nQ10: \"To yield sparse word codes, we choose the Laplace distribution as the variational distribution of word codes, \" ... sorry, I am confused. Either word codes have a uniform distribution or they have a Laplace distribution, they cannot have both. The variational encoder is just an inference algorithm ... it should change what the prior is.\nA10: Thank you for reminding it. We have revised this. In the generative process, we replace the super-Gaussian with a uniform distribution. In the inference process, we adopt the variational posterior $Laplace(s_{d,nk};0,\\sigma_{d,nk}(w_{d,n}))$ to approximate the intractable posterior $p(s_{d,nk}|w_{d,n})$ for the sparse word codes, where $\\sigma_{d,nk}$ is the scale parameter of Laplace distribution.\nWe thank again for your constructive comments. Your comments are valuable for improving our paper and research."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1514359516103,"tcdate":1514359516103,"number":3,"cdate":1514359516103,"id":"HkNkspgmz","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Comment","forum":"Sy33KapTb","replyto":"ByQd-bhJz","signatures":["ICLR.cc/2018/Conference/Paper79/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper79/Authors"],"content":{"title":"Response to Reviewer 3","comment":"We thank you for your careful readings and valuable comments. Your comments are valuable for improving our paper and research.  In the new manuscript, all revised texts are highlighted in blue. We address the major concerns below:\n\nQ1: a lot of recent work here, barely mentioned, other deep neural topic models, traditional topic models effectively using word embeddings.\nA1: Thank you for the valuable advice. We have added the introduction and related work about word embedding as prior knowledge in topic models in section 1, paragraph 2, and section 2, paragraph 2. We also included “Gaussian LDA” and “DocNADE” as our baselines, and have updated the experimental results in our new manuscript section 4.\n\nQ2: Table 2 looks very informative, but the words in the table are illegible.\nA2: We are sorry for it. We have shrunk table 2 (which have been modified to table 4 in the new manuscript) to make it more clear.\n\nQ3: Please properly capitalise some words in the references.\nA3: We are sorry for this mistake. The mistake is corrected in our new manuscript.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1514359628328,"tcdate":1514359109654,"number":2,"cdate":1514359109654,"id":"BJ0rYpeQf","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Comment","forum":"Sy33KapTb","replyto":"SynN9CuxG","signatures":["ICLR.cc/2018/Conference/Paper79/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper79/Authors"],"content":{"title":"Response to Reviewer 1","comment":"We thank you for your careful readings and valuable comments. Your comments are valuable for improving our paper and research.  In the new manuscript, all revised texts are highlighted in blue.  We address the major concerns below:\n\nA1: It means the document code is removed from our model as in [1], as the document code can be generated via a simple aggregation of the individual codes of all its words on each topic. We detail it in our new manuscript, section 3.2, line 3. We don’t remove the prior of word codes, but choose the uniform distribution as the prior of word codes, and the Laplace distribution as its variational posterior. The reason for the choice is also detailed in section 3.2.\n\nA2: We are sorry for describing them unclear. We constraint the word codes to be non-negative via applying the relu activation function on the output s of encoder. We also use the simplex projection on each column of topic dictionary which is the same as the sparsemax activation function in [2], which  declares how the Jacobian of the projection can be efficiently computed, providing the theoretical base of its employment in a neural network trained with backpropagation. After the simplex projection, each column of the topic dictionary is promised to be sparse, non-negative and united.\n\nA3: We apologize for the mistake in equation (5), and have updated the correct equation in the new manuscript.\n\nA4: According to STC, the topic dictionary is corresponding to the parameter of topic word distribution \\beta in LDA, which is treated as a common variable rather than a hidden variable, and optimized in the M-step of variational EM. \n\nA5: Thank you for reminding it. The equation 3 refers to the lower bound of the marginal log likelihood. We have corrected the mistake.\n\nA6: We are sorry for describing them unclear. We don’t use the default parameters . We have detail describe the parameter settings for the baselines in section 4.1.\n\nA7: Thank you for pointing it out. We have corrected it in our new manuscript (section 3.2).\n\n[1] Lu Bai, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. Group sparse topical coding: from code to topic. In Proceedings of the sixth ACM international conference on Web search and data mining, pp. 315–324. ACM, 2013.\n[2] Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In International Conference on Machine Learning, pp. 1614–1623, 2016."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1514359592681,"tcdate":1514358775606,"number":1,"cdate":1514358775606,"id":"ryx-d6e7z","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Comment","forum":"Sy33KapTb","replyto":"rJNnP3Fgz","signatures":["ICLR.cc/2018/Conference/Paper79/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper79/Authors"],"content":{"title":"Response to Reviewer 2","comment":"We thank you for your constructive comments. Your comments are valuable for improving our paper and research.  In the new manuscript, all revised texts are highlighted in blue.  We address the major concerns below:\n\nA1: Rather than the word-occurrence information, NVSTM treats the word count as the input feature to learn the sparse word code and topic dictionary as STC. We remove the document code to simplify the structure of our model according to [1], as the document code can obtain via a simple aggregation of the individual codes of all its words on each topic. We further detail it in our new manuscript, section 3.2, line 3.\n\nA2: We are sorry for describing them unclear. A detail description of them is provided in the new manuscript section 3.3, paragraph 1. Moreover, the prior p(s_n) is a uniform distribution U (-0.5,0.5) which is wrongly presented as Laplace distribution in our previous manuscript, Figure 1.  We have corrected it. To yield sparse word codes, we choose the Laplace distribution as the variational distribution of word codes, whose base or noise distribution is uniform distribution according to the reparameterisation method. \n\nA3: The NVSTM utilizes the word count information rather than the word co-occurrence information and sparsity mechanism in learning representations as STC, which make it more effective to short texts than other topic models. Generally, the assumptions of topic models aiming at short texts are also suitable for long texts, but significant improvement in performance with limited word co-occurrence information.\n\nA4: Thank you for the valuable advice. We have appended the introduction and related work about short-text and word embedding as prior knowledge in topic models in section 1, paragraph 2, and section 2, paragraph 2. We also include “Gaussian LDA” and “DocNADE” as our baselines, whose models incorporate word embeddings with topic models. We have updated the experimental results in our new manuscript section 4.\n\nA5: Thank you for the valuable advice. We have added two short text datasets “Biomedical” and “BBC”. The detail results are updated in the new manuscript section 4.\n\nA6: We set the size of hidden layer above the output layer to the number of topics, and the size of other hidden layers in encoder and decoder network to 500 as we described in section 4.1, the last paragraph.\n\nA7: Thank you for the valuable advice. We have presented the parameter settings for the baselines in section 4.1.\n\n[1] Lu Bai, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. Group sparse topical coding: from code to topic. In Proceedings of the sixth ACM international conference on Web search and data mining, pp. 315–324. ACM, 2013.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1515642511486,"tcdate":1511798700688,"number":3,"cdate":1511798700688,"id":"rJNnP3Fgz","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Review","forum":"Sy33KapTb","replyto":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference/Paper79/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice topic, but inadequate investigation","rating":"3: Clear rejection","review":"The paper presents a novel model called Neural Variational Sparse Topic Model (NVSTM), that captures word-level sparse representation by employing a Laplacian prior, and enriches semantics by using external word embeddings. The experimental results have shown that NVSTM yields good results in terms of document classification and sparse word representation against 3 related models. \n\nThere are several issues that the authors should address in the future.\n-\tWord co-occurrence information is very important to shape topics. The paper also mentioned about that.  However, in model perspective, it is surprised that NVSTM does not have any variables (or hyper-parameters) to model each document. It means that NVSTM seems not to capture the word co-occurrence information. \n-\tThe paper only uses the variational autoencoder (VAE) to do inference for NVSTM. The authors should detail the approximate posterior (q), variational parameters $\\Theta$, model parameters $\\Phi$, and objective function $L$ in NVSTM. It is confusing when the encoder’s distribution q(s_n) and the prior p(s_n) of the latent variable $s_n$ are both Laplace(0,b_n)  with the same parameter. There might be some typos here. \n-\tThe assumptions of NVSTM are not different between short and normal text. Why does the paper target at short-text? \n-\tThe paper should survey more about short-text and word embedding as prior knowledge in topic models. The authors should discuss related work, and take some baselines which use word embedding as prior into comparison.\n-\tThe authors should do experiment on more datasets to have higher confidence. Many short and normal text datasets are freely available. \n-\tIn the experiment settings, what is the size of the hidden layer? Is it the number of topics $K$? \n-\tThe settings for the baselines were not described in the paper. To make a fair comparison, the settings for the baselines should be chosen thoughtfully.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1515642511522,"tcdate":1511742004309,"number":2,"cdate":1511742004309,"id":"SynN9CuxG","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Review","forum":"Sy33KapTb","replyto":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference/Paper79/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The manuscript needs to be clarified more precisely.","rating":"3: Clear rejection","review":"This paper proposes a neural variational sparse topic model to model topics of short text via variational auto encoding and sparse coding. \n\nOverall, many parts of the paper need to be further clarified or justified. I tried to list some major and minor concerns/comments below:\n\n* 'Collapsed document code \\theta': In section 3.2, I'm not sure what 'collapse the document code' means, but from the context, I assume that it means removing prior of the word codes. This is still a valid choice, however, there are many pieces of evidence that removing prior of word codes may lead to a serious overfitting problems such as the case of pLSI. Maybe the structure of the variational distribution relaxes the overfitting problem, but even it's the case, this choice is not justified well in the text. Additionally, if we remove the prior over word codes, then the inference can be tractable in some way I guess, which means we may not need the variational approximation method.\n\n* The word codes need to be non-negative to obtain distributions in Table 2, however, it is not clear how this non-negativity can be obtained during the inference procedure. In the STC, the authors made a strong assumption of non-negativity on document and word codes in order to keep the interpretability of the model. As a consequence, they had to rely on some property of convex function during training, while I cannot find these detail in the current manuscript. In a similar sense, which projection method has been used to project topic dictionary after relu layer? Did you normalise the vector using the sum of elements?\n\n* Reparameterisation: From Equation (5) I found that the natural log can take a negative value if the epsilon is greater than 0.5, over which the log is undefined. In addition, why do you need sign function since the epsilon is always positive by definition?\n\n* Non-stochastic beta?: the word codes are defined as stochastic variables. Why not for the topic dictionary beta? Which variable needs to be stochastic or not?\n\n* The second term in RHS of Equation 3: 's' should be removed from the term if you meant the marginal probability of observations here.\n\n* Experiment details: How the parameters of models are chosen? Are the default parameters used throughout the experiments? \n\n* Please use a single character to denote a matrix. Word embedding matrix WE seems a multiplication of two matrices.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}},{"tddate":null,"ddate":null,"tmdate":1515642511558,"tcdate":1510900075543,"number":1,"cdate":1510900075543,"id":"ByQd-bhJz","invitation":"ICLR.cc/2018/Conference/-/Paper79/Official_Review","forum":"Sy33KapTb","replyto":"Sy33KapTb","signatures":["ICLR.cc/2018/Conference/Paper79/AnonReviewer3"],"readers":["everyone"],"content":{"title":"good illustration of deep-neural network machinery but poor connection to related/prior work","rating":"5: Marginally below acceptance threshold","review":"\"without complicated mathematical inference\" (on end of page 10)\nHeaven forbid we should expect our graduate students to know a bit of statistics!  Sorry,  I am being sarcastic.  Yes, I agree its good to automate as much of the \"tedium\" as possible.  Perhaps word this differently.\n\nTable 5 should probably be in appendix.  With comparison to other methods, this isn't\ntoo informative.\n\nPlease properly capitalise some words in the references. The good Reverend bayes I'm sure is turning in his grave.  I note \"bayes\", \"lda\" and \"ibp\" still occur in the revision!\n\nSection 3.4 is a nice trick.  \n\nFirst version experimental work\n==========================\nFor short text, you missed a whole body of work that uses word background knowledge, like word correlations or embeddings, to regularise topics or as an informed prior on topics.  A lot of progress on this recently, and it produces dramatic improvements using GloVe embeddings, for instance, for the prior or regulariser.  Given the lack of comparison with these, it makes your experimental work lacking.   There are also many more deep NN methods for topic models, e.g., DocNADE.\n\nRevised experimental work\n==================\nBig improvement.  Its now believably good at classification.  Still, I'd argue better existing\nalgorithms could be used, i.e.,  much better ones still exist (in my view)\nbut at least you have made an effort this time.  \n\nMathematical formulation of 3.1 is very sloppy\n==============================================\nA)       $\\theta$ looks to be scalar in line 1, but you earlier\n        said it was a vector; also should $\\theta$ be indexed by $d$?\nB)   \t from 2(b) I presume $s_n$ is a vector but appears\n\t  to be a scalar in 2(a)\nC)       what is a supergaussian, and what are the 2 parameters for it?\n         my only thought is that it is a scalar distribution e^{-|x|^c}\n\t for 1<c<2 and x>0, but how does $\\theta$ fit in?\nC)       if $s_n$ is scalar, the whole distribution is too trivial to be\n         a topic model\nD)      $\\beta$ is a topic dictionary, I presume a matrix, what then\n        is $\\beta_n$ ... I'm guessing a $K$ dim vector\nE)      is $n$ the $n$-th word in the document or the word with\n       dictionary index $n$ ... very different!\nSo I'll guess $\\beta$ is $N$ (vocab dim) times $K$, and the supergaussian\nis K dimensional, but only on the positive quadrant, so all entries of the\nvector $s_n$ are positive. Not sure about $\\theta$!\nThat is a lot of work for me to do just to read 3.1.\nAnd I'm still unsure.  Had to clear everything by reading\nthe STC paper.   Note STC was a UAI paper.   You have it listed as CoRR.\nThey define super-Gaussian, which is non-standard, so you need to\ndefine it in your paper.\n\nDetails of 3.2 also confusing\n=============================\nI gather when you say $s_n \\sim U(....)$ you really mean\n$s_{d,k} \\sim U(....)$, since $s_n$ has to be $K$ dimnesional.\nGenerally, a lot more care needed with definition\nof distributions.\n\nLine 4 you present how to collapse the doc code.\nI'm lost by this.  Doesn't that make $\\theta_d$ a scalar?\nShouldn't it be a vector?  Or did you mean $\\theta_k$?\nWhat is $s_{d,nk}$ mean?  What is $nk$?  Presume you meant\n$s_{d,k}$.  Regardless, I don't see how the super-Gaussian\nand the Poisson (2a) and 2b)) can be collapsed\nto give a simple summation estimate for $\\theta$.\nBasically, I don't believe Bai et als, formula\nis strictly correct, though its probably a\ngood summary statistic.\n\nHowever, since the $s_n$ are vectors for each word, right,\nyou have plenty of latent variable dimensions floating around,\nall the $s_n$, so you have a rich latent space.\nIts not easy to intepret, but seems OK.\n\n\"To yield sparse word codes, we choose the Laplace distribution as the variational distribution of word codes, \" ... sorry, I am confused.\nEither word codes have a uniform distribution or they have a Laplace\ndistribution, they cannot have both.  The variational encoder is just\nan inference algorithm ... it should change what the prior is.\n\nMathematical formulation of 3.1 is very sloppy\n==============================================\nA)       $\\theta$ looks to be scalar in line 1, but you earlier\n        said it was a vector; also should $\\theta$ be indexed by $d$?\nB)   \t from 2(b) I presume $s_n$ is a vector but appears\n\t  to be a scalar in 2(a)\nC)       what is a supergaussian, and what are the 2 parameters for it?\n         my only thought is that it is a scalar distribution e^{-|x|^c}\n\t for 1<c<2 and x>0, but how does $\\theta$ fit in?\nC)       if $s_n$ is scalar, the whole distribution is too trivial to be\n         a topic model\nD)      $\\beta$ is a topic dictionary, I presume a matrix, what then\n        is $\\beta_n$ ... I'm guessing a $K$ dim vector\nE)      is $n$ the $n$-th word in the document or the word with\n       dictionary index $n$ ... very different!\nSo I'll guess $\\beta$ is $N$ (vocab dim) times $K$, and the supergaussian\nis K dimensional, but only on the positive quadrant, so all entries of the\nvector $s_n$ are positive. Not sure about $\\theta$!\nThat is a lot of work for me to do just to read 3.1.\nAnd I'm still unsure.  Had to clear everything by reading\nthe STC paper.   Note STC was a UAI paper.   You have it listed as CoRR.\nThey define super-Gaussian, which is non-standard, so you need to\ndefine it in your paper.\n\nDetails of 3.2 also confusing\n=============================\nI gather when you say $s_n \\sim U(....)$ you really mean\n$s_{d,k} \\sim U(....)$, since $s_n$ has to be $K$ dimnesional.\nGenerally, a lot more care needed with definition\nof distributions.\n\nLine 4 you present how to collapse the doc code.\nI'm lost by this.  Doesn't that make $\\theta_d$ a scalar?\nShouldn't it be a vector?  Or did you mean $\\theta_k$?\nWhat is $s_{d,nk}$ mean?  What is $nk$?  Presume you meant\n$s_{d,k}$.  Regardless, I don't see how the super-Gaussian\nand the Poisson (2a) and 2b)) can be collapsed\nto give a simple summation estimate for $\\theta$.\nBasically, I don't believe Bai et als, formula\nis strictly correct, though its probably a\ngood summary statistic.\n\nHowever, since the $s_n$ are vectors for each word, right,\nyou have plenty of latent variable dimensions floating around,\nall the $s_n$, so you have a rich latent space.\nIts not easy to intepret, but seems OK.\n\n\"To yield sparse word codes, we choose the Laplace distribution as the variational distribution of word codes, \" ... sorry, I am confused.\nEither word codes have a uniform distribution or they have a Laplace\ndistribution, they cannot have both.  The variational encoder is just\nan inference algorithm ... it should change what the prior is.\n\nSummary\n==============\nThis is good looking research.  \nPros:\n* its a good illustration of deep-neural network machinery at work, and well put together\n* the experimental results show it works very well\n* good experimental work (different data sets, different algorithms)\nCons:\n* sloppy mathematical presentation\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Neural Variational Sparse Topic Model","abstract":"Effectively inferring discriminative and coherent latent topics of short texts is a critical task for many real world applications. Nevertheless, the task has been proven to be a great challenge for traditional topic models due to the data sparsity problem induced by the characteristics of short texts. Moreover, the complex inference algorithm also become a bottleneck for these traditional models to rapidly explore variations. In this paper, we propose a novel model called Neural Variational Sparse Topic Model (NVSTM) based on a sparsity-enhanced topic model named Sparse Topical Coding (STC). In the model, the auxiliary word embeddings are utilized to improve the generation of representations. The Variational Autoencoder (VAE) approach is applied to inference the model efficiently, which makes the model easy to explore extensions for its black-box inference process. Experimental results onWeb Snippets, 20Newsgroups, BBC and Biomedical datasets show the effectiveness and efficiency of the model.","pdf":"/pdf/04862af3bf61af3447a8371d948baf04aead15a8.pdf","TL;DR":"a neural sparsity-enhanced topic model based on VAE","paperhash":"anonymous|neural_variational_sparse_topic_model","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Variational Sparse Topic Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy33KapTb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper79/Authors"],"keywords":["Variational Autoencoder","Sparse Topical Coding","Neural Variational Inference"]}}],"limit":2000,"offset":0}