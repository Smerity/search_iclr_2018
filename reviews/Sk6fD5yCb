{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642398965,"tcdate":1511983962815,"number":3,"cdate":1511983962815,"id":"HymwoY3lM","invitation":"ICLR.cc/2018/Conference/-/Paper148/Official_Review","forum":"Sk6fD5yCb","replyto":"Sk6fD5yCb","signatures":["ICLR.cc/2018/Conference/Paper148/AnonReviewer3"],"readers":["everyone"],"content":{"title":"fast implementation of binary networks","rating":"7: Good paper, accept","review":"The paper presents an implementation strategy (with code link anonymized for review) for fast computations of binary forward inference. The paper makes the approach seem straightforward (clever?) and there has been lots of work on fast inference of quantized, low-bit-width neural networks, but if indeed the implementation is significantly faster than commercial alternatives (e.g. from Intel) then I expect the authors have made a novel and useful contribution.\n\nThe paper is written clearly, but I am not an expert in alternative approaches in this area.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Espresso: Efficient Forward Propagation for Binary Deep Neural Networks","abstract":"  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.","pdf":"/pdf/fbfa7b5ae6ff7c80cbe439cc083c1be1ce09f4fc.pdf","TL;DR":"state-of-the-art computational performance implementation of binary neural networks","paperhash":"anonymous|espresso_efficient_forward_propagation_for_binary_deep_neural_networks","_bibtex":"@article{\n  anonymous2018espresso:,\n  title={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk6fD5yCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper148/Authors"],"keywords":["binary deep neural networks","optimized implementation","bitwise computations"]}},{"tddate":null,"ddate":null,"tmdate":1515963637560,"tcdate":1511759958537,"number":2,"cdate":1511759958537,"id":"HJCLeXtgM","invitation":"ICLR.cc/2018/Conference/-/Paper148/Official_Review","forum":"Sk6fD5yCb","replyto":"Sk6fD5yCb","signatures":["ICLR.cc/2018/Conference/Paper148/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review revised","rating":"6: Marginally above acceptance threshold","review":"The paper presents a library written in C/CUDA that features all the functionalities required for the forward propagation of BCNNs. The library is significantly faster than existing implementations of optimized binary neural networks (≈ 2 orders of magnitude), and will be released on github.\n\nBCNNs have been able to perform well on large-scale datasets with increased speed and decreased  energy consumption, and implementing efficient kernels for them can be very useful for mobile applications. The paper describes three implementations CPU, GPU and GPU_opt, but it is not entirely clear what the differences are and why GPU_opt is faster than GPU implementation.\n\nAre BDNN and BCNN used to mean the same concept? If yes, could you please use only one of them?\n\nThe subsection title “Training Espresso” should be changed to “Converting a network to Espresso”, or “Training a network for Espresso”.\n\nWhat is the main difference between GPU and GPU_opt implementations?\n\nThe unrolling and lifting operations are shown in Figure 2. Isn’t accelerating convolution by this method a very well known one which is implemented in many deep learning frameworks for both CPU and GPU?\n\nWhat is the main contribution that makes the framework here faster than the other compared work? In Figure1, Espresso implementations are compared with other implementations in (a)dense binary matrix multiplication and (b)BMLP and not (c)BCNN. Can others ( BinaryNet\n(Hubara et al., 2016) or Intel Nervana/neon (NervanaSystems)) run CNNs?\n\n6.2 MULTI-LAYER PERCEPTRON ON MNIST – FIGURE 1B AND FIGURE 1E. It should be Figure 1d instead of 1e? \n\nAll in all, the novelty in this paper is not very clear to me. Is it bit-packing?\n\n\nUPDATE: \nThank you for the revision and clarifications. I increase my rating to 6.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Espresso: Efficient Forward Propagation for Binary Deep Neural Networks","abstract":"  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.","pdf":"/pdf/fbfa7b5ae6ff7c80cbe439cc083c1be1ce09f4fc.pdf","TL;DR":"state-of-the-art computational performance implementation of binary neural networks","paperhash":"anonymous|espresso_efficient_forward_propagation_for_binary_deep_neural_networks","_bibtex":"@article{\n  anonymous2018espresso:,\n  title={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk6fD5yCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper148/Authors"],"keywords":["binary deep neural networks","optimized implementation","bitwise computations"]}},{"tddate":null,"ddate":null,"tmdate":1515642399040,"tcdate":1510781734271,"number":1,"cdate":1510781734271,"id":"SyRQ7Vq1G","invitation":"ICLR.cc/2018/Conference/-/Paper148/Official_Review","forum":"Sk6fD5yCb","replyto":"Sk6fD5yCb","signatures":["ICLR.cc/2018/Conference/Paper148/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A full implementation of binary CNN with code","rating":"7: Good paper, accept","review":"This paper builds on Binary-NET [Hubara et al. 2016] and expands it to CNN architectures. It also provides optimizations that substantially improve the speed of the forward pass: packing layer bits along the channel dimension, pre-allocation of CUDA resources and binary-optimized CUDA kernels for matrix multiplications. The authors compare their framework to BinaryNET and Nervana/Neon and show a 8x speedup for 8092 matrix-matrix multiplication and a 68x speedup for MLP networks. For CNN, they a speedup of 5x is obtained from the GPU to binary-optimizimed-GPU. A gain in memory size of 32x is also achieved by using binary weight and activation during the forward pass.\n\nThe main contribution of this paper is an optimized code for Binary CNN. The authors provide the code with permissive licensing. As is often the case with such comparisons, it is hard to disentangle from where exactly come the speedups. The authors should provide a table with actual numbers instead of the hard-to-read bar graphs. Otherwise the paper is well written and relatively clear, although the flow is somewhat unwieldy. \n\nOverall, i think it makes a good contribution to a field that is gaining importance for mobile and embedded applications of deep convnets. I think it is a good fit for a poster.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Espresso: Efficient Forward Propagation for Binary Deep Neural Networks","abstract":"  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.","pdf":"/pdf/fbfa7b5ae6ff7c80cbe439cc083c1be1ce09f4fc.pdf","TL;DR":"state-of-the-art computational performance implementation of binary neural networks","paperhash":"anonymous|espresso_efficient_forward_propagation_for_binary_deep_neural_networks","_bibtex":"@article{\n  anonymous2018espresso:,\n  title={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk6fD5yCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper148/Authors"],"keywords":["binary deep neural networks","optimized implementation","bitwise computations"]}},{"tddate":null,"ddate":null,"tmdate":1513383415727,"tcdate":1509037845450,"number":148,"cdate":1509739457105,"id":"Sk6fD5yCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk6fD5yCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Espresso: Efficient Forward Propagation for Binary Deep Neural Networks","abstract":"  There are many applications scenarios for which the computational\n  performance and memory footprint of the prediction phase of Deep\n  Neural Networks (DNNs) need to be optimized. Binary Deep Neural\n  Networks (BDNNs) have been shown to be an effective way of achieving\n  this objective. In this paper, we show how Convolutional Neural\n  Networks (CNNs) can be implemented using binary\n  representations. Espresso is a compact, yet powerful\n  library written in C/CUDA that features all the functionalities\n  required for the forward propagation of CNNs, in a binary file less\n  than 400KB, without any external dependencies. Although it is mainly\n  designed to take advantage of massive GPU parallelism, Espresso also\n  provides an equivalent CPU implementation for CNNs. Espresso\n  provides special convolutional and dense layers for BCNNs,\n  leveraging bit-packing and bit-wise computations\n  for efficient execution. These techniques provide a speed-up of\n  matrix-multiplication routines, and at the same time, reduce memory\n  usage when storing parameters and activations. We experimentally\n  show that Espresso is significantly faster than existing\n  implementations of optimized binary neural networks (~ 2\n  orders of magnitude). Espresso is released under the Apache 2.0\n  license and is available at http://github.com/organization/project.","pdf":"/pdf/fbfa7b5ae6ff7c80cbe439cc083c1be1ce09f4fc.pdf","TL;DR":"state-of-the-art computational performance implementation of binary neural networks","paperhash":"anonymous|espresso_efficient_forward_propagation_for_binary_deep_neural_networks","_bibtex":"@article{\n  anonymous2018espresso:,\n  title={Espresso: Efficient Forward Propagation for Binary Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk6fD5yCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper148/Authors"],"keywords":["binary deep neural networks","optimized implementation","bitwise computations"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}