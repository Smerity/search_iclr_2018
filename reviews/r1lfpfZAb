{"notes":[{"tddate":null,"ddate":null,"tmdate":1515687226830,"tcdate":1515687226830,"number":4,"cdate":1515687226830,"id":"ByGHp-S4M","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Comment","forum":"r1lfpfZAb","replyto":"rks9NupQG","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper993/AnonReviewer2"],"content":{"title":"Response to author response","comment":"While the paper was improved, it didn't address my main concern, that it is unclear whether the model really implements Gricean maxims. Assessing the repetitions and finding that the language model repeats slightly more often is not much evidence in my opinion. Also, the re-ranker should be trained on appropriately generated data, as it happens with the approach proposed. Thus my assessment of the paper remains the same."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515189395170,"tcdate":1515189395170,"number":3,"cdate":1515189395170,"id":"rks9NupQG","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Comment","forum":"r1lfpfZAb","replyto":"BJFJrHcgz","signatures":["ICLR.cc/2018/Conference/Paper993/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper993/Authors"],"content":{"title":"Thank you for your detailed feedback.","comment":"Regarding our approach to implementing Grice's maxims:\n\n- Our repetition module is trained to recognize both exact repetitions and repetitions involving lexical paraphrases, as indicated by the cosine similarity between word embeddings. We believe that this is a more robust approach than placing hard constraints on repetition, and has the advantage that the model can learn to distinguish between desirable and undesirable similarity patterns in human-produced and machine-produced text. \n\n- For the entailment module, while there is a risk that relevant sentences will be penalized, in the training data most of the entailments are direct enough that they are not likely to occur in writing, while the neural class training examples still often contains relevant information. \nIn terms of the model, we chose to use a lightweight bag-of-words model for time and memory efficiency reasons (as it is expensive to do pairwise sentence comparisons to compute the entailment scores), even though a state-of-the-art model is likely to somewhat increase the performance. \n\nWe added an analysis to the paper of the frequency of repetitions in the training data, finding that they indeed occur more frequently in the samples from the language model, which are used as negative examples for training the repetition model, than in the reference endings. \n\nWe added an equation in description of the length module in order to clarify its objective. \n\nThe purpose of the maximum length restriction is simply to guarantee that the beam-search will terminate. In practice the generated sequence (which is the highest-scoring sequence ending with the termination token) is always shorter than the maximum length allowed. \n\nAs suggested, we include a reranker baseline in the results to re-score the n-best outputs after doing beam search decoding using only the language model: We found that it performs much worse due to a lack of diversity in the beam.\n\nWe added more details in the paper to support the claim that the objective is being learned. The scoring function learned in one stage informs the objective in the following stages. First, the expert classifiers are learned to improve the language model by using samples from the language model as negative training data. Subsequently, these expert classifiers are combined in a mixed objective where the weights of the classifiers are learned discriminatively. As a result, the overall objective function for training the generator changes dynamically as the mixture weights are updated because the objective itself depends on those weights. The mixture weights are learned to optimize a discriminative objective, which updates the overall generation objective; this in turn changes the discriminative objective for the next training iteration. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515189341239,"tcdate":1515189341239,"number":2,"cdate":1515189341239,"id":"SJrPVdp7f","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Comment","forum":"r1lfpfZAb","replyto":"ByWqV4YlG","signatures":["ICLR.cc/2018/Conference/Paper993/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper993/Authors"],"content":{"title":"Thank you for your concrete and constructive feedback.","comment":"We added an analysis to the paper of the frequency of repetitions in the training data, finding that they indeed occur more frequently in the samples from the language model, which are used as negative examples for training the repetition model, than in the reference endings. \n\nEntailment examples in our training data are often but not always a form of paraphrasing, but usually not instances of direct repetition. Therefore we believe that we still need a seperate repetition model to handle more direct repetitions at a lexical level. A separate paraphrasing model is an interesting suggestion for future work, although we believe that the repetition and entailment models together are able to capture most of the paraphrases we are aiming to avoid.\n\nWe improved the description of the entailment score formulation (eq 6). \n\nThe very low BLEU scores observed in our results in the TripAdvisor domain are an artifact of the BLEU metric’s length penalty. The average length of reference completions is 12 sentences, which is much longer than the average length of endings generated by our Learning to Write models. This forces the BLEU score's length penalty to drive down the scores, despite our observation that there is still a significant amount of word and phrase overlap. The completions generated by the base language model are longer on average (as it tends to repeat itself over and over) and therefore do not suffer from this problem. \n\nWhile we agree that more labels per example will be valuable, we believe that the the test sets (of 1000 examples per domain) are large enough that to obtain a reasonably accurate aggregate score, despite the fact that not all of the individual annotations will be reliable."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515189257558,"tcdate":1515189257558,"number":1,"cdate":1515189257558,"id":"HkMz4Oamz","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Comment","forum":"r1lfpfZAb","replyto":"HkN9lyRxG","signatures":["ICLR.cc/2018/Conference/Paper993/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper993/Authors"],"content":{"title":"Thank you for your feedback and suggestions.  ","comment":"                                                                                                                                                                                                                                                                                                                                                                                                                                  We added more details in the paper to support the claim that the objective is being learned. The scoring function learned in one stage informs the objective in the following stages. First, the expert classifiers are learned to improve the language model by using samples from the language model as negative training data. Subsequently, these expert classifiers are combined in a mixed objective where the weights of the classifiers are learned discriminatively. As a result, the overall objective function for training the generator changes dynamically as the mixture weights are updated because the objective itself depends on those weights. The mixture weights are learned to optimize a discriminative objective, which updates the overall generation objective; this in turn changes the discriminative objective for the next training iteration. \n\nThe recommendation to tackle grounded language tasks is a great suggestion, and we are eager to explore this avenue for future work. We believe incorporating grounding introduces novel challenges and so falls out of the scope of this paper, which we have scoped to focus on open-ended, ungrounded generation. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515830469319,"tcdate":1512071307870,"number":3,"cdate":1512071307870,"id":"HkN9lyRxG","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Review","forum":"r1lfpfZAb","replyto":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neat contribution that integrates previous work","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to bring together multiple inductive biases that hope to correct for inconsistencies in sequence decoding. Building on previous works that utilize modified objectives to generate sequences, this work proposes to optimize for the parameters of a pre-defined combination of various sub-objectives. The human evaluation is straight-forward and meaningful to compensate for the well-known inaccuracies of automatic evaluation. \n\nWhile the paper points out that they introduce multiple inductive biases that are useful to produce human-like sentences, it is not entirely correct that the objective is being learnt as claimed in portions of the paper. I would like this point to be clarified better in the paper. \n\nI think showing results on grounded generation tasks like machine translation or image-captioning would make a stronger case for evaluating relevance. I would like to see comparisons on these tasks. \n\n---- \nAfter reading the paper in detail again and the replies, I am downgrading my rating for this paper. While I really like the motivation and the evaluation proposed by this work, I believe that fixing the mismatch between the goals and the actual approach will make for a stronger work. \n\nAs pointed out by other reviewers, while the goals and evaluation seem to be more aligned with Gricean maxims, some components of the objective are confusing. For instance, the length penalty encourages longer sentences violating quantity, manner (be brief) and potentially relevance. Further, the repetition model address the issue of RNNs failing to capture long-term contextual dependencies -- how much does such a modified objective affect models with attention / hierarchical models is not clear from the formulation. \n\nAs pointed out in my initial review evaluation of relevance on the current task is not entirely convincing. A very wide variety of topics are feasible for a given context sentence. Grounded generation like MT / captioning would have been a more convincing evaluation. For example, Wu et al. (and other MT works) use a coverage term and this might be one of the indicators of relevance. \n\nFinally, I am not entirely convinced by the update regarding \"learning the objective\". While I agree with the authors that the objective function is being dynamically updated, the qualities of good language is encoded manually using a wide variety of additional objectives and only the relative importance of each of them is learnt. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642538806,"tcdate":1511834849508,"number":2,"cdate":1511834849508,"id":"BJFJrHcgz","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Review","forum":"r1lfpfZAb","replyto":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well-motivated goals, but the methods don't achieve them.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to improve RNN language model generation using augmented objectives inspired by Grice's maxims of communication. The idea is to combine the standard word-by-word decoding objective with additional objectives that reward sentences following these maxims. The proposed decoding objective is not new; reseachers in machine translation \n have worked on it referring to it as loss-augmented decoding: http://www.cs.cmu.edu/~nasmith/papers/gimpel+smith.naacl12.pdf\nThe use of RNNs in this context might be novel though.\n\nPros:\n- Well-motivated and ambitious goals\n\n- Human evaluation conducted on the outputs.\n\nCons:\n- My main concern is that it is unclear whether the models introduced are indeed implementing the Gricean maxims. For eaxample, the repetition model would not only discourage the same word occurring twice, but also a similar word (according to the word vectors used) to follow another one. \n\n- Similary, for the entailment model, what is an \"obvious\" entailment\"? Not sure we have training data for this in particular. Also, entailment suggests textual cohesion, which is conducive to the relation maxim. If this kind of model is what we need, why not take a state-of-the-art model?\n\n- The results seem to be inconsistent. The working vocabulary doesn't help in the tripAdvior experiment, while the RNN seems to work very well on the ROCstory data. While there might be good reasons for these, the point for me is that we cannot trust that the models added to the objective do what they are supposed to do.\n\n- Are the negative examples generated for the repetition model checked that they contain repetitions? Shouldn't be difficult to do. \n\n- Would be better to give the formula for the length model, the description is intuition but it is difficult to know exactly what the objective is\n\n- In algorithm 1, it seems like we fix in advance the max length of the sentence (max-step).  Is this the case? If so why? Also, the proposed learning algorithm only learns how to mix pre-trained models, not sure I agree they learn the objective. It is more of an ensembling.\n\n- As far as I can tell these ideas could have been more simply implemented by training a re-ranker to score the n-best outputs of the decoder. Why not try it? They are very popular in text generation tasks.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642538842,"tcdate":1511765128833,"number":1,"cdate":1511765128833,"id":"ByWqV4YlG","invitation":"ICLR.cc/2018/Conference/-/Paper993/Official_Review","forum":"r1lfpfZAb","replyto":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference/Paper993/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper combines RNN language model with several discriminatively trained models to improve the language generation. I like the idea of using Grice’s Maxims of communication to improve the language generation. However, some parts need to be further clarified and it would be nice to see more related analysis. ","rating":"5: Marginally below acceptance threshold","review":"This paper argues that the objective of RNN is not expressive enough to capture the good generation quality. In order to address the problems of RNN in generating languages, this paper combines the RNN language model with several other discriminatively trained models, and the weight for each sub model is learned through beam search. \n\nI like the idea of using Grice’s Maxims of communication to improve the language generation. Human evaluation shows significant improvement over the baseline. I have some detailed comments as follows:\n\n- The repetition model uses the samples from the base RNNs as negative examples. More analysis is needed to show it is a good negative sampling method.\n\n- As Section 3.2.3 introduced, “the unwanted entailment cases include repetitions and paraphrasing”. Does it mean the entailment model also handles repetition problem? Do we still need a separate repetition model? How about a separate paraphrasing model?\n\n- Equation 6 and the related text are not very clearly represented. It would be better to add more intuition and better explained. \n\n- In the Table 2, the automated bleu scores of L2W algorithm for Tripadvisor is very low (0.34 against 24.11). Is this normal? More explanation is needed here.\n\n- For human judgement, how many scores does each example get? It would be better to get multiple workers on M-Turk to label the same example, and compute the mean and variance. One score per example may not be reliable. \n\n- It would be interesting to see deeper analysis about how each model in the objectives influence the actual language generation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515189156217,"tcdate":1509137685713,"number":993,"cdate":1510092360822,"id":"r1lfpfZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1lfpfZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Write by Learning the Objective","abstract":"Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models for learning prevalent patterns in natural language.   Yet language generated by RNNs often shows several degenerate characteristics that are uncommon in human language; while fluent, RNN language production can be overly generic, repetitive, and even self-contradictory.  We postulate that the objective function optimized by RNN language models, which amounts to the overall perplexity of a text, is not expressive enough to capture the abstract qualities of good generation such as Grice’s Maxims. In this paper, we introduce a general learning framework that can construct a decoding objective better suited for generation. Starting with a generatively trained RNN language model, our framework learns to construct a substantially stronger generator by combining several discriminatively trained models that can collectively address the limitations of RNN generation.  Human evaluation demonstrates that text generated by the resulting generator is preferred over  that  of  baselines  by  a  large  margin  and  significantly  enhances  the  overall coherence, style, and information content of the generated text.","pdf":"/pdf/5f9bd9e4ec64e1fd981afed94af1e8f10e2a8de6.pdf","TL;DR":"We build a stronger natural language generator by discriminatively training scoring functions that rank candidate generations with respect to various qualities of good writing.","paperhash":"anonymous|learning_to_write_by_learning_the_objective","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Write by Learning the Objective},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1lfpfZAb}\n}","keywords":["natural language generation"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper993/Authors"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}