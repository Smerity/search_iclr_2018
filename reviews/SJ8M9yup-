{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222611452,"tcdate":1511907690858,"number":3,"cdate":1511907690858,"id":"HkQOWPieM","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Review","forum":"SJ8M9yup-","replyto":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference/Paper28/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Recovering sparse signals via ReLU/Sigmoid functions","rating":"5: Marginally below acceptance threshold","review":"This paper considers the following model of a signal x = W^T h + b, where h is an m-dimensional random sparse vector, W is an m by n matrix, b is an n dimensional fixed bias vector. The random vector h follows an iid sparse signal model, each coordinate independently have some probability of being zero, and the remaining probability is distributed  among nonzero values according to some reasonable pdf/pmf. The task is to recover h, from the observation x via the activation functions like Sigmoid or ReLU. For example, \\hat{h} = Sigmoid(W^T h + b).\n\nThe authors then show that, under the random sparsity model of h, it is possible to upper bound the probability P(||h-\\hat{h}|| > \\delta. m) in terms of the parameters of the distribution of h and W and b. In some cases noise can also be tolerated. In particular, if W is incoherent (columns being near-orthonormal), then the guarantee is stronger. As far as I understood, the proofs make sense - they basically use Chernoff-bound type argument. \n\nIt is my impression that a lot of conditions have to be satisfied for the recovery guarantee to be meaningful. I am unsure if real datasets will satisfied so many conditions. Also, the usual objective of autoencoders is to denoise  - i.e. recover x, without any access to W. The authors approach in this vein seem to be only empirical. Some recent works on associative memory also assume the sparse recovery model - connections to this literature would have been of interest. It is also not clear why compressed sensing-type recovery using a single ReLU or Sigmoid would be of interest: are their complexity benefits?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/5897ccd66bc1adfe67f0bcffd1fc9e6f89ae6a2c.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1512222611495,"tcdate":1511719836228,"number":2,"cdate":1511719836228,"id":"B1NjQYOeG","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Review","forum":"SJ8M9yup-","replyto":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference/Paper28/AnonReviewer3"],"readers":["everyone"],"content":{"title":"On optimality conditions for auto-encoder signal recovery","rating":"5: Marginally below acceptance threshold","review":"This papers proposes to analyze auto-encoders under sparsity constraints of an underlying signal to be recovered.\nBased on concentration inequality, the reconstruction provided for a simple class of functions is guaranteed to be accurate in l1 norm with high probability.\nThe proof techniques are classical, but the results seem novel as far as I know.\nAs an open question, could the results be given for other lp norms, in particular for infinity-norm? Indeed, this is a privileged norm for support recovery.\n\n\n\nPresentation issues:\n- section should be Section when stating for instance \"section 1\". Idem for eq, equation, assumption...\n- bold fonts for vectors are randomly used: some care should be given to harmonizing symbols fonts.\n- equations should be cited with brackets\n\nReferences issues:\n- harmonize citations: if you add first name for some authors add it for all references: why writing Roland Makhzani and J. Wright?\n\n- Candes -> Cand\\`es\n\n- Consider citing \"Sparse approximate solutions to linear systems\", Natarajan 1995 when mentioning Amaldi and Kann 1998.\n\n\n\nSpecific comments:\npage 1:\n- hasn't -> has not.\n\npage 2:\n- \"activation function\": at this stage s_e and s_d are just functions. What is the \"activation\" refers to? Also a clarification on the space they act on should be stated. Idem for b_e and b_d.\n- \"the identity of h in eq. 1 is only well defined in the presence of l1 regularization due to the over-completeness of the dictionary\" : this is implicitly stating the uniqueness of the Lasso. Not that it is well known that there are cases where the Lasso is non-unique. Please, clarify your statement accordingly.\n- for simplicity b_d could be removed here.\n- in (4) it would be more natural to write f_j(h_j) instead of f(h_j)\n- \"has is that to be bounded\"-> is boundedness?\n- what is l_max_j here? moreover the bold letters seem to represent vectors but this should be state explicitly somewhere.\n\npage 3:\n- what is the precise meaning of \"distributed\" when referring to representation\n- In remark 1: the font has changed weirdly for W and h.\n- \"two class\"->two classes\n- Definition 1: again what is a precise definition of activation function?\n- \"if we set\": bold issue.\n- b should b_e in Theorem 1, right? Also, please recall the definition of the sigmoid function here. Moreover l_max and mu_h seem useless in this theorem... why referring to them?\n- \"if the rows of the weight matrix is\"-> if the rows of the weight matrix are\n\npage 4:\n- Proposition 1 could be stated as a theorem and Th.1 as a corollary (with e=0). The same is true for proposition 2 I suspect.\n- Again the influence of l_max and mu_h are none here...\n- Please, provide the definition of the ReLu function here. Is this just x->x_+ ?\n\npage 6:\n- R^+m -> font issue again.\n- \"are maximally incoherent\": what is the precise meaning of this statement?\n- what the motivation for Theorem 3? This should be discussed.\n- De-noising -> de-noising\n- the discussion after (15) should be made more precise.\n\npage 7:\n- Figure 1 and 2 should be postponed to page 8.\n- in Eq. (16) one needs to known E_h(x) and E_h_i(h_i), but I suspect this quantity are usually unknown to the practitioner. Can the author comment on that?\n\npage 8:\n- \"the recovery is denoised through thresholding\": where is this step analyzed?\n\npage 9:\n- figure 3: sparseness-> sparsity; also what is the activation function used here?\n- \"are then generate\"->are then generated\n- \"by greedily select\"->by greedily selecting\n- \"the the\"\n- \"and thus pre-process\"-> and thus pre-processing\n\n\nSupplementary:\npage 1:\n- please define \\sigma, and its simple properties used along the proof.\n\npage 2:\n- g should be g_j (in eq 27 - > 31)\n- overall this proof relies on ingredients such as the one used for Hoeffding's inequality.\nMost ingredients could be taken from standard tools on concentration (see for instance Boucheron, Lugosi, Massart: \"Concentration Inequalities: A Nonasymptotic Theory of Independence\", 2013).\nMoreover, some elements should be factorized as they are shared among the next proofs. This should reduce the size of the supplementary dramatically.\n\npage 7:\n- Eq. (99): it should be reminded that W_ii=1 here.\n- the upper bound used on \\mu to get equation 105 seems to be in the wrong order.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/5897ccd66bc1adfe67f0bcffd1fc9e6f89ae6a2c.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1512222611539,"tcdate":1511364284837,"number":1,"cdate":1511364284837,"id":"HySa8MQgz","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Review","forum":"SJ8M9yup-","replyto":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference/Paper28/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Recovery guarantees for auto-encoders; a rather unclear paper, with insufficient comparisons to previous work, and no algorithmic/practical perspectives.","rating":"4: Ok but not good enough - rejection","review":"*Summary*\nThe paper studies recovery guarantees within the context of auto-encoders. Assuming a noise-corrupted linear model for the inputs x's, the paper looks at some sufficient properties (e.g., over the generating dictionary denoted by W) to recover the true underlying sparse signals (denoted by h). Several settings of increasing complexity are considered (from binary signals with no noise to noisy continuous signals). Evaluations are carried out on synthetic examples to highlight the theoretical findings.\n\nThe paper is overall difficult to read. Moreover, and importantly, no algorithmic perspectives are presented in the paper, in the sense that we do not know whether practical procedures would lead to W's satisfying the appropriate properties (unlike (not-mentioned) recent results for dictionary learning/ICA; see detailed comments). Also, assumptions are made (e.g., knowledge about expectations of h and x) for which it is unclear to see how practical/limiting they are. Finally (and as further discussed below), the paper does not sufficiently discuss related work.\n\n(note: I have not reviewed the appendix and supplementary material)\n\n*Detailed comments*\n\n-I think there is an insufficient literature review about recent recovery results in the context of sparse coding, dictionary learning and ICA (see some references at the bottom of the review). I think this is all the more important as the paper tries to draw connections with ICA (see Sec. 4.4).\nGiven that the paper positions itself on a theoretical level, detailed comparisons with existing sample complexities obtained in previous work for related models (e.g., sparse coding, dictionary learning and ICA) must be provided.\n\n-To the best of my understanding of the paper, the guarantees are about h_hat and the true h. It therefore seems that the paper's approach is very close to standard sparse inverse problems, up to the difference due to the (non-identity) activation function. If this is indeed the case, the paper should discuss its results when the activation is identity to see whether known results are recovered. \n\n-\"...we consider linear activation s_d because it is a more general case.\": Just after this statement, it is mentioned that non-linear activations are used in practice. Could this statement be therefore clarified?\n\n-Sec. 2 is unclear. For instance, it is not easy to see how one go from (1) to (2). Moreover, the concept of \"AE framework\" is not well defined.\n\n-In the bottom of page 3, why are p_i and (1-p_i) discarded?\n\n-In practice, how can we set the appropriate value of b_i?\n\n-What is the practical sense of being able to have access to E_h[x], E_x[x], and E_h[h]?\n\n-In Proposition 1 and 2, if the noise e is indeed random, it means the right-hand sides are also random variables. Then, what does the probability statement Pr mean on the left-hand side? Is is conditioned on the draw of e? Some clarifications are required.\n\n-Typo page 7: \"...that used to generate the data.\" --> \"... used to generate the data.\"\n-Typo page 9: \"...data are then generate...\" --> \"...data are then generated...\"\n\n-In Sec. 5.3, to match W_hat and W, the Hungarian algorithm can probably be used.\n\n*References*\n\n(Arora2012) Arora, S.; Ge, R.; Moitra, A. & Sachdeva, S. Provable ICA with unknown Gaussian noise, with implications for Gaussian mixtures and autoencoders Advances in Neural Information Processing Systems (NIPS), 2012, 2375-2383\n\n(Arora2013) Arora, S.; Ge, R. & Moitra, A. New algorithms for learning incoherent and overcomplete dictionaries preprint arXiv:1308.6273, 2013\n\n(Chatterji2017) Chatterji, N. S. & Bartlett, P. L. Alternating minimization for dictionary learning with random initialization preprint arXiv:1711.03634, 2017\n\n(Gribonval2015) Gribonval, R.; Jenatton, R. & Bach, F. Sparse and spurious: dictionary learning with noise and outliers IEEE Transactions on Information Theory, 2015, 61, 6298-6319\n\n(Sun2015) Sun, J.; Qu, Q. & Wright, J. Complete dictionary recovery over the sphere Sampling Theory and Applications (SampTA), 2015 International Conference on, 2015, 407-410\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/5897ccd66bc1adfe67f0bcffd1fc9e6f89ae6a2c.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1509739523168,"tcdate":1508534798355,"number":28,"cdate":1509739520514,"id":"SJ8M9yup-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/5897ccd66bc1adfe67f0bcffd1fc9e6f89ae6a2c.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}