{"notes":[{"tddate":null,"ddate":null,"tmdate":1515129595641,"tcdate":1515129595641,"number":3,"cdate":1515129595641,"id":"BkNZsY27f","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Comment","forum":"SJ8M9yup-","replyto":"HySa8MQgz","signatures":["ICLR.cc/2018/Conference/Paper28/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper28/Authors"],"content":{"title":"Response to reviewer 2","comment":"Thank you for your comments. \n\nWe would like to stress that our goal in this paper was to study the signal  recovery properties of auto-encoders rather than studying them from a dictionary learning point of view which is a separate problem on its own.\n\nRegarding not providing algorithmic perspective.\nWe are interested in properties that would lead to optimal solutions using auto-encoders from a signal recovery perspective, and thus better understand auto-encoders. There is no algorithmic perspective since 1) when the dictionary is known, we can get the solution analytically, 2) in case the dictionary is unknown we solve it using gradient descent. In the second case, we do not have guarantees for the recovery of the hidden signals, however, we have shown that empirically the recovery is strong using the theories that we developed. \n\nRegarding the linearity of the activation function s_d, we apologize for the confusion, we have reworded to make it more clear. s_d is the decoding activation function, linear activation is more general as it covers a wider numerical range. The latter sentence meant to say 1) the activations for both s_d and s_e can be non-linear in practice; 2) since linear s_d can handle the case for non-linear s_d, the previous statement is still true.\n\nOn page 3, p_i and 1-p_i are constants with respect to the data and therefore it is suffice to analyse the other terms.\n\nRegarding how to set the value of b_i, as stated in theorem 1 and 2, b_i can be set analytically based on weights and p_i. In practice, since we do not know the sparsity level, we can set it in two ways, 1) treat p_i as hyper parameters, 2) treat p_i as parameters of the model.\n\nE_h[x] and E_x[x] are all data mean, and E_h[h] are mean of the hidden activations.\n\nWe have fixed other minor problems mentioned in your reviews.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1515129437078,"tcdate":1515129437078,"number":2,"cdate":1515129437078,"id":"r1bwcKhQz","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Comment","forum":"SJ8M9yup-","replyto":"B1NjQYOeG","signatures":["ICLR.cc/2018/Conference/Paper28/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper28/Authors"],"content":{"title":"Response to reviewer 3","comment":"Thank you for your diligent reviews and detailed comments.\n\nOur results can be extended trivially to other norms using the standard norm equalities; however a non-trivial bound for specific norms (Eg. infinity norm) may need additional work. \n\nWe have harmonized the citations as suggested by the reviewer.  Following are clarifications to some of the questions.\n\nWhat is the \"activation\" refers to? \nWe use the term activation function in accordance to the terminology used commonly for auto-encoders. \n\nThere are cases where the Lasso is non-unique.\nIndeed there are cases when the solution to LASSO is not unique (Eg. when the line y=Dw aligns with |w|=c for some c). We were referring to the general case when the solution to y=Dw is not unique at all when D is over-complete. In these cases some sort of constraint like L1 or L2 penalty is needed to make the solution unique. \n\nin (4) it would be more natural to write f_j(h_j) instead of f(h_j)\nWe use the notation f(h_j) instead of f_j(h_j) to stress that all units are identical.\n\n what is the precise meaning of \"distributed\" when referring to representation\nDistributed representation is a terminology used in deep learning which implies multiple hidden units participate together to represent one sample instead of a single hidden unit corresponding to a single sample. It is an efficient way of encoding patterns.\n\nl_max and mu_h seem useless in this theorem\nI_max and mu_h is required for the data generating distribution BINS defined in our paper; they are the sufficient statistics.\n\nReLu\nReLU is x -> max(0, x)\n\n\"are maximally incoherent\": what is the precise meaning of this statement?\nThe vectors in a matrix are maximally incoherent if the minimum of the angle between every pair of vectors is maximized. This angle is given by the Welch bound.\n\nwhat the motivation for Theorem 3?\nAs mentioned at the beginning of this subsection, the motivation behind theorem 3 is to gain some insights for the generated data.\n\nQuantities of  E_h(x) and E_h_i(h_i)\nYes the quantities E_h(x) and E_h_i(h_i) are unknown, but notice this value is equal to W E_x[x] where x is observed. So as long as W can be recovered, the quantity E_h[h] can be computed.\n\nRegarding \"the recovery is denoised through thresholding\", we didn't analyze it. We mention this based on the intuition that the signal is recovered with epsilon error with high probability. So if the signal magnitude is large enough compared to noise, a simple thresholding should work in separating signal from noise.\n\nIn figure 3, ReLU is used for continuous recovery and sigmoid is used for binary case, as mentioned in theorem 1 in section 4.1 and theorem 2 in section 4.2\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1515177924560,"tcdate":1515129307745,"number":1,"cdate":1515129307745,"id":"r1Nk9t2Xz","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Comment","forum":"SJ8M9yup-","replyto":"HkQOWPieM","signatures":["ICLR.cc/2018/Conference/Paper28/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper28/Authors"],"content":{"title":"Response to reviewer 1","comment":"Thanks for your comments.\n\nRegarding the questions. All real datasets of course do not exactly satisfy the assumptions made in the paper. It also depends on how we model the data. For instance, modeling images at patch levels would be conducive to the assumptions made for real images while modeling them at the image level itself may not. To further defend that the assumptions would hold at patch level, consider that independent component analysis (ICA) is commonly used to model real images at patch level. As discussed in the paper, ICA is a special case of our model where the signal is further assumed to be sparse for recovery using the mechanism discussed in the paper. It is well known that the sparsity assumption usually holds in practice. \n\nRegarding the use of ReLU and Sigmoid, other non-linearities can be used for signal recovery as well as long as they satisfy certain criteria.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1515642424300,"tcdate":1511907690858,"number":3,"cdate":1511907690858,"id":"HkQOWPieM","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Review","forum":"SJ8M9yup-","replyto":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference/Paper28/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Recovering sparse signals via ReLU/Sigmoid functions","rating":"5: Marginally below acceptance threshold","review":"This paper considers the following model of a signal x = W^T h + b, where h is an m-dimensional random sparse vector, W is an m by n matrix, b is an n dimensional fixed bias vector. The random vector h follows an iid sparse signal model, each coordinate independently have some probability of being zero, and the remaining probability is distributed  among nonzero values according to some reasonable pdf/pmf. The task is to recover h, from the observation x via the activation functions like Sigmoid or ReLU. For example, \\hat{h} = Sigmoid(W^T h + b).\n\nThe authors then show that, under the random sparsity model of h, it is possible to upper bound the probability P(||h-\\hat{h}|| > \\delta. m) in terms of the parameters of the distribution of h and W and b. In some cases noise can also be tolerated. In particular, if W is incoherent (columns being near-orthonormal), then the guarantee is stronger. As far as I understood, the proofs make sense - they basically use Chernoff-bound type argument. \n\nIt is my impression that a lot of conditions have to be satisfied for the recovery guarantee to be meaningful. I am unsure if real datasets will satisfied so many conditions. Also, the usual objective of autoencoders is to denoise  - i.e. recover x, without any access to W. The authors approach in this vein seem to be only empirical. Some recent works on associative memory also assume the sparse recovery model - connections to this literature would have been of interest. It is also not clear why compressed sensing-type recovery using a single ReLU or Sigmoid would be of interest: are their complexity benefits?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1515642424346,"tcdate":1511719836228,"number":2,"cdate":1511719836228,"id":"B1NjQYOeG","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Review","forum":"SJ8M9yup-","replyto":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference/Paper28/AnonReviewer3"],"readers":["everyone"],"content":{"title":"On optimality conditions for auto-encoder signal recovery","rating":"5: Marginally below acceptance threshold","review":"This papers proposes to analyze auto-encoders under sparsity constraints of an underlying signal to be recovered.\nBased on concentration inequality, the reconstruction provided for a simple class of functions is guaranteed to be accurate in l1 norm with high probability.\nThe proof techniques are classical, but the results seem novel as far as I know.\nAs an open question, could the results be given for other lp norms, in particular for infinity-norm? Indeed, this is a privileged norm for support recovery.\n\n\n\nPresentation issues:\n- section should be Section when stating for instance \"section 1\". Idem for eq, equation, assumption...\n- bold fonts for vectors are randomly used: some care should be given to harmonizing symbols fonts.\n- equations should be cited with brackets\n\nReferences issues:\n- harmonize citations: if you add first name for some authors add it for all references: why writing Roland Makhzani and J. Wright?\n\n- Candes -> Cand\\`es\n\n- Consider citing \"Sparse approximate solutions to linear systems\", Natarajan 1995 when mentioning Amaldi and Kann 1998.\n\n\n\nSpecific comments:\npage 1:\n- hasn't -> has not.\n\npage 2:\n- \"activation function\": at this stage s_e and s_d are just functions. What is the \"activation\" refers to? Also a clarification on the space they act on should be stated. Idem for b_e and b_d.\n- \"the identity of h in eq. 1 is only well defined in the presence of l1 regularization due to the over-completeness of the dictionary\" : this is implicitly stating the uniqueness of the Lasso. Not that it is well known that there are cases where the Lasso is non-unique. Please, clarify your statement accordingly.\n- for simplicity b_d could be removed here.\n- in (4) it would be more natural to write f_j(h_j) instead of f(h_j)\n- \"has is that to be bounded\"-> is boundedness?\n- what is l_max_j here? moreover the bold letters seem to represent vectors but this should be state explicitly somewhere.\n\npage 3:\n- what is the precise meaning of \"distributed\" when referring to representation\n- In remark 1: the font has changed weirdly for W and h.\n- \"two class\"->two classes\n- Definition 1: again what is a precise definition of activation function?\n- \"if we set\": bold issue.\n- b should b_e in Theorem 1, right? Also, please recall the definition of the sigmoid function here. Moreover l_max and mu_h seem useless in this theorem... why referring to them?\n- \"if the rows of the weight matrix is\"-> if the rows of the weight matrix are\n\npage 4:\n- Proposition 1 could be stated as a theorem and Th.1 as a corollary (with e=0). The same is true for proposition 2 I suspect.\n- Again the influence of l_max and mu_h are none here...\n- Please, provide the definition of the ReLu function here. Is this just x->x_+ ?\n\npage 6:\n- R^+m -> font issue again.\n- \"are maximally incoherent\": what is the precise meaning of this statement?\n- what the motivation for Theorem 3? This should be discussed.\n- De-noising -> de-noising\n- the discussion after (15) should be made more precise.\n\npage 7:\n- Figure 1 and 2 should be postponed to page 8.\n- in Eq. (16) one needs to known E_h(x) and E_h_i(h_i), but I suspect this quantity are usually unknown to the practitioner. Can the author comment on that?\n\npage 8:\n- \"the recovery is denoised through thresholding\": where is this step analyzed?\n\npage 9:\n- figure 3: sparseness-> sparsity; also what is the activation function used here?\n- \"are then generate\"->are then generated\n- \"by greedily select\"->by greedily selecting\n- \"the the\"\n- \"and thus pre-process\"-> and thus pre-processing\n\n\nSupplementary:\npage 1:\n- please define \\sigma, and its simple properties used along the proof.\n\npage 2:\n- g should be g_j (in eq 27 - > 31)\n- overall this proof relies on ingredients such as the one used for Hoeffding's inequality.\nMost ingredients could be taken from standard tools on concentration (see for instance Boucheron, Lugosi, Massart: \"Concentration Inequalities: A Nonasymptotic Theory of Independence\", 2013).\nMoreover, some elements should be factorized as they are shared among the next proofs. This should reduce the size of the supplementary dramatically.\n\npage 7:\n- Eq. (99): it should be reminded that W_ii=1 here.\n- the upper bound used on \\mu to get equation 105 seems to be in the wrong order.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1515642424389,"tcdate":1511364284837,"number":1,"cdate":1511364284837,"id":"HySa8MQgz","invitation":"ICLR.cc/2018/Conference/-/Paper28/Official_Review","forum":"SJ8M9yup-","replyto":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference/Paper28/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Recovery guarantees for auto-encoders; a rather unclear paper, with insufficient comparisons to previous work, and no algorithmic/practical perspectives.","rating":"4: Ok but not good enough - rejection","review":"*Summary*\nThe paper studies recovery guarantees within the context of auto-encoders. Assuming a noise-corrupted linear model for the inputs x's, the paper looks at some sufficient properties (e.g., over the generating dictionary denoted by W) to recover the true underlying sparse signals (denoted by h). Several settings of increasing complexity are considered (from binary signals with no noise to noisy continuous signals). Evaluations are carried out on synthetic examples to highlight the theoretical findings.\n\nThe paper is overall difficult to read. Moreover, and importantly, no algorithmic perspectives are presented in the paper, in the sense that we do not know whether practical procedures would lead to W's satisfying the appropriate properties (unlike (not-mentioned) recent results for dictionary learning/ICA; see detailed comments). Also, assumptions are made (e.g., knowledge about expectations of h and x) for which it is unclear to see how practical/limiting they are. Finally (and as further discussed below), the paper does not sufficiently discuss related work.\n\n(note: I have not reviewed the appendix and supplementary material)\n\n*Detailed comments*\n\n-I think there is an insufficient literature review about recent recovery results in the context of sparse coding, dictionary learning and ICA (see some references at the bottom of the review). I think this is all the more important as the paper tries to draw connections with ICA (see Sec. 4.4).\nGiven that the paper positions itself on a theoretical level, detailed comparisons with existing sample complexities obtained in previous work for related models (e.g., sparse coding, dictionary learning and ICA) must be provided.\n\n-To the best of my understanding of the paper, the guarantees are about h_hat and the true h. It therefore seems that the paper's approach is very close to standard sparse inverse problems, up to the difference due to the (non-identity) activation function. If this is indeed the case, the paper should discuss its results when the activation is identity to see whether known results are recovered. \n\n-\"...we consider linear activation s_d because it is a more general case.\": Just after this statement, it is mentioned that non-linear activations are used in practice. Could this statement be therefore clarified?\n\n-Sec. 2 is unclear. For instance, it is not easy to see how one go from (1) to (2). Moreover, the concept of \"AE framework\" is not well defined.\n\n-In the bottom of page 3, why are p_i and (1-p_i) discarded?\n\n-In practice, how can we set the appropriate value of b_i?\n\n-What is the practical sense of being able to have access to E_h[x], E_x[x], and E_h[h]?\n\n-In Proposition 1 and 2, if the noise e is indeed random, it means the right-hand sides are also random variables. Then, what does the probability statement Pr mean on the left-hand side? Is is conditioned on the draw of e? Some clarifications are required.\n\n-Typo page 7: \"...that used to generate the data.\" --> \"... used to generate the data.\"\n-Typo page 9: \"...data are then generate...\" --> \"...data are then generated...\"\n\n-In Sec. 5.3, to match W_hat and W, the Hungarian algorithm can probably be used.\n\n*References*\n\n(Arora2012) Arora, S.; Ge, R.; Moitra, A. & Sachdeva, S. Provable ICA with unknown Gaussian noise, with implications for Gaussian mixtures and autoencoders Advances in Neural Information Processing Systems (NIPS), 2012, 2375-2383\n\n(Arora2013) Arora, S.; Ge, R. & Moitra, A. New algorithms for learning incoherent and overcomplete dictionaries preprint arXiv:1308.6273, 2013\n\n(Chatterji2017) Chatterji, N. S. & Bartlett, P. L. Alternating minimization for dictionary learning with random initialization preprint arXiv:1711.03634, 2017\n\n(Gribonval2015) Gribonval, R.; Jenatton, R. & Bach, F. Sparse and spurious: dictionary learning with noise and outliers IEEE Transactions on Information Theory, 2015, 61, 6298-6319\n\n(Sun2015) Sun, J.; Qu, Q. & Wright, J. Complete dictionary recovery over the sphere Sampling Theory and Applications (SampTA), 2015 International Conference on, 2015, 407-410\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]}},{"tddate":null,"ddate":null,"tmdate":1515129642102,"tcdate":1508534798355,"number":28,"cdate":1509739520514,"id":"SJ8M9yup-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ8M9yup-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On Optimality Conditions for Auto-Encoder Signal Recovery","abstract":"Auto-Encoders are unsupervised models that aim to learn  patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this signal recovery perspective enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the true hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically also demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.","pdf":"/pdf/e21c2e8c1a73c0137fbf115110a023bd50cd92c8.pdf","paperhash":"anonymous|on_optimality_conditions_for_autoencoder_signal_recovery","_bibtex":"@article{\n  anonymous2018on,\n  title={On Optimality Conditions for Auto-Encoder Signal Recovery},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ8M9yup-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper28/Authors"],"keywords":["Auto Encoder","Signal Recovery","Sparse Coding"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}