{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222621670,"tcdate":1511815209706,"number":3,"cdate":1511815209706,"id":"rkGE_e9ez","invitation":"ICLR.cc/2018/Conference/-/Paper334/Official_Review","forum":"HyDu7FgAW","replyto":"HyDu7FgAW","signatures":["ICLR.cc/2018/Conference/Paper334/AnonReviewer2"],"readers":["everyone"],"content":{"title":"-","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to use a hybrid of convolutional and recurrent networks to predict the DSL specification of a GUI given a screenshot of the GUI.\n\n\nPros:\n\n\nThe paper is clear and the proposed problem is novel and well-defined.\n\nThe training data is synthetic, allowing for arbitrarily large training sets to be generated.  The authors have made their synthetic dataset publicly available.\n\nThe method seems to work well based on the samples and ROC curves presented.\n\n\nCons:\n\nThis is mostly an application of an existing method to a new domain -- as stated in the related work section, effectively the same convnet+RNN architecture has been in common use for image captioning and other vision applications.\n\nThe UIs that are represented in the dataset seem quite simple; it’s not clear that this will transfer to arbitrarily complex and multi-page UIs.\n\nThe main motivation for the proposed system seems to be for non-technical designers to be able to implement UIs just by drawing a mockup screenshot.  However, the paper hasn’t shown that this is necessarily possible assuming the hand-designed mockups aren’t pixel-for-pixel matches with a screenshot that could be generated by the “DSL code -> screenshot” mapping that this system learns to invert.\n\nThere exist a number of “drag and drop” style UI design products (at least for HTML) that would seem to accomplish the same basic goal as the proposed system in a more reliable way. (Though the proposed system does have the advantage of only requiring a screenshot created using any software, rather than being restricted to a particular piece of software.)\n\n\nOverall, the paper is well-written but the novelty and applicability seems a bit limited.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"pix2code: Generating Code from a Graphical User Interface Screenshot","abstract":"Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).","pdf":"/pdf/55a2d5f274c299db7ccd03a019a2a5774e39754d.pdf","TL;DR":"CNN and LSTM to generate markup-like code describing graphical user interface images.","paperhash":"anonymous|pix2code_generating_code_from_a_graphical_user_interface_screenshot","_bibtex":"@article{\n  anonymous2018pix2code:,\n  title={pix2code: Generating Code from a Graphical User Interface Screenshot},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyDu7FgAW}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper334/Authors"],"keywords":["computer vision","scene understanding","text processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222621712,"tcdate":1511729524062,"number":2,"cdate":1511729524062,"id":"Sy3OtodxG","invitation":"ICLR.cc/2018/Conference/-/Paper334/Official_Review","forum":"HyDu7FgAW","replyto":"HyDu7FgAW","signatures":["ICLR.cc/2018/Conference/Paper334/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting task and dataset, but limited technical contribution and experiments","rating":"5: Marginally below acceptance threshold","review":"The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface. Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time. Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released.\n\nPros:\n- Generating programs with neural networks is an exciting direction\n- Novel task of generating UI code from UI screenshots\n- Three new datasets of UI images and corresponding code\n- Paper is clearly written\n\nCons:\n- Limited technical novelty\n- Limited experiments\n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research. Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest. The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task.\n\nMy main concern with this paper is a lack of technical novelty. The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now. Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand.\n\nThe experiments in this paper are also lacking. Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments. The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples. I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods. I would have also liked to see more innovation in evaluation. Are there metrics other than overall accuracy that could be used to measure performance? Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task? In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics? Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases? How does performance change with different amounts of training data? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another? I don’t expect the authors to answer any of these questions in particular; I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset.\n\nOn the whole I appreciate the novelty of the task and dataset, but the paper suffers from a lack of technical novelty in the model and limited experimental validation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"pix2code: Generating Code from a Graphical User Interface Screenshot","abstract":"Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).","pdf":"/pdf/55a2d5f274c299db7ccd03a019a2a5774e39754d.pdf","TL;DR":"CNN and LSTM to generate markup-like code describing graphical user interface images.","paperhash":"anonymous|pix2code_generating_code_from_a_graphical_user_interface_screenshot","_bibtex":"@article{\n  anonymous2018pix2code:,\n  title={pix2code: Generating Code from a Graphical User Interface Screenshot},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyDu7FgAW}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper334/Authors"],"keywords":["computer vision","scene understanding","text processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222621753,"tcdate":1511691105723,"number":1,"cdate":1511691105723,"id":"rJqwmMueG","invitation":"ICLR.cc/2018/Conference/-/Paper334/Official_Review","forum":"HyDu7FgAW","replyto":"HyDu7FgAW","signatures":["ICLR.cc/2018/Conference/Paper334/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Even though this paper tackles an interesting problem, it lacks novelty and originally on the model side and it doesn't contain enough experiments.","rating":"2: Strong rejection","review":"This paper proposed an end-to-end network that generates computer tokens from a single GUI screenshot as input. \n\nEven though the introduced dataset in this paper is interesting, there are some issues:\n- On the model side, this paper used the same architecture as the Karpathy & Fei-Fei (2015). As a result, in my view, the paper has limited novelty and originality.\n- Experiments: The experiments are not enough at all. More specifically, the paper didn't establish any baseline to show the difficulty of this problem and the dataset. Without a reasonable baseline, it is hard to see how difficult is this dataset and this problem and can't say anything about the significance of this problem in this paper.\n- The related works: Some recent papers in program synthesis are missing and should have been included in this paper such as:\nRobustFill: Neural Program Learning under Noisy I/O, ICML 2017\n\nSome other comments:\n- In the last paragraph of section 3 and first paragraph section 3.1, the authors mentioned that \"...CNN to perform unsupervised feature learning...\". In my view, it is not a correct statement to call the feature extraction from a CNN unsupervised feature learning as the referred CNNs in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as well.     \n \n\nUnfortunately, At this point, I do not see a sufficient contribution to warrant publication in ICLR.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"pix2code: Generating Code from a Graphical User Interface Screenshot","abstract":"Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).","pdf":"/pdf/55a2d5f274c299db7ccd03a019a2a5774e39754d.pdf","TL;DR":"CNN and LSTM to generate markup-like code describing graphical user interface images.","paperhash":"anonymous|pix2code_generating_code_from_a_graphical_user_interface_screenshot","_bibtex":"@article{\n  anonymous2018pix2code:,\n  title={pix2code: Generating Code from a Graphical User Interface Screenshot},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyDu7FgAW}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper334/Authors"],"keywords":["computer vision","scene understanding","text processing"]}}],"limit":2000,"offset":0}