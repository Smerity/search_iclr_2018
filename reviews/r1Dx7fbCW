{"notes":[{"tddate":null,"ddate":null,"tmdate":1512320830983,"tcdate":1512320830983,"number":4,"cdate":1512320830983,"id":"rkDrknW-G","invitation":"ICLR.cc/2018/Conference/-/Paper800/Official_Review","forum":"r1Dx7fbCW","replyto":"r1Dx7fbCW","signatures":["ICLR.cc/2018/Conference/Paper800/AnonReviewer4"],"readers":["everyone"],"content":{"title":"A novel data augmentation method guided by domain specific information","rating":"7: Good paper, accept","review":"The authors proposed to perturbed the estimated domain features for data augmentation, which is done by using the gradients of label and domain classification losses. The idea is interesting and new. And the paper is well written.\n\nMy major conerns are as follows:\n1. Section 3 seems a bit too lengthy or redundant to derive the data augmentation by introducing the latent domain features g. In fact, without g, it also makes sense to perturb x as done in lines 6 and 7 in Alg. 1.\n2. The assumption in (A1) can only be guaranteed under certain theoretical conditions. The authors should provide more explanations to better convey the assumption to readers.\n\nMinors:\n1. LabelGrad was not defined when firstly being used in Section 4.\n2. Fig. 1 looks redundant.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING","abstract":"We present CROSSGRAD , a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD jointly trains a label and a domain classifier on examples perturbed by loss gradients of each other’s objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that\n (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and \n(2) data augmentation is a more stable and accurate method than domain adversarial training.","pdf":"/pdf/d829e0feb3e194ff2f90b6164f258866657b1410.pdf","TL;DR":"Domain guided augmentation of data provides a robust and stable method of domain generalization","paperhash":"anonymous|generalizing_across_domains_via_crossgradient_training","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1Dx7fbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper800/Authors"],"keywords":["domain generalization","domain adaptation","adversarial learning","adversarial examples"]}},{"tddate":null,"ddate":null,"tmdate":1512243913855,"tcdate":1512243913855,"number":3,"cdate":1512243913855,"id":"SJbAMFl-z","invitation":"ICLR.cc/2018/Conference/-/Paper800/Official_Review","forum":"r1Dx7fbCW","replyto":"r1Dx7fbCW","signatures":["ICLR.cc/2018/Conference/Paper800/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A novel domain-robust training method","rating":"8: Top 50% of accepted papers, clear accept","review":"Quality, clarity : Very well written, well motivated, convincing experiments and analysis\nOriginality: I think they framed the problem of domain-robustness very well: how to obtain a \"domain level embedding\" which generalizes to unseen domains. To do this the  authors introduce the CrossGrad method, which trains both a label classification task and a domain classification task (from which the domain-embedding is obtained)\nSignificance: Robustness in new domains is a very important practical and theoretical issue.\n\nPros:\n- It's novel, interesting, well written, and appears to work very well in the experiments provided.\n\nCons:\n- Formally, for the embedding to generalize one needs to make the \"domain continuity assumption\", which is not guaranteed to hold in any realistic settings (e.g. when there are no underlying continuous  factors) \n- The training set needs to be in the form (x,y,d) where 'd' is a domain, this information might not exist or be only partially present.\n- A single step required 2 forward and 2 backward passes - thus is twice as expensive. \n\nConstructive comments:\n- Algorithm 1 uses both X_l and X_d, yet the text only discusses X_d, there is some symmetry, but more discussion will help.\n- LabelGrad is mentioned in section 4 but defined in section 4.1, it should be briefly defined in the first mention.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING","abstract":"We present CROSSGRAD , a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD jointly trains a label and a domain classifier on examples perturbed by loss gradients of each other’s objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that\n (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and \n(2) data augmentation is a more stable and accurate method than domain adversarial training.","pdf":"/pdf/d829e0feb3e194ff2f90b6164f258866657b1410.pdf","TL;DR":"Domain guided augmentation of data provides a robust and stable method of domain generalization","paperhash":"anonymous|generalizing_across_domains_via_crossgradient_training","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1Dx7fbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper800/Authors"],"keywords":["domain generalization","domain adaptation","adversarial learning","adversarial examples"]}},{"tddate":null,"ddate":null,"tmdate":1512222769312,"tcdate":1512075460730,"number":2,"cdate":1512075460730,"id":"Syaaxl0lf","invitation":"ICLR.cc/2018/Conference/-/Paper800/Official_Review","forum":"r1Dx7fbCW","replyto":"r1Dx7fbCW","signatures":["ICLR.cc/2018/Conference/Paper800/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors introduce the CROSSGRAD method to address the problem of  multi-domain data to adaptation used to learn a classifier that generalizes to new domains. The  experimental validation on benchmark datasets shows that the proposed 'data augmentation' using multi-domain data leads to improved performance compared to traditional approaches for the task. ","rating":"7: Good paper, accept","review":"The method is posed in the Bayesian setting, the main idea being to achieve the data augmentation through domain-guided perturbations of input instances. Different from traditional adaptation methods, where the adaptation step is applied explicitly, in this paper the authors exploit labeled instances from several domains to collectively train a system that can handle new domains without the adaptation step. While this is another way of looking at domain adaptation, it may be misleading to say 'without' adaptation step. By the gradient perturbations on multi-domain training data, the learning of the adaptation step is effectively done. This should be clarified in the paper. The notion of using 'scarce' training domains to cover possible choices for the target domain is interesting and novel. The experimental validation should also include a deeper analysis of this factor: how the proposed adaptation performance is affected by the scarcity of the training multi-domain data. While this is partially shown in Table 8, it seems that by adding more domains the performance is compromised (compared to the baseline) (?).  It would be useful to see how the model ranks the multiple domains in terms of their relatedness to the target domain. Figs 6-7 are unclear and difficult to read. The captions should provide more information about the main point of these figures. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING","abstract":"We present CROSSGRAD , a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD jointly trains a label and a domain classifier on examples perturbed by loss gradients of each other’s objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that\n (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and \n(2) data augmentation is a more stable and accurate method than domain adversarial training.","pdf":"/pdf/d829e0feb3e194ff2f90b6164f258866657b1410.pdf","TL;DR":"Domain guided augmentation of data provides a robust and stable method of domain generalization","paperhash":"anonymous|generalizing_across_domains_via_crossgradient_training","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1Dx7fbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper800/Authors"],"keywords":["domain generalization","domain adaptation","adversarial learning","adversarial examples"]}},{"tddate":null,"ddate":null,"tmdate":1512222769355,"tcdate":1511818022836,"number":1,"cdate":1511818022836,"id":"rkJNQW5gM","invitation":"ICLR.cc/2018/Conference/-/Paper800/Official_Review","forum":"r1Dx7fbCW","replyto":"r1Dx7fbCW","signatures":["ICLR.cc/2018/Conference/Paper800/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novel approach and extensive experiments. Many related works are missing.","rating":"7: Good paper, accept","review":"This paper proposed a domain generalization approach by domain-dependent data augmentation. The augmentation is guided by a network that is trained to classify a data point to different domains. Experiments on four datasets verify the effectiveness of the proposed approach. \n\nStrengths:\n+ The proposed classification model is domain-dependent, as opposed to being domain-invariant. This is new and differs from most existing works on domain adaptation/generalization, to the best of my knowledge. \n+ The experiments show that the proposed method outperforms two baselines. However, more related approaches could be included to strengthen the experiments (see below for details).\n\n\nWeaknesses:\n- The paper studies domain generalization and yet fails to position it in the right literature. By a simple search of \"domain generalization\" using Google Scholar, I found several existing works on this problem and have listed some below. The authors may consider to include them in both the related works and the experiments. \n\nQuestions: \n1. It is intuitive to directly define the data augmentation by x_i+Grad_x J_d. Why is it necessary to instead define it as the inverse transformation G^{-1}(g') and then go through the approximations to derive the final augmentation? \n2. Is the CrossGrad training necessary? What if one trains the network in two steps? Step 1: learn G using J_d and a regularization to avoid misclassification over the labels using the original data. Step 2: Learn the classification network (possibly different from G) by the domain-dependent augmentation.\n\n\nSaeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Unified deep supervised\ndomain adaptation and generalization. In IEEE International Conference on Computer\nVision (ICCV), 2017.\n\nMuandet, K., Balduzzi, D. and Schölkopf, B., 2013. Domain generalization via invariant feature representation. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) (pp. 10-18).\n\nXu, Z., Li, W., Niu, L. and Xu, D., 2014, September. Exploiting low-rank structure from latent domains for domain generalization. In European Conference on Computer Vision (pp. 628-643). Springer, Cham.\n\nGhifary, M., Bastiaan Kleijn, W., Zhang, M. and Balduzzi, D., 2015. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision (pp. 2551-2559).\n\nGan, C., Yang, T. and Gong, B., 2016. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 87-97).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING","abstract":"We present CROSSGRAD , a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD jointly trains a label and a domain classifier on examples perturbed by loss gradients of each other’s objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that\n (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and \n(2) data augmentation is a more stable and accurate method than domain adversarial training.","pdf":"/pdf/d829e0feb3e194ff2f90b6164f258866657b1410.pdf","TL;DR":"Domain guided augmentation of data provides a robust and stable method of domain generalization","paperhash":"anonymous|generalizing_across_domains_via_crossgradient_training","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1Dx7fbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper800/Authors"],"keywords":["domain generalization","domain adaptation","adversarial learning","adversarial examples"]}},{"tddate":null,"ddate":null,"tmdate":1509739094698,"tcdate":1509135086938,"number":800,"cdate":1509739092035,"id":"r1Dx7fbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1Dx7fbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING","abstract":"We present CROSSGRAD , a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD jointly trains a label and a domain classifier on examples perturbed by loss gradients of each other’s objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that\n (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and \n(2) data augmentation is a more stable and accurate method than domain adversarial training.","pdf":"/pdf/d829e0feb3e194ff2f90b6164f258866657b1410.pdf","TL;DR":"Domain guided augmentation of data provides a robust and stable method of domain generalization","paperhash":"anonymous|generalizing_across_domains_via_crossgradient_training","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={GENERALIZING ACROSS DOMAINS VIA CROSS-GRADIENT TRAINING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1Dx7fbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper800/Authors"],"keywords":["domain generalization","domain adaptation","adversarial learning","adversarial examples"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}