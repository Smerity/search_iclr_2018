{"notes":[{"tddate":null,"ddate":null,"tmdate":1512428428364,"tcdate":1512428428364,"number":3,"cdate":1512428428364,"id":"By4qQIQ-f","invitation":"ICLR.cc/2018/Conference/-/Paper190/Official_Review","forum":"SkFvV0yC-","replyto":"SkFvV0yC-","signatures":["ICLR.cc/2018/Conference/Paper190/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Interesting paper, but requires more clarifications on novelty and experiment results","rating":"5: Marginally below acceptance threshold","review":"This paper proposed an iterative learning scheme to train a very deep convolutional neural network. Instead of learning a deep network from scratch, the authors proposed to gradually increase the depth of the network while transferring the knowledge obtained from the shallower network by applying network morphism. \n\nOverall, the paper is clearly written and the proposed ideas are interesting. However, many parts of the ideas discussed in the paper (Section 3.3) are already investigated in Wei et al., 2016, which limits the novel contribution of the paper. Besides, the best performances obtained by the proposed method are generally much lower than the ones reported by the existing methods (e.g. He et al., 2016) except cifar-10 experiment, which makes it hard for the readers to convince that the proposed method is superior than the existing ones. More thorough discussions are required.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network Iterative Learning for Dynamic Deep Neural Networks via Morphism","abstract":"In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.","pdf":"/pdf/7bc2e45e0c4c6ffd4d6919980bfd6cd95bb275a0.pdf","paperhash":"anonymous|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism","_bibtex":"@article{\n  anonymous2018network,\n  title={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFvV0yC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper190/Authors"],"keywords":["Network Iterative Learning","Morphism"]}},{"tddate":null,"ddate":null,"tmdate":1512222587053,"tcdate":1511823183973,"number":2,"cdate":1511823183973,"id":"BkdUwGqgz","invitation":"ICLR.cc/2018/Conference/-/Paper190/Official_Review","forum":"SkFvV0yC-","replyto":"SkFvV0yC-","signatures":["ICLR.cc/2018/Conference/Paper190/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting discussion, but not novel enough","rating":"5: Marginally below acceptance threshold","review":"This paper proposes an iterative approach to train deep neural networks based on morphism of the network structure into more complex ones. The ideas are rather simple, but could be potentially important for improving the performance of the networks. On the other hand, it seems that an important part of the work has already been done before (in particular Wei et al. 2016), and that the differences from there are very ad-hoc and intuition for why they work is not present. Instead, the paper justifies its approach by arguing that the experimental results are good. Personally, I am skeptical with that, because interesting ideas with great added value usually have some cool intuition behind them. The paper is easy to read, and there does not seem to exist major errors. Because I am not an active researcher in the topic, I cannot judge if the benefits that are shown in the experiments are enough for publication (the theoretical part is not the strongest of the paper).","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network Iterative Learning for Dynamic Deep Neural Networks via Morphism","abstract":"In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.","pdf":"/pdf/7bc2e45e0c4c6ffd4d6919980bfd6cd95bb275a0.pdf","paperhash":"anonymous|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism","_bibtex":"@article{\n  anonymous2018network,\n  title={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFvV0yC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper190/Authors"],"keywords":["Network Iterative Learning","Morphism"]}},{"tddate":null,"ddate":null,"tmdate":1512222587090,"tcdate":1511800609284,"number":1,"cdate":1511800609284,"id":"S1YX1ptgz","invitation":"ICLR.cc/2018/Conference/-/Paper190/Official_Review","forum":"SkFvV0yC-","replyto":"SkFvV0yC-","signatures":["ICLR.cc/2018/Conference/Paper190/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The submission looks interesting and is well-written","rating":"7: Good paper, accept","review":"This submission develops a learning scheme for training deep neural networks with adoption of network morphism (Wei et al., 2016), which optimizes a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized, instead of directly optimizing a static objective function. Overall, the idea looks interesting and the manuscript is well-written. The shown experimental results should be able to validate the effectiveness of the learning scheme to some extent.\n\nIt would be more convincing to include the performance evaluation of the learning scheme in some representative applications, since the optimality of the training objective function is not necessarily the same as that of the trained network in the application of interest.\n\nBelow are two minor issues:\n\n- In page 2, it is stated that Fig. 2(e) illustrates the idea of the proposed network iterative learning scheme for deep neural networks based on network morphism. However, the idea seems not clear from Fig. 2(e).\n\n- In page 4, “such network iterative learning process” should be “such a network iterative learning process”.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Network Iterative Learning for Dynamic Deep Neural Networks via Morphism","abstract":"In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.","pdf":"/pdf/7bc2e45e0c4c6ffd4d6919980bfd6cd95bb275a0.pdf","paperhash":"anonymous|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism","_bibtex":"@article{\n  anonymous2018network,\n  title={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFvV0yC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper190/Authors"],"keywords":["Network Iterative Learning","Morphism"]}},{"tddate":null,"ddate":null,"tmdate":1509739436998,"tcdate":1509053537113,"number":190,"cdate":1509739434337,"id":"SkFvV0yC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkFvV0yC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Network Iterative Learning for Dynamic Deep Neural Networks via Morphism","abstract":"In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.","pdf":"/pdf/7bc2e45e0c4c6ffd4d6919980bfd6cd95bb275a0.pdf","paperhash":"anonymous|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism","_bibtex":"@article{\n  anonymous2018network,\n  title={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFvV0yC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper190/Authors"],"keywords":["Network Iterative Learning","Morphism"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}