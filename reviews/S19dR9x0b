{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222625550,"tcdate":1511884287829,"number":2,"cdate":1511884287829,"id":"HyOWIZjeM","invitation":"ICLR.cc/2018/Conference/-/Paper361/Official_Review","forum":"S19dR9x0b","replyto":"S19dR9x0b","signatures":["ICLR.cc/2018/Conference/Paper361/AnonReviewer2"],"readers":["everyone"],"content":{"title":"multi-bit quantization method for recurrent neural networks","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper introduces a multi-bit quantization method for recurrent neural networks, which is built on alternating the minimization formulated by Guo et al. 2017 by first fixing the \\alpha values and then finding the optimal binary codes b_i with a BST, to then estimate \\alpha with the refined approximation by Guo et al. 2017, iteratively. The observation that the optimal binary code can be computed with a BST is simple and elegant.\n\nThe paper is easy to follow and the topic of reducing memory and speeding up computations for RNN and DNN is interesting and relevant to the community.\n\nThe overall contribution on model quantization is based on existing methods, which makes the novelty of the paper suffer a bit. Said that, applying it to RNN is a convincing and a strong motivation. Also, in the paper it is shown how the matrix multiplications of the quantized model can be speeded up using 64 bits operation in CPU. This is, not only saves memory storage and usage, but also on runtime calculation using CPU, which is an important characteristic when there are limited computational resources.\n\nResults on language models show that the models with quantized weights with 3 bits obtain the same or even slightly better performance on the tested datasets with impressive speed-ups and memory savings.\n\nFor completeness, it would be interesting, and I would strongly encourage to add a discussion or even an experiment using feedforward DNN with a simple dataset as MNIST, as most of previous work discussed in the paper report experiments on DNN that are feedforward. Would the speed-ups and memory savings obtained for RNN hold also for feedforward networks?\n\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Alternating Multi-bit Quantization for Recurrent Neural Networks","abstract":"Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gate recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  potential ~13.5x   inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and potential ~6.5x inference acceleration. Both results beat the exiting quantization works with large margins.  ","pdf":"/pdf/f9324f8aff50050cf350fafbc52452934489f4fd.pdf","TL;DR":"We propose a  new  quantization method and apply it to quantize RNNs for both compression and acceleration","paperhash":"anonymous|alternating_multibit_quantization_for_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018alternating,\n  title={Alternating Multi-bit Quantization for Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S19dR9x0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper361/Authors"],"keywords":["Alternating Minimization","Quantized Recurrent Neural Network","Binary Search Tree"]}},{"tddate":null,"ddate":null,"tmdate":1512222625584,"tcdate":1511810700333,"number":1,"cdate":1511810700333,"id":"BJz5LyclM","invitation":"ICLR.cc/2018/Conference/-/Paper361/Official_Review","forum":"S19dR9x0b","replyto":"S19dR9x0b","signatures":["ICLR.cc/2018/Conference/Paper361/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nicely written paper","rating":"7: Good paper, accept","review":"\nSummary of the paper\n-------------------------------\n\nThe authors propose a new way to perform multi-bit quantization based on greedy approximation and binary search tree for RNNs. They first show how this method, applied to the parameters only, performs on pre-trained networks and show great performances compared to other existing techniques on PTB. Then they present results with the method applied to both parameters and activations during training on 3 NLP datasets, showing again great performances compared to existing technique.\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: The paper is clearly written.\n\nSignificance: I'm not familiar with the quantization literature, so I'll let more knowledgeable reviewers evaluate this point.\n\nCorrectness: The paper is technically correct.\n\nQuestions\n--------------\n\n1. It would be nice to have those memory and speed gains for training as well. Is it possible to use those quantization methods to train networks from scratch, i.e. without using a pre-train model?\n\nPros\n------\n\n1. The paper defines clear goals and contributions.\n2. Existing methods (and their differences) are clearly and concisely presented.\n3. The proposed method is well explained.\n4. The experimental setup shows clear results compared to the non-quantized baselines and other quantization techniques.\n\nCons\n-------\n\n1. It would be nice to have another experiment not based on text (speech recognition / synthesis, audio, biological signals, ...) to see how it generalizes to other kind of data (although I can't see why it wouldn't).\n\nTypos\n--------\n\n1. abstract: \"gate recurrent unit\" -> \"gated recurrent unit\"\n2. equation (6): remove parenthesis in c_(t-1)\n3. section 4, paragraph 1: \"For the weight matrices, instead of on the whole, we quantize them row by row.\" -> \"We don't apply quantization on the full matrices but rather row by row.\"\n4. section 4, paragraph 2: Which W matrix is it? W_h? (2x)\n\nNote\n-------\n\nSince I'm not familiar with the quantization literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Alternating Multi-bit Quantization for Recurrent Neural Networks","abstract":"Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gate recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  potential ~13.5x   inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and potential ~6.5x inference acceleration. Both results beat the exiting quantization works with large margins.  ","pdf":"/pdf/f9324f8aff50050cf350fafbc52452934489f4fd.pdf","TL;DR":"We propose a  new  quantization method and apply it to quantize RNNs for both compression and acceleration","paperhash":"anonymous|alternating_multibit_quantization_for_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018alternating,\n  title={Alternating Multi-bit Quantization for Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S19dR9x0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper361/Authors"],"keywords":["Alternating Minimization","Quantized Recurrent Neural Network","Binary Search Tree"]}},{"tddate":null,"ddate":null,"tmdate":1509739343919,"tcdate":1509105265711,"number":361,"cdate":1509739341237,"id":"S19dR9x0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S19dR9x0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Alternating Multi-bit Quantization for Recurrent Neural Networks","abstract":"Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied.  We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gate recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and  potential ~13.5x   inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and potential ~6.5x inference acceleration. Both results beat the exiting quantization works with large margins.  ","pdf":"/pdf/f9324f8aff50050cf350fafbc52452934489f4fd.pdf","TL;DR":"We propose a  new  quantization method and apply it to quantize RNNs for both compression and acceleration","paperhash":"anonymous|alternating_multibit_quantization_for_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018alternating,\n  title={Alternating Multi-bit Quantization for Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S19dR9x0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper361/Authors"],"keywords":["Alternating Minimization","Quantized Recurrent Neural Network","Binary Search Tree"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}