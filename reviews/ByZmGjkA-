{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222577985,"tcdate":1511845197841,"number":3,"cdate":1511845197841,"id":"HJULpw9gz","invitation":"ICLR.cc/2018/Conference/-/Paper156/Official_Review","forum":"ByZmGjkA-","replyto":"ByZmGjkA-","signatures":["ICLR.cc/2018/Conference/Paper156/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting work that could be grounded more strongly in cognitive science","rating":"7: Good paper, accept","review":"This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information. The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. I think this is a nice example of a detailed analysis of the representations acquired by a reinforcement learning agent. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Nonetheless the paper takes on an ambitious goal of relating questions in machine learning in cognitive science and does a reasonably good job of analyzing the results.\n\nComments:\n\n1. The results on word learning biases are not particularly surprising given previous work in this area, much of which has used similar neural network models. Linda Smith and Eliana Colunga have published a series of papers that explore these questions in detail:\n\nhttp://www.iub.edu/~cogdev/labwork/kinds.pdf\nhttp://www.iub.edu/~cogdev/labwork/Ontology2003.pdf\n\n2. In figure 2 and the associated analyses, why were 20 shape terms used rather than 8 to parallel the other cases? It seems like there is a strong basic color bias. This seems like one of the most novel findings in the paper and is worth highlighting.\n\nThis figure and the corresponding analysis could be made more systematic by mapping out the degree of shape versus color bias as a function of the number of shape and color terms in a 2D plot. The resulting plot would show the degree of bias towards color.\n\n3. The section on curriculum learning does not mention relevant work on “starting small”  and the “less is more\" hypothesis in language development by Jeff Elman and Elissa Newport:\n\nhttps://pdfs.semanticscholar.org/371b/240bebcaa68921aa87db4cd3a5d4e2a3a36b.pdf\nhttp://www.sciencedirect.com/science/article/pii/0388000188900101\n\n4. The section on learning speeds could include more information on the actual patterns that are found with human learners, for example the color words are typically acquired later. I found these human results hard to reconcile with the results from the models. I also found it hard to understand why colors were hard to learn given the bias towards colors shown earlier in the paper.\n\n5. The section on layerwise attention claims to give a “computational level” explanation, but this is a misleading term to use — it is not a computational level explanation in the sense introduced by David Marr which is the standard use of this term in cognitive science. The explanation of layerwise attention could be clearer.\n\nMinor:\n\n“analagous” -> “analogous”\n\nThe paper runs longer than eight pages, and it is not obvious that the extra space is warranted.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Grounded Language Learning Agents","abstract":"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","pdf":"/pdf/3ade44df946e4d039d5f051cfe63aed510ed94d4.pdf","TL;DR":"Analysing and understanding how neural network agents learn to understand simple grounded language","paperhash":"anonymous|understanding_grounded_language_learning_agents","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Grounded Language Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByZmGjkA-}\n}","keywords":["Language AI Learning Reinforcement Deep"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper156/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222578037,"tcdate":1511741353902,"number":2,"cdate":1511741353902,"id":"r1-nPRulz","invitation":"ICLR.cc/2018/Conference/-/Paper156/Official_Review","forum":"ByZmGjkA-","replyto":"ByZmGjkA-","signatures":["ICLR.cc/2018/Conference/Paper156/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An innovating and interesting approach, but there are serious, lingering concerns about the approach ","rating":"5: Marginally below acceptance threshold","review":"In this manuscript, the authors connect psychological experimental methods to understand how the black box of the mind solves problems with current issues in understanding how the black box of deep learning methods solves problems. The authors used situated versions of human language learning tasks as simulation environments to test a CNN + LSTM deep learning network. They examined a few key phenomena: shape/color bias, learning negation concepts, incremental learning, and how learning affects the representation of objects via attention-like processes. They illustrated conditions in which their deep learning network acts similarly to people in simulations.\nDeveloping methods that enable humans to understand how deep learning models solve problems is an important problem for many reasons (e.g., usability of models for science, ethical concerns) that has captured the interest of a wide range of researchers. By adapting experimental methodology from psychology to test that have been used to understand and explain the internal workings of the mind, the authors approach the problem in a novel and innovative manner. I was impressed by the range of phenomena they tackled and their analyses were informative in understanding the behavior of deep learning models \nI found the analogy persuasive in theory, but I was not convinced that the current manuscript really demonstrates its value. In particular, I did not see the value of situating their model in a grounded environment. One analysis that would have helped convince me is a comparison to an equivalent non-grounded deep learning model (e.g., a CNN trained to make equivalent classifications), and show how this would not help us understand human behavior. However, the more I thought about the logic of this type of analysis, the more concerned I became about the logic of their approach. \nWhat would it mean if the equivalent non-situated model does not show the phenomena? If it does not, it could illustrate the efficacy of using situated environments. But, it also could mean that their technique acts differently for equivalent situated and non-situated models. In this case though, what would we learn about the more general non-situated case then? It does not seem like we would learn much, which would defeat the purpose of the technique. Alternatively, if the equivalent non-situated model does show the phenomena, then using the situated version would not be useful because the model acts equivalently in both cases. I am not fully convinced by the argument I just sketched, but leaves me very concerned about the usefulness of their approach. (note that the “controlled” vs. “naturalistic” analyses in the word learning section did not convince me. This argues for the importance of using naturalistic statistics – not necessarily cross-modal, situated environments as the authors argue for).\nAdditionally, I was unconvinced that simpler models could not be used to examine the phenomena that they analyzed. Although combining LSTM with CNN via a “mixing” module was interesting, it added another layer of complexity that made it more difficult to assess what the results meant. This left me less convinced of the usefulness of their paradigm. If we need to create a novel deep learning method to illustrate its efficacy, how will it be useful for solving the problem that motivated everything: understanding how pre-existing deep learning methods solve problems. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Grounded Language Learning Agents","abstract":"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","pdf":"/pdf/3ade44df946e4d039d5f051cfe63aed510ed94d4.pdf","TL;DR":"Analysing and understanding how neural network agents learn to understand simple grounded language","paperhash":"anonymous|understanding_grounded_language_learning_agents","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Grounded Language Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByZmGjkA-}\n}","keywords":["Language AI Learning Reinforcement Deep"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper156/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222578081,"tcdate":1510520789454,"number":1,"cdate":1510520789454,"id":"SyTAvN8yf","invitation":"ICLR.cc/2018/Conference/-/Paper156/Official_Review","forum":"ByZmGjkA-","replyto":"ByZmGjkA-","signatures":["ICLR.cc/2018/Conference/Paper156/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Worthy goal, but implementation felt a bit underwhelming","rating":"4: Ok but not good enough - rejection","review":"This paper presents an analysis of an agent trained to follow linguistic commands in a 3D environment.  The behaviour of the agent is analyzed by means of a set of \"psycholinguistic\" experiments probing what it learned, and by inspection of its visual component through an attentional mechanism.\n\nOn the positive side, it is nice to read a paper that focuses on understanding what an agent is learning. On the negative side, I did not get many new insights from the analyses presented in the study.\n\n3 A situated language learning agent\n\nI can't make up the chair from the refrigerator in the figure.\n\n4.1 Word learning biases\n\nThis experiment shows that, when an agent is trained on shapes only, it will exhibit a shape bias when tested on new shapes and colors. Conversely, when it is exposed to colors only, it will have a color bias. When the training set is balanced, the agent shows a mild bias for the simpler color property. How is this interesting or surprising? The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human-like shape bias. This, however, is not addressed in the paper.\n\nMinor comments about this section:\n\n- Was there noise also in shape generation, or were all object instances identical?\n\n- propensity to select o_2: rather o_1?\n\n- I did not follow the paragraph starting with \"This effect provides\".\n\n4.2 The problem of learning negation\n\nI found this experiment very interesting.\n\nPerhaps, the authors could be more explicit about the usage of negation here. The meaning of commands containing negation are, I think, conjunctions of the form \"pick something and do not pick X\" (as opposed to the more natural \"do not pick X\").\n\nmodifiation: modification\n\n4.3 Curriculum learning\n\nPerhaps the difference in curriculum effectiveness in language modeling vs grounded language learning simulations is due to the fact that the former operates on large amounts of natural data, where it's hard to define the curriculum, while the latter are typically grounded in toy worlds with a controlled language, where it's easier to construct the curriculum.\n\n4.4 Processing and representation differences\n\nThere is virtually no discussion of what makes the naturalistic setup naturalistic, and thus it's not clear which conclusions we should derive from the corresponding experiments. Also, I don't see what we should learn from Figure 5 (besides the fact that in the controlled condition shapes are easier than categories). For the naturalistic condition, the current figure is misleading, since different classes contain different numbers of instances. It would be better to report proportions.\n\nConcerning the attention analysis, it seems to me that all it's saying is that lower layers of a CNN detect lower-level properties such as colors, higher layers detect more complex properties, such as shapes characterizing objects. What is novel here?\n\nAlso, since introducing attention changes the architecture, shouldn't the paper report the learning behaviour of the attention-augmented network?\n\nThe explanation of the attention mechanism is dense, and perhaps could be aided by a diagram (in the supplementary materials?). I think the description uses \"length\" when \"dimensional(ity)\" is meant.\n\n6. Supplementary material\n\nIt would be good to have an explicit description of the architecture, including number of layers of the various components, structure of the CNN, non-linearities, dimensionality of the layers, etc. (some of this information is inconsistently provided in the paper).\n\nIt's interesting that the encoder is actually a BOW model. This should be discussed in the paper, as it raises concerns about the linguistic interest of the controlled language that was used.\n\nTable 3: indicates is: indicates if\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Understanding Grounded Language Learning Agents","abstract":"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","pdf":"/pdf/3ade44df946e4d039d5f051cfe63aed510ed94d4.pdf","TL;DR":"Analysing and understanding how neural network agents learn to understand simple grounded language","paperhash":"anonymous|understanding_grounded_language_learning_agents","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Grounded Language Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByZmGjkA-}\n}","keywords":["Language AI Learning Reinforcement Deep"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper156/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739454275,"tcdate":1509040664677,"number":156,"cdate":1509739451604,"id":"ByZmGjkA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByZmGjkA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Understanding Grounded Language Learning Agents","abstract":"Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.","pdf":"/pdf/3ade44df946e4d039d5f051cfe63aed510ed94d4.pdf","TL;DR":"Analysing and understanding how neural network agents learn to understand simple grounded language","paperhash":"anonymous|understanding_grounded_language_learning_agents","_bibtex":"@article{\n  anonymous2018understanding,\n  title={Understanding Grounded Language Learning Agents},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByZmGjkA-}\n}","keywords":["Language AI Learning Reinforcement Deep"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper156/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}