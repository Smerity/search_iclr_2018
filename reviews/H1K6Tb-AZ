{"notes":[{"tddate":null,"ddate":null,"tmdate":1515172694945,"tcdate":1515172694945,"number":3,"cdate":1515172694945,"id":"SykDmN6Xz","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Comment","forum":"H1K6Tb-AZ","replyto":"SJF0AbKgG","signatures":["ICLR.cc/2018/Conference/Paper735/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper735/Authors"],"content":{"title":"Responses to the questions in review","comment":"Thanks for your comments. The followings are the response to your questions in the review:\n\n1. Application to another dataset, VGG-16 on CIFAR-100\nIn this revision, we evaluate the effectiveness of TESLA over a large dataset, CIFAR-100, and TESLA does outperform the original IDP design by a great margin, 60% accuracy, at low computation costs. Please find the experiment result on CIFAR-100 in Figure 4.\n\n2. In actual, for both fully connected layer and convolution layer, the profile coefficient is applied equivalently. For example, in a fully connected layer, an input element first multiplies its weights and then multiplies its corresponding profile coefficients.\n\n4. It is worthy to clarify that TESLA takes one new task into consideration at one time, and aggregate the loss of that new task into the current objective function and try to optimize the aggregated loss until meeting the early stopping criterion. Once early stopped, we add another new task and repeat the process until all tasks are optimized.\n\n3, 5 and 6. Thanks for pointing out these errors. We have corrected them in this revision.\n\nGreatly thanks for your valuable comments."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515170388897,"tcdate":1515170388897,"number":2,"cdate":1515170388897,"id":"ryaL9Qp7G","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Comment","forum":"H1K6Tb-AZ","replyto":"rJyYwFhlz","signatures":["ICLR.cc/2018/Conference/Paper735/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper735/Authors"],"content":{"title":"Difference between the original IDP paper","comment":"Thanks for your detailed comments.\n\n1. To answer the question about \"how the learning process differs from the original IDP paper, if they also have a scheduled learning setting.\":\n\n(i) original IDP only trains the network using complete dot product, so, in our terminology, original IDP optimizes one task of 100% DP. And that is why original IDP does not perform well at inference time if low computation costs. On the other hand, TESLA adds a new task at a time and tries to optimize multiple tasks incrementally and jointly for the sake of improving the performance of the new task without contaminating the performance of the past tasks. \n\n(ii) A scheduled learning is similar but not equivalent to the effect of our early stopping design. For example, ideally, task A needs to 10 epochs to reach the optimal performance. If only allocates less than 10 epochs on task A, the model will underfit in task A; if allocates more than 10 epochs, says 20 epoches, the model will overfit. Thus, we believe the early stopping is a better way to adapt across tasks with different convergence rates.\n\n2. Revision on the description of TESLA\nWe have revised our paper and elaborate the details of the TESLA algorithm, which was originally cut for space considerations. Briefly, TESLA is designed to optimize multiple tasks incrementally and jointly by:\n\n(i) Task-wise early stopping: due to different learning difficulties and convergence rates of tasks, we apply an early stopping mechanism to adjust the training process of each task to reduce overfitting and optimize across all tasks sequentially.\n\n(ii) Task-wise loss aggregation: because of shared weights between any two tasks, when adding a new task at a time, we aggregate the loss of the new task into the current objective function and optimize jointly.\n\nPlease check the revised section 3 to better understand the design intuition of our algorithm.\n\n3. Application to another dataset, CIFAR-100\nIn this revision, we also evaluate the effectiveness of TESLA over a large dataset, CIFAR-100, and TESLA does outperform the original IDP design. Please find the experiment result in Figure 4.\n\nThank you."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515168449297,"tcdate":1515167831507,"number":1,"cdate":1515167831507,"id":"S11DgXamf","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Comment","forum":"H1K6Tb-AZ","replyto":"r1VT-4J-z","signatures":["ICLR.cc/2018/Conference/Paper735/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper735/Authors"],"content":{"title":"Thanks for your review and our paper is updated","comment":"Thanks for your comments. \n\n1. Revision on the description of TESLA\nWe have revised our paper and elaborate the details of the TESLA algorithm, which was originally cut for space considerations. Briefly, TESLA is designed to optimize multiple tasks incrementally and jointly by:\n\n(a) Task-wise early stopping: due to different learning difficulties and convergence rates of tasks, we apply an early stopping mechanism to adjust the training process of each task to reduce overfitting and optimize across all tasks sequentially.\n\n(b) Task-wise loss aggregation: because of shared weights between any two tasks, when adding a new task at a time, we aggregate the loss of the new task into the current objective function and optimize jointly.\n\nPlease check the revised section 3 to better understand the design intuition of our algorithm.\n\n2. Application to another dataset, CIFAR-100\nIn this revision, we also evaluate the effectiveness of TESLA over a large dataset, CIFAR-100, and TESLA does outperform the original IDP design. Please find the experiment result in Figure 4."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642500640,"tcdate":1512157628434,"number":3,"cdate":1512157628434,"id":"r1VT-4J-z","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Review","forum":"H1K6Tb-AZ","replyto":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference/Paper735/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A potentially useful method, but not well motivated or explained","rating":"4: Ok but not good enough - rejection","review":"The authors propose a method for reducing the computational burden when performing inference in deep neural networks. The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients. The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product. \n\nUnfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not. While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure.\n\nPositive points:\n- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.\n- Application to two well-known benchmarking datasets.\n\nNegative points:\n- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches.\n- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task.\n\nMinor point:\n- Figure 1 is unclear and requires a better caption. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642500679,"tcdate":1511982967229,"number":2,"cdate":1511982967229,"id":"rJyYwFhlz","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Review","forum":"H1K6Tb-AZ","replyto":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference/Paper735/AnonReviewer3"],"readers":["everyone"],"content":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference ","rating":"5: Marginally below acceptance threshold","review":"This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints. The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized. These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity. \n\nThe original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities. This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting. They also present results for a 'harmonic' combination which was not explained in the paper at all.\n\nOverall the paper has very good motivation and significance. \nHowever the writing is not very clear and the paper is not self-contained at all. I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting. \n\nAdditionally, there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in Figure  3. As is, while results are promising, I can't fully assess that the paper has major contributions.  ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642500745,"tcdate":1511755472701,"number":1,"cdate":1511755472701,"id":"SJF0AbKgG","invitation":"ICLR.cc/2018/Conference/-/Paper735/Official_Review","forum":"H1K6Tb-AZ","replyto":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference/Paper735/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated.\n\nThe approach is based on `profile coefficients’ which are learned for every channel in a convolution layer, or for every column in the fully connected layer. Based on the magnitude of this profile coefficient, which determines the importance of this `filter,’ individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner.\n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn’t fully utilize the deep net performance. To address this issue a `loss aggregation’ is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used.\n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime.\n\nSummary:\n——\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method. The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing. In addition, the writing should be improved as it is often ambiguous. See below for details.\n\nReview:\n—————\n1. Experiments are only provided on very small datasets. According to my opinion, this isn’t sufficient to illustrate the effectiveness of the proposed approach. As a reader I wouldn’t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2. Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent. More specifically, while the profile coefficient is applied for every input element in Eq. (1), it’s applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layer’ is more confusing than clarifying.\n\n3. The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where \\leq x \\geq 100, means the x% of terms used in dot products’. While sentences like those can be deciphered, they aren’t that appealing.\n\n4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc.\n\n5. Algorithm 1 and Algorithm 2 call functions that aren’t described/defined.\n\n6. Baseline numbers for training on datasets without incomplete dot products should be provided.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515166174499,"tcdate":1509133761070,"number":735,"cdate":1509739130821,"id":"H1K6Tb-AZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1K6Tb-AZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference","abstract":"For inference operations in deep neural networks on end devices, it is desirable to deploy a single pre-trained neural network model, which can dynamically scale across a computation range without comprising accuracy. To achieve this goal, Incomplete Dot Product (IDP) has been proposed to use only a subset of terms in dot products during forward propagation. However, there are some limitations, including noticeable performance degradation in operating regions with low computational costs, and essential performance limitations since IDP uses hand-crafted profile coefficients. In this paper, we extend IDP by proposing new training algorithms involving a single profile, which may be trainable or pre-determined, to significantly improve the overall performance, especially in operating regions with low computational costs. Specifically, we propose the Task-wise Early Stopping and Loss Aggregation (TESLA) algorithm, which is showed in our 3-layer multilayer perceptron on MNIST that outperforms the original IDP by 32\\% when only 10\\% of dot products terms are used and achieves 94.7\\% accuracy on average. By introducing trainable profile coefficients, TESLA further improves the accuracy to 95.5\\% without specifying coefficients in advance. Besides, TESLA is applied to the VGG-16 model, which achieves 80\\% accuracy using only 20\\% of dot product terms on CIFAR-10 and also keeps 60\\% accuracy using only 30\\% of dot product terms on CIFAR-100, but the original IDP performs like a random guess in these two datasets at such low computation costs. Finally, we visualize the learned representations at different dot product percentages by class activation map and show that, by applying TESLA, the learned representations can adapt over a wide range of operation regions.","pdf":"/pdf/d3074c96cafb6d75194c73e4565f4ad43e152700.pdf","paperhash":"anonymous|tesla_taskwise_early_stopping_and_loss_aggregation_for_dynamic_neural_network_inference","_bibtex":"@article{\n  anonymous2018tesla:,\n  title={TESLA: Task-wise Early Stopping and Loss Aggregation for Dynamic Neural Network Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1K6Tb-AZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper735/Authors"],"keywords":[]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}