{"notes":[{"tddate":null,"ddate":null,"tmdate":1512018686870,"tcdate":1512018686870,"number":3,"cdate":1512018686870,"id":"SyDbmz6gf","invitation":"ICLR.cc/2018/Conference/-/Paper27/Public_Comment","forum":"rJ7yZ2P6-","replyto":"BkpgMaheG","signatures":["~Alex_Wong1"],"readers":["everyone"],"writers":["~Alex_Wong1"],"content":{"title":"Reply to \"Request for Code\"","comment":"Hi,\n\nThanks for all of this, will definitely take time to go through your notes. For more communication, my email is alexander.wong4@mail.mcgill.ca\n\nCheers"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1511997941171,"tcdate":1511997941171,"number":3,"cdate":1511997941171,"id":"BkpgMaheG","invitation":"ICLR.cc/2018/Conference/-/Paper27/Official_Comment","forum":"rJ7yZ2P6-","replyto":"B15ZQj2gf","signatures":["ICLR.cc/2018/Conference/Paper27/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper27/Authors"],"content":{"title":"Reply to \"Request for Code\"","comment":"Hi, Alex,\n      I could not see your email in open review profile (\"a****4@cs\").  Open source the code in the paper  is in progress.  I don't know whether I can share the code for this reproduction challenge now and need to check the legal department in my company. \n\n> 1. What random seed did you use to generate the Ubuntu corpus?\njust used the default one (default = 1234) (see: https://github.com/rkadlec/ubuntu-ranking-dataset-creator) so that results are comparable with others.\n\n> 2. How did you implement the character-composed embedding? \n> 3. could you clarify on the concatenation of word and character embeddings?\n\nI used the tensorflow (tf.nn.bidirectional_dynamic_rnn)  to  conduct all experiments in the paper.\nFor example, you can define function below:\ndef lstm_layer(inputs, input_seq_len, rnn_size, dropout_keep_prob, scope, scope_reuse=False):\n    with tf.variable_scope(scope, reuse=scope_reuse) as vs:\n        fw_cell = tf.contrib.rnn.LSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True, reuse=scope_reuse)\n        fw_cell  = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=dropout_keep_prob)\n        bw_cell = tf.contrib.rnn.LSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True, reuse=scope_reuse)\n        bw_cell  = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=dropout_keep_prob)\n        rnn_outputs, rnn_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, cell_bw=bw_cell,\n                                                                inputs=inputs,\n                                                                sequence_length=input_seq_len,\n                                                                dtype=tf.float32)\n        return rnn_outputs, rnn_states\n\nThen\n#context_char_embedded: [batch_size * max_sequence_length, max_word_length, embed_char_dim]\n#context_char_length: [batch_size * max_sequence_length] (define number of character per word)\n#charRNN_size:  40 \n#max_word_length: 18\n#max_sequence_length: 180\n#dropoutput_keep_prob: 1.0\n#embed_char_dim: 69\n#batch_size: 128\nchar_rnn_output_context,  char_rnn_state_context = lstm_layer(context_char_embedded, context_char_length, charRNN_size,  dropout_keep_prob, charRNN_scope_name, scope_reuse=False)\n\n#response_char_embedded: [batch_size * max_sequence_length, max_word_length, embed_char_dim]\n#response_char_length: [batch_size * max_sequence_length]\n\nchar_rnn_output_response, char_rnn_state_response = lstm_layer(response_char_embedded,\nresponse_char_length, charRNN_size,  dropout_keep_prob, charRNN_scope_name, scope_reuse=True)\n\n#context char representation\nchar_embed_dim = charRNN_size * 2\n#context_char_state: [batch_size * max_sequence_length,  char_embed_dim]\ncontext_char_state = tf.concat(axis=1, values=[char_rnn_state_context[0].h, char_rnn_state_context[1].h])\n#reshape \ncontext_char_state = tf.reshape(context_char_state, [-1, max_sequence_length, char_embed_dim])\n\nThe similar operations are applied to char_rnn_state_response.\n\nFor word embedding, I assume that you can get \"context_word_output and response_word_output\"\nBoth tensors will have shape [batch_size, max_sequence_length, word_embedding_dim]\nThen you can use tf.concat to get the combined representation.\n\n> Regarding your ESIM, what settings did you use for the following hyper-parameters: patience, gradient clipping threshold, max epochs?\nI am not familiar with patience.  No gradient clipping was used.  In my experiments, training usually achieved the best performance (MRR) on the validation set at around 22000 - 25000 batch steps. \n\nNote:  tensorflow version (tensorflow-gpu (1.1.0)). \n\nIf you share your email, we can communicate through email or another channel.\n\n\n\n\n\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1511990017844,"tcdate":1511990017844,"number":2,"cdate":1511990017844,"id":"B15ZQj2gf","invitation":"ICLR.cc/2018/Conference/-/Paper27/Public_Comment","forum":"rJ7yZ2P6-","replyto":"r1UNAmogz","signatures":["~Alex_Wong1"],"readers":["everyone"],"writers":["~Alex_Wong1"],"content":{"title":"Reply to \"Request for Code\"","comment":"Thank you for your willingness to help!\n\nUnfortunately, we are not in a position to enter a legal contractual agreement on behalf of the University, but if there is still a way to share any source code anonymously that would be helpful. We would only be using your source material for this reproduction challenge. If not, then you probably do not need to stay anonymous for further correspondence. You can find my email address on my OpenReview profile.\n\nIn terms of implementation details, we do have some questions:\n1. What random seed did you use to generate the Ubuntu corpus?\n2. How did you implement the character-composed embedding? More specifically, could you give more detail on you are describing in this line from section 3.1: \"The character-composed embedding is generated by concatenating the final state vector of the forward and backward direction of bi-directional LSTM (BiLSTM)\"\n3. Could you clarify on the concatenation of word and character embeddings?\n4. Regarding your ESIM, what settings did you use for the following hyper-parameters: patience, gradient clipping threshold, max epochs?\n\nAgain, if you'd rather communicate through email or another channel, feel free.\n\nThanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1511901906279,"tcdate":1511901906279,"number":2,"cdate":1511901906279,"id":"rk9R5BjgM","invitation":"ICLR.cc/2018/Conference/-/Paper27/Official_Comment","forum":"rJ7yZ2P6-","replyto":"HkFtNMclM","signatures":["ICLR.cc/2018/Conference/Paper27/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper27/Authors"],"content":{"title":"Reply to \"Request  for code\"","comment":"Hi, Alex,\n     Thank for your interest in our paper. Open source approval process in our company may take time.  At the same time,  if further clarification about technical implementation details (e.g hyper-parameter setting) is needed, feel free to ask here. We like to help you reproduce the results in the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1511894574144,"tcdate":1511894574144,"number":1,"cdate":1511894574144,"id":"r1UNAmogz","invitation":"ICLR.cc/2018/Conference/-/Paper27/Official_Comment","forum":"rJ7yZ2P6-","replyto":"HkFtNMclM","signatures":["ICLR.cc/2018/Conference/Paper27/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper27/Authors"],"content":{"title":"Reply to \"Request for Code\"","comment":"We are extremely excited that you have selected our paper for reproducibility.  We are going through our employer's open source approval process which will take a much longer time than the Dec 15 deadline.  Few questions that may help us with alternatives.  \n\n1. Do we need to stay anonymous to continue further our correspondence?\n2. Are you open for us to enter a legal contractual agreement to access our source code between our employer and your school for the \"reproducibility\" purpose?  This would be potentially a faster process to give you access to our source code.  I can explore this route to get more affirmative answers on the timing if you are open to enter a legal contractual agreement, like \"no cost collaboration\".\n\nAt this time, we believe open source process would take beyond your Dec 15 deadline, but we hope to finish the open source approval for the conference date.  \nIf you have any further questions about our paper, please let us know as well.  \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1511822465059,"tcdate":1511822465059,"number":1,"cdate":1511822465059,"id":"HkFtNMclM","invitation":"ICLR.cc/2018/Conference/-/Paper27/Public_Comment","forum":"rJ7yZ2P6-","replyto":"rJ7yZ2P6-","signatures":["~Alex_Wong1"],"readers":["everyone"],"writers":["~Alex_Wong1"],"content":{"title":"Request for Code","comment":"Dear Authors,\n\nI am part of a team at McGill University participating in the ICLR 2018 Reproducibility Challenge (linked below). We have chosen to reproduce your study and are wondering if you would like to share some or all of the code you used.\n\nhttp://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html\n\nThank you!"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1512222609722,"tcdate":1511814166099,"number":3,"cdate":1511814166099,"id":"H1RMVeqgz","invitation":"ICLR.cc/2018/Conference/-/Paper27/Official_Review","forum":"rJ7yZ2P6-","replyto":"rJ7yZ2P6-","signatures":["ICLR.cc/2018/Conference/Paper27/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Promising results but insufficient clarity and focus in write-up","rating":"5: Marginally below acceptance threshold","review":"The main contributions in this paper are:\n1) New variants of a recent LSTM-based model (\"ESIM\") are applied to the task of response-selection in dialogue modeling -- ESIM was originally introduced and evaluated for natural language inference. In this new setting, the ESIM model (vanilla and extended) outperform previous models when trained and evaluated on two distinct conversational datasets.\n\n2) A fairly trivial method is proposed to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to these conversational datasets.\nThe method itself is to combine d1-dimensional word embeddings that were pretrained on a large unannotated corpus (vocabulary S) with distinct d2-dimensional word embeddings that are trained on the task-specific training data (vocabulary T). The enhanced (d1+d2)-dimensional representation for a word is constructed by concatenating its vectors from the two embeddings, setting either the d1- or d2-dimensional subvector to zeros when the word is absent from either S or T, respectively. This method is incorporated as an extension into ESIM and evaluated on the two conversation datasets.\n\nThe main results can be characterized as showing that this vocabulary extension method leads to performance gains on two datasets, on top of an ESIM-model extended with character-based word embeddings, which itself outperforms the vanilla ESIM model.\n\nThese empirical results are potentially meaningful and could justify reporting, but the paper's organization is very confusing, and too many details are too unclear, leading to low confidence in reproducibility. \n\nThere is basic novelty in applying the base model to a new task, and the analysis of the role of the special conversational boundary tokens is interesting and can help to inform future modeling choices. The embedding-enhancing method has low originality but is effective on this particular combination of model architecture, task and datasets. I am left wondering how well it might generalize to other models or tasks, since the problem it addresses shows up in many other places too...\n\nOverall, the presentation switches back and forth between the Douban corpus and the Ubuntu corpus, and between word2vec and Glove embeddings, and this makes it very challenging to understand the details fully.\n\nS3.1 - Word representation layer: This paragraph should probably mention that the character-composed embeddings are newly introduced here, and were not part of the original formulation of ESIM. That statement is currently hidden in the figure caption.\n\nAlgorithm 1:\n- What set does P denote, and what is the set-theoretic relation between P and T?\n- Under one possible interpretation, there may be items in P that are in neither T nor S, yet the algorithm does not define embeddings for those items even though its output is described as \"a dictionary with word embeddings ... for P\". This does not seem consistent? I think the sentence in S4.2 about initializing remaining OOV words as zeros is relevant and wonder if it should form part of the algorithm description?\n\nS4.1 - What do the authors mean by the statement that response candidates for the Douban corpus were \"collected by Lucene retrieval model\"?\n\nS4.2 - Paragraph two is very unclear. In particular, I don't understand the role of the Glove vectors here when Algorithm 1 is used, since the authors refer to word2vec vectors later in this paragraph and also in the Algorithm description.\n\nS4.3 - It's insufficiently clear what the model definitions are for the Douban corpus. Is there still a character-based LSTM involved, or does FastText make it unnecessary?\n\nS4.3 - \"It can be seen from table 3 that the original ESIM did not perform well without character embedding.\" This is a curious way to describe the result, when, in fact, the ESIM model in table 3 already outperforms all the previous models listed.\n\nS4.4 - gensim package -- for the benefit of readers unfamiliar with gensim, the text should ideally state explicitly that it is used to create the *word2vec* embeddings, instead of the ambiguous \"word embeddings\".\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1512222609769,"tcdate":1511734819370,"number":2,"cdate":1511734819370,"id":"BkomChuxf","invitation":"ICLR.cc/2018/Conference/-/Paper27/Official_Review","forum":"rJ7yZ2P6-","replyto":"rJ7yZ2P6-","signatures":["ICLR.cc/2018/Conference/Paper27/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A minor solution to resolving OOV word representations","rating":"3: Clear rejection","review":"The paper considers a setting (Ubuntu Dialogue Corpus and Douban Conversation Corpus) where most word types in the data are not covered by pretrained representations. The proposed solution is to combine (1) external pretrained word embeddings and (2) pretrained word embeddings on the training data by keeping them as two views: use the view if it's available, otherwise use a zero vector. This scheme is shown to perform well compared to other methods, specifically combinations of pretraining vs not pretraining embeddings on the training data, updating vs not updating embeddings during training, and others. \n\nQuality: Low. The research is not very well modularized: the addressed problem has nothing specifically to do with ESIM and dialogue response classification, but it's all tangled up. The proposed solution is reasonable but rather minor. Given that the model will learn task-specific word representations on the training set anyway, it's not clear how important it is to follow this procedure, though minor improvement is reported (Table 5). \n\nClarity: The writing is clear. But the point of the paper is not immediately obvious because of its failure to modularize its contributions (see above).\n\nOriginality: Low to minor.\n\nSignificance: It's not convincing that an incremental improvement in the pretraining phase is so significant, for instance compared to developing a novel better architecture actually tailored to the dialogue task. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1512222609815,"tcdate":1511564192073,"number":1,"cdate":1511564192073,"id":"rJdjmmLez","invitation":"ICLR.cc/2018/Conference/-/Paper27/Official_Review","forum":"rJ7yZ2P6-","replyto":"rJ7yZ2P6-","signatures":["ICLR.cc/2018/Conference/Paper27/AnonReviewer5"],"readers":["everyone"],"content":{"title":"Good paper with important practical and engineering relevance. Little methodological novelty, though.","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThis paper proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations. The proposed approach uses generic embeddings and combines them with the embeddings trained on the training dataset in a straightforward string-matching algorithm. In addition, the paper also makes a couple of improvements to Chen et. al's enhanced LSTM by adding character-level embeddings and replacing average pooling by LSTM last state summary vector. The results are shown on the standard Ubuntu dialogue dataset as well as a new Douban conversation dataset. The proposed approach gives sizable gains over the baselines.\n\n\nComments:\n\nThe paper is well written and puts itself nicely in context of previous work. Though, the proposed extension to handle out-of-vocabulary items is a simple and straightforward string matching algorithm, but nonetheless it gives noticeable increase in empirical performance on both the tasks. All in all, the methodological novelty of the paper is small but it has high practical relevance in terms of giving improved accuracy on an important task of dialogue conversation.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]}},{"tddate":null,"ddate":null,"tmdate":1509739523698,"tcdate":1508520154896,"number":27,"cdate":1509739521048,"id":"rJ7yZ2P6-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJ7yZ2P6-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus","abstract":"Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end\ndeep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is \nthe large number of out-of-vocabulary words. In this paper we proposed an algorithm which combines the general pre-trained word embedding vectors with those  generated on the task-specific training set to address this issue.  We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags. ","pdf":"/pdf/ce64d77a6cbf81b0facd7cb93e09f2aa3ab6ab8b.pdf","TL;DR":"Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue","paperhash":"anonymous|enhance_word_representation_for_outofvocabulary_on_ubuntu_dialogue_corpus","_bibtex":"@article{\n  anonymous2018enhance,\n  title={Enhance Word Representation for Out-of-Vocabulary on Ubuntu Dialogue Corpus},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJ7yZ2P6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper27/Authors"],"keywords":["next utterance selection","ubuntu dialogue corpus","out-of-vocabulary","word representation"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}