{"notes":[{"tddate":null,"ddate":null,"tmdate":1514502446741,"tcdate":1513644158760,"number":2,"cdate":1513644158760,"id":"S1PKgkIGz","invitation":"ICLR.cc/2018/Conference/-/Paper872/Official_Comment","forum":"Hk3ddfWRW","replyto":"Hk3ddfWRW","signatures":["ICLR.cc/2018/Conference/Paper872/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper872/Authors"],"content":{"title":"Review response","comment":"We thank the reviewers for the thoughtful comments.\n\nThe paper has been updated with additional simulation experiments.\n\nWe start by describing the additional experiments, and then address each reviewer separately. \n\nFollowing the reviewers suggestions, we include results that compare our approach to a state-of-the-art conditional VAE on a simulated domain. These results were omitted in our initial submission with the interest of keeping the paper at the suggested page limits. We briefly summarize the results here, see Appendix D for more details.\n\nThe experiments were conducted on a simple simulated domain: given an image with N randomly positioned targets with different colors, predict the location (i.e., x-y position) of one of them. For training, we randomly selected one of the targets and provided its location as the supervisory signal. \nThis task captures the essence of the robotic task in the paper - image input and a low dimensional multi-modal output (with N modes). It simplifies the image processing, and the fact that there is no trajectory - it’s a single step decision making problem.  \n\nTo make the comparison fair, we chose the latent variable z in IDS to be a standard Gaussian, just as for the CVAE. All network sizes and training parameters were the same for both methods (except for the additional recognition and conditional prior network for CVAE), and we did not apply any pretraining to the conv layer.\n\nWe have tried various CVAE parameter settings, and also annealing of the KL term in the cost. The CVAE works well for N=2 targets, and with careful tuning also for N=3, but despite genuine efforts we could not get it to work for N=5 targets. These results actually motivated us to follow the IDS approach in the first place, which worked well and robustly for all values of N we tried. The convergence of IDS in all cases was also an order of magnitude faster. \n\nThese results show that:\n1) In some domains our IDS algorithm works significantly better than state of the art algorithms for variational inference.\n2) Pretraining is not required for our approach (though it definitely helps speed it up).\n\nWhile it could definitely be the case that with more parameter tuning, or that by applying other improvements to CVAEs such as normalizing flows we could make them work in this task, we believe that the simplicity of our approach and its robust performance is worth reporting. \n\nA similar result was recently reported by Fragkiadaki et al. (2017), comparing CVAEs to backpropping through top-k samples in video prediction. Our contribution, compared to that work, is grounding this method in a formal mathematical treatment, proposing optimistic sampling which significantly improves its performance, and showing its importance in a real world robotic imitation learning domain.\n\nReferences:\nFragkiadaki, Katerina, et al. \"Motion Prediction Under Multimodality with Conditional Stochastic Networks.\" arXiv preprint arXiv:1705.02082 (2017).\n\n\n\nAnonReviewer1:\n\nComparison to value iteration networks (VIN): \nThe VIN work does not consider multiple modes in the data, which is the main focus in our work. In particular, the target position in the VIN paper is *explicitly provided* as a separate image channel of the input, and the VIN output is deterministic - it cannot reproduce multiple modes of reaching to different targets. Thus, VINs cannot solve the problems we tackle in this paper.\n\nExtending VINs with latent variables or using VINs inside a generative model is an interesting direction, but one that would require a separate investigation.\n\nWe believe our related work section covers most relevant works on imitation learning with multiple intentions/modes in the data.\n\nAnswers to specific comments:\n1) In our setting (and in many realistic industrial setting) knowing when the demonstrations start and end is trivial, as the demonstrator records demonstrations sequentially.\n2) Adding context would require to either label the context or infer it. Labelling adds burden on the demonstrator, which we wish to minimize. Inferring the context is the approach we pursue, and we added additional experiments comparing our approach to a state of the art variational inference method.\n3+4) See above for additional simulation results.\n\nAnonReviewer2:\nSee above - we added a comparison with conditional VAEs. \n\nAnonReviewer3:\n\nPretrained weights - see above. Pretraining is not necessary, but definitely helps speed up training. \n\nExtra rollouts: We did not fully understand this comment. Generally rollouts are better understood in the context of an RL setting, however our approach is not RL and thus no rollout is involved. While extra RL rollouts can be used to improve the policy, in many realistic scenarios taking extra rollouts on the robot can be costly/unsafe/time consuming.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Imitation Learning from Visual Data with Multiple Intentions","abstract":"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. \nLfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.\nWe demonstrate our method on real robot visual object reaching tasks, and show that\nit can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9.","pdf":"/pdf/04d263fbe8898831447a14f774de406f5824f4de.pdf","TL;DR":"multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention. ","paperhash":"anonymous|imitation_learning_from_visual_data_with_multiple_intentions","_bibtex":"@article{\n  anonymous2018imitation,\n  title={Imitation Learning from Visual Data with Multiple Intentions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk3ddfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper872/Authors"],"keywords":["multi-modal imitation learning","deep learning","generative models","stochastic neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642524333,"tcdate":1511832251885,"number":3,"cdate":1511832251885,"id":"r1ET9Ncgf","invitation":"ICLR.cc/2018/Conference/-/Paper872/Official_Review","forum":"Hk3ddfWRW","replyto":"Hk3ddfWRW","signatures":["ICLR.cc/2018/Conference/Paper872/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper focuses on imitation learning with intentions sampled \nfrom a multi-modal distribution. The papers encode the mode as a hidden \nvariable in a stochastic neural network and suggest stepping around posterior \ninference over this hidden variable (which is generally required to \ndo efficient maximum likelihood) with a biased importance \nsampling estimator. Lastly, they incorporate attention for large visual inputs. \n\nThe unimodal claim for distribution without randomness is weak. The distribution \ncould be replaced with a normalizing flow. The use of a latent variable \nin this setting makes intuitive sense, but I don't think multimodality motivates it.\n\nMoreover, it really felt like the biased importance sampling approach should be \ncompared to a formal inference scheme. I can see how it adds value over sampling \nfrom the prior, but it's unclear if it has value over a modern approximate inference \nscheme like a black box variational inference algorithm or stochastic gradient MCMC.\n\nHow important is using the pretrained weights from the deterministic RNN?\n\nFinally, I'd also be curious about how much added value you get from having \naccess to extra rollouts.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Imitation Learning from Visual Data with Multiple Intentions","abstract":"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. \nLfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.\nWe demonstrate our method on real robot visual object reaching tasks, and show that\nit can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9.","pdf":"/pdf/04d263fbe8898831447a14f774de406f5824f4de.pdf","TL;DR":"multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention. ","paperhash":"anonymous|imitation_learning_from_visual_data_with_multiple_intentions","_bibtex":"@article{\n  anonymous2018imitation,\n  title={Imitation Learning from Visual Data with Multiple Intentions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk3ddfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper872/Authors"],"keywords":["multi-modal imitation learning","deep learning","generative models","stochastic neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642524368,"tcdate":1511690609824,"number":2,"cdate":1511690609824,"id":"r1cOWGdgz","invitation":"ICLR.cc/2018/Conference/-/Paper872/Official_Review","forum":"Hk3ddfWRW","replyto":"Hk3ddfWRW","signatures":["ICLR.cc/2018/Conference/Paper872/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This work shows how to learn several modalities using Imitation learning from visual data using stochastic Neural Network.","rating":"4: Ok but not good enough - rejection","review":"The authors provide a method for learning from demonstrations where several modalities of the same task are given. The authors argue that in the case where several demonstrations exists and a deterministic (i.e., regular network) is given, the network learns some average policy from the demonstrations.\n\nThe paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming. They put the this specific work in the right context of imitation learning and IRL. Afterward, the authors argue that deterministic network cannot adequately several modalities. The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs. I find that recent paper by Tamar et al 2016. on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network. Even the control task is very similar to the current proposed task in this paper.\n\nThe authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs. Again, Tamar et al. 2016 deals with this 3 points.\n\nI went over the math. It seems right and valid. Indeed, SNN is a good choice for adding (Bayesian) context to a task. Also, I see the advantage of referring only to the \"good\" quantiles when needed. It is indeed a good method for dealing with the variance. \n\nI must say that I was impressed with the authors making the robot succeed in the tasks in hand (although reaching to an object is fairly simple task). \n\nMy concerns are as follows:\n1) Seems like that the given trajectories are naturally divided with different tasks, i.e., a single trajectory consists only a single task. For me, this is not the pain point in this tasks. the pain point is knowing when tasks are begin and end. \n2) I'm not sure, and I haven't seen evidence in the paper (or other references) that SNN is the only (optimal?) method for this context. Why not adding (non Bayesian) context (not label) to the task will not work as well? \n3) the robot task is impressive. but proving the point, and for the ease of comparing to different tasks, and since we want to show the validity of the work on more than 200 trials, isn't showing the task on some simulation is better for understanding the different regimes that this method has advantage? I know how hard is to make robotic tasks work...   \n4) I’m not sure that the comparison of the suggested architecture to one without any underlying additional variable Z or context (i.e., non-Bayesian setup) is fair. \"Vanilla\" NN indeed may fail miserably . So, the comparison should be to any other work that can deal with \"similar environment but different details\".\n\nTo summarize, I like the work and I can see clearly the motivation. But I think some more work is needed in this work: comparing to the right current state of the art, and show that in principal (by demonstrating on other simpler simulations domains) that this method is better than other methods.  \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Imitation Learning from Visual Data with Multiple Intentions","abstract":"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. \nLfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.\nWe demonstrate our method on real robot visual object reaching tasks, and show that\nit can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9.","pdf":"/pdf/04d263fbe8898831447a14f774de406f5824f4de.pdf","TL;DR":"multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention. ","paperhash":"anonymous|imitation_learning_from_visual_data_with_multiple_intentions","_bibtex":"@article{\n  anonymous2018imitation,\n  title={Imitation Learning from Visual Data with Multiple Intentions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk3ddfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper872/Authors"],"keywords":["multi-modal imitation learning","deep learning","generative models","stochastic neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642524408,"tcdate":1511286157830,"number":1,"cdate":1511286157830,"id":"ryU5B1zxf","invitation":"ICLR.cc/2018/Conference/-/Paper872/Official_Review","forum":"Hk3ddfWRW","replyto":"Hk3ddfWRW","signatures":["ICLR.cc/2018/Conference/Paper872/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Latent variable model for multi-modal imitation learning","rating":"6: Marginally above acceptance threshold","review":"The authors propose a new sampling based approach for inference in latent variable models. They apply this approach to multi-modal (several \"intentions\") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks. \n\nThe proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled. The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior. The objective to be maximized is a lower bound to 1/alpha * the likelihood. \n\nQuality: The empirical results (including a video of an actual robotic arm system performing the task) looks good. This reviewer is a bit sceptical to the methodology. I am not convinced that the proposed bound will have low enough variance. It is mentioned in a footnote that variational autoencoders were tested but that they failed. Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework. Also, the recently proposed continuous relaxation of random variables seemed relevant. \n\nClarity: The paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribution.\n\nSignificance: Hard to say. New method proposed and shown to work well in one case. Too early to tell about significance.\n\nPro:\n1. Challenging and relevant problem solved better than other approaches.\n2. New latent variable model bound that might work better than classic approaches.\nCon:\n1. Not entirely convincing that it should work better than already existing methods.\n2. Missing some investigation of the properties of the estimator on simple problem to be compared to standard methods.     ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Imitation Learning from Visual Data with Multiple Intentions","abstract":"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. \nLfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.\nWe demonstrate our method on real robot visual object reaching tasks, and show that\nit can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9.","pdf":"/pdf/04d263fbe8898831447a14f774de406f5824f4de.pdf","TL;DR":"multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention. ","paperhash":"anonymous|imitation_learning_from_visual_data_with_multiple_intentions","_bibtex":"@article{\n  anonymous2018imitation,\n  title={Imitation Learning from Visual Data with Multiple Intentions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk3ddfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper872/Authors"],"keywords":["multi-modal imitation learning","deep learning","generative models","stochastic neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1513643262297,"tcdate":1509136499986,"number":872,"cdate":1509739052897,"id":"Hk3ddfWRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk3ddfWRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Imitation Learning from Visual Data with Multiple Intentions","abstract":"Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. \nLfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module.\nWe demonstrate our method on real robot visual object reaching tasks, and show that\nit can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9.","pdf":"/pdf/04d263fbe8898831447a14f774de406f5824f4de.pdf","TL;DR":"multi-modal imitation learning from unstructured demonstrations using stochastic neural network modeling intention. ","paperhash":"anonymous|imitation_learning_from_visual_data_with_multiple_intentions","_bibtex":"@article{\n  anonymous2018imitation,\n  title={Imitation Learning from Visual Data with Multiple Intentions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk3ddfWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper872/Authors"],"keywords":["multi-modal imitation learning","deep learning","generative models","stochastic neural networks"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}