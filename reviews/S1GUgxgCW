{"notes":[{"tddate":null,"ddate":null,"tmdate":1516574847916,"tcdate":1516574847916,"number":3,"cdate":1516574847916,"id":"By_Kd9frz","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Comment","forum":"S1GUgxgCW","replyto":"rkTnC_MVG","signatures":["ICLR.cc/2018/Conference/Paper207/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper207/Authors"],"content":{"title":"Learning rate and epoch","comment":"Thank you for the comments. We used Adam with an initial learning rate of 0.1. The convergence varies case-by-case, but most of the models start to converge around 7-8th epoch."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1516574605419,"tcdate":1516574605419,"number":2,"cdate":1516574605419,"id":"S1B9wqGrz","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Comment","forum":"S1GUgxgCW","replyto":"HyY3SM9NM","signatures":["ICLR.cc/2018/Conference/Paper207/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper207/Authors"],"content":{"title":"Contribution/Novelty of the paper","comment":"The main contribution of the paper is three-fold as mentioned in this post (– General comments on Contributions–): \n1) We were first to be able to jointly learn the neural topic and seq2seq models.\n2) The paper offers a better understanding/training of latent models for languages.\n3) Both an extensive evaluation and a comprehensive analysis were conducted to validate the results."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1516017072651,"tcdate":1516017072651,"number":6,"cdate":1516017072651,"id":"HyY3SM9NM","invitation":"ICLR.cc/2018/Conference/-/Paper207/Public_Comment","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"A combination of VAE and TopicRNN?","comment":"It seems that the model is a combination of VAE (https://arxiv.org/abs/1605.06069) and TopicRNN (https://arxiv.org/pdf/1611.01702.pdf). Any new insights?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1515152983604,"tcdate":1515152983604,"number":1,"cdate":1515152983604,"id":"HJeDIJaXG","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Comment","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper207/Authors"],"content":{"title":"Notification of the newest paper update.","comment":"The authors would like to notify reviewers about the newest update of the paper where a quick analysis of the learned topic gate $l_t$ has been added to the paper based on reviewer1's request."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1514972547950,"tcdate":1514972547950,"number":4,"cdate":1514972547950,"id":"rJnFHQ5XM","invitation":"ICLR.cc/2018/Conference/-/Paper207/Public_Comment","forum":"S1GUgxgCW","replyto":"H1E8RNcxz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"* Responses to R2","comment":"Hyperparameter search of LV-S2S\nThe experiments were conducted in a careful way where a small set of hyper-parameters were tuned to find the best model in each category. We didn’t do an exhaustive grid search over all possible network configurations, however, given the recent understanding of latent variable models (Higgins et al, 2016, Bowman et al., 2015, Dieng et al., 2017), the result of this work has shown good evidences that LTCM is generally more capable of learning diverse and interesting responses than latent variable S2S models.\n\nHuman evaluation\nWe ran 5000 pairwise comparisons between the 8 models in Table 1 (~90 comparisons per pair) and reported only the top performing ones in each of the model categories. The number of tasks each MTurk can work on was capped at 20. This results in about >=250 unique workers. The meaning of each rating is presented in Section 4.2 Human evaluation. We have added these details, please see our revision.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1515152570993,"tcdate":1514972483686,"number":3,"cdate":1514972483686,"id":"B1nrBm57f","invitation":"ICLR.cc/2018/Conference/-/Paper207/Public_Comment","forum":"S1GUgxgCW","replyto":"rypOZF5eG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"* Responses to R1","comment":"Literature survey\nThe authors have updated the paper per the reviewer’s suggestion to add more citations for topic-aware models. We would like to point out that the suggested work (Xing et al., 2017) is quite different from ours in that they used a pretrained LDA model whereas our LTCM model trains the topic and seq2seq component jointly.\n[1] Xing et al., 2017. Topic Aware Neural Response Generation. https://arxiv.org/pdf/1606.08340.pdf \n\nInterpret topical information\nThe topic information learned in LTCM was not as easy to interpret as in other topic models that trained on document sets. This is because the word co-occurrence statistics in short text datasets are too sparse to train interpretable topic representations (Yan et al, 2013). However, we found that sampling from this learned latent representation does give us diversified sentences, at both syntactic and semantic levels. We do acknowledge the suggestion to visualize the values of $l_t$ which we have included in the newest revision of the paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1514972407786,"tcdate":1514972219825,"number":2,"cdate":1514972219825,"id":"HkES4X57M","invitation":"ICLR.cc/2018/Conference/-/Paper207/Public_Comment","forum":"S1GUgxgCW","replyto":"r1ahhhJWM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"* Responses to R3","comment":"We thank the reviewer for the comments. Please see this post (– General comments on Contributions –) for the contributions of our paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1514972105831,"tcdate":1514972105831,"number":1,"cdate":1514972105831,"id":"SJG0X7cQf","invitation":"ICLR.cc/2018/Conference/-/Paper207/Public_Comment","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"– General comments on Contributions – ","comment":"We thank all the reviewers for the comments and feedback, which have helped us improve the paper (please see our revision). However, we are disappointed about the low review scores of the paper and that our contributions were not fully appreciated. To help reviewers better evaluate our paper, we would like to re-emphasize the contributions of this work:\n\n(a) Novelty\nWe were first to be able to jointly learn the neural topic and seq2seq models. The key idea is to utilize the hard-decision trick from TopicRNN (Dieng et al., 2017) to prevent the latent variable from catastrophic mode collapsing. Previous work such as [1, 2] only incorporated pre-trained models (LDA, counting grid) into seq2seq models instead of joint learning.\n[1] Xing et al., 2017. Topic Aware Neural Response Generation. https://arxiv.org/pdf/1606.08340.pdf \n[2] Wang et al., 2017. Steering Output Style and Topic in Neural Response Generation.\nhttps://arxiv.org/abs/1709.03010.pdf\n\n(b) Better understanding/training of latent models for languages\nLatent models for languages are notoriously hard to train [3, 4]. This work contributes to better training/understanding of latent models by observing and investigating in correlations of many training metrics. For examples, we found that:\n  (i)  approximated perplexity has much more to do with the generation quality comparing to variational lower bound; \n  (ii) a lower lowerbound isn’t necessarily better because the higher KL can lead to a higher sentence diversity.\n  (iii) BoW encoder works just fine in the topic component of LTCM. It is also easier to optimise.\nThese could serve as valuable rules of thumb for future model development. \n\n[3] Bowman et al., 2015. Generating Sentences from a Continuous Space. https://arxiv.org/pdf/1511.06349.pdf \n[4] Miao and Blunsom, 2016. Language as a Latent Variable: Discrete Generative Models for Sentence Compression. https://arxiv.org/pdf/1609.07317.pdf\n\n(c) Standard and comprehensive evaluation\nWe acknowledge that evaluating chat-based systems is hard. To our best effort, we included previous metrics [5, 6] to provide a comprehensive and extensive evaluation that demonstrates the superiority of our models over strong baselines. The evaluation includes both corpus-based metrics (perplexity, lowerbound, KL divergence, uniqueness, Zipf coefficients) and human judgments (interestingness, appropriateness, as well as a pairwise comparison).\n\n[5] Serban et al., 2016. A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. https://arxiv.org/pdf/1605.06069.pdf \n[6] Cao & Clark, 2017. Latent Variable Dialogue Models and their Diversity. https://arxiv.org/pdf/1702.05962.pdf\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1515642408977,"tcdate":1512193205301,"number":3,"cdate":1512193205301,"id":"r1ahhhJWM","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Review","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/AnonReviewer3"],"readers":["everyone"],"content":{"title":"topic modeling + seq2seq","rating":"6: Marginally above acceptance threshold","review":"I enjoyed this paper a lot. The paper addresses the issue of enduring topicality in conversation models. The model proposed here is basically a mash-up between a neural topic model and a seq2seq-based dialog system. The exposition is relatively clear and a reader with sufficient background in ML should have no following the model. My only concern about the paper is that is very incremental in nature -- the authors combine two separate models into a relatively straight-forward way. The results do are good and validate the approach, but the paper has little to offer beyond that.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1515642409016,"tcdate":1511850357212,"number":2,"cdate":1511850357212,"id":"rypOZF5eG","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Review","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting combination of seq2seq and neural topic models, but weak evaluation","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a conversational model with topical information, by combining seq2seq model with neural topic models. The experiments and human evaluation show the model outperform some the baseline model seq2seq and the other latent variable model variant of seq2seq.\n\nThe paper is interesting, but it also has certain limitations:\n\n1) To my understanding, it is a straightforward combination of seq2seq and one of the neural topic models without any justification.\n2) The evaluation doesn't show how the topic information could influence word generation. No of the metrics in table 2 could be used to justify the effect of topical information.\n3) There is no analysis about the model behavior, therefore there is no way we could get a sense about how the model actually works. One possible analysis is to investigate the values $l_t$ and the corresponding words, which to some extent will tell us how the topical information be used in generation. In addition, it could be even better if there are some analysis about topics extracted by this model.\n\nThis paper also doesn't pay much attention to the existing work on topic-driven conversational modeling. For example \"Topic Aware Neural Response Generation\" from Xing et al., 2017.\n\nSome additional issues:\n\n1) In the second line under equation 4, y_{t-1} -> y_{t}\n2) In the first paragraph of section 3, two \"MLP\"'s are confusing\n3) In the first paragraph of page 6, words with \"highest inverse document frequency\" are used as stop words?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1515642409056,"tcdate":1511833163874,"number":1,"cdate":1511833163874,"id":"H1E8RNcxz","invitation":"ICLR.cc/2018/Conference/-/Paper207/Official_Review","forum":"S1GUgxgCW","replyto":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference/Paper207/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The evaluation and details of experiments are not sufficient.","rating":"4: Ok but not good enough - rejection","review":"This paper proposed the combination of topic model and seq2seq conversational model.\nThe idea of this combination is not surprising but the attendee of ICLR might be interested in the empirical results if the model clearly outperforms the existing method in the experimental results.\nHowever, I'm not sure that the empirical evaluation shows the really impressive results.\nIn particular, the difference between LV-S2S and LTCM seem to be trivial.\nThere are many configurations in the LSTM-based model.\nCan you say that there is no configuration of LV-S2S that outperforms your model?\nMoreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]}},{"tddate":null,"ddate":null,"tmdate":1515152439185,"tcdate":1509060682464,"number":207,"cdate":1509739426720,"id":"S1GUgxgCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1GUgxgCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Latent Topic Conversational Models","abstract":"Despite much success in many large-scale language tasks, sequence-to-sequence (seq2seq) models have not been an ideal choice for conversational modeling as they tend to generate generic and repetitive responses. In this paper, we propose a Latent Topic Conversational Model (LTCM) that augments the seq2seq model with a neural topic component to better model human-human conversations. The neural topic component encodes information from the source sentence to build a global “topic” distribution over words, which is then consulted by the seq2seq model to improve generation at each time step. The experimental results show that the proposed LTCM can generate more diverse and interesting responses by sampling from its learnt latent representations. In a subjective human evaluation, the judges also confirm that LTCM is the preferred option comparing to competitive baseline models.\n","pdf":"/pdf/2abcc0d2d4fd220e99a58f0869fc4e054fcdbdf9.pdf","TL;DR":"Latent Topic Conversational Model, a hybrid of seq2seq and neural topic model to generate more diverse and interesting responses.","paperhash":"anonymous|latent_topic_conversational_models","_bibtex":"@article{\n  anonymous2018latent,\n  title={Latent Topic Conversational Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GUgxgCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper207/Authors"],"keywords":["conversational modeling","dialogue","chitchat","open-domain dialogue","topic model","neural variational inference","human evaluation","latent variable model","gaussian reparameterisation trick"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}