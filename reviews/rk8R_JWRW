{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222670416,"tcdate":1511826009081,"number":2,"cdate":1511826009081,"id":"SyWwzQceM","invitation":"ICLR.cc/2018/Conference/-/Paper495/Official_Review","forum":"rk8R_JWRW","replyto":"rk8R_JWRW","signatures":["ICLR.cc/2018/Conference/Paper495/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A spiking implementation of LSTMs","rating":"5: Marginally below acceptance threshold","review":"The authors propose a first implementation of spiking LSTMs. This is an interesting and open problem. However, the present work somewhat incomplete, and requires further experiments and clarifications.\n\nPros:\n1. To my best knowledge, this is the first mapping of LSTMs to spiking networks\n2. The authors tackle an interesting and challenging problem.\n\nCons:\n1. In the abstract the authors mention that another approach has been taken, but is never stated what’s the problem that this new one is trying to address. Also, H&S 1997 tested several tasks, which is the one that the authors are referring to?\n2. Figure 1 is not very easy to read. The authors can spell out the labels of the axis (e.g. S could be input, S)\n3. Why are output and forget gates not considered here?\n4. A major point in mapping LSTMs to spiking networks is its biological plausibility. However, the authors do not seem to explore this. Of particular interest is its relationship to a recent proposal of a cortical implementation of LSTMs (Cortical microcircuits as gated-RNNs, NIPS 2017).\n5. The text should be improved, for example in the abstract: “that almost all resulting spiking neural network equivalents correctly..”, please rephrase.\n6. Current LSTMs are applied in much more challenging problems than the original ones. It would be important to test one of this, perhaps the relatively simple pixel-by-pixel MNIST task. If this is not feasible, please comment.\n\nMinor comments:\n1. Change in the abstract “can be substituted for” > “can be substituted by”\n2. A new body of research aims at using backprop in spiking RNNs (e.g. Friedemann and Ganguli 2017). The present work gets around this by training the analog version instead. It would be of interesting to discuss how to train spiking-LSTMs as this is an important topic for future research. \n3. As the main promise of using spiking nets (instead of rate) is their potential efficiency in neuromorphic systems, it would be interesting to contrast in the text the two options for LSTMs, and give some more quantitative analyses on the gain of spiking-LSTM versus rate-LSTMs in terms of efficiency.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gating out sensory noise in a spike-based Long Short-Term Memory network","abstract":"Spiking neural networks are being investigated both as biologically plausible models of neural computation and also as a potentially more efficient type of neural network. While convolutional spiking neural networks have been demonstrated to achieve near state-of-the-art performance, only one solution has been proposed to convert gated recurrent neural networks, so far.\nRecurrent neural networks in the form of networks of gating memory cells have been central in state-of-the-art solutions in problem domains that involve sequence recognition or generation. Here, we design an analog gated LSTM cell where its neurons can be substituted for efficient stochastic spiking neurons. These adaptive spiking neurons implement an adaptive form of sigma-delta coding to convert internally computed analog activation values to spike-trains. For such neurons, we approximate the effective activation function, which resembles a sigmoid. We show how analog neurons with such activation functions can be used to create an analog LSTM cell; networks of these cells can then be trained with standard backpropagation. We train these LSTM networks on a noisy and noiseless version of the original sequence prediction task from Hochreiter & Schmidhuber (1997), and also on a noisy and noiseless version of a classical working memory reinforcement learning task, the T-Maze. Substituting the analog neurons for corresponding adaptive spiking neurons, we then show that almost all resulting spiking neural network equivalents correctly compute the original tasks.","pdf":"/pdf/00d851a73c1f010821e0893cc3161eaa7c442fc0.pdf","TL;DR":" We demonstrate a gated recurrent asynchronous spiking neural network that corresponds to an LSTM unit.","paperhash":"anonymous|gating_out_sensory_noise_in_a_spikebased_long_shortterm_memory_network","_bibtex":"@article{\n  anonymous2018gating,\n  title={Gating out sensory noise in a spike-based Long Short-Term Memory network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8R_JWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper495/Authors"],"keywords":["spiking neural networks","LSTM","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222671215,"tcdate":1511769495841,"number":1,"cdate":1511769495841,"id":"BkeiHSFxz","invitation":"ICLR.cc/2018/Conference/-/Paper495/Official_Review","forum":"rk8R_JWRW","replyto":"rk8R_JWRW","signatures":["ICLR.cc/2018/Conference/Paper495/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting idea, but it seems that the main claims of are not sufficiently well proven","rating":"5: Marginally below acceptance threshold","review":"First the authors suggest an adaptive analog neuron (AAN) model which can be trained by back-propagation and then mapped to an Adaptive Spiking Neuron (ASN). Second, the authors suggest a network module called Adaptive Analog LSTM Cell (AA-LSTM) which contains input cells, input gates, constant error carousels (CEC) and output cells. Jointly with the AA-LSTM, the authors describe a spiking model (AS-LSTM) that is meant to reproduce its transfer function. It is shown quantitatively that the transfer functions of isolated AAN and AA-LSTM units are well approximated by their spiking counterparts. Two sets of experiments are reported, a sequence prediction task taken from the original LSTM paper and a T-maze task solved with reward based learning.\n\nIn general, the paper presents an interesting idea. However, it seems that the main claims of the introduction are not sufficiently well proven later. Also, I believe that the tasks are rather simple and therefore it is not demonstrated that the approach performs well on practically relevant tasks.\n\nOn general level, it should be clarified whether the model is meant to reproduce features of biology or whether the model is meant to be efficient. If the model is meant to reproduce biology, some features of the model are problematic. In particular, that the CEC is modeled with an infinitely long integration time constant of the input current. This would produce infinitely long EPSPs. However, I think there is a chance that minor changes of the model could still work while being more realistic. For example, I would find it more convincing to put the CEC into the adaptation time constants by using a large tau_gamma or tau_eta.\n\nIf the model is meant to provide efficient spiking neural networks, I find the tasks too simple and too artificial. This is particularly true in comparison to the speech recognition tasks VAD and TIMIT which were already solved in Esser et al. with spiking and efficient feedforward networks. \n\nThe authors say in the introduction that they target to model recurrent neural networks. This is an important open question. The usage of the CEC is an interesting idea toward this goal.\nHowever, beside the presence of CEC I do not see any recurrence in the used networks. This seems in contradiction with what is implicitly claimed in the introduction, title and abstract. There are only input-output neuron connections in the sequence prediction task, and a single hidden layer for the T-maze (which does not seem to be recurrently connected). This is problematic as the authors mention that their goal is to reproduce the functionality of LSTMs with spiking neurons for which the network recurrence is an important feature. \n\n\nRegarding more low-level comments:\n\n- The authors used a truncated version of RTRL to train LSTMs and standard back-propagation for single neurons. I wonder why two different algorithms were used, as, in principle, they compute the same gradient\neither forward or backward.\nIs there a reason for this? Did the truncated RTRL bring any\nadditional benefit compared to the exact backpropagation already\nimplemented in automatic differentiation software?\n\n- The sigma-delta neuron model seems quite ad-hoc and incompatible\nwith most simulators and dedicated hardware. I wonder whether the\nAS-LSTM model would still be valid if the ASN model is replaced with a\nstandard SRM model for instance.\n\n- The authors claim in the introduction that they made an analytical conversion from discrete to continuous time. I did not find this in the main text.\n\n- The axes in Figure 1 are not defined (what is Delta S?) and the\ncaption does not match. \"Average output signal [...] as a function of its incoming PSC I\" output signal is not defined, and S is presented in the graph, but not I.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gating out sensory noise in a spike-based Long Short-Term Memory network","abstract":"Spiking neural networks are being investigated both as biologically plausible models of neural computation and also as a potentially more efficient type of neural network. While convolutional spiking neural networks have been demonstrated to achieve near state-of-the-art performance, only one solution has been proposed to convert gated recurrent neural networks, so far.\nRecurrent neural networks in the form of networks of gating memory cells have been central in state-of-the-art solutions in problem domains that involve sequence recognition or generation. Here, we design an analog gated LSTM cell where its neurons can be substituted for efficient stochastic spiking neurons. These adaptive spiking neurons implement an adaptive form of sigma-delta coding to convert internally computed analog activation values to spike-trains. For such neurons, we approximate the effective activation function, which resembles a sigmoid. We show how analog neurons with such activation functions can be used to create an analog LSTM cell; networks of these cells can then be trained with standard backpropagation. We train these LSTM networks on a noisy and noiseless version of the original sequence prediction task from Hochreiter & Schmidhuber (1997), and also on a noisy and noiseless version of a classical working memory reinforcement learning task, the T-Maze. Substituting the analog neurons for corresponding adaptive spiking neurons, we then show that almost all resulting spiking neural network equivalents correctly compute the original tasks.","pdf":"/pdf/00d851a73c1f010821e0893cc3161eaa7c442fc0.pdf","TL;DR":" We demonstrate a gated recurrent asynchronous spiking neural network that corresponds to an LSTM unit.","paperhash":"anonymous|gating_out_sensory_noise_in_a_spikebased_long_shortterm_memory_network","_bibtex":"@article{\n  anonymous2018gating,\n  title={Gating out sensory noise in a spike-based Long Short-Term Memory network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8R_JWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper495/Authors"],"keywords":["spiking neural networks","LSTM","recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739271728,"tcdate":1509124302436,"number":495,"cdate":1509739269072,"id":"rk8R_JWRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk8R_JWRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gating out sensory noise in a spike-based Long Short-Term Memory network","abstract":"Spiking neural networks are being investigated both as biologically plausible models of neural computation and also as a potentially more efficient type of neural network. While convolutional spiking neural networks have been demonstrated to achieve near state-of-the-art performance, only one solution has been proposed to convert gated recurrent neural networks, so far.\nRecurrent neural networks in the form of networks of gating memory cells have been central in state-of-the-art solutions in problem domains that involve sequence recognition or generation. Here, we design an analog gated LSTM cell where its neurons can be substituted for efficient stochastic spiking neurons. These adaptive spiking neurons implement an adaptive form of sigma-delta coding to convert internally computed analog activation values to spike-trains. For such neurons, we approximate the effective activation function, which resembles a sigmoid. We show how analog neurons with such activation functions can be used to create an analog LSTM cell; networks of these cells can then be trained with standard backpropagation. We train these LSTM networks on a noisy and noiseless version of the original sequence prediction task from Hochreiter & Schmidhuber (1997), and also on a noisy and noiseless version of a classical working memory reinforcement learning task, the T-Maze. Substituting the analog neurons for corresponding adaptive spiking neurons, we then show that almost all resulting spiking neural network equivalents correctly compute the original tasks.","pdf":"/pdf/00d851a73c1f010821e0893cc3161eaa7c442fc0.pdf","TL;DR":" We demonstrate a gated recurrent asynchronous spiking neural network that corresponds to an LSTM unit.","paperhash":"anonymous|gating_out_sensory_noise_in_a_spikebased_long_shortterm_memory_network","_bibtex":"@article{\n  anonymous2018gating,\n  title={Gating out sensory noise in a spike-based Long Short-Term Memory network},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk8R_JWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper495/Authors"],"keywords":["spiking neural networks","LSTM","recurrent neural networks"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}