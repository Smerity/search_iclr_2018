{"notes":[{"tddate":null,"ddate":null,"tmdate":1512420402017,"tcdate":1512420402017,"number":3,"cdate":1512420402017,"id":"S1c4VEXWz","invitation":"ICLR.cc/2018/Conference/-/Paper69/Official_Review","forum":"HJtEm4p6Z","replyto":"HJtEm4p6Z","signatures":["ICLR.cc/2018/Conference/Paper69/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Detailed tech report, missing some motivation and comparison experiments","rating":"6: Marginally above acceptance threshold","review":"This paper provides an overview of the Deep Voice 3 text-to-speech system. It describes the system in a fair amount of detail and discusses some trade-offs w.r.t. audio quality and computational constraints. Some experimental validation of certain architectural choices is also provided.\n\nMy main concern with this work is that it reads more like a tech report: it describes the workings and design choices behind one particular system in great detail, but often these choices are simply stated as fact and not really motivated, or compared to alternatives. This makes it difficult to tell which of these aspects are crucial to get good performance, and which are just arbitrary choices that happen to work okay.\n\nAs this system was clearly developed with actual deployment in mind (and not purely as an academic pursuit), all of these choices must have been well-deliberated. It is unfortunate that the paper doesn't demonstrate this. I think this makes the work less interesting overall to an ICLR audience. That said, it is perhaps useful to get some insight into what types of models are actually used in practice.\n\nAn exception to this is the comparison of \"converters\", model components that convert the model's internal representation of speech into waveforms. This comparison is particularly interesting because some of the results are remarkable, i.e. Griffin-Lim spectrogram inversion and the WORLD vocoder achieving very similar MOS scores in some cases (Table 2). I wish there would be more of that kind of thing in the paper. The comparison of attention mechanisms is also useful.\n\nI'm on the fence as I think it is nice to get some insight into a practical pipeline which benefits from many current trends in deep learning research (autoregressive models, monotonic attention, ...), but I also feel that the paper is a bit meager when it comes to motivating all the architectural aspects. I think the paper is well written so I've tentatively recommended acceptance.\n\n\nOther comments:\n\n- The separation of the \"decoder\" and \"converter\" stage is not entirely clear to me. It seems that the decoder is trained to predict spectrograms autoregressively, but its final layer is then discarded and its hidden representation is then used as input to the converter stage instead? The motivation for doing this is unclear to me, surely it would be better to train everything end-to-end, including the converter? This seems like an unnecessary detour, what's the reasoning behind this?\n\n- At the bottom of page 2 it is said that \"the whole model is trained end-to-end, excluding the vocoder\", which I think is an unfortunate turn of phrase. It's either end-to-end, or it isn't.\n\n- In Section 3.3, the point of mixing of h_k and h_e is unclear to me. Why is this done?\n\n- The gated linear unit in Figure 2a shows that speaker embedding information is only injected in the linear part. Has this been experimentally validated to work better than simpler mechanisms such as adding conditioning-dependent biases/gains?\n\n- When the decoder is trained to do autoregressive prediction of spectrograms, is it autoregressive only in time, or also in frequency? I'm guessing it's the former, but this means there is an implicit independence assumption (the intensities in different frequency bins are conditionally independent, given all past timesteps). Has this been taken into consideration? Maybe it doesn't matter because the decoder is never used directly anyway, and this is only a \"feature learning\" stage of sorts?\n\n- Why use the L1 loss on spectrograms?\n\n- The recent work on Parallel WaveNet may allow for speeding up WaveNet when used as a vocoder, this could be worth looking into seeing as inference speed is used as an argument to choose different vocoder strategies (with poorer audio quality as a result).\n\n- The title heavily emphasizes that this model can do multi-speaker TTS with many (2000) speakers, but that seems to be only a minor aspect that is only discussed briefly in the paper. And it is also something that preceding systems were already capable of (although maybe it hasn't been tested with a dataset of this size before). It might make sense to rethink the title to emphasize some of the more relevant and novel aspects of this work.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Voice 3: 2000-Speaker Neural Text-to-Speech","abstract":"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server.","pdf":"/pdf/f862db0c4998567b34789981132e5bf80df75b07.pdf","paperhash":"anonymous|deep_voice_3_2000speaker_neural_texttospeech","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Voice 3: 2000-Speaker Neural Text-to-Speech},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJtEm4p6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper69/Authors"],"keywords":["Attention","Fully-convolutional","Text-to-Speech","Multi-speaker"]}},{"tddate":null,"ddate":null,"tmdate":1512222720468,"tcdate":1511819091414,"number":2,"cdate":1511819091414,"id":"rJo8vWqgM","invitation":"ICLR.cc/2018/Conference/-/Paper69/Official_Review","forum":"HJtEm4p6Z","replyto":"HJtEm4p6Z","signatures":["ICLR.cc/2018/Conference/Paper69/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The paper presents a speech synthesis system based on convolution neural networks. The proposed approach is an end-to-end characters to spectrogram system, trained on a very large dataset. The paper also introduces a attention model and can be used with various waveform synthesis methods. The proposed model is shown to match the state -of-the-art approaches performance in speech naturalness. \n\nThe paper is clearly written and easy to follow. The relation to previous works is detailed and clear.\n\nThe contributions of the paper are significants and an important step towards practical and efficient neural TTS system. The ability to train on a large corpus of speaker 10 times faster than current models is impressive and important for deployment, as is the cost-effective inference and the monotonic attention model. \nThe experiments on naturalness (Table 2) are convincing and show the viability of the approach. However, the experiments on multi-speaker synthesis (Table 3) are not very strong. The proposed model seems to need to use Wavenet as a vocoder to possibly outperform Deep Voice 2, which will slow down the inference time, one of the strong aspect of the proposed model.\n\nOther comments:\n\n* In Section 2, it is mentioned that RNN-based approaches can leads to attention errors, can the authors elaborate more on that aspect ? It seems important as the proposed approach alleviates these issues, but it is not clear from the paper what these errors are and why they happen.\n\n* In Table 3 there seems to be missing models compared to Table 2, like Tacotron with Wavenet, the authors should explain why in the text.  \n\n* The footnote 2 on page 3 looks important enough to be part of the main text.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Voice 3: 2000-Speaker Neural Text-to-Speech","abstract":"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server.","pdf":"/pdf/f862db0c4998567b34789981132e5bf80df75b07.pdf","paperhash":"anonymous|deep_voice_3_2000speaker_neural_texttospeech","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Voice 3: 2000-Speaker Neural Text-to-Speech},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJtEm4p6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper69/Authors"],"keywords":["Attention","Fully-convolutional","Text-to-Speech","Multi-speaker"]}},{"tddate":null,"ddate":null,"tmdate":1512222720507,"tcdate":1511672478869,"number":1,"cdate":1511672478869,"id":"r1Ps9aPez","invitation":"ICLR.cc/2018/Conference/-/Paper69/Official_Review","forum":"HJtEm4p6Z","replyto":"HJtEm4p6Z","signatures":["ICLR.cc/2018/Conference/Paper69/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Impressive results, but missing some details and insights","rating":"6: Marginally above acceptance threshold","review":"This paper discusses a text-to-speech system which is based on a convolutional attentive seq2seq architecture.  It covers experiments on a few datasets, testing the model's ability to handle increasing numbers of speakers.\n\nBy and large, this is a \"system\" paper - it mostly describes the successful application of many different existing ideas to an important problem (with some exceptions, e.g. the novel method of enforcing monotonic alignments during inference).  In this type of paper, I typically am most interested in hearing about *why* a particular design choice was made, what alternatives were tried, and how different ideas worked.  This paper is lacking in this regard - I frequently was left looking for more insight into the particular system that was designed.  Beyond that, I think more detailed description of the system would be necessary in order to reimplement it suitably (another important potential takeaway for a \"system\" paper).  Separately, I the thousands-of-speakers results are just not that impressive - a MOS of 2 is not really useable in the real-world.  For that reason, I think it's a bit disingenuous to sell this system as \"2000-Speaker Neural Text-to-Speech\".\n\nFor the above reasons, I'm giving the paper a \"marginally above\" rating.  If the authors provide improved insight, discussion of system specifics, and experiments, I'd be open to raising my review.  Below, I give some specific questions and suggestions that could be addressed in future drafts.\n\n- It might be worth giving a sentence or two defining the TTS problem - the paper is written assuming background knowledge about the problem setting, including different possible input sources, what a vocoder is, etc.  The ICLR community at large may not have this domain-specific knowledge.\n- Why \"softsign\" and not tanh?  Seems like an unusual choice.\n- What do the \"c\" and \"2c\" in Figure 2a denote?\n- Why scale (h_k + h_e) by \\sqrt{0.5} when computing the attention value vectors?\n- \"An L1 loss is computed using the output spectrograms\" I assume you mean the predicted and target spectrograms are compared via an L1 loss.  Why L1?\n- In Vaswani et al., it was shown that a learned positional encoding worked about as well as the sinusoidal position encodings despite being potentially more flexible/less \"hand-designed\" for machine translation.  Did you also try this for TTS?  Any insight?\n- Some questions about monotonic attention: Did you use the training-time \"soft\" monotonic attention algorithm from Raffel et al. during training and inference, or did you use the \"hard\" monotonic attention at inference time?  IIUC the \"soft\" algorithm doesn't actually force strict monotonicity.  You wrote \"monotonic attention results in the model frequently mumbling words\", can you provide evidence/examples of this?  Why do you think this happens?  The monotonic attention approach seems more principled than post-hoc limiting softmax attention to be monotonic, why do you think it didn't work as well?\n- I can't find an actual reference to what you mean by a \"wavenet vocoder\".  The original wavenet paper describes an autoregressive model for waveform generation.  In order to use it as a vocoder, you'd have to do conditioning in some way.  How?  What was the structure of the wavenet you used?   Why?  These details appear to be missing.  All you write is the sentence (which seems to end without a period) \"In the WaveNet vocoder, we use mel-scale spectrograms from the decoder to condition a Wavenet, which was trained separated\".\n- Can you provide examples of the mispronunciations etc. which were measured for Table 1?  Was the evaluation of each attention mechanism done blindly?\n- The 2.07 MOS figure produced for tacotron seems extremely low, and seems to indicate that something went wrong or that insufficient care was taken to report this baseline.  How did you adapt tacotron (which as I understand is a single-speaker model) to the multi-speaker setting?\n- Table 3 begs the question of whether Deep Voice 3 can outperform Deep Voice 2 when using a wavenet vocoder on VCTK (or improve upon the poor 2.09 MOS score reported).  Why wasn't this experiment run?\n- The paragraph and appendix about deploying at scale is interesting and impressive, but seems a bit out of place - it probably makes more sense to include this information in a separate \"systems\" paper.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Voice 3: 2000-Speaker Neural Text-to-Speech","abstract":"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server.","pdf":"/pdf/f862db0c4998567b34789981132e5bf80df75b07.pdf","paperhash":"anonymous|deep_voice_3_2000speaker_neural_texttospeech","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Voice 3: 2000-Speaker Neural Text-to-Speech},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJtEm4p6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper69/Authors"],"keywords":["Attention","Fully-convolutional","Text-to-Speech","Multi-speaker"]}},{"tddate":null,"ddate":null,"tmdate":1509739503264,"tcdate":1508881201180,"number":69,"cdate":1509739500604,"id":"HJtEm4p6Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJtEm4p6Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Voice 3: 2000-Speaker Neural Text-to-Speech","abstract":"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server.","pdf":"/pdf/f862db0c4998567b34789981132e5bf80df75b07.pdf","paperhash":"anonymous|deep_voice_3_2000speaker_neural_texttospeech","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Voice 3: 2000-Speaker Neural Text-to-Speech},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJtEm4p6Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper69/Authors"],"keywords":["Attention","Fully-convolutional","Text-to-Speech","Multi-speaker"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}