{"notes":[{"tddate":null,"ddate":null,"tmdate":1516648251705,"tcdate":1516648251705,"number":18,"cdate":1516648251705,"id":"SkNSwnmrG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"BJNCqPqxM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"About the analysis of the temporal ensembling trick [Samuli & Timo, 2017]","comment":"AnonReviewer2: \"But still, without the analysis of the temporal ensembling trick [Samuli & Timo, 2017]\" \n\nWe have actually reported the ablation study about this temporal ensemebling technique in the rebuttal. Please read our answer to Q4 in the rebuttal. \n\n=Q4=\n\"... which part of the model works\"\n\nPlease see either Appendix C of the revised paper or the following for our answer to this question.\n\nWe have done some ablation studies about our semi-supervised learning approach on CIFAR-10. \nMethod, Error\nw/o CT, 15.0\nw/o GAN (note 1), 12.0\nw batch norm (note 2), --\nw/o D_, 10.7\nOURS, 10.0\n\nNote 1: This almost reduces to TE (Laine & Aila, 2016). All the settings here are the same as TE except that we use the extra regularization ($D\\_(.,.)$ in CT) over the second-to-last layer.\nNote 2: We use the weight normalization as in (Salimans et al., 2016), which becomes a core constituent of our approach. The batch normalization would actually invalidate the feature matching in (Salimans et al., 2016).\n\nWe can see that both GAN and the temporal ensembling effectively contribute to our final results. The results without our consistency regularization (w/o CT) drop more than those without GAN. We are running the experiments without any data augmentation and will include the corresponding results in the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516648075321,"tcdate":1516648075321,"number":17,"cdate":1516648075321,"id":"r17cU27HG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"BJNCqPqxM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"About data augmentation","comment":"Following (Laine & Aila, 2016, Miyato et al., 2017, Tarvainen & Valpola, 2017), we do not apply any augmentation to MNIST and yet augment the CIFAR10 images in the following way. We flip the images horizontally and randomly translate the images within [-2,2] pixels horizontally. \n\nSamuli Laine and Timo Aila.  Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.\nAntti Tarvainen and Harri Valpola.  Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516647938543,"tcdate":1516647938543,"number":16,"cdate":1516647938543,"id":"rJJbI3QHM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"B11HQF84M","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Experiments on LSUN finished","comment":"FYI, we have finally finished the experiments on LSUN-Bedroom. The results are comparable to those reported in (Gulrajani et al. 2017) except that our generated images are more diverse in terms of the color theme. \n\n1. https://goo.gl/MvK2x8\n2. https://goo.gl/Cidqgu\n3. https://goo.gl/f6WMeJ\n4. https://goo.gl/N3Jc6M\n5. https://goo.gl/XCpmaK"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515782995497,"tcdate":1515782966991,"number":13,"cdate":1515782966991,"id":"B11HQF84M","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"SywG98VGM","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer3"],"content":{"title":"Rebuttal response","comment":"Thank you for the rebuttal! I'm glad you took the comments into account and updated the manuscript.\n\nA few big picture comments and conclusions, and why I decided to keep my score based on rebuttal, revision and newer comments:\n- I still think the justification of *why* the regularizer should give an increase in performance is not on the level of a venue like this, and most of the arguments on the theoretical motivation are not strong. I don't find the rebuttal sufficient in this context. If keeping the gradient penalty makes it safe to only check the data manifold, it's still not clear why your method should give an improvement over it (for example, what's a solid theoretical argument for saying enforcing 1-lip in the data manifold more important than elsewhere?).\n- The authors did a good job at showing improvement in their models on the CIFAR dataset. However, this is not sufficient evidence to prove scalability of the regularizer, especially considering that adding hyperparameters and tuning well in a small dataset like CIFAR can give drastically different results. Furthermore, the results in LSUN and Imagenet are very unconvincing.\n- I appreciate Appendices D and E. These are very important sanity checks and I'm glad you added them.\n\nI think the paper is in a better state now, but taking all of these things into account, the given score is accurate in my opinion."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515123668591,"tcdate":1513566987170,"number":10,"cdate":1513566987170,"id":"r1QGQhVfG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"SksBJ-fzz","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Thank you for checking out our code!","comment":"Thank you for checking out our code! If you are interested, please also test M’=0.2 for CIFAR-10 and you should be able to see a slightly higher inception score than M’=0. We fix M’=0 for all the experiments in the paper for consistency, but as we wrote in the paper, the best results are obtained between M’=0 and M’=0.2. \n\nWe noted that the assumption of d(x_1,x_2) being a constant can be relaxed. Our derivations still hold as long as d(x’,x’’) is bounded by a constant, and we can absorb the constant to M’. \n\nWe have actually reported two sets of experiments for CIFAR-10 in the paper. The first set is done using 1000 images and the second uses the whole CIFAR-10 dataset to train a ResNet. These setups are the same as in [1]. Additionally, we are running experiments on ImageNet and LSUN; we will update the response once the experiments are done. \n\nAbout the overfitting, please see Appendix E of the revised paper for the experimental results of GP-WGAN+Dropout on CIFAR-10 using 1000 training images. The corresponding inception score is better than GP-WGAN and yet still significantly lower than ours (2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12). Figure 12, which is about the convergence curves of the discriminator cost over both training and testing sets, shows that dropout is indeed able to reduce the overfitting, but it is not as effective as ours.\n\nIn Appendix F of the revised paper, we further present  experimental results on the large-scale ImageNet and LSUN bedroom datasets. The experiment setup (e.g., network architecture, learning rates, etc.) is exactly the same as in the GP-WGAN work. After 200,000 generator iterations on ImageNet, the inception score of the proposed CT-GAN is 10.27+-0.15, whereas GP-WGAN's is 9.85+-0.17. Since there is only one class in LSUN bedroom, the inception score is not a proper evaluation metric for the experiments on this dataset. Visually, there is no clear difference between the generated samples of GP-WGAN and CT-GAN."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515124011622,"tcdate":1513544207384,"number":9,"cdate":1513544207384,"id":"SywG98VGM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"HkbRNKwef","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Response to ICLR 2018 Conference Paper1144 AnonReviewer3","comment":"We thank the reviewer for the insightful comments and suggestions! The paper has been revised accordingly. Next, we answer the questions in detail.\n\n== Q1: The motivation ==\nWe acknowledge that the duality uses 1-Lipschitz continuity in the entire space between Pr and Pg, and it is impossible to visit everywhere of the space in the experiments. We instead focus on the region around the real data manifold to complement the region checked by GP-WGAN --- the gradient penalty term is kept in our overall approach. We have clarified this point by the following in the revised paper.\n\nArguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution $\\mathbb{P}_r$ and its surrounding regions mainly for two reasons. First, we  keep the gradient penalty term  and improve it by the proposed consistency term in our overall approach. While the former enforces the continuity over the points sampled between the real and generated points, the latter complement the former by focusing on the region around the real data manifold instead. Second, the distribution of the generative model $\\mathbb{P}_G$ is virtually desired to be as close as possible to $\\mathbb{P}_r$.\n\n\n== Q2: That 1-Lip may not be enforced in non-examined samples is checkable ==\nThe non-examined samples can refer to all the possible samples in the continuous space which cannot be traversed in a discrete manner. Figure 8 plots the norm of the gradients (of the critic with respect to the input) over the real data points only. In other words, Figure 8 is only part of the consequence, and certainly not the cause, of the discriminators trained by GP-WGAN and our CT-GAN, respectively. It is not surprising that the norms by CT-GAN are closer to 1 than by GP-WGAN because we explicitly enforce the continuity around the real data. \n\nWe have run more experiments with larger \\lambda values in GP-GAN, and found the gradient norms can indeed reach those of CT-GAN when the \\lambda is four times larger than the original one used in the authors’ code. However, the inception score on CIFAR-10 drops a little, and the overfitting remains. \n\nStronger evidence? In addition to the gradient norm, we have also examined the 1-Lipschitz continuity of the critic using the basic definition. For any two inputs x and x', the difference of the critic's outputs should be no more than M*|x-x'|. This notion is captured by our CT term defined in eq. (4). We plot the CT versus the training iterations as Figure 9 in the revised paper. In particular, for every 100 iterations, we randomly pick up 64 real examples and split them into two subsets of the same size. We compute d(D(x1)-D(x2))/d(x1-x2) for all the (x1,x2) pairs, where x1 is from the first subset and x2 is from the second. The maximum of d(D(x1)-D(x2))/d(x1-x2) is plotted in Figure 9. We can see that the CT-GAN curve converges under a certain value much faster than GP-WGAN.\n\n==  Q3: Plot the value of the CT regularizer ==\nPlease see Figures 9 and 10 in the revised paper for the plots. Note that M’ has absorbed the term d(x’,x’’) in the final consistency term (eq. (5)), so we have to tune its value as opposed to fixing it to 1. Also, because of this fact, we agree with the comment that “Thus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant.” We will clarify this part in the final paper, if it is accepted. \n\n== Q4: Line 11 ==\nIt is correct and is another way of denoting the gradient.\n\n== Q5: MNIST ==\nWe understand your concern with the use of MNIST and appreciate that you agree the overfitting experiments (Figure 4) are relevant. The other results (e.g., the generated samples and the test error in semi-supervised learning) can give the readers a concrete understanding about our model, but we agree one should not use MNIST to compare different algorithms.\n\n\n== Q6: GP-WGAN + Dropout ==\nPlease see Appendix E for the experimental results of GP-WGAN+Dropout on CIFAR-10 using 1000 training images. The corresponding inception score is better than GP-WGAN and yet still significantly lower than ours (2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12). Figure 12, which is about the convergence curves of the discriminator cost over both training and testing sets, shows that dropout is indeed able to reduce the overfitting, but it is not as effective as ours.\n\n\n== Q7: Experiments in larger datasets ==\nIn Appendix F of the revised paper, we present results on the ImageNet and LSUN bedroom datasets following the experiment setup of GP-WGAN. After 200,000 generator iterations on ImageNet, the inception score of CT-GAN is 10.27+-0.15, whereas GP-WGAN's is 9.85+-0.17. Since there is only one class in LSUN bedroom, the inception score is not a proper evaluation metric for the experiments on this dataset. Visually, there is no clear difference between the generated samples of GP-WGAN and CT-GAN up to the 124,000th generator iteration.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513567442373,"tcdate":1513534860756,"number":8,"cdate":1513534860756,"id":"ryr5SEEfG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"ryT2f8KgM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Response to ICLR 2018 Conference Paper1144 AnonReviewer1","comment":"We thank the reviewer for the very positive and affirmative comments about our work. \n\nWe also appreciate the suggestions for improving the writing clarify of the paper. The following has been incorporated in the revised paper.\n\n== M vs. M' == \nWe use the notation $M$ in eq. (3) and a different $M'$ in eq. (4) to reflect the fact that the continuity will be checked only sparsely at some data points in practice. ... ... Note that, however, it becomes impossible to compute the distance $d(\\bm{x}',\\bm{x}'')$ between the two virtual data points. In this work, we assume it is bounded by a constant and absorb the constant to $M'$. Accordingly, we tune $M'$ in our experiments to take account of this unknown constant; the best results are obtained between $M'=0$ and $M'=0.2$. \n\n== Semi-supervised experiment details and the CIFAR-10 augmentation process ==\n\nMNIST: There are 60,000 images in total. We randomly choose 10 data points for each digit as the labeled set. No data augmentation is used.\n\nCIFAR-10: There are 50,000 image in total. We randomly choose 400 images for each class as the labeled set. We augment the data by horizontally flipping the images and randomly translating the images within [-2,2] pixels. No ZCA whitening is used.\n\nModel Configuration\n\nTable 1: MNIST\n--------------\nClassifier C                        | Generator G\nInput: Labels y, 28*28 Images x     | Input: Noise 100 z\nGaussian noise 0.3, MLP 1000, ReLU  | MLP 500, Softplus, Batch norm \nGaussian noise 0.5, MLP 500, ReLU   | MLP 500, Softplus, Batch norm \nGaussian noise 0.5, MLP 250, ReLU   | MLP 784, Sigmoid, Weight norm \nGaussian noise 0.5, MLP 250, ReLU   |                               \nGaussian noise 0.5, MLP 250, ReLU   |                               \nGaussian noise 0.5, MLP 10, Softmax |                              \n\nTable 2: CIFAR-10\n-----------------\nInput: Labels y, 32*32*3 Colored Image x,               |   Input: Noise 50 z\n------------------------------------------------------------------------------\n0.2 Dropout                                             |   MLP 8192, ReLU, BN      \n3*3 conv. 128, Pad =1, Stride =1, lReLU, Weight norm    |   Reshape 512*4*4                         \n3*3 conv. 128, Pad =1, Stride =1, lReLU, Weight norm    |   5*5 deconv. 256*8*8,    \n3*3 conv. 128, Pad =1, Stride =2, lReLU, Weight norm    |   ReLU, Batch norm                         \n------------------------------------------------------------------------------\n0.5 Dropout                                             |\n3*3 conv. 256, Pad =1, Stride =1, lReLU, Weight norm    |\n3*3 conv. 256, Pad =1, Stride =1, lReLU, Weight norm    |   5*5 deconv. 128*16*16,  \n3*3 conv. 256, Pad =1, Stride =2, lReLU, Weight norm    |   ReLU, Batch norm                             \n------------------------------------------------------------------------------\n0.5 Dropout                                             |    \n3*3 conv. 512, Pad =0, Stride =1, lReLU, Weight norm    |\n3*3 conv. 256, Pad =0, Stride =1, lReLU, Weight norm    |   5*5 deconv. 3*32*32,   \n3*3 conv. 128, Pad =0, Stride =1, lReLU, Weight norm    |   Tanh, Weight norm  \n-----------------------------------------------------------------------------\nGlobal pool                                             |\nMLP 10, Weight norm, Softmax                            |\n\n== Hyper-parameters == \nWe set \\lambda = 1.0 in Eq.(7) in all our experiments. For CIFAR-10, the number of training epochs is set to 1,000 with a constant learning rate of 0.0003. For MNIST, the number of training epochs is set to 300 with a constant learning rate of 0.003.  The other hyper-parameters are exactly the same as in the improved GAN (Salimans et al., 2016).\n\n== New Title == \nImproving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513533671778,"tcdate":1513533671778,"number":7,"cdate":1513533671778,"id":"Bkee-E4zG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"BJNCqPqxM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Response to ICLR 2018 Conference Paper1144 AnonReviewer2","comment":"We are pleased to see that the reviewer thinks our \"generated data showed very good qualities\" and \"the SSL-GANs results were impressive\".\n\n=Q1=\n\"Missing citations: the most related work of this one is the DRAGAN\"\n\nWe would consider WGAN and WGAN-GP as the most related works to ours and DRAGAN ranks after them. As a matter of fact, DRAGAN is an unpublished work and has not been peer-reviewed. As another matter of fact, the gradient penalty in DRAGAN is the same as in WGAN-GP except that it is imposed around the real data while WGAN-GP applies it to the points sampled between the real and the generated ones. \n\nNext, we highlight some key differences between DRAGAN and ours. \n\nWe propose to improve Wasserstein GAN, while DRAGAN works with GAN. \n\nDRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs. In contrast, we propose an approach to enforcing the 1-Lipschitz continuity over the critic of WGANs. \n\nOne of our key observations is that it blurs the generated samples if we add noise directly to the data points, as done in DRAGAN. Instead, we perturb the hidden layers of the discriminator. \n\nDRAGAN perturbs a data point once while we do it twice in each iteration. After the perturbations, DRAGAN penalizes the gradients while we enforce the consistency of the outputs.\n\nOne of the most distinct features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila (2016) with GANs. \n\n=Q2=\n\"the paper should discuss ... [Arjovsky & Bottou 2017], [Wu et al. 2016]\"\n\nWe had included both in our paper. Arjovsky & Bottou 2017 analyzes some distribution divergences and their effects in training GANs. Wu et al. 2016 propose to quantitatively evaluate the decoder-based generative models by annealed importance sampling. In our paper, we focus on a different subject, i.e., to design an algorithmic solution to the difficulty of training GANs.\n\n=Q3=\n\"the paper missed several details\"\n\nPlease see either Appendices A and B of the revised paper or the following for our answer to this question.\n\nGiven the context of the question, we believe it is about SSL. We follow the experiment setups in the prior works so that our results are directly comparable to theirs. Please see below for more details. If you are interested, you may also check out our code: https://github.com/biuyq/CT-GAN/blob/master/CT-GANs/Theano_classifier/CT_CIFAR-10_TE.py.\n\nMNIST: There are 60,000 images in total. We randomly choose 10 data points for each digit as the labeled set. No data augmentation is used.\n\nCIFAR-10: There are 50,000 image in total. We randomly choose 400 images for each class as the labeled set. We augment the data by horizontally flipping the images and randomly translating the images within [-2,2] pixels. No ZCA whitening is used.\n\nModel Configurations: We had included them in the appendix.\n\nHyper-parameters: We set lambda = 1.0 in Eq.(7) in all our experiments. For CIFAR-10, the number of training epochs is set to 1,000 with a constant learning rate of 0.0003. For MNIST, the number of training epochs is set to 300 with a constant learning rate of 0.003.  The other hyper-parameters are exactly the same as in the improved GAN (Salimans et al., 2016).\n\n=Q4=\n\"... which part of the model works\"\n\nPlease see either Appendix C of the revised paper or the following for our answer to this question.\n\nWe have done some ablation studies about our semi-supervised learning approach on CIFAR-10. \nMethod, Error\nw/o CT, 15.0\nw/o GAN (note 1), 12.0\nw batch norm (note 2), --\nw/o D_, 10.7\nOURS, 10.0\n\nNote 1: This almost reduces to TE (Laine & Aila, 2016). All the settings here are the same as TE except that we use the extra regularization ($D\\_(.,.)$ in CT) over the second-to-last layer.\nNote 2: We use the weight normalization as in (Salimans et al., 2016), which becomes a core constituent of our approach. The batch normalization would actually invalidate the feature matching in (Salimans et al., 2016).\n\nWe can see that both GAN and the temporal ensembling effectively contribute to our final results. The results without our consistency regularization (w/o CT) drop more than those without GAN. We are running the experiments without any data augmentation and will include the corresponding results in the paper.\n\n=Q5=\n\"...it is better to evaluate it on more datasets\"\n\nWe have run some new experiments on the SVHN dataset. Ours is the best among all the GAN based semi-supervised learning methods, and is on par with the state of the arts.\nMethod, Error\nPI Laine & Aila 2016, 4.8\nTE Laine & Aila 2016, 4.4\nTarvainen & Valpola 2017, 4.0\nMiyato et al. 2017, 3.86\nSalimans et al. 2016, 8.1\nDumoulin et al. 2016, 7.4\nKumar et al. 2017, 5.9\nOurs, 4.2"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1513389891420,"tcdate":1513389891420,"number":16,"cdate":1513389891420,"id":"SksBJ-fzz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Review for paper 1144","comment":"This paper points out a potential caveat for the improved training of WGAN approach. The gradient penalty term takes effects only upon points sampled on the lines connecting pairs of data points sampled from the real distribution and the model distribution. At the beginning of the training the Lipschitz continuity over the manifold supporting the real distribution is not enforced because at the beginning stage the synthetic data points G(z), and hence the sampled points $\\hat{x}$, could be far away from the manifold. The author introduces a natural solution to overcome that problem, that is to additionally impose the Lipschitz continuity condition over the manifold supporting the real data distribution. This paper showed that WGAN with consistency term can generate sharper and more realistic samples than most state-of-art GANs. Moreover, they proposed a framework for semi-supervised training which is able to train a decent GAN model.\n\nGenerally speaking, the author did a pretty good job on improving the training of WGANs, and the results are very impressive. We re-conducted some of the experiments using the code released by author, and here are several comments based on our findings:\n\nMathematical rigorousness: Since throughout the experiments M' has been held at 0, it drops out from the consistency term (4); furthermore, since the denominator d(x_1,x_2) is a constant number, and the numerator is by the definition of metrics a value greater than or equal to 0, the consistency term effectively reduces to \n\n     CT|_{x_1,x_2}= E_{x_1,x_2 ~ P_r}[d(D(x_1),D(x_2))].\n\nWe find it hard to infer how the Lipschitz continuity is enforced from merely adding a metric d(D(x_1),D(x_2)) as an additional constraint, and we suspect that the actual training has deviated from the initial motivation which was to enforce the Lipschitz continuity over the manifold supporting the real data distribution.\n\nMore experiments on larger and higher dimensional dataset: The paper has shown a noticeable improvement on the generated CIFAR-10 image quality. Nevertheless, higher dimensional datasets should be considered in the experiments. This quality improvement on higher dimensional images can be more noticeable. Furthermore, in the paper, the authors only used 1000 samples to train the generator; we think that using larger datasets can also verify their claims more persuasively. We also tried to use the original code from [1] for MNIST data. Our results show that using the whole MNIST dataset leads to less vague digits (low contrast between foreground and background) than only using 1000 images.\n\nOverfitting analysis: The paper stated that CT-WGAN is less prone to overfitting. We have verified this claim in our experiment. However, the reason for that is less clear in the paper. We think the dropout perturbation plays an important role in avoiding overfitting. We modified GP-WGAN architecture by adding the dropout layers in the same way as in CT-WGAN. Our results show that GP-WGAN becomes less prone to overfitting after adding dropout. Therefore, we suspect that the non-overfitting property of CT-WGAN is a direct consequence of adding dropout regularization, instead of adding consistency term.\n\n[1]  I.Gulrajani, F.Ahmed, M.Arjovsky, V.Dumoulin, and A.Courville,  \"Improved  training  of  wasserstein  gans,\"arXiv  preprintarXiv:1704.00028, 2017."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512145700642,"tcdate":1512145700642,"number":6,"cdate":1512145700642,"id":"SkTmQbkbz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"BJhxQVieM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"The dual roles of that formulation","comment":"That’s a great question! We were actually wondering about a similar one. We can certainly interpret the formulation as that it encourages the network to be resilient to the dropout noise --- one of the notions that motivates the temporal ensembling semi-supervised learning method. In addition to that, however, it also enforces the Lipschitz continuity over the discriminator because of equation (3). Thanks to the dual roles of this formulation, we are able to use it to both improve the training of WGANs and connect GAN with the temporal ensembling method. \n\nWe have re-run the experiments using margin $M’=0.2$. To show that the margin, albeit small, plays an active role, we have got some statistics of the $d(D,D)+0.1 d(D_,D_)$ term over the last 10 epochs. We can see that the median values of that term are smaller and the max values are larger than the margin. \n\nMin          0.0162    0.0153    0.0149    0.0171    0.0170    0.0159    0.0140    0.0146    0.0159    0.0144\nMedian    0.1130    0.1133     0.1124    0.1138    0.1114      0.1123    0.1124     0.1122    0.1125     0.1111\nMax          7.1718    6.1229    7.1985    7.3505    4.9636    5.2252   5.2559   5.3058    5.9905    4.8519\n\n\n\n\n\n\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511895796243,"tcdate":1511895796243,"number":15,"cdate":1511895796243,"id":"BJhxQVieM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Does the intuition agree with what you are doing in the end?","comment":"Section 1.2 and Figure 1 outline the general idea behind the approach.\n\nI wonder however, how much of the intuitive explanation is still valid in the actual CT loss.\n\nTo dissect this a little:\nd(x1, x2) being a metric should always be positive. that means all instances of max(0, d(x1,x2)) reduce to d(x1,x2)\n\nLooking at Eq 4 and 5, given M=0, d(x1,x2) is assumed to be constant, it reduces to \nCT_(x1,x2) = E_{x1,x2} d(D(x1),D(x2))\nwith the constant d(x1,x2) absorbed into d.\n\nHowever, since the input is not changed but rather the network, a better notation would probably be\n\nCT = E_(x, \\theta_1, \\theta_2) d(D(x, \\theta_1), D(x, \\theta_2))\n\nwhere \\theta_1, \\theta_2 are the noise vectors used for dropout.\n\nLooking at that formulation, does this still mean it penalizes the gradient in the original input space or would it be more appropriate to say it encourages the resilience to dropout (or is that actually the same thing)?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516584792654,"tcdate":1511844555829,"number":3,"cdate":1511844555829,"id":"BJNCqPqxM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Review","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Official review for paper 1144","rating":"4: Ok but not good enough - rejection","review":"Updates: thanks for the authors' hard rebuttal work, which addressed some of my problems/concerns. But still, without the analysis of the temporal ensembling trick [Samuli & Timo, 2017] and data augmentation, it is difficult to figure out the real effectiveness of the proposed GAN. I would insist my previous argument and score. \n\nOriginal review:\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nThis paper presented an improved approach for training WGANs, by applying some Lipschitz constraint close to the real manifold in the pixel level.  The framework can also be integrated to boost the SSL performances. In experiments, the generated data showed very good qualities, measured by inception score. Meanwhile, the SSL-GANs results were impressive on MNIST and CIFAR-10, demonstrating its effectiveness. \n\nHowever, the paper has the following weakness: \n\nMissing citations: the most related work of this one is the DRAGAN work. However, it did not cite it. I think the author should cite it, make a clear justification for the comparison and emphasize the main contribution of the method. Also, it suggested that the paper should discuss its relation to other important work, [Arjovsky & Bottou 2017], [Wu et al. 2016].\n\nExperiments: as for the experimental part, it is not solid. Firstly, although the SSL results are very good, it is guaranteed the proposed GAN is good [Dai & Almahairi, et al. 2017]. Secondly, the paper missed several details, such as settings, model configuration, hyper-parameters, making it is difficult to justify which part of the model works. Since the paper using the temporal ensembling trick [Samuli & Timo, 2017],  most of the gain might be from there. Data augmentation might also help to improve. Finally, except CIFAR-10, it is better to evaluate it on more datasets. \n\nGiven the above reason, I think this paper is not ready to be published in ICLR. The author can submit it to the workshop and prepare for next conference. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642389607,"tcdate":1511772853534,"number":2,"cdate":1511772853534,"id":"ryT2f8KgM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Review","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper continues a line of improvement to Wasserstein GANs, and suggests an approach based a double perturbation of each data point, penalizing deviations from Lipshitz-ness. Empirical results demonstrate the effectiveness of the proposal. ","rating":"7: Good paper, accept","review":"This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al.  [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty \"encouraging\" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning.  \n\nIn terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator’s response on the perturbed points.  The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10.  They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. \n\nThe authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. \n\nWhere the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. \n[1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan.\n\n[2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642389646,"tcdate":1511654601307,"number":1,"cdate":1511654601307,"id":"HkbRNKwef","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Review","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference/Paper1144/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review for \"Improving the Improved Training of Wasserstein GANs\"","rating":"6: Marginally above acceptance threshold","review":"Summary:\n\nThe paper proposes a new regularizer for wgans, to be combined with the traditional gradient penalty. The theoretical motivation is bleak, and the analysis contains some important mistakes. The results are very good, as noticed by the comments, the fact that the method is also less susceptible to overfitting is also an important result, though this might be purely due to dropout. One of the main problems is that the largest dataset used is CIFAR, which is small. Experiments on something like bedrooms or imagenet would make the paper much stronger. \n\nIf the authors fix the theoretical analysis and add evidence in a larger dataset I will raise the score.\n\nDetailed comments:\n\n- The motivation of 1.2 and the sentence \"Arguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution P_r and its surrounding regions\" are incredibly wrong. First of all, it should be noted that the duality uses 1-Lip in the entire space between Pr and Pg, not in Pr alone. If the manifolds are not extremely close (such as in the beginning of training), then the discriminator can be almost exactly 1 in the real data, and 0 on the fake. Thus the discriminator would be almost exactly constant (0-Lip) near the real manifold, but will fail to be 1-lip in the decision boundary, this is where interpolations fix this issue. See Figure 2 of the wgan paper for example, in this simple example an almost perfect discriminator would have almost 0 penalty.\n\n- In the 'Potential caveats' section, the implication that 1-Lip may not be enforced in non-examined samples is checkable by an easy experiment, which is to look for samples that have gradients of the critic wrt the input with norm > 1. I performed the exp in figure 8 and saw that by taking a slightly higher lambda, one reaches gradients that are as close to 1 as with ct-gan. Since ct-gan uses an extra regularizer, I think the authors need some stronger evidence to support the claim that ct-gan better battles this 'potential caveat'.\n\n- It's important to realize that the CT regularizer with M' = 1 (1-Lip constraint) will only be positive for an almost 1-Lip function if x and x' are sampled when x - x' has a very similar direction than the gradient at x. This is very hard in high dimensional spaces, and when I implemented a CT regularizer indeed the ration of eq (4) was quite less than the norm of the gradient. It would be useful to plot the value of the CT regularizer (the eq 4 version) as the training iterations progresses. Thus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant. This difference is non-trivial and should be discussed.\n\n- Line 11 of the algorithm is missing L^(i) inside the sum.\n\n- One shouldn't use MNIST for anything else than deliberately testing an overfitting problem. Figure 4 is thus relevant, but the semi-supervised results of MNIST or the sample quality experiments give hardly any evidence to support the method.\n\n- The overfitting result is very important, but one should disambiguate this from being due to dropout. Comparing with wgangp + dropout is thus important in this experiment.\n\n- The authors should provide experiments in at least one larger dataset like bedrooms or imagenet (not faces, which is known to be very easy). This would strengthen the paper quite a bit.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511389406380,"tcdate":1511389406380,"number":13,"cdate":1511389406380,"id":"HkLJt_QxG","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"BynjqwxJf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Ablated study results","comment":"The table below shows the ablated study results:\n\nMethod                                                                          |                  Test Error\nOURS w/o CT                                                               |                 14.98+-0.43\nOURS w/o GAN *                                                         |                  11.98+-0.32\nOURS w batch norm **                                                |                           --\nOURS w/o D_(.,.) over the second-to-last layer        |                  10.70+-0.24\nOURS                                                                             |                  9.98+-0.21\n\n* This almost reduces to TE (Laine & Aila, 2016). All the settings are exactly the same as in TE  (Laine & Aila, 2016) except that we use the extra regularization (D_(.,.) in CT) over the second-to-last layer.\n** We use the weight normalization as in (Salimans et al., 2016), which becomes a core constituent of our approach. The batch normalization would actually invalidate the feature matching in  (Salimans et al., 2016).\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprintarXiv:1610.02242, 2016.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.Improved techniques for training gans. In Advances in Neural Information Processing Systems,pp. 2234–2242, 2016."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510795715364,"tcdate":1510795715364,"number":10,"cdate":1510795715364,"id":"BJopYw5yM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Code","comment":"Here is the code for this paper: https://github.com/biuyq/CT-GAN\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510140580230,"tcdate":1510140580230,"number":5,"cdate":1510140580230,"id":"BynjqwxJf","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"r1mlEqyyz","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Ablated studies","comment":"We have done some ablated studies but they are not as thorough as you suggested. We will complete them and then get back to you soon. Thanks! \n\nObservations thus far: Both the consistent regularization and GAN are necessary to arrive at the report results, and the results without the consistency drop more than those without GAN.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510140148720,"tcdate":1510140148720,"number":4,"cdate":1510140148720,"id":"SkpgYPxyz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"rJm5I9kJM","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Will add a column about the data agumentations","comment":"Hello Hongyi, \n\nWe will add a column or a new table about the results with and without the data augmentations. Thank you for the pointer! \n\nBest,\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510086283416,"tcdate":1510086283416,"number":9,"cdate":1510086283416,"id":"rJm5I9kJM","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"BJzbrHC0-","signatures":["~Hongyi_Zhang1"],"readers":["everyone"],"writers":["~Hongyi_Zhang1"],"content":{"title":"Thanks (and additional comments)","comment":"Thanks for your clarification, it's very helpful.\n\nI think it is good to explicitly compare the data augmentation used by different methods in Table 2, so that the interested readers don't assume they all use the same augmentation, or don't have to look up each paper to figure out what augmentation each method used. For example, AFAIK, the Ladder Networks paper (Table 3, https://arxiv.org/pdf/1507.02672.pdf) reported results without data augmentation."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510085610893,"tcdate":1510085610893,"number":8,"cdate":1510085610893,"id":"r1mlEqyyz","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Source of semi-supervised learning gains","comment":"I'm impressed by your semi-supervised learning results. However, without an ablation study it's hard to tell why your method works so well. Do you have any ideas about what's causing the improvement? It could be\n(1) You use both a GAN and consistency regularization (prior work uses one or the other).\n(2) Your GAN works better.\n(3) Your consistency regularization is better (either because dropout is better than Gaussian noise or because the second-to-last layer consistency term helps).\n(4) Improvements to the architecture/hyperparameters (e.g., using weight-norm instead of batch-norm as you mention in the appendix).\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509999866432,"tcdate":1509999866432,"number":7,"cdate":1509999866432,"id":"BJzbrHC0-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"Syb85wYR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for your concern.","comment":"Dear Hongyi,\n\nSorry for the late response. We did not receive or had missed the notification from Openreview about your comment. Following (Laine & Aila, 2016, Miyato et al., 2017, Tarvainen & Valpola, 2017), we do not apply any augmentation to MNIST and yet augment the CIFAR10 images in the following way. We flip the images horizontally and randomly translate the images within [-2,2] pixels horizontally. \n\nSamuli Laine and Timo Aila.  Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.\nAntti Tarvainen and Harri Valpola.  Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509681736930,"tcdate":1509681736930,"number":6,"cdate":1509681736930,"id":"Syb85wYR-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["~Hongyi_Zhang1"],"readers":["everyone"],"writers":["~Hongyi_Zhang1"],"content":{"title":"Any data augmentation?","comment":"In Figure 2, there is an \"augmentation\" process before feeding the input x into the network D. Could you clarify what \"augmentation\" means here? In particular, what kind of data preprocessing did you use in the semi-supervised learning experiments?\n\nThanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509410645475,"tcdate":1509410595464,"number":5,"cdate":1509410595464,"id":"r1s7vBrCb","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"Hk9rHlS0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for the clarification","comment":"Thanks for the clarification. Comparing with DRAGAN in your experiments would have helped to understand where the benefit is coming from. They show improvements over WGAN-GP as well but use only DCGAN architecture. \n\nGood luck!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509397858477,"tcdate":1509397858477,"number":4,"cdate":1509397858477,"id":"B1cwHGBA-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"BkWTPbHCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for your reply.","comment":"Now I can understand the experimental details. Thanks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092427332,"tcdate":1509394361112,"number":3,"cdate":1509394361112,"id":"BkWTPbHCW","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"HJ-LeemC-","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"More experiment details","comment":"Thank you for the interest in our work. Please see below for the answers to your questions. \n\n(1) Dropout ratio: \n\n## The network used to learn from 1000 labeled CIFAR10 images only\nDiscriminator                                                                             Generator\nInput: 3*32*32 Image x                                                            Input: Noise z 128\n5*5 conv. 128, Pad = same, Stride = 2, lReLU                        MLP 8192, ReLU, Batch norm\n0.5 Dropout                                                                               Reshape     512*4*4\n5*5 conv. 256, Pad = same, Stride = 2, lReLU                       5*5 deconv. 256*8*8,\n0.5 Dropout                                                                              ReLU, Bach norm\n5*5 conv. 512, Pad = same Stride = 2, lReLU                       5*5 deconv. 128*16*16\n0.5 Dropout                                                                              ReLU, Batch norm\nReshape 512*4*4      (D_)                                                        5*5 deconv. 3*32*32\nMLP 1                          (D)                                                         Tanh\n\n## ResNet:\nDiscriminator                                                           |                Generator\nInput: 3*32*32 Image x                                          |                Input: Noise z 128\n[3*3]*2 Residual Block, Resample = DOWN        |                MLP 2048\n128*16*16                                                               |                Reshape 128*4*4\n[3*3]*2 Residual Block, Resample = DOWN        |                [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.2 Dropout                                             |                 128*8*8\n[3*3]*2 Residual Block, Resample = None           |                 [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.5 Dropout                                             |                 128*16*16\n[3*3]*2 Residual Block, Resample = None           |                 [3*3]*2 Residual Block, Resample = UP\n128*8*8 0.5 Dropout                                             |                 128*32*32\nReLU, Global mean pool    (D_)                              |                 3*3 conv.  3*32*32\nMLP 1                                  (D)                                |                 Tanh\n\n## The network for MNIST\nDiscriminator                                                                               Generator\nInput: 1*28*28 Image x                                                              Input: Noise z 128\n5*5 conv. 64, Pad = same, Stride = 2, lReLU                            MLP 4096, ReLU\n0.5 Dropout                                                                                 Reshape 256*4*4\n5*5 conv. 128, Pad = same, Stride = 2, lReLU                         5*5 deconv. 128*8*8\n0.5 Dropout                                                                                 ReLU, Cut 128*7*7\n5*5 conv. 256, Pad = same, Stride = 2, lReLU                         5*5 deconv. 64*14*14\n0.5 Dropout                                                                                 ReLU\nReshape 256*4*4       (D_)                                                          5*5 deconv. 1*28*28\nMLP 1                         (D)                                                             Sigmoid\n\n(2) No. There are only two perturbations, as denoted by x' and x’’, for a data point x in each iteration. They are independently generated by the dropout as shown in my answer to your question (1). In other words, the two terms equation (5) are actually calculated over the same pair of x' and x'' for each draw x ~ P_r.\n\nWe will try to release the code in one or two weeks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092427376,"tcdate":1509389716127,"number":2,"cdate":1509389716127,"id":"BJhcHerRb","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"HkMNKONR-","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Thank you and we will correct it in the updated version","comment":"Sorry about that and Thank you for noting it! We will correct it in the updated version. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092427414,"tcdate":1509389634003,"number":1,"cdate":1509389634003,"id":"Hk9rHlS0-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Official_Comment","forum":"SJx9GQb0-","replyto":"ry2iCPQRb","signatures":["ICLR.cc/2018/Conference/Paper1144/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1144/Authors"],"content":{"title":"Thank you for directing us to DRAGAN","comment":"Thank you for directing us to DRAGAN. Sorry for missing it in our paper. We will include it in the updated version.\n\nGoing back to your question, the short answer is no because we do not actually aim to smooth the discriminator though the approach may have that effect. The long answer below clarifies it further and additionally highlights some differences between ours and DRAGAN.\n\nMotivations: DRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs by drawing results from the minimax theorem for zero-sum game. In sharp contrast, we propose an alternative way of enforcing the 1-Lipschitz continuity over the “critic” of WGANs thanks to the recent results by Arjovsky & Bottou (2017). \n\nHow to add the perturbations: One of the key observations in our experiments is that it reduces the quality of the generated samples if we add noise directly to the data points, as what is done in DRAGAN. Similar observations are reported by Arjovsky & Bottou (2017) and Wu et al. (2016). After many painstaking trials, we find good results by perturbing the hidden layers of the discriminator instead (as opposed to perturbing the original data). Besides, DRAGAN perturbs a data point once while we do it twice in an iteration. \n\nHow to use the perturbation: Similar to the gradient penalty proposed in (Gulrajani et al., 2017), DRGAN introduces a same regularization whereas for different reasons. In contrast, ours is a consistent regularization derived from the basic definition of Lipschitz continuous functions. \n\nSemi-supervised learning: One of the most notable features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila (2016) with GANs. \n\nFinally, here is the DRAGAN paper we found on ArXiv: https://arxiv.org/abs/1705.07215 just to confirm it with you. Going back to the DRGAN work, it would be interesting to investigate whether it generates blurry images too, for example by comparing the results of different amount of noise including no noise. It may do not because it constraints the gradient as oppose to the discriminator’s output. \n \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509357866515,"tcdate":1509357866515,"number":3,"cdate":1509357866515,"id":"HkMNKONR-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["~Antti_Tarvainen1"],"readers":["everyone"],"writers":["~Antti_Tarvainen1"],"content":{"title":"Citation typo","comment":"Looks like an interesting paper!\n\nI noticed you accidentally cited Salimans et al. on the fourth row of Table 2 when you (probably) meant to cite our work: https://arxiv.org/abs/1703.01780"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509290271795,"tcdate":1509289636439,"number":2,"cdate":1509289636439,"id":"ry2iCPQRb","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Smoothing the discriminator near data manifold using local perturbations sounds familiar ","comment":"Your contribution looks like a relaxed version of DRAGAN's regularization scheme, which you don't cite anywhere. Is that correct? \n\nKodali, N., Abernethy, J., Hays, J. and Kira, Z., 2017. How to Train Your DRAGAN. arXiv preprint arXiv:1705.07215."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509257321034,"tcdate":1509257289032,"number":1,"cdate":1509257289032,"id":"HJ-LeemC-","invitation":"ICLR.cc/2018/Conference/-/Paper1144/Public_Comment","forum":"SJx9GQb0-","replyto":"SJx9GQb0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Missing Experiment Parameters","comment":"I appreciate your contribution to generative model research. The results seem to be great.\nI was trying to reproduce the paper's result, but I think it is difficult to get some details:\n(1) Dropout ratio for generative models is not specified. It might be better to have Tables like Tables 4 and 5 for generative modeling tasks.\n(2) I could not understand the meaning of \"We find that it slightly improves the performance by further controlling the second-to-last layer D_(.) of the discriminator.\" Are we generating two more perturbed points x''' and x'''' by inserting a dropout layer at second-to-last layer -- as opposed to perturbed points x' and x''?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515124237367,"tcdate":1509139082615,"number":1144,"cdate":1510092359481,"id":"SJx9GQb0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJx9GQb0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Improving the Improved Training of Wasserstein GANs","abstract":"Despite being impactful on a  variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, namely, Wasserstein GAN (WGAN) hinges on the 1-Lipschitz continuity of the discriminators. In this paper, we propose a novel approach for enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning approaches. As  a result, it gives rise to not only better photo-realistic samples  than the previous methods  but also state-of-the-art semi-supervised learning results.  In particular, to the best of our knowledge, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR10 images and is the first that exceeds the accuracy of 90\\% the CIFAR10 datasets using only 4,000 labeled images.\n","pdf":"/pdf/f91940db8043f84929503b774f6dac1be4f9beb1.pdf","paperhash":"anonymous|improving_the_improved_training_of_wasserstein_gans","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving the Improved Training of Wasserstein GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJx9GQb0-}\n}","keywords":["GAN","WGAN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1144/Authors"]},"nonreaders":[],"replyCount":30,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}