{"notes":[{"tddate":null,"ddate":null,"tmdate":1515205959628,"tcdate":1515157440843,"number":4,"cdate":1515157440843,"id":"ByFTDgp7G","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Comment","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper944/Authors"],"content":{"title":"Summary of the updates","comment":"We summarize the updates made in the revision below:\n\nMain updates:\n- To address the comment from AnonReviewer 3 that the base networks are too small, we replaced the experimental results on CIFAR-100 dataset with results from a deeper network, that is a slight modification of AlexNet (8 layers: 5 Conv and 3 FC). Our algorithm achieved significant performance gain over the baseline models on this dataset as well.\n\n- Following the suggestion from AnonReviewer 2, we performed new experiments on the Permuted MNIST dataset with feedforward networks, and included the experimental results in the Appendix Section. The results show DEN achieves comparable performance to batch models (STL and MTL), while significantly outperforming both DNN-EWC and DNN-Progressive.\n\nPage 2: Updated the value of %p to reflect the updates in the CIFAR-100 experimental results. \nPage 7: 1) Corrected the typo: 3) DNN. Dase → 3) DNN. Base\n              2) Replaced the description of LeNet with the description of the deeper CNN used in the new experiments.\nPage 8: Updated the CIFAR-100 plot in Figure 3 with the results from the modified AlexNet.\nPages 7-8, 10: Updated the value of %p to reflect the updates in the CIFAR-100 results.\nPage 11: Included the Appendix section, which contains the results and discussions of the Permuted MNIST experiments.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1515160135339,"tcdate":1514268294777,"number":3,"cdate":1514268294777,"id":"HJJ5UP1XG","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Comment","forum":"Sk7KsfW0-","replyto":"SJAGR15lz","signatures":["ICLR.cc/2018/Conference/Paper944/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper944/Authors"],"content":{"title":"Answers to questions and the results on permuted MNIST dataset.","comment":"Q1. Did the authors also check the performance on the permuted MNIST benchmark, as studied by Kirkpatrick et al. and Zenke et al.? It would be important to see how the method fares in this setting, where the tasks are the same, but the inputs have to be remapped, and network expansion is less of an issue.\n\nA.  Following your suggestion, we have also experimented on the permuted MNIST dataset using feedforward network with 2 hidden layers and included the results in the Appendix section (See Figure 6). As expected, DEN achieves performance comparable to batch models such as STL or MTL, significantly outperforming both DNN-EWC and DNN-Progressive while obtaining a network that has significantly less number of parameters.  \n\nQ2. Fig. 4 would be clearer if the authors showed also the performance and how much the selected connection subsets would change if instead of using the last layer lasso + BFS, the full L1-penalized problem was solved, while keeping the rest of the pipeline intact.\n\nA.   The suggested comparative study is already done in Fig. 4. DNN-L1 shows the results using the full L1-penalized regularizer instead of the last layer lasso + BFS. This result shows that selective retraining is indeed useful in reducing time complexity of training and perform selective knowledge transfer to obtain better accuracy.  \n\n     For better understanding of the BFS process, we updated the figure that illustrates the selective retraining process to include arrows (Leftmost figure of Figure 2).\n \nQ3. Still regarding the proposed selective retraining, the special role played by the last hidden layer seems slightly arbitrary. It may well be that it has the highest task-specificity, though this is not trivial to me. This special role might become problematic when dealing with deeper networks.\n\nA. The last hidden layer is not the only layer that is learned to be task-specific, as the BFS process selects units that are useful for the given task at all layers of the network and retrains them. \n\n     To show that selective retraining does not become problematic with deeper networks, we performed additional experiments on the CIFAR-100 dataset with a 8-layer network which is a slight modification of AlexNet. The results show that DEN obtains similar performance gain over baselines even with this deeper network. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1515160166336,"tcdate":1514268127580,"number":2,"cdate":1514268127580,"id":"BkOyLPJXG","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Comment","forum":"Sk7KsfW0-","replyto":"SkcwXXqxM","signatures":["ICLR.cc/2018/Conference/Paper944/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper944/Authors"],"content":{"title":"Answers to the questions","comment":"Q1. What does the \"batch model\" refer to?\nA.  “Batch model” refers to models that are not trained in an incremental manner; in other words, a batch model is trained with all tasks at hand, such as DNN-MTL or DNN-STL. \n\nQ2. re. \" 11.9%p − 51.8%p\"; remove \"p\"?\nA. %p stands for percent point and is a more accurate way of denoting absolute performance improvements compared to %.\n\nQ3. Reference for CIFAR-100? Explain abbreviation for both CIFAR-100 and AWA-Class?\nA.   Thank you for the suggestion. We updated the reference for the CIFAR-100 dataset and included the full dataset name for AWA in the revision. CIFAR is simply a dataset  \n \n \nQ4. re. \"... but when the number of tasks is large, STL works better since it has larger learning capacity than MTL\": isn't the number of parameters matched? If so, why is the \"learning capacity\" different? What do the authors mean exactly by \"learning capacity\"?\nA.   By “learning capacity”, we are referring to the number of parameters in a network. DNN-MTL learns only a single network for all T tasks whereas DNN-STL learns T networks for T tasks. For the experiments that generated the plots in the top row of Figure 3, we used the same network size for both DNN-STL and DNN-MTL, and therefore, DNN-STL used T times more parameters than DNN-MTL. For accuracy / network capacity experiments in the bottom row, we diversified the base network capacity for both baselines.\n \nQ5. \"DNN: dase (sic) DNN\": how is this trained?\nA.   Thank you for pointing out the typo. We have corrected it in the revision.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1515160224421,"tcdate":1514267704561,"number":1,"cdate":1514267704561,"id":"ryZHNvkmz","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Comment","forum":"Sk7KsfW0-","replyto":"HJnaoMagf","signatures":["ICLR.cc/2018/Conference/Paper944/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper944/Authors"],"content":{"title":"Description of breath-first search and the experimental results with deeper networks","comment":"Q1. One question would be how sparse training is done, and how this saves computation, especially with the breadth-first search described on page 5. \n\nA. Sparse training is done at both initial network training (Eq. (2)) and selective retraining step (Eq. (3)), using L1 regularizer. First, the initial network training obtains a network with sparse connectivity between neurons at consecutive layers. \n\nThen, the selective retraining selects the neurons at the layer just before the output neurons of this sparse network, and then using the topmost layer neurons as starting vertices, it selects neurons at each layer that have connections to the selected upper-layer neurons (See the leftmost model illustration of Figure 2). \n\nThis results in obtaining a subnetwork of the original network that has much less number of parameters (Figure 4.(b)) that can be trained with significantly less training time (Figure 4.(a)). The selected subnetwork also obtains substantially higher accuracy since it leverages only the relevant parts of the network for the given task (Figure 4.(a)).\n\nQ2. A critique would be that the base networks (a two layer FF net and LeNet) are not very compelling.\n\nA. To show that our algorithm obtains performance improvements on any generic networks, we experimented with a larger network that consists of 8 layers, which is a slight modification of AlexNet. With this larger network, our algorithm achieved similar performance gain over the baseline models as in the original experiments. We included the new results in the revision.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1515642534074,"tcdate":1512020932461,"number":3,"cdate":1512020932461,"id":"HJnaoMagf","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Review","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Method for lifelong learning with neural networks","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors propose a method (Dynamically Expandable Network) that addresses issues of training efficiency, how to dynamically grow the network, and how to prevent catastrophic forgetting.\n\nThe paper is well written with a clear problem statement and description of the method for preventing each of the described issues. Interesting points include the use of an L1 regularization term to enforce sparsity in the weights, as well as the method for identifying which neurons have “drifted” too far and should be split. The use of timestamps is a clever addition as well.\n\nOne question would be how sparse training is done, and how this saves computation, especially with the breadth-first search described on page 5. A critique would be that the base networks (a two layer FF net and LeNet) are not very compelling.\n\nExperiments indicate that the method works well, with a clear improvement over progressive networks. Thus, though there isn’t one particular facet of the paper that leaps out, overall the method and results seem solid and worthy of publication.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1515642534112,"tcdate":1511826273945,"number":2,"cdate":1511826273945,"id":"SkcwXXqxM","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Review","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Investigation of DENs and variants applied to lifelong learning","rating":"6: Marginally above acceptance threshold","review":"The topic is of great interest to the community, and the ideas explored by the authors are reasonable, but I found the conclusion less-than-clear. Mainly, I was not sure how to interpret the experimental findings, and did not have a clear picture of the various models being investigated (e.g. \"base DNN regularized with l2\"), or even of the criteria being examined. What is \"learning capacity\"? (If it's number of model parameters, the authors should just say, \"number of parameters\"). The relative performance of the different models examined, plotted in the top row of Figure 3, is quite different, and though the authors do devote a paragraph to interpreting the results, I found it slightly hard to follow, and was not sure what the bottom line was.\n\nWhat does the \"batch model\" refer to?\n\nre. \" 11.9%p − 51.8%p\"; remove \"p\"?\n\nReference for CIFAR-100? Explain abbreviation for both CIFAR-100 and AWA-Class?\n\nre. \"... but when the number of tasks is large, STL works better since it has larger learning capacity than MTL\": isn't the number of parameters matched? If so, why is the \"learning capacity\" different? What do the authors mean exactly by \"learning capacity\"?\n\nre. Figure 3, e.g. \"Average per-task performance of the models over number of task t\": this is a general point, but usually the expression \"<f(x)> vs. <x>\" is used rather than \"<f(x)> over <x>\" when describing a plot.\n\n\"DNN: dase (sic) DNN\": how is this trained?\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1515642534149,"tcdate":1511812629737,"number":1,"cdate":1511812629737,"id":"SJAGR15lz","invitation":"ICLR.cc/2018/Conference/-/Paper944/Official_Review","forum":"Sk7KsfW0-","replyto":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference/Paper944/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach to continual learning","rating":"7: Good paper, accept","review":"The paper was clearly written and pleasant to read. I liked the use of sparsity- and group-sparsity-promoting regularizers to select connections and decide how to expand the network.\n\nA strength of the paper is that the proposed algorithm is interesting and intuitive, even if relatively complex, as it requires chaining a sequence of sub-algorithms. It was good to see the impact of each sub-algorithm studied separately (to some degree) in the experimental section. The results are overall strong.\n\nIt’s hard for me to judge the novelty of the approach though, as I’m not an expert on this topic.\n\nJust a few points below:\n- The experiments focus on a relevant continual learning problem, where each new task corresponds to learning a new class. In this setup, the method consistently outperforms EWC (e.g., Fig. 3), as well as the progressive network baseline.\nDid the authors also check the performance on the permuted MNIST benchmark, as studied by Kirkpatrick et al. and Zenke et al.? It would be important to see how the method fares in this setting, where the tasks are the same, but the inputs have to be remapped, and network expansion is less of an issue.\n\n- Fig. 4 would be clearer if the authors showed also the performance and how much the selected connection subsets would change if instead of using the last layer lasso + BFS, the full L1-penalized problem was solved, while keeping the rest of the pipeline intact.\n\n- Still regarding the proposed selective retraining, the special role played by the last hidden layer seems slightly arbitrary. It may well be that it has the highest task-specificity, though this is not trivial to me. This special role might become problematic when dealing with deeper networks.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]}},{"tddate":null,"ddate":null,"tmdate":1514618125551,"tcdate":1509137280893,"number":944,"cdate":1510092362115,"id":"Sk7KsfW0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk7KsfW0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Lifelong Learning with Dynamically Expandable Networks","abstract":"We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters. ","pdf":"/pdf/69498cecf34f7b48d42f236b2b336179b330a5ce.pdf","TL;DR":"We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.","paperhash":"anonymous|lifelong_learning_with_dynamically_expandable_networks","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Learning with Dynamically Expandable Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk7KsfW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper944/Authors"],"keywords":["Transfer learning","Lifelong learning","Selective retraining","Dynamic network expansion"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}