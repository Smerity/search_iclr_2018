{"notes":[{"tddate":null,"ddate":null,"tmdate":1513850111534,"tcdate":1513850111534,"number":4,"cdate":1513850111534,"id":"HywWHWKGM","invitation":"ICLR.cc/2018/Conference/-/Paper566/Official_Comment","forum":"H1-nGgWC-","replyto":"Hk4JEb5eM","signatures":["ICLR.cc/2018/Conference/Paper566/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper566/Authors"],"content":{"title":"AnonReviewer1 author response","comment":"Thank you for your detailed and thought provoking review.  We will acknowledge your anonymous contribution in the final version of the paper.  \n\n--On the deep kernel learning papers of Wilson et al and Al-Shedivat et al:\n\nWe agree that this deep kernel literature is useful and relevant in this context. We are sure you would agree that is not the only promising approach. In Section 6 we did point out that the “emergent kernels in our case are hyperparameter free” and that “any Gaussian process with a fixed kernel does not use a learnt hierarchical representation”. Therefore we respectfully disagree with your assessment that we “erroneously attributed” this behaviour to GP methods with a learnt kernel. Nevertheless, we agree that the paper would be clearer with more discussion of learnt representations and we have added additional material to Section 6 along with the citations you kindly suggested.\n\n--On significance and practical value:\n\nWe agree that, in your words: “this infinite limit loses much of the interesting representation in neural networks because the variance of the weights goes to zero.” Indeed, the careful extension of the mathematics underlying this intuition to networks with more than one hidden layer is part of the contribution of our paper. We view the cautionary message of the paper as one of its key scientific contributions. Furthermore, Neal's original 1996 work \"suffers\" from the same issue yet has become extremely influential and led to many invaluable insights. Our analysis moves the careful study of random networks beyond what was known. This requires considerable technical insight. The theoretical assumptions we make are less restrictive than for instance Daniely et al. (2016), which was (correctly in our opinion) regarded as impactful at that NIPS. \n\nAlthough, as we acknowledge, it is difficult to do exhaustive experiments in the fully Bayesian regime, our experiments with the base network architecture of Hernandez-Lobato and Adams (2015) suggest that the Gaussian process limit is relevant to wide finite Bayesian neural networks in the regime studied. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513849947988,"tcdate":1513849947988,"number":3,"cdate":1513849947988,"id":"Sy4PN-KGz","invitation":"ICLR.cc/2018/Conference/-/Paper566/Official_Comment","forum":"H1-nGgWC-","replyto":"BykFw7cxz","signatures":["ICLR.cc/2018/Conference/Paper566/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper566/Authors"],"content":{"title":"AnonReviewer3 author response","comment":"Thank you for your review which raises some important questions. We will endeavour here to answer them more clearly. We will acknowledge your anonymous contribution in the final version of the paper. \n\nIf you will excuse us we will start with your last question first since it relates to your criticism of the significance of the work. \n\nFrom your review: Page 4: ` \"This is because for finite H the input activations do not have a multivariate normal distribution\".  Can you elaborate on this? Since we are interested in the infinite limit, why is this a problem?'\n\nThis is an important point. There is a general answer and a more specific answer: \n    1) In general, weak convergence is exactly that - many general manipulations that we might want to perform with it don't actually hold. For instance if the sequence of distributions ( a_n ) converges weakly/in-distribution to a and the sequence of distributions ( b_n ) converges weakly/in-distribution to b then the sequence of independent product distributions (a_n,b_n) doesn't necessarily converge weakly/in-distribution to (a,b). See Billingsley 1999 page 23. Care and rigour is required in this domain.\n    2) More specifically to this example, the rate at which the convergence of the activations occurs could have a ``knock on effect'' on the convergence of the activation distributions further through the network. We've added a comment about this second point to the main text just after the sentence you quote.\n\nAs an example of point 2). Suppose that the sequence of distributions (P_n) converges in distribution to some P_*. Consider the limit of a sequence of expectations (\\int \\psi_n d P_n ) where the integrand is also changing. This will not in general be the same as if we first substitute the limit measure (\\int \\psi_n d P_*) and then take the n limit of the new integral. The rate of convergence will in general matter. \n\nFrom your review: \"The result itself does not feel very novel because variants of it were already available.\" \n\nWe have already argued that improving rigorous results in this area is very desirable. Therefore we must respectfully disagree. To the best of our knowledge there are no rigorous results about convergence in this area since Neal (1996).\n \nIt is fair to point out that our empirical analysis does not extend to high dimensional functions- thank you. We've updated the discussion to reflect this. Note that the content of Theorem 1 does not depend on the dimensionality of the inputs.\n\nAlso from your review: \"Are you sure this is discussed in Section 3?\"\n\nYou are correct - we do not allude to this. Thank you for pointing this out. This is an orphaned cross reference to some material that did not make the cut because it is orthogonal to the main thrust of the paper. Essentially, carefully scaling the weight variances can help mitigate the onset of the depth pathologies discussed in Duvenaud et al (2014). We apologize and have now removed this. The exact code we used is available in our anonymous repository."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513849686245,"tcdate":1513849686245,"number":2,"cdate":1513849686245,"id":"Hk08QWFff","invitation":"ICLR.cc/2018/Conference/-/Paper566/Official_Comment","forum":"H1-nGgWC-","replyto":"rJVpI8hgz","signatures":["ICLR.cc/2018/Conference/Paper566/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper566/Authors"],"content":{"title":"AnonReviewer2 author response","comment":"We thank the reviewer for their careful reading of the paper. We will acknowledge your anonymous contribution in the final version of the paper. \n\nRegarding the technical query for the proof of Lemma 2, we now have slightly rearranged the material to make clear what constitutes a “sufficiently large H_\\mu for the bound to hold, and to show that this is consistent with the growth rates in the statement of Lemma 2. In fact, we require a rate which grows faster than 2^{n H_\\mu}, for all n to deal with this, and so have adjusted the stated growth rates in Lemma 2 to H_{\\mu-1} = O(2^{H_{\\mu}^2}).\n\nRegarding Lemma 4, the bound is actually independent of H_{\\mu-2}. This is because \\tilde g^{\\mu - 1} is a deterministic transformation of Z^{\\mu - 1} with the known n-dimensional normal distribution from Lemma~1, independent of H_{\\mu-2}. The original proof incorrectly bounded the norm of \\tilde g^{\\mu - 1} in terms of \\tilde f^{\\mu - 1} instead of Z^{\\mu - 1} which we noticed thanks to your comment.\n\nThese details have now been incorporated into the relevant sections of the appendix. We emphasise that the conclusion of Theorem 1 in the main paper remains unchanged, and thank the reviewer once again for their close attention to the details of the proof.\n\nRegarding notational issues:\nBottom of p3 - we have modified the notation to be consistent with the rest of the paper.\nEq (9) - we have now defined the distributions in the text.\nx^{(i)} notation - we now use notation of the form x[i] to refer to the different input points to the neural network.\nTheorem 1 - we have made the suggested change of wording, and added the phrase “strictly increasing” to emphasise that the convergence happens as layer widths go to infinity.\nRecursive kernel comment: Cho & Saul (2009) indeed solve the~integral recursion of Hazan and Jaakkola (2015); we have provided more details to make the link clearer.\n\n\nMinor mistakes:\nThank you for spotting these, we have made the relevant changes in the revised version of the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642470855,"tcdate":1511970492438,"number":3,"cdate":1511970492438,"id":"rJVpI8hgz","invitation":"ICLR.cc/2018/Conference/-/Paper566/Official_Review","forum":"H1-nGgWC-","replyto":"H1-nGgWC-","signatures":["ICLR.cc/2018/Conference/Paper566/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper focuses on proving and discussing properties of wide deep neural networks, and more particularly of their behaviour when priors on the weights are assumed.","rating":"6: Marginally above acceptance threshold","review":"In part 1, the authors introduce motivation for studying wide neural networks and summarize related work. \nIn part 2,  they present a theorem (main theoretical result) stating that under conditions on the weight priors, the output function of a multi-layer neural network (conditionally to a given input) weakly converges to a gaussian process as the size of the hidden layers go to infinity.\nremark on theorem 1: This result generalizes a result proven in 2015 stating that the normality of a layer propagates to the next as the size of the first layer goes to infinity. The result stated in this paper is proven by bounding the gap between the output distribution and the corresponding gaussian process, and by propagating this bound across layers (appendix). \nIn part 3, the authors discuss the choice of a nonlinearity function that enables easy computation of the kernels introduced in the covariance matrix of the limit normal distribution. Their choice lands on ReLU.\nIn part 4, the focus is on the speed of the convergence presented in theorem 1. Experiments are conducted to show how the distance (maximum mean disrepancy) between the output distribution and its theoretical gaussian process limit vary when the sizes of the hidden layers increase. The results show that the convergence (in MMD) happens consistently, although it is slower when the number of hidden layers gets bigger.\nIn part 5, the authors compare the distributions (finite Bayesian deep networks and their analogues Gaussian processes) in yet another way: by studying their agreement in terms of inference. For this purpose, the authors chose several crieteria: the first two moments of the posterior, the log marginal likelihood and the predictive log-likelihood. The authors judge that the distributions agree on those criteria, but do not provide further analysis.\nIn part 6, now that It has been shown that the output distributions of Bayesian neural nets do not only weakly converge to Gaussian processes but also behave similarly in terms of inference, the authors discuss ways to avoid the gaussian process behaviour. Indeed, it seems that Gaussian processes with a fixed kernel cannot learn hierarchical representations, that are essential in deep learning.\nThe idea to avoid the Gaussian process behaviour is to contradict one of the hypothesis of the CLT (so that it does not hold anymore), either by controlling the size of intermediate layers, by using networks with infinite variance in the activities, or by choosing non-independent weights.\nIn part 7, it is concluded that the result that has been proven for size of layers going to infinity (Theorem 1) seems to empirically be verified on finite networks similar to those used in the literature. This can be used to simplify inference in cases were the gaussian process behaviour is desired, and opens questions on how to avoid this behaviour the rest of the time.\n\nPros: The authors line of thought of the authors is overall quite easy to follow. The main theoretical convergence result is stated early on, and the remaining of the article is dedicated to observing this result empirically from different angles (MMD, inference, predictive capability..). The last part contains a discussion concerning the extent to which it is actually a desired or a undesired result in classical deep learning use-cases, and the authors provide intuitive conditions under which the convergence would not hold. The stated theorem is a clear improvement on the past literature and is promising in a context where multi-layers neural networks are more and more studied.\nFinally, the work is well documented.\n\nCons: \nI have a some concerns with the main result (Theorem 1) and found that some of the notations / formulas were not very clear.\n Concerns with Theorem 1:\n* at the end of the proof of Lemma 2, H_\\mu is to be chosen large enough in order to get the \\epsilon bound of the statement. However, I think that  H_\\mu is constrained by the statement of Proposition 2, not to be larger than a constant times 2^(H_{\\mu+1}). Isn't that a problem?\n* In the proof of Lemma 4, it looks like matrix \\Psi, from the schur decomposition of \\tilde f, actually depends on H_{\\mu-2}, thus making \\psi_max depend on it too, as well as the final \\beta bound, which would contradict the statement that it depends only on n and H_{\\mu}. Could you please double check?\n\nUnclear statements/notations:\n* end of page 3, notations are not entirely consist with previous notations\n* I do not understand which distribution is assumed on epsilon and gamma when taking the expectancy in equation (9).\n* the notation x^(i) (in the theorem and the proof notably) could be changed, for the ^(i) index refers to the depth of the layer in the rest of the notations, and is here surprisingly referring to a set of observations.\n* the statement of Theorem 1:\n    * I would change \"for a countable input set\" to \"for any countable input set\", if this holds true.\n    * does not say that the width has to go to infinity for the convergence to happen, which goes a bit in contradiction with the adjective \"wide\". However, the authors say that in practice, they use the identity as width function.\n* I understood that the conclusion of part 3 was that the expectation of eq (9) was elegantly computable for certain non-linearity (including ReLU). However I don't see the link with the \"recursive kernel\" idea (maybe it's just the way to do the computation described in Cho&Saul(2009) ?)\n\nSome places where it appears that there are minor mistakes:\n* 7th line from the bottom of page 3, the vector f^{(2)}(x) contains f_i^{(1)}(x) but should contain f_i^{(2)}(x)\n* last display of page 3: change x and x', and indicate upper limit of the sum\n* please double check variances C_w and/or \\hat{C}_w appearing in equations in (9) and (13).\n* line 2 of second paragraph after equations (8) and (9). The authors refer to equation (8) concerning the independence of the components of the output. I think they rather wanted to refer to (9). Same for first sentence before eq (14).\n* middle of page 12: matrix LY should be RY.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642470893,"tcdate":1511827319350,"number":2,"cdate":1511827319350,"id":"BykFw7cxz","invitation":"ICLR.cc/2018/Conference/-/Paper566/Official_Review","forum":"H1-nGgWC-","replyto":"H1-nGgWC-","signatures":["ICLR.cc/2018/Conference/Paper566/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Adds to the theoretical understanding of the deep wide regime in NNs, no clear application","rating":"6: Marginally above acceptance threshold","review":"- Summary\n\nThe paper is well written and proves how deep, wide, fully connected NNs are equivalent to GPs in the limit. This result, which was well known for single-layer NNs, is now extended to the multilayer case. Although there was already previous work suggesting GP this behavior, there was no formal proof under the specific conditions presented here.\n\nThe convergence to a GP is also verified experimentally on some toy examples.\n\n\n- Relevance\n\nThe result itself does not feel very novel because variants of it were already available.\n\nUnfortunately, although making other researchers aware of this is worthy, the application of this result seems limited, since in fact it describes and lets us know more about a regime that we would rather avoid, rather than one we want to exploit. Most of the applications of deep learning benefit from strong structured priors that cannot be represented as a GP. This is properly acknowledged in the paper.\n\nThe lack of practical relevance combined with the not-groundbreaking novelty of the result makes this paper less appealing.\n\n\n- Other comments\n\nPage 6: \"It does mean however that our empirical study does not extend to larger datasets where such inference is prohibitively expensive (...) prior dominated problems are generally regarded as an area of strength for Bayesian approaches and in this context our results are directly relevant.\"\n\nAlthough that argument can hold for datasets that are large in terms of amount of data points, it doesn't for datasets that are large in terms of number of dimensions. The empirical study could have used very high-dimensional datasets with comparatively low amounts of training data. That would maintain a regime were the prior does matter but and better show the generality of the results.\n\nPage 6: \"We use rectified linear units and correct the variances to avoid a loss of prior variance as depth is increased as discussed in Section 3\" \n\nAre you sure this is discussed in Section 3?\n\nPage 4: \"This is because for finite H the input activations do not have a multivariate normal distribution\". \n\nCan you elaborate on this? Since we are interested in the infinite limit, why is this a problem?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642470933,"tcdate":1511818204585,"number":1,"cdate":1511818204585,"id":"Hk4JEb5eM","invitation":"ICLR.cc/2018/Conference/-/Paper566/Official_Review","forum":"H1-nGgWC-","replyto":"H1-nGgWC-","signatures":["ICLR.cc/2018/Conference/Paper566/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A nicely written paper for the most part, but practical value is unclear","rating":"6: Marginally above acceptance threshold","review":"The authors study the limiting behaviour for wide Bayesian neural networks, comparing to Gaussian processes. \n\nThe paper is well written, and the experiments are enlightening. This work is a nice follow up to Neal (1994), and recent work considering similar results for neural networks with more than one hidden layer. It does add to our understanding of this body of work.\n\nThe weakness of this paper is in its significance and practical value. This infinite limit loses much of the interesting representation in neural networks because the variance of the weights goes to zero. Thus it’s unclear whether these formulations will have many of the benefits of standard neural networks, and whether they’re particularly related to standard neural networks at all. There also don’t seem to be many practical takeaways from the experiments, and the experiments themselves do not consider any predictive tasks at all. It would be nice to see some practical benefit for a predictive task actually demonstrated in the paper. I am not sure what exactly I would do differently in training large neural networks based on the results of this paper, and the possible takeaways are not tested here on real applications.\n\nThis paper also seems to erroneously attribute this limitation of the Neal (1994) limit, and its multilayer extensions, to Gaussian processes in the section “avoiding Gaussian process behaviour”. The problems with that construction are not a profound limitation of Gaussian processes in general. If we can learn the kernel function, then we can learn an interesting representation that does not have these limitations and still use a GP. We could alternatively treat the kernel parameters probabilistically, but the fact that in this case we would not marginally have a GP any longer is mostly incidental. The discussed limitations are more about specific kernel choices, and lack of kernel learning, than about “GP behaviour”.\n\nIndeed, while the discussion of related work is mostly commendable, the authors should also discuss the recent papers on “deep kernel learning”:\ni) http://proceedings.mlr.press/v51/wilson16.pdf\nii) https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf\niii) http://www.jmlr.org/papers/volume18/16-498/16-498.pdf\n\nIn particular, these papers do indeed learn flexible representations with Gaussian processes by using kernels constructed with neural networks. They avoid the behaviour discussed in the last section of your paper, but still use a Gaussian process. The network structures themselves are trained through the marginal likelihood of the Gaussian process. This approach effectively learns an infinite number of adaptive basis functions, parametrized through the structural properties of a neural network. Computations are made scalable and practical through exploiting algebraic structure. \n\t\t\t\nOverall I enjoyed reading your paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]}},{"tddate":null,"ddate":null,"tmdate":1513849363385,"tcdate":1509126824727,"number":566,"cdate":1509739229657,"id":"H1-nGgWC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1-nGgWC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gaussian Process Behaviour in Wide Deep Neural Networks","abstract":"Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.","pdf":"/pdf/7c6cadc22542e52a4f1d21cef9bb357c504331cb.pdf","paperhash":"anonymous|gaussian_process_behaviour_in_wide_deep_neural_networks","_bibtex":"@article{\n  anonymous2018gaussian,\n  title={Gaussian Process Behaviour in Wide Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1-nGgWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper566/Authors"],"keywords":["Gaussian Processes","Bayesian Deep Learning","Theory of Deep Neural Networks"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}