{"notes":[{"tddate":null,"ddate":null,"tmdate":1512406005961,"tcdate":1512406005961,"number":3,"cdate":1512406005961,"id":"BJAg3e7ZM","invitation":"ICLR.cc/2018/Conference/-/Paper979/Official_Review","forum":"HJrJpzZRZ","replyto":"HJrJpzZRZ","signatures":["ICLR.cc/2018/Conference/Paper979/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"\n1) Summary\nThis paper proposes a flow-based neural network architecture and adversarial training for multi-step video prediction. The neural network in charge of predicting the next frame in a video implicitly generates flow that is used to transform the previously observed frame into the next. Additionally, this paper proposes a new quantitative evaluation criteria based on the observed flow in the prediction in comparison to the groundtruth. Experiments are performed on a new robot arm dataset proposed in the paper where they outperform the used baselines.\n\n\n2) Pros:\n+ New quantitative evaluation criteria based on motion accuracy.\n+ New dataset for robot arm pushing objects.\n\n3) Cons:\nOverall architectural prediction network differences with baseline are unclear:\nThe differences between the proposed prediction network and [1] seem very minimal. In Figure 3, it is mentioned that the network uses a U-Net with recurrent connections. This seems like a very minimal change in the overall architecture proposed. Additionally, there is a paragraph of “architecture improvements” which also are minimal changes. Based on the title of section 3, it seems that there is a novelty on the “prediction with flow” part of this method. If this is a fact, there is no equation describing how this flow is computed. However, if this “flow” is computed the same way [1]  does it, then the title is misleading.\n\n\nAdversarial training objective alone is not new as claimed by the authors:\nThe adversarial objective used in this paper is not new. Works such as [2,3] have used this objective function for single step and multi-step frame prediction training, respectively. If the authors refer to the objective being new in the sense of using it with an action conditioned video prediction network, then this is again an extremely minimal contribution. Essentially, the authors just took the previously used objective function and used it with a different network. If the authors feel otherwise, please comment on why this is the case.\n\n\nIncomplete experiments:\nThe authors only show experiments on videos containing objects that have already been seen, but no experiments with objects never seen before. The missing experiment concerns me in the sense that the network could just be memorizing previously seen objects. Additionally, the authors present evaluation based on PSNR and SSIM on the overall predicted video, but not in a per-step paradigm. However, the authors show this per-step evaluation in the Amazon Mechanical Turk, and predicted object position evaluations.\n\n\nUnclear evaluation:\nThe way the Amazon Mechanical Turk experiments are performed are unclear and/or not suited for the task at hand.\nBased on the explanation of how these experiments are performed, the authors show individual images to mechanical turkers. If we are evaluating the video prediction task for having real or fake looking videos, the turkers need to observe the full video and judge based on that. If we are just showing images, then they are evaluating image synthesis, which do not necessarily contain the desired properties in videos such as temporal coherence.\n\n\nAdditional comments:\nThe paper needs a considerable amount of polishing.\n\n\n4) Conclusion:\nThis paper seems to contain very minimal changes in comparison to the baseline by [1]. The adversarial objective is not novel as mentioned by the authors and has been used in [2,3]. Evaluation is unclear and incomplete.\n\n\nReferences:\n[1] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NIPS, 2016.\n[2] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016.\n[3] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee. Decomposing Motion and Content for Natural Video Sequence Prediction. In ICLR, 2017\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Supervised Learning of Object Motion Through Adversarial Video Prediction","abstract":"Can we build models that automatically learn about object motion from raw, unlabeled videos? In this paper, we study the problem of multi-step video prediction, where the goal is to predict a sequence of future frames conditioned on a short context. We focus specifically on two aspects of video prediction: accurately modeling object motion, and producing naturalistic image predictions. Our model is based on a flow-based generator network with a discriminator used to improve prediction quality. The implicit flow in the generator can be examined to determine its accuracy, and the predicted images can be evaluated for image quality. We argue that these two metrics are critical for understanding whether the model has effectively learned object motion, and propose a novel evaluation benchmark based on ground truth object flow. Our network achieves state-of-the-art results in terms of both the realism of the predicted images, as determined by human judges, and the accuracy of the predicted flow. Videos and full results can be viewed on the supplementary website: \\url{https://sites.google.com/site/omvideoprediction}.","pdf":"/pdf/f12ed6b3f7d214e767e4bb91ab2987bdf3569a6c.pdf","paperhash":"anonymous|selfsupervised_learning_of_object_motion_through_adversarial_video_prediction","_bibtex":"@article{\n  anonymous2018self-supervised,\n  title={Self-Supervised Learning of Object Motion Through Adversarial Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJrJpzZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper979/Authors"],"keywords":["adversarial","video prediction","flow"]}},{"tddate":null,"ddate":null,"tmdate":1512222830599,"tcdate":1511809738470,"number":2,"cdate":1511809738470,"id":"ByW0MJqlM","invitation":"ICLR.cc/2018/Conference/-/Paper979/Official_Review","forum":"HJrJpzZRZ","replyto":"HJrJpzZRZ","signatures":["ICLR.cc/2018/Conference/Paper979/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Incremental contributions and incomplete evaluations","rating":"3: Clear rejection","review":"This paper is concerned with video prediction, for use in robotic motion planning. The task is performed on tabletop videos of a robotic arm manipulator interacting with various small objects. They use a prior model proposed in Finn et al. 2016, make several incremental architectural improvements, and use an adversarial loss function instead of an L2 loss. They also propose a new metric, motion accuracy, which uses the accuracy of the predicted position of the object instead of conventional metrics like PSNR, which is more relevant for robotic motion planning.\n\nThey obtain significant quantitative improvements over the previous 2 papers in this domain (video prediction on tabletop with robotic arm and objects) on both type of metrics - image assessment and motion accuracy. They also evaluate realism images using AMT fooling - asking turks to chose the fake between between real and generated images, and obtain substantial improvements on this metric as well. \n\nA major point of concern is that they do not use the public dataset proposed in Finn et al. 2016, but use their own (smaller) dataset. They do not mention whether they train the previous methods on the new dataset, and some of their reported improvements may be because of this. They also do not report results on unseen objects, when occlusions are present, and on human motion video prediction, unlike the other papers.\n\nThe adversarial loss helps significantly only with AMT fooling or realism of images, as expected because GANs produce sharp images rather than distributions, and is not very relevant for robot motion planning. The incremental architectural changes, different dataset and training are responsible for most of the other improvements.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Supervised Learning of Object Motion Through Adversarial Video Prediction","abstract":"Can we build models that automatically learn about object motion from raw, unlabeled videos? In this paper, we study the problem of multi-step video prediction, where the goal is to predict a sequence of future frames conditioned on a short context. We focus specifically on two aspects of video prediction: accurately modeling object motion, and producing naturalistic image predictions. Our model is based on a flow-based generator network with a discriminator used to improve prediction quality. The implicit flow in the generator can be examined to determine its accuracy, and the predicted images can be evaluated for image quality. We argue that these two metrics are critical for understanding whether the model has effectively learned object motion, and propose a novel evaluation benchmark based on ground truth object flow. Our network achieves state-of-the-art results in terms of both the realism of the predicted images, as determined by human judges, and the accuracy of the predicted flow. Videos and full results can be viewed on the supplementary website: \\url{https://sites.google.com/site/omvideoprediction}.","pdf":"/pdf/f12ed6b3f7d214e767e4bb91ab2987bdf3569a6c.pdf","paperhash":"anonymous|selfsupervised_learning_of_object_motion_through_adversarial_video_prediction","_bibtex":"@article{\n  anonymous2018self-supervised,\n  title={Self-Supervised Learning of Object Motion Through Adversarial Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJrJpzZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper979/Authors"],"keywords":["adversarial","video prediction","flow"]}},{"tddate":null,"ddate":null,"tmdate":1512222830643,"tcdate":1511568282282,"number":1,"cdate":1511568282282,"id":"BkGsQ4Ixz","invitation":"ICLR.cc/2018/Conference/-/Paper979/Official_Review","forum":"HJrJpzZRZ","replyto":"HJrJpzZRZ","signatures":["ICLR.cc/2018/Conference/Paper979/AnonReviewer3"],"readers":["everyone"],"content":{"title":"fine paper, but the authors should aim higher","rating":"7: Good paper, accept","review":"This is a fine paper that generally reads as a new episode in a series on motion-based video prediction with an eye towards robotic manipulation [Finn et al. 2016, Finn and Levine 2017, Ebert et al. 2017]. The work is rather incremental but is competently executed. It is in line with current trends in the research community and is a good fit for ICLR. The paper is well-written, reasonably scholarly, and contains stimulating insights.\n\nI recommend acceptance, despite some reservations. My chief criticism is a matter of research style: instead of this deluge of barely distinguishable least-publishable-unit papers on the same topic, in every single conference, I wish the authors didn’t slice so thinly, devoted more time to each paper, and served up a more substantial dish.\n\nSome more detailed comments:\n\n- The argument for evaluating visual realism never quite gels and is not convincing. The paper advocates two primary metrics: accuracy of the predicted motion and perceptual realism of the synthesized images. The argument for motion accuracy is clear and is clearly stated: it’s the measure that is actually tied to the intended application, which is using action-conditional motion prediction for control. A corresponding argument for perceptual realism is missing. Indeed, a skeptical reviewer may suspect that the authors needed to add perceptual realism to the evaluation because that’s the only thing that justifies the adversarial loss. The adversarial loss is presented as the central conceptual contribution of the paper, but doesn’t actually make a difference in terms of task-relevant metrics. A skeptical perspective on the paper is that the adversarial loss just makes the images look prettier but makes no difference in terms of task performance (control). This is an informative negative result. It's not how the paper is written, though.\n\n- The “no adversary”/“no adv” condition in Table 1 and Figure 4 is misleading. It’s not properly controlled. It is not the case that the adversarial loss was simply removed. The regression loss was also changed from l_1 to l_2. This is not right. The motivation for this control is to evaluate the impact of the adversarial loss, which is presented as the key conceptual contribution of the paper. It should be a proper control. The other loss should remain what it is in the full “Ours” condition (i.e., l_1).\n\n- The last sentence in the caption of Table 1 -- “Slight improvement in motion is observed by training with an adversary as well” -- should be removed. The improvement is in the noise.\n\n- Generally, the quantitative impact of the adversarial loss never comes together. The only statistically significant improvement is on perceptual image realism. The relevance of perceptual image realism to the intended task (control) is not substantiated, as discussed earlier.\n\n- In the perceptual evaluation procedure, the “1 second” restriction is artificial and makes the evaluated methods appear better than they are. If we are serious about evaluating image realism and working towards passing the visual Turing test, we should report results without an artificial time limit. They won’t look as flattering, but will properly report our progress on this journey. If desired, the results of timed comparisons can also be reported, but reporting just a timed comparison with an artificial limit of 1 second may mislead some readers into thinking that we are farther along than we actually are.\n\n\nThere are some broken sentences that mar an otherwise well-written paper:\n\n- End of Section 1, “producing use a learned discriminator and show improvements in visual quality”\n\n- Beginning of Section 3, “We first present the our overall network architecture”\n\n- page 4, “to choose to copy pixels from the previous frame, used transformed versions of the previous frame”\n\n- page 4, “convolving in the input image with”\n\n- page 5, “is know to produce”\n\n- page 5, “an additional indicating”\n\n- page 5, “Adam Kingma & Ba (2015)” (use the other cite command)\n\n- page 5, “we observes”\n\n- page 5, “smaller batch sizes degrades”\n\n- page 5, “larger batch sizes provides”\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Supervised Learning of Object Motion Through Adversarial Video Prediction","abstract":"Can we build models that automatically learn about object motion from raw, unlabeled videos? In this paper, we study the problem of multi-step video prediction, where the goal is to predict a sequence of future frames conditioned on a short context. We focus specifically on two aspects of video prediction: accurately modeling object motion, and producing naturalistic image predictions. Our model is based on a flow-based generator network with a discriminator used to improve prediction quality. The implicit flow in the generator can be examined to determine its accuracy, and the predicted images can be evaluated for image quality. We argue that these two metrics are critical for understanding whether the model has effectively learned object motion, and propose a novel evaluation benchmark based on ground truth object flow. Our network achieves state-of-the-art results in terms of both the realism of the predicted images, as determined by human judges, and the accuracy of the predicted flow. Videos and full results can be viewed on the supplementary website: \\url{https://sites.google.com/site/omvideoprediction}.","pdf":"/pdf/f12ed6b3f7d214e767e4bb91ab2987bdf3569a6c.pdf","paperhash":"anonymous|selfsupervised_learning_of_object_motion_through_adversarial_video_prediction","_bibtex":"@article{\n  anonymous2018self-supervised,\n  title={Self-Supervised Learning of Object Motion Through Adversarial Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJrJpzZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper979/Authors"],"keywords":["adversarial","video prediction","flow"]}},{"tddate":null,"ddate":null,"tmdate":1510525852682,"tcdate":1510525852682,"number":2,"cdate":1510525852682,"id":"rJrjsrIkf","invitation":"ICLR.cc/2018/Conference/-/Paper979/Public_Comment","forum":"HJrJpzZRZ","replyto":"rk2BBWM1z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Links to publicly available robot pushing datasets","comment":"The dataset will be released upon publication. In the meantime, there are similar datasets that are publicly available such as the following:\nhttps://sites.google.com/site/brainrobotdata/home/push-dataset\nhttps://sites.google.com/view/sna-visual-mpc"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Supervised Learning of Object Motion Through Adversarial Video Prediction","abstract":"Can we build models that automatically learn about object motion from raw, unlabeled videos? In this paper, we study the problem of multi-step video prediction, where the goal is to predict a sequence of future frames conditioned on a short context. We focus specifically on two aspects of video prediction: accurately modeling object motion, and producing naturalistic image predictions. Our model is based on a flow-based generator network with a discriminator used to improve prediction quality. The implicit flow in the generator can be examined to determine its accuracy, and the predicted images can be evaluated for image quality. We argue that these two metrics are critical for understanding whether the model has effectively learned object motion, and propose a novel evaluation benchmark based on ground truth object flow. Our network achieves state-of-the-art results in terms of both the realism of the predicted images, as determined by human judges, and the accuracy of the predicted flow. Videos and full results can be viewed on the supplementary website: \\url{https://sites.google.com/site/omvideoprediction}.","pdf":"/pdf/f12ed6b3f7d214e767e4bb91ab2987bdf3569a6c.pdf","paperhash":"anonymous|selfsupervised_learning_of_object_motion_through_adversarial_video_prediction","_bibtex":"@article{\n  anonymous2018self-supervised,\n  title={Self-Supervised Learning of Object Motion Through Adversarial Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJrJpzZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper979/Authors"],"keywords":["adversarial","video prediction","flow"]}},{"tddate":null,"ddate":null,"tmdate":1510245700415,"tcdate":1510245700415,"number":1,"cdate":1510245700415,"id":"rk2BBWM1z","invitation":"ICLR.cc/2018/Conference/-/Paper979/Public_Comment","forum":"HJrJpzZRZ","replyto":"HJrJpzZRZ","signatures":["~Oleksandr_Zaytsev1"],"readers":["everyone"],"writers":["~Oleksandr_Zaytsev1"],"content":{"title":"Requesting access to the dataset","comment":"Hello,\nwe want to take part in the ICLR 2018 Reproducibility Challenge (http://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html) and replicate the experiments described in this paper. Would it be possible to access the trajectories dataset that was described in a paper and used for training?\n\nThank you!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Self-Supervised Learning of Object Motion Through Adversarial Video Prediction","abstract":"Can we build models that automatically learn about object motion from raw, unlabeled videos? In this paper, we study the problem of multi-step video prediction, where the goal is to predict a sequence of future frames conditioned on a short context. We focus specifically on two aspects of video prediction: accurately modeling object motion, and producing naturalistic image predictions. Our model is based on a flow-based generator network with a discriminator used to improve prediction quality. The implicit flow in the generator can be examined to determine its accuracy, and the predicted images can be evaluated for image quality. We argue that these two metrics are critical for understanding whether the model has effectively learned object motion, and propose a novel evaluation benchmark based on ground truth object flow. Our network achieves state-of-the-art results in terms of both the realism of the predicted images, as determined by human judges, and the accuracy of the predicted flow. Videos and full results can be viewed on the supplementary website: \\url{https://sites.google.com/site/omvideoprediction}.","pdf":"/pdf/f12ed6b3f7d214e767e4bb91ab2987bdf3569a6c.pdf","paperhash":"anonymous|selfsupervised_learning_of_object_motion_through_adversarial_video_prediction","_bibtex":"@article{\n  anonymous2018self-supervised,\n  title={Self-Supervised Learning of Object Motion Through Adversarial Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJrJpzZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper979/Authors"],"keywords":["adversarial","video prediction","flow"]}},{"tddate":null,"ddate":null,"tmdate":1510092382950,"tcdate":1509137640633,"number":979,"cdate":1510092360923,"id":"HJrJpzZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJrJpzZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Self-Supervised Learning of Object Motion Through Adversarial Video Prediction","abstract":"Can we build models that automatically learn about object motion from raw, unlabeled videos? In this paper, we study the problem of multi-step video prediction, where the goal is to predict a sequence of future frames conditioned on a short context. We focus specifically on two aspects of video prediction: accurately modeling object motion, and producing naturalistic image predictions. Our model is based on a flow-based generator network with a discriminator used to improve prediction quality. The implicit flow in the generator can be examined to determine its accuracy, and the predicted images can be evaluated for image quality. We argue that these two metrics are critical for understanding whether the model has effectively learned object motion, and propose a novel evaluation benchmark based on ground truth object flow. Our network achieves state-of-the-art results in terms of both the realism of the predicted images, as determined by human judges, and the accuracy of the predicted flow. Videos and full results can be viewed on the supplementary website: \\url{https://sites.google.com/site/omvideoprediction}.","pdf":"/pdf/f12ed6b3f7d214e767e4bb91ab2987bdf3569a6c.pdf","paperhash":"anonymous|selfsupervised_learning_of_object_motion_through_adversarial_video_prediction","_bibtex":"@article{\n  anonymous2018self-supervised,\n  title={Self-Supervised Learning of Object Motion Through Adversarial Video Prediction},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJrJpzZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper979/Authors"],"keywords":["adversarial","video prediction","flow"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}