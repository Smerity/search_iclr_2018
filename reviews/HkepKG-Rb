{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222809207,"tcdate":1512095319509,"number":3,"cdate":1512095319509,"id":"SJJw0N0eM","invitation":"ICLR.cc/2018/Conference/-/Paper891/Official_Review","forum":"HkepKG-Rb","replyto":"HkepKG-Rb","signatures":["ICLR.cc/2018/Conference/Paper891/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Overall the idea is interesting, but the paper does not seem ready for publication. ","rating":"4: Ok but not good enough - rejection","review":"This paper suggest a method for including symbolic knowledge into the learning process. The symbolic knowledge is given as logical constraints which characterize the space of legal solutions.   This knowledge is \"injected\" into the learning process by augmenting the loss function with a symbolic-loss term that, in addition to the traditional loss, increases the probability of legal states (which also includes incorrect, yet legal, predictions). \n\nOverall the idea is interesting, but the paper does not seem ready for publication.  The  idea of semantic-loss function is appealing and is nicely motivated in the paper, however the practical aspects of how it can be applied are extremely vague and hard to understand.  Specifically, the authors define it over all assignments to the output variables that satisfy the constraints. For any non-trivial prediction problem, this would be at least computationally challenging. The author discuss it briefly mentioning a method by Darwiche-2003, but do not offer much intuition or analysis beyond that.  Their experiments focus on multiclass classification, which implicitly has a \"one-vs.-all\" constraint, although it's not clear why defining a formal loss function is needed (instead of just taking the argmax of the multiclass net), and even beyond that - why would it result in such significant improvements (when there are a few annotated data points)?   \n\nThe more interesting case is where the loss needs to decompose over the parts of a structural decision, where symbolic knowledge can help  constrain the output space. This has been addressed in the literature (e.g., [1], [2]) it's not clear why the authors don't compare to these models, or even attempt any meaningful evaluation.\n\n\n[1] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural\nnetworks with logic rules. ACL, 2016.\n\n[2]Posterior Regularization for Structured Latent Variable Models.  Ganchev et-al 2010.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Semantic Loss Function for Deep Learning with Symbolic Knowledge","abstract":"This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and shortest paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.","pdf":"/pdf/432d927483521f91358d6dc63833a1973ab1ef5c.pdf","paperhash":"anonymous|a_semantic_loss_function_for_deep_learning_with_symbolic_knowledge","_bibtex":"@article{\n  anonymous2018a,\n  title={A Semantic Loss Function for Deep Learning with Symbolic Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkepKG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper891/Authors"],"keywords":["deep learning","symbolic knowledge","semi-supervised learning","constraints"]}},{"tddate":null,"ddate":null,"tmdate":1512222809256,"tcdate":1511871261380,"number":2,"cdate":1511871261380,"id":"ByEQXA5lM","invitation":"ICLR.cc/2018/Conference/-/Paper891/Official_Review","forum":"HkepKG-Rb","replyto":"HkepKG-Rb","signatures":["ICLR.cc/2018/Conference/Paper891/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes a new loss function that penalizes semantic features of the data, and shows some experiments; overall the writing is good and the ideas are nice, even though the contribution is relatively small.","rating":"5: Marginally below acceptance threshold","review":"The authors propose a new loss function that is directed to take into account Boolean constraints involving the variables of a classification problem. This is a nice idea, and certainly relevant. The authors clearly describe their problem, and overall the paper is well presented. The contributions are a loss function derived from a set of axioms, and experiments indicating that this loss function captures some valuable elements of the input. This is a valid contribution, and the paper certainly has some significant strengths.\n\nConcerning the loss function, I find the whole derivation a bit distracting and unnecessary. Here we have some axioms, that are not simple when taken together, and that collectively imply a loss function that makes intuitive sense by itself. Well, why not just open the paper with Definition 1, and try to justify this definition on the basis of its properties. The discussion of axioms is just something that will create debate over questionable assumptions. Also it is frustrating to see some axioms in the main text, and some axioms in the appendix (why this division?). \n\nAfter presenting the loss function, the authors consider some applications. They are nicely presented; overall the gains are promising but not that great when compared to the state of the art --- they suggest that the proposed semantic loss makes sense. However I find that the proposal is still in search of a \"killer app\". Overall, I find that the whole proposal seems a bit premature and in need of more work on applications (the work on axiomatics is fine as long as it has something to add).\n\nConcerning the text, a few questions/suggestions:\n- Before Lemma 3, \"this allows...\" is the \"this including the other axioms in the appendix?\n- In Section 4, line 3: I suppose that the constraint is just creating a problem with a class containing several labels, not really a multi-label classification problem (?).\n- The beginning of Section 4.1 is not very clear. By reading it, I feel that the best way to handle the unlabeled data would be to add a direct penalty term forcing the unlabeled points to receive a label. Is this fair?\n- Page 6: \"a mor methodological\"... should it be \"a more methodical\"?\n- There are problems with capitalization in the references. Also some references miss page numbers and some do not even indicate what they are (journal papers, conference papers, arxiv, etc).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Semantic Loss Function for Deep Learning with Symbolic Knowledge","abstract":"This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and shortest paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.","pdf":"/pdf/432d927483521f91358d6dc63833a1973ab1ef5c.pdf","paperhash":"anonymous|a_semantic_loss_function_for_deep_learning_with_symbolic_knowledge","_bibtex":"@article{\n  anonymous2018a,\n  title={A Semantic Loss Function for Deep Learning with Symbolic Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkepKG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper891/Authors"],"keywords":["deep learning","symbolic knowledge","semi-supervised learning","constraints"]}},{"tddate":null,"ddate":null,"tmdate":1512222809303,"tcdate":1511717221874,"number":1,"cdate":1511717221874,"id":"SkRvF__xf","invitation":"ICLR.cc/2018/Conference/-/Paper891/Official_Review","forum":"HkepKG-Rb","replyto":"HkepKG-Rb","signatures":["ICLR.cc/2018/Conference/Paper891/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Deep learning with symbolic information","rating":"7: Good paper, accept","review":"SUMMARY \n\nThe paper proposes a new form of regularization utilizing logical constraints. The semantic loss function is built on the exploitation of symbolic knowledge extracted from data and connecting the logical constraints to the outputs of a neural network. The use of Boolean logic as a constraint provides a secondary regularization term to prevent over-fitting and improve predictions. The benefit of using the function is found primarily with semi-supervised tasks where data is partially unlabelled. The logical constraints provided by the semantic loss function allow for improved classification of unlabeled data.\nOutput constraints for the semantic loss function are represented with one-hot encoding, prefer- ence rankings, and paths in a grid. These three different output constraints are designed to explore different learning purposes. The semantic function was tested on both semi-supervised classifica- tion tasks as well as structure learning. The paper primarily focuses on the one-hot encoding constraint as it is viewed as a capable technique for multi-class classification.\n\nPOSITIVES \n\nIn terms of structure, the paper was written very well. Sufficient background information was con- veyed which helped in understanding the proposed semantic loss function. A thorough breakdown is also carried out on the semantic loss function itself by explaining its axioms which help explain how the outputs of a neural network match a given constraint.\nAs a scientific contribution, I would say results from the experiments were able to justify the proposal of the semantic loss function. The function was able to perform better than most other implementations for semi-supervised learning tasks, and the function was tested on multiple datasets. The paper also made use of testing the function against other notable machine learning approaches, and in most cases the function performed better, but this usually was confined to semi-supervised learning tasks. During supervised learning tasks the function did not perform markedly better than older implementations. Given that, the semantic loss function did prove to be a seemingly simple approach to improving semi-supervised classification tasks.\n• The background section covers the knowledge required in understanding the semantic loss function. The paper also clearly explains the meaning for some of the notation used in the definitions.\n• Experiments which clearly show the benefit of using the semantic loss function. Multiple experiment types were done as well which showed evidence of the broad applicability of the function.\n• In depth description of the definitions, axioms, and propositions of the semantic loss function.\n• A large number of experiments exploring the usefulness of the function for multiple learning tasks, and on multiple datasets.\n\nNEGATIVES \n\nI was not clear if the logical constraints are to be instantiated before learning, i.e. they are defined by hand prior to being implemented in the neural network. This is a pretty important question and drastically changes the nature of the learning process. Beyond that complaint, the paper did not suffer from any critical issues. There were some issues with spelling, and the section titled ’Algorithm’ fails to clearly define a complete algorithm using the semantic loss function. It would have helped to have two algorithms. One defining the pipeline for the semantic loss function, and another showing the implementation of the function in a machine learning framework. The semantic loss function found success only in cases were the learning task was semi-supervised, and not in cases of total supervised learning. This is not a true negative, but an observation on the effectiveness of the function.\n\n- A few typos in the paper.\n- The axioms for the semantic loss function where defined but there seemed to be a lack of a clear algorithm provided showing the pipeline implementation of the semantic loss function.\n- While the semantic loss function does improve learning performance in most cases, the im- provements are confined to semi-supervised learning tasks, and with the MNIST dataset another methodology, Ladder Nets, was able to outperform the semantic loss function.\n\nRELATED WORK\n\nThe paper proposed that logic constraints applied to the output of neural networks have the capacity to improve semi-supervised classification tasks as well as finding the shortest path. In the introduction, the paper lists Zhiting Hu et al. paper titled Harnessing Deep Neural Networks with Logic Rules as an example of a similar approach. Hu et al. paper utilized logic constraints in conjunction with neural nets as well. A key difference was that Hu et al. applied their network architecture to supervised classification tasks. Since the performance of the current papers semantic loss function with supervised tasks did not improve upon other methods, it may benefit to utilize the research by Hu et al. as a means of direct comparison for supervised learning tasks, and possibly incorporate their methods with the semantic loss function in order to improve upon supervised learning tasks.\n\nCONCLUSION\n\nGiven the success of the semantic loss function with semi-supervised tasks, I would accept this paper. The semantic loss was able to improve learning with respect to the tested datasets, and the paper clearly described the properties of the functions. The paper would benefit by including a more concrete algorithm describing the flow of data through a given neural net to the semantic loss function, as well as the process by which the semantic loss function constrains the data based on propositional logic, but in general this complaint is more nit picking. The semantic loss function and the experiments which tested the function showed clearly that there is a benefit to this research and there are areas for it to improve.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Semantic Loss Function for Deep Learning with Symbolic Knowledge","abstract":"This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and shortest paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.","pdf":"/pdf/432d927483521f91358d6dc63833a1973ab1ef5c.pdf","paperhash":"anonymous|a_semantic_loss_function_for_deep_learning_with_symbolic_knowledge","_bibtex":"@article{\n  anonymous2018a,\n  title={A Semantic Loss Function for Deep Learning with Symbolic Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkepKG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper891/Authors"],"keywords":["deep learning","symbolic knowledge","semi-supervised learning","constraints"]}},{"tddate":null,"ddate":null,"tmdate":1510092386246,"tcdate":1509136844140,"number":891,"cdate":1510092362707,"id":"HkepKG-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkepKG-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Semantic Loss Function for Deep Learning with Symbolic Knowledge","abstract":"This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and shortest paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.","pdf":"/pdf/432d927483521f91358d6dc63833a1973ab1ef5c.pdf","paperhash":"anonymous|a_semantic_loss_function_for_deep_learning_with_symbolic_knowledge","_bibtex":"@article{\n  anonymous2018a,\n  title={A Semantic Loss Function for Deep Learning with Symbolic Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkepKG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper891/Authors"],"keywords":["deep learning","symbolic knowledge","semi-supervised learning","constraints"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}