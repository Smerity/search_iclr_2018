{"notes":[{"tddate":null,"ddate":null,"tmdate":1514156153041,"tcdate":1514156153041,"number":3,"cdate":1514156153041,"id":"ryZFenTGf","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Comment","forum":"rkcya1ZAW","replyto":"r1_fQy9lz","signatures":["ICLR.cc/2018/Conference/Paper522/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper522/Authors"],"content":{"title":"Thanks for positive feeback","comment":"We appreciate your consistent support for our work. The draft is updated again, hopefully it could be easier for the future readers to understand."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1513832582362,"tcdate":1513832582362,"number":2,"cdate":1513832582362,"id":"SyAtlT_zG","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Comment","forum":"rkcya1ZAW","replyto":"HkCNqISxM","signatures":["ICLR.cc/2018/Conference/Paper522/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper522/Authors"],"content":{"title":"Thank you for valuable feedback, we have updated our draft","comment":"Thank you for valuable feedback, which make us aware of some presentation issues of our original submission. We hope we could engage in constructive discussions to fully clarify and address your concerns and questions. We have fixed the problems by re-writing Section 3, and hopefully address your concerns. We wish to take the opportunities to emphasize that the main proposed methodologies/algorithms are still valid, and the pointed problems are only relates to the writing. Below are our initial responses to your three comments.\n\n1. You are right. Proposition 1 is not correct for all CTFs, but it is correct for some specific CTFs such as the Hamiltonian flow. Sorry for the mistake, we have removed it and re-written this section. Note that the proposed algorithm for inference does not rely on this proposition. This is because the Jacobian term is only necessary in explicit methods ( i.e. maintaining distribution forms) in representing the normalizing flows, while our amortized approach is implicit (i.e. sample-based approximation) in representing flows at each step. Please see Section 3.2 on the detailed learning algorithm.\n\n2. Eq.5 is not directly implemented in practice, We have clarified it in our revision to avoid confusion.  Eq.5 is derived from the principle theory of CTF, and presented in the paper to justify (1) some potential advantages of using CTF and (2) the sequential procedure of approximating the unknown \\rho_T. \n\nIn practice, we build the algorithm on the sequential procedure in Eq.5, and amortized the inference in an implicit manner. Specifically, at each step, we (1) first simulated samples from the corresponding diffusion, which is equivalent to optimizing one step in Eq.5, i.e., the resulting sample distribution (implicit) equals that from optimizing eq.5, and (2) proposed to use a neural network to match (i.e., â€œdistill the knowledge\") the simulated sample distributions. Directly handling the optimization problem to obtained its explicit distribution forms is an interesting direction of future work.\n\n3. In the original Eq.6, we meant to show the ELBO, assuming \\bar{\\rho} is continuous (in the infinite-data setting). We agree this is a little misleading, thus we have removed it, and reformulated the objective in our revision (still Eq.6). Thanks for pointing out this issue."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1513832534051,"tcdate":1513832534051,"number":1,"cdate":1513832534051,"id":"ByCUxp_GG","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Comment","forum":"rkcya1ZAW","replyto":"Hy8TV-qgf","signatures":["ICLR.cc/2018/Conference/Paper522/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper522/Authors"],"content":{"title":"Thank you for your recognition","comment":"Thank you for recognizing our work. We are happy to address the two questions raised.\n\nWe agree that our way to approximate \\rho_T is not optimal. We use the simple sample averaging for the convenience of analysis. Better approximation by assigning more weights to the more recent samples leads to more challenges in theoretical analysis. We have added some discussion about this in the Section 3.1.\n\nThe stepsize parameter of the discretized Langevin chain does not affect model performance a lot as long as the stepsize lies in an appropriate range. To verify this, following SteinGAN with a simple Gaussian-Bernoulli Restricted Boltzmann Machines as the energy-based model (https://github.com/DartML/SteinGAN), we conducted an extra experiment on the MNIST dataset with MacGAN. We used the annealed importance sampling to evaluate log-likelihoods. Below are the log-likelihoods by varying the stepsize. More details are included in the appendix D.4.\n\nstepsize: \t\t6e-4\t2.4e-3\t3.6e-3\t6e-3\t1e-2\t1.5e-2\nlog-likelihood:\t-800\t-760\t-752\t-762\t-758\t-775"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1515642460698,"tcdate":1511818429772,"number":3,"cdate":1511818429772,"id":"Hy8TV-qgf","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Review","forum":"rkcya1ZAW","replyto":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference/Paper522/AnonReviewer3"],"readers":["everyone"],"content":{"title":"No great novelty, but above the threshold.","rating":"6: Marginally above acceptance threshold","review":"The authors propose the use of first order Langevin dynamics as a way to transition from one latent variable to the next in the VAE setting, as opposed to the deterministic transitions of normalizing flow. The extremely popular Fokker-Planck equation is used to analyze the steady state distributions in this setting. The authors also propose the use of CTF in density estimation, as a generator of samples from the ''true'' distribution, and show competitive performance w.r.t. inception score for some common datasets.\n\nThe use of Langevin diffusion for latent transitions is a good idea in my opinion; though quite simple, it has the benefit of being straightforward to analyze with existing machinery. Though the discretized Langevin transitions in \\S 3.1 are known and widely used, I liked the motivation afforded by Lemma 2. \n\nI am not convinced that taking \\rho to be the sample distribution with equal probabilities at the z samples is a good choice in \\S 3.1; it would be better to incorporate the proximity of the langevin chain to a stationary point in the atom weights instead of setting them to 1/K. However to their credit the authors do provide an estimate of the error in the distribution stemming from their choice.   \n\nTo the best of my knowledge the use of CTF in density estimation as described in \\S 4 is new, and should be of interest to the community; though again it is fairly straightforward. Regarding the experiments, the difference in ELBO between the macVAE and the vanilla ones with normalizing flows is only about 2%; I wish the authors included a discussion on how the parameters of the discretized Langevin chain affects this, if at all.\n\nOverall I think the theory is properly described and has a couple of interesting formulations, in spite of being not particularly novel. I think CTFs like the one described here will see increased usage in the VAE setting, and thus the paper will be of interest to the community.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1515642460739,"tcdate":1511809807663,"number":2,"cdate":1511809807663,"id":"r1_fQy9lz","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Review","forum":"rkcya1ZAW","replyto":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference/Paper522/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Previous reviewer; interesting ideas furthering continuous-time flows","rating":"6: Marginally above acceptance threshold","review":"\nThe authors propose continuous-time flows as a flexible family of\ndistributions for posterior inference of latent variable models as\nwell as explicit density estimation. They build primarily on the work\nof normalizing flows from Rezende and Mohamed (2015). They derive an\ninteresting objective based on a sequence of sub-optimization\nproblems, following a variational formulation of the Fokker-Planck\nequations.\n\nI reviewed this paper for NIPS with a favorable decision toward weak\nacceptance; and the authors also addressed some of my questions in\nthis newer version (namely, some comparisons to related work; clearer\nwriting).\n\nThe experiments are only \"encouraging\"; they do not illustrate clear\nimprovements over previous methods. However, I think the work\ndemonstrates useful ideas furthering the idea of continuous-time\ntransformations that warrants acceptance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1515642460778,"tcdate":1511512629821,"number":1,"cdate":1511512629821,"id":"HkCNqISxM","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Review","forum":"rkcya1ZAW","replyto":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference/Paper522/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not convinced!","rating":"3: Clear rejection","review":"The authors try to use continuous time generalizations of normalizing flows for improving upon VAE-like models or for standard density estimation problems.\n\nClarity: the text is mathematically very sloppy / hand-wavy.\n\n1. I do not understand proposition (1). I do not think that the proof is correct (e.g. the generator L needs to be applied to a function -- the notation L(x) does not make too much sense): indeed, in the case when the volatility is zero (or very small), this proposition would imply that any vector field induces a volume preserving transformation, which is indeed false.\n\n2. I do not really see how the sequence of minimization Eq(5) helps in practice. The Wasserstein term is difficult to hand.\n\n3. in Equation (6), I do not really understand what $\\log(\\bar{\\rho})$ is if $\\bar{\\rho}$ is an empirical distribution. One really needs $\\bar{\\rho}$ to be a probability density to make sense of that.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1513832767499,"tcdate":1509125345906,"number":522,"cdate":1509739254129,"id":"rkcya1ZAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/b62e8abe24c071df14013e00b04d1d382336a798.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}