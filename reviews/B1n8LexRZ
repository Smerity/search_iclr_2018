{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222589915,"tcdate":1511821833536,"number":3,"cdate":1511821833536,"id":"rkZzfMqef","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Review","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper on fast mixing extension of HMC","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions. These functions are trained to maximize expected squared jump distance.\nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016.\n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases. A few downsides are commented on below.\n\nThe experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. \nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper. I can't see where the number \"124x\" in sec 5.1 stems from. As a user, I would be interested in the typical computational cost of both \"MCMC sampler training\" and MCMC sampler usage (inference?), compared to competing methods. This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation. \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC?\n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC).\n\nThe number \"124x ESS\" in sec5.1 seems at odds with the number in the abstract, \"50x\".\n\n# Minor errors\n- sec1: \"The sampler is trained to minimize a variation\": should be maximize\n\"as well as on a the real-world\"\n- sec3.2 \"and 1/2 v^T v the kinetic\": \"energy\" missing\n- sec4: the acronym L2HMC is not expanded anywhere in the paper\nThe sentence \"We will denote the complete augmented...p(d)\" might be moved to after \"from a uniform distribution\" in the same paragraph. \nIn paragraph starting \"We now update x\":\n    - specify for clarity: \"the first update, which yields x' \"/ \"the second update, which yields x''  \"\n    - \"only affects $x_{\\bar{m}^t}$\": should be $x'_{\\bar{m}^t}$  (prime missing)\n    - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \"mask(x',m^t)\"?\n    - inside zeta_2 and zeta_3, do you not mean $m^t\" and $\\bar{m}^t$ ?\n- sec5: add reference for first mention of \"A NICE MC\"\n- Appendix A: \n    - \"Let's\" -> \"Let\"\n    - eq12 should be x''=...\n- Appendix C: space missing after \"Section 5.1\"\n- Appendix D1: \"In this section is presented\" : sounds odd\n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov Chain Monte Carlo kernels (parameterized by deep neural networks) that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate significant empirical gains (up to 50 times greater effective sample size) on a collection of simple but challenging distributions. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/c29721db351c837f2c25134833ba71c59f6f09e6.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1512222589956,"tcdate":1511799759948,"number":2,"cdate":1511799759948,"id":"HJdCshKgf","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Review","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposed a generalized HMC with neural networks, but it lacks theoretical justifications why it could mix with multiple modes","rating":"6: Marginally above acceptance threshold","review":"The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. The derivations look correct to me. In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov Chain Monte Carlo kernels (parameterized by deep neural networks) that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate significant empirical gains (up to 50 times greater effective sample size) on a collection of simple but challenging distributions. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/c29721db351c837f2c25134833ba71c59f6f09e6.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1512222589991,"tcdate":1511718322719,"number":1,"cdate":1511718322719,"id":"Hksh6uugz","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Review","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice idea; but the paper could be improved.","rating":"7: Good paper, accept","review":"In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance). In the literature, specially in machine learning, there is ``fever’’ about HMC, in my opinion, partially unjustified.\n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point.\n\nHowever, I have other additional comments:\n\n- Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader.\n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes\n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques, \n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223–242, April 2001,\n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337–348, 199. \n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov Chain Monte Carlo kernels (parameterized by deep neural networks) that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate significant empirical gains (up to 50 times greater effective sample size) on a collection of simple but challenging distributions. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/c29721db351c837f2c25134833ba71c59f6f09e6.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1509739426672,"tcdate":1509062227787,"number":212,"cdate":1509739424015,"id":"B1n8LexRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov Chain Monte Carlo kernels (parameterized by deep neural networks) that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate significant empirical gains (up to 50 times greater effective sample size) on a collection of simple but challenging distributions. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/c29721db351c837f2c25134833ba71c59f6f09e6.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}