{"notes":[{"tddate":null,"ddate":null,"tmdate":1516583839974,"tcdate":1516583839974,"number":8,"cdate":1516583839974,"id":"B1_so3fSM","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"SJGzn0pNz","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"A sub-gradient exists everywhere","comment":"While it is true that for some value of \\xi, the integrand is not differentiable, it does admit a sub-gradient everywhere. This is sufficient for optimization ( https://en.wikipedia.org/wiki/Subgradient_method ). Also note that ReLU networks demonstrate this same characteristic (continuous function, with discontinuities in the first derivative), but are routinely trained in deep learning.\n\nThank you again for your interest and thoughtful reading of our work!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1516267905894,"tcdate":1516264457749,"number":5,"cdate":1516264457749,"id":"SJGzn0pNz","invitation":"ICLR.cc/2018/Conference/-/Paper212/Public_Comment","forum":"B1n8LexRZ","replyto":"ryeffj94z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for the prompt response","comment":"I think the key issue here is establishing whether the integrand in Eq. (8) is an absolutely continuous function of \\theta for almost all \\xi. Then you can use e.g. Theorem 3 here http://planetmath.org/differentiationundertheintegralsign to validate the interchange. The easier to validate Theorem 2, which is sufficient for most cases in stochastic gradient-based VI, does not hold for your integrand because assumption 2 is not valid for the function A(|). This because the derivative of A(|) does not exist whenever the ratio of densities is exactly one in Eq. (3). But perhaps it is easy to show the absolute continuous property of A(|) wrt \\theta for almost all \\xi?\n\nI do agree that this is not an issue of any discrete random variables nor the function described by the numerical integration of an Hamiltonian flow. My concern is merely if the discontinuity in the gradient of A(|) wrt \\theta will be an issue.\n\nAlso, thanks for a very interesting read!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1516052999803,"tcdate":1516052999803,"number":7,"cdate":1516052999803,"id":"ryeffj94z","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"rJ0a6k9Nf","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"We believe our derivations are correct and the gradients unbiased","comment":"Thank you for your question, we believe our derivations are correct and the gradients unbiased.\n\nIn Eq. (8), p(\\xi) and q(\\xi) are not functions of the parameters \\theta and the loss inside the expectation is an (almost everywhere) differentiable function of \\xi. This allows us, when differentiating w.r.t \\theta to easily exchange (under mild assumptions) derivative and integration.\n\nIt is important to note that when optimizing, we are NOT sampling through the accept/reject step. Given a state \\xi, we move it forward using our (differentiable) generalized Hamiltonian dynamics and use that new proposed state for the loss; explicitly marginalizing over the accept/reject decision. We thus do not need to back-propagate through a discrete decision variable, making our gradients unbiased. This is additionally detailed in Pasarica and Gelman 2010.\n\nWe hope this answers your question!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1516006854417,"tcdate":1516006854417,"number":4,"cdate":1516006854417,"id":"rJ0a6k9Nf","invitation":"ICLR.cc/2018/Conference/-/Paper212/Public_Comment","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"How is the discontinuity in A(.|.) handled?","comment":"The accept-reject step of the MCMC kernel introduces a discontinuity in the function A( ) that depends on both the random variable xi AND the parameter being optimized with respect to. This means that interchanging the order of expectation (integration) and differentiation in eq. (8) is invalid in general. Have the authors considered this in their derivations? Can you prove that the gradients used for learning in Alg. 1 are still unbiased, and thus will lead to convergence by standard stochastic approximation results? If the gradients are not unbiased (which I suspect is the case), have you studied the impact this has on the learning procedure?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515465836251,"tcdate":1515465836251,"number":6,"cdate":1515465836251,"id":"By4O2iZVM","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"r1-mLY6XG","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"Re:","comment":"We believe we are the most flexible parameterization of a Markov kernel to date. However, there has been previous work that proposes general purpose kernels. Most relevant is Song et al, which trains a flexible class of volume-constrained Markov proposals using an adversarial objective. (We discuss and experimentally compare against this approach in our paper.) \n\nThanks for the question!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515193881223,"tcdate":1515193881223,"number":3,"cdate":1515193881223,"id":"r1-mLY6XG","invitation":"ICLR.cc/2018/Conference/-/Paper212/Public_Comment","forum":"B1n8LexRZ","replyto":"HyMf9EiXz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re:","comment":"Thanks for the response! As I understand it then, your method is the first in literature to be able to train expressive MCMC kernels? (as if I recall correctly, in the past, the focus has been more on tuning a very limited number of parameters associated with the proposal distribution, like the variance of a gaussian proposal for ex.)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515045958676,"tcdate":1515045958676,"number":5,"cdate":1515045958676,"id":"BkkI4roQM","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"Updated paper","comment":"We thank the reviewers for their valuable time and comments.\n\nWe updated the paper with the following modifications:\n- Clarified some points and fixed typos pointed out by the reviewers.\n- Added a ``Future Work section as well as additional relevant references.\n- Added a comparison with Look-Ahead HMC (LAHMC; Sohl-Dickstein et al. 2014) in the Appendix.\n\nAdditionally, in the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515043338088,"tcdate":1515043338088,"number":4,"cdate":1515043338088,"id":"HyMf9EiXz","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"ryQkGp_XG","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"Response","comment":"Thank you for your question.\n\nYou are correct that in general our method’s proposals cannot be interpreted as (approximately) integrating the dynamics of any Hamiltonian. Ultimately our goal (and that of HMC) is to produce a proposal that mixes efficiently, not to simulate Hamiltonian dynamics accurately.\n\nThere are many other trainable proposals for which we could compute the Jacobian, but not all will mix efficiently. By choosing a parameterized family of proposals that can mimic the behavior of HMC (and initializing it to do so), we ensure that our learned proposal performs at least as well as HMC.\n\nThe momentum-resampling step is essential, since it is the only source of randomness in the proposal. Using gradient information (d_x U(x)) is essential for giving the proposal information about the local geometry of the target distribution."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1514882645635,"tcdate":1514881499089,"number":2,"cdate":1514881499089,"id":"ryQkGp_XG","invitation":"ICLR.cc/2018/Conference/-/Paper212/Public_Comment","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Question regarding Section 4.1.2","comment":"I had one question -- in Equations 4-6, you have functions Q, T to rescale and translate the momentum and position. However it seems that Q, T are vectors and thus you are learning arbitrary transformations to d_x U(x)? \n\nIf that is the case, then I'm unclear on how your leapfrog operator guarantees (approximate) integration of the Hamiltonian. And if it does not and your goal is simply to learn proposals for which you can compute the Jacobian, then what's the purpose of the momentum resampling step and/or having the d_x U(x) term in the update at all?\n\nIf you could shed some light on that, that would be great!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1513760407704,"tcdate":1513760407704,"number":3,"cdate":1513760407704,"id":"Bylo8owGM","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"rkZzfMqef","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"Clarifications and comparison with LAHMC","comment":"We first and foremost want to thank you for your time and extremely valuable comments. We have uploaded a new version of the paper based on the feedback, and have addressed specific points below.\n\nClarification about 50x vs 124x:\nWe decided against advertising the 124x number as it is misleading considering that HMC completely failed on this task; the correct ratio was too large for us to experimentally measure. As such, we reported the one for the Strongly-Correlated Gaussian. We clarified this in the text and detail that L2HMC can succeed when HMC fails.\n\nIntuition on Eq 7.:\nWe define this reciprocal loss to encourage mixing across the entire state space. The second term corresponds exactly to Expected Square Jump Distance, which we want to maximize as a proxy for mixing. The first term discourages a particle from not-moving at all in a region of state space -- if d(x, x’) = 0, the first term would be infinite. We clarified that part in the text.\n\nTime encoding:\nOur operator L_\\theta consists of the composition of M augmented leapfrog steps. For each of those leapfrog, the timestep t is provided as input to the networks Q, S and T. Instead of providing it as a single scalar value, we provide it as a 2-d vector [cos(2 * pi * t / M), sin(2 * pi * t / M)].\n\nRegarding samples in Fig5:\nSample quality and sharpness are inherently hard things to evaluate. Our observation was that many digits generated by L2HMC-DGLM look very sharp (Line 1 Column 2, Line 2 Column 8, Line 5 Column 2, Line 7 Columns 3 and 7…). However, we will weaken the claim in the caption.\n\nComparison with LAHMC:\nWe compared our method to LAHMC on the evaluated energy functions. L2HMC significantly outperforms LAHMC on all tasks, for the same number of gradient evaluations. LAHMC is also unable to mix between modes in the MoG case. Results are reported in Appendix C.1. \n\nWe also note that L2HMC could be easily combined with LAHMC, by replacing the leapfrog integrator of LAHMC with the learned one of L2HMC.\n\nIn the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1).\n\nQuestion about computation:\nFor the 2d-SCG case, on CPU, the training of the sampler took 160 seconds. The L2HMC overhead for sampling, with a batch-size of 200, was about 36%. This is negligible compared to an 106x improved ESS.  We also should note that for the latent generative model case, we train the sampler online with the same computations used to train everything else; in that case L2HMC and HMC perform the exact same number of gradient evaluation of the energy and thus requires no training budget.\n\nThank you once again for your valuable feedback, we hope this helps answer your questions!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1513760264536,"tcdate":1513760264536,"number":2,"cdate":1513760264536,"id":"rkeGIjvMf","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"Hksh6uugz","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"Adaptive techniques can be complementary to our method","comment":"Thank you for your review and the pointer to references.\n\nWe wish to emphasize that our method is able, but not limited to, automatically tuning HMC parameters (which systems like Stan already have well-tested heuristics for). Our approach generalizes HMC, and is capable of learning proposal distributions that do not correspond to any tuned HMC proposal (but which can still be plugged into the Metropolis-Hastings algorithm to generate a valid MCMC algorithm). Indeed, in our experiments, we find that our approach significantly outperforms well-tuned HMC kernels.\n\nThe training is done during the burn-in phase, and the trained sampler is then frozen. This is a common approach to adapting transition-kernel hyperparameters in the MCMC literature. \n\nRegarding the references, we added those in the text. We also want to emphasize that all of these are complementary to and could be combined with our method. For example, we could incorporate the intuition behind MTM by having several parametric operators and training each one when used. \n\nAdditionally, in the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1513760280210,"tcdate":1513760180687,"number":1,"cdate":1513760180687,"id":"Syp2SsvzG","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Comment","forum":"B1n8LexRZ","replyto":"HJdCshKgf","signatures":["ICLR.cc/2018/Conference/Paper212/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper212/Authors"],"content":{"title":"Expected Square Jump Distance is related to mixing speed","comment":"Thank you very much for your review and comments. Guaranteeing mixing between modes is a fundamental (#P-Hard) problem. As such, we do not hope to solve it in the general case. Rather, we propose a method to greatly increase the flexibility and adaptability of a class of samplers which is already state of the art in many contexts. The relation between mixing time and expected square jump distance is thoroughly treated in [Pasarica & Gelman, 2010], and is the theoretical inspiration for our choice of training loss.\n\nWe further emphasize that, barring optimization issues, our method should always fare at least as well as HMC in terms of mixing.\n\nThank you once again, we have updated the text to more clearly discuss why our approach might be expected to lead to better mixing.\n\nAdditionally, in the process of revisiting our experiments to compare against LAHMC, we empirically found that weighting the second term of our loss (the ‘burn-in’ term) could lead to even more improved auto-correlation and ESS on the diagnostic distributions. We therefore updated the paper and report the results obtained with slightly tuning that parameter (setting it to 0 or 1)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515642409886,"tcdate":1511821833536,"number":3,"cdate":1511821833536,"id":"rkZzfMqef","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Review","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper on fast mixing extension of HMC","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions. These functions are trained to maximize expected squared jump distance.\nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016.\n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases. A few downsides are commented on below.\n\nThe experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. \nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper. I can't see where the number \"124x\" in sec 5.1 stems from. As a user, I would be interested in the typical computational cost of both \"MCMC sampler training\" and MCMC sampler usage (inference?), compared to competing methods. This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation. \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC?\n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC).\n\nThe number \"124x ESS\" in sec5.1 seems at odds with the number in the abstract, \"50x\".\n\n# Minor errors\n- sec1: \"The sampler is trained to minimize a variation\": should be maximize\n\"as well as on a the real-world\"\n- sec3.2 \"and 1/2 v^T v the kinetic\": \"energy\" missing\n- sec4: the acronym L2HMC is not expanded anywhere in the paper\nThe sentence \"We will denote the complete augmented...p(d)\" might be moved to after \"from a uniform distribution\" in the same paragraph. \nIn paragraph starting \"We now update x\":\n    - specify for clarity: \"the first update, which yields x' \"/ \"the second update, which yields x''  \"\n    - \"only affects $x_{\\bar{m}^t}$\": should be $x'_{\\bar{m}^t}$  (prime missing)\n    - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \"mask(x',m^t)\"?\n    - inside zeta_2 and zeta_3, do you not mean $m^t\" and $\\bar{m}^t$ ?\n- sec5: add reference for first mention of \"A NICE MC\"\n- Appendix A: \n    - \"Let's\" -> \"Let\"\n    - eq12 should be x''=...\n- Appendix C: space missing after \"Section 5.1\"\n- Appendix D1: \"In this section is presented\" : sounds odd\n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515642409923,"tcdate":1511799759948,"number":2,"cdate":1511799759948,"id":"HJdCshKgf","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Review","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposed a generalized HMC with neural networks, but it lacks theoretical justifications why it could mix with multiple modes","rating":"6: Marginally above acceptance threshold","review":"The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. The derivations look correct to me. In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1515642409962,"tcdate":1511718322719,"number":1,"cdate":1511718322719,"id":"Hksh6uugz","invitation":"ICLR.cc/2018/Conference/-/Paper212/Official_Review","forum":"B1n8LexRZ","replyto":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference/Paper212/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice idea; but the paper could be improved.","rating":"7: Good paper, accept","review":"In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance). In the literature, specially in machine learning, there is ``fever’’ about HMC, in my opinion, partially unjustified.\n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point.\n\nHowever, I have other additional comments:\n\n- Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader.\n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes\n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques, \n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223–242, April 2001,\n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337–348, 199. \n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]}},{"tddate":null,"ddate":null,"tmdate":1513952925386,"tcdate":1509062227787,"number":212,"cdate":1509739424015,"id":"B1n8LexRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1n8LexRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generalizing Hamiltonian Monte Carlo with Neural Networks","abstract":"We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code will be open-sourced with the camera-ready paper.","pdf":"/pdf/6c1280e1e04b0cd2fe1ab0bc7e6a8265510cbafd.pdf","TL;DR":"General method to train expressive MCMC kernels parameterized with deep neural networks. Given a target distribution p, our method provides a fast-mixing sampler, able to efficiently explore the state space.","paperhash":"anonymous|generalizing_hamiltonian_monte_carlo_with_neural_networks","_bibtex":"@article{\n  anonymous2018generalizing,\n  title={Generalizing Hamiltonian Monte Carlo with Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1n8LexRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper212/Authors"],"keywords":["markov","chain","monte","carlo","sampling","posterior","deep","learning","hamiltonian"]},"nonreaders":[],"replyCount":15,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}