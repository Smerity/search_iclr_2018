{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642411075,"tcdate":1511772630427,"number":3,"cdate":1511772630427,"id":"B10RWItgz","invitation":"ICLR.cc/2018/Conference/-/Paper221/Official_Review","forum":"rybAWfx0b","replyto":"rybAWfx0b","signatures":["ICLR.cc/2018/Conference/Paper221/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper present a simple but effective approach to utilize language model information in a seq2seq framework. The experimental results show improvement for both baseline and adaptation scenarios.\n\nPros:\nThe approach is adapted from deep fusion but the results are promising, especially for the off-domain setup. The analysis also well-motivated about why cold-fusion outperform deep-fusion.\n\nCons:\n(1) I have some question about the baseline. Why the decoder is single layer but for LM it is 2 layer? I suspect the LM may add something to it.  For my own Seq2seq model, 2 layer decoder always better than one. Also, what is HMM/DNN/CTC baseline ? Since they use a internal dataset, it's hard to know how was the seq2seq numbers. The author also didn't compare with re-scoring method.\n\n(2) It would be more interesting to test it on more standard speech corpus, for example, SWB (conversational based) and librispeech (reading task). Then it's easier to reproduce and measure the quality of the model.\n\n(3) This paper only report results on speech recognition. It would be more interesting to test it on more area, e.g. Machine Translation. \n\nMissing citation: In (https://arxiv.org/pdf/1706.02737.pdf) section 3.3, they also pre-trained RNN-LM on more standard speech corpus. Also, need to compare with this type of shallow fusion.\n\nUpdates: \n\nhttps://arxiv.org/pdf/1712.01769.pdf (Google's End2End system) use 2-layer LSTM decoder. \nhttps://arxiv.org/abs/1612.02695,  https://arxiv.org/abs/1707.07413 and https://arxiv.org/abs/1506.07503) are small task. \nBattenberg et al. paper (https://arxiv.org/abs/1707.07413) use Seq2Seq as a baseline and didn't show any combined results of different #decoder layer vs. different LM integration method. My point is how a stronger decoder affect the results with different LM integration methods. In the paper, it still only compared with deep fusion with one decoder layer. \n\nAlso, why it only compared shallow fusion w/ CTC model? I suspect deep decoder + shallow fusion already could provide good results. Or the gain is additive?\n\nThanks a lot adding Librispeech results. But why use Wav2Letter paper (instead of refer to a peer reviewed paper)? The Wav2letter paper didn't compare with any baseline on librispeech (probably because librispeech isn't a common dataset, but at least the Kaldi baseline is there). \n\nIn short, I'm still think this is a good paper but still slightly below the acceptance threshold.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS","abstract":"Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.","pdf":"/pdf/b683baa101e58773bed8d984f95a1fdc7e8c207c.pdf","TL;DR":"We introduce a novel method to train Seq2Seq models with language models that converge faster, generalize better and can almost completely transfer to a new domain using less than 10% of labeled data.","paperhash":"anonymous|cold_fusion_training_seq2seq_models_together_with_language_models","_bibtex":"@article{\n  anonymous2018cold,\n  title={COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybAWfx0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper221/Authors"],"keywords":["Sequence-to-Sequence Models","Speech Recognition","Language Models"]}},{"tddate":null,"ddate":null,"tmdate":1515642411115,"tcdate":1511715865734,"number":2,"cdate":1511715865734,"id":"ryGQ4uugM","invitation":"ICLR.cc/2018/Conference/-/Paper221/Official_Review","forum":"rybAWfx0b","replyto":"rybAWfx0b","signatures":["ICLR.cc/2018/Conference/Paper221/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Better integration of language models into sequence 2 sequence networks.","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new way of integrating a language model into a seq2seq network: instead of adding the language model only during decoding, the model has access to a pretrained language model during training too. This makes the training and testing conditions more similar. Moreover, only the logits of the pretrained language model are used, making it possible to swap language models post-training.\n\nThe experiments show that the proposed language model fusion is effective, and works well even when different, domain-dependent language models are used during training and testing. Further experiments indicate that through the integration of a language model at training time the seq2seq's decoder can be smaller as it is relieved of language modeling.\n\nQuality:\nThe paper is well executed, the experiments do basic validation of the model (ablation plus a specially designed task to show model effectiveness)\n\nClarity:\nWell written, easy to understand.\n\nOriginality:\nThe main idea is new.\n\nSignificance:\nBetter language model integration and easier adaptation to new domains of seq2seq models is important.\n\nPros and cons:\npros : see above\n\ncons:\nMy problem with the paper is lack of experiments on public datasets. The efficacy of the method is shown on only one task on a proprietary corpus engineered for domain mismatch and the method may be not so efficient under other circumstances.  Besides presenting results on publicly available data, the paper would also be improved by adding a baseline in which the logits of the language model are added to the logits of the seq2seq decoder at training time. Similarly to cold-fusion, this baseline also allows swapping of language models at test time. In contrast, the baselines presented in the paper are weaker because they don't use a language model during training time.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS","abstract":"Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.","pdf":"/pdf/b683baa101e58773bed8d984f95a1fdc7e8c207c.pdf","TL;DR":"We introduce a novel method to train Seq2Seq models with language models that converge faster, generalize better and can almost completely transfer to a new domain using less than 10% of labeled data.","paperhash":"anonymous|cold_fusion_training_seq2seq_models_together_with_language_models","_bibtex":"@article{\n  anonymous2018cold,\n  title={COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybAWfx0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper221/Authors"],"keywords":["Sequence-to-Sequence Models","Speech Recognition","Language Models"]}},{"tddate":null,"ddate":null,"tmdate":1515642411152,"tcdate":1511539189778,"number":1,"cdate":1511539189778,"id":"Sy0xMaHlG","invitation":"ICLR.cc/2018/Conference/-/Paper221/Official_Review","forum":"rybAWfx0b","replyto":"rybAWfx0b","signatures":["ICLR.cc/2018/Conference/Paper221/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The paper proposes a novel approach to integrate a language model (LM) to a seq2seq based speech recognition system (ASR). The LM is pretrained on separate data (presumably larger, potentially not the same exact distribution). It has a similar flavor as DeepFusion (DF), a previous work which also integrated an LM to a ASR in a similar way, but where the fusion is also trained. This paper argues this is not good as the ASR decoder and LM are trying to solve the same problem. Instead, ColdFusion first trains the LM, then fixes it and trains the ASR, so it can concentrate on what the LM doesn't do. This makes a lot of sense.\n\nExperiments on private data show that the ColdFusion approach works better than the DeepFusion approach. Sadly these experiments are done on private data and it is thus hard to compare with benchmark models and datasets.\n\nFor instance, it is possible that the relative capacity (number of layers, number of cells, etc) for each of the blocs need to vary differently between the baseline, the ColdFusion approach and the DeepFusion approach. It is hard to say with results on private data only, as it cannot be compared with strong baselines available in the literature.\n\nUnless a second series of experiments on known benchmarks is provided, I cannot propose this paper for acceptance.\n\n***********\nI have read the revised version. I applaud the use of a public dataset to\ndemonstrate some of the results of the new algorithm, and for this I am raising\nmy score. I am concerned, though, that while ColdFusion is indeed better than\nDeepFusion on LibriSpeech, both of them are significantly worse than the\nresults provided by Wav2Letter on word error rates (although better on\ncharacter error rates, which are usually not as important in that literature).\nIs there any reason for this?\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS","abstract":"Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.","pdf":"/pdf/b683baa101e58773bed8d984f95a1fdc7e8c207c.pdf","TL;DR":"We introduce a novel method to train Seq2Seq models with language models that converge faster, generalize better and can almost completely transfer to a new domain using less than 10% of labeled data.","paperhash":"anonymous|cold_fusion_training_seq2seq_models_together_with_language_models","_bibtex":"@article{\n  anonymous2018cold,\n  title={COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybAWfx0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper221/Authors"],"keywords":["Sequence-to-Sequence Models","Speech Recognition","Language Models"]}},{"tddate":null,"ddate":null,"tmdate":1515194162864,"tcdate":1509069256563,"number":221,"cdate":1509739419989,"id":"rybAWfx0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rybAWfx0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS","abstract":"Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.","pdf":"/pdf/b683baa101e58773bed8d984f95a1fdc7e8c207c.pdf","TL;DR":"We introduce a novel method to train Seq2Seq models with language models that converge faster, generalize better and can almost completely transfer to a new domain using less than 10% of labeled data.","paperhash":"anonymous|cold_fusion_training_seq2seq_models_together_with_language_models","_bibtex":"@article{\n  anonymous2018cold,\n  title={COLD FUSION: TRAINING SEQ2SEQ MODELS TOGETHER WITH LANGUAGE MODELS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybAWfx0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper221/Authors"],"keywords":["Sequence-to-Sequence Models","Speech Recognition","Language Models"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}