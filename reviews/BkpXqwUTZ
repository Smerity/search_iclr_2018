{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222585471,"tcdate":1511768765687,"number":3,"cdate":1511768765687,"id":"r18pMHFgM","invitation":"ICLR.cc/2018/Conference/-/Paper18/Official_Review","forum":"BkpXqwUTZ","replyto":"BkpXqwUTZ","signatures":["ICLR.cc/2018/Conference/Paper18/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper claims to work towards a more biological version of error-backpropagation.","rating":"2: Strong rejection","review":"The paper is incomplete and nowhere near finished, it should have been withdrawn. \n\nThe theoretical results are presented in a bitmap figure and only referred to in the text (not explained), and  the results on datasets are not explained either (and pretty bad). A waste of my time.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning","abstract":"In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.","pdf":"/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf","TL;DR":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.","paperhash":"anonymous|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkpXqwUTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper18/Authors"],"keywords":["Iterative temporal differencing","feedback alignment","spike-time dependent plasticity","vanilla backpropagation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222585515,"tcdate":1511729536932,"number":2,"cdate":1511729536932,"id":"rkFKYjdlz","invitation":"ICLR.cc/2018/Conference/-/Paper18/Official_Review","forum":"BkpXqwUTZ","replyto":"BkpXqwUTZ","signatures":["ICLR.cc/2018/Conference/Paper18/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not up to a professional standard.","rating":"2: Strong rejection","review":"The paper falls far short of the standard expected of an ICLR submission. \n\nThe paper has little to no content. There are large sections of blank page throughout. The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description. The experiments are only performed on MNIST. The subfigures are not labeled. The paper over-uses acronyms; sentences like “In this figure, VBP, VBP with FBA, and ITD using FBA for VBP…” are painful to read. \n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning","abstract":"In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.","pdf":"/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf","TL;DR":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.","paperhash":"anonymous|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkpXqwUTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper18/Authors"],"keywords":["Iterative temporal differencing","feedback alignment","spike-time dependent plasticity","vanilla backpropagation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222585552,"tcdate":1511630254199,"number":1,"cdate":1511630254199,"id":"rJUnrQDlG","invitation":"ICLR.cc/2018/Conference/-/Paper18/Official_Review","forum":"BkpXqwUTZ","replyto":"BkpXqwUTZ","signatures":["ICLR.cc/2018/Conference/Paper18/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Not clear what exactly the authors want to achieve","rating":"3: Clear rejection","review":"- This paper is not well written and incomplete. There is no clear explanation of what exactly the authors want to achieve in the paper, what exactly is their approach/contribution, experimental setup, and analysis of their results. \n\n- The paper is hard to read due to many abbreviations, e.g., the last paragraph in page 2. \n\n- The format is inconsistent. Section 1 is numbered, but not the other sections. \n\n- in page 2, what do the numbers mean at the end of each sentence? Probably the figures? \n\n- in page 2, \"in this figure\": which figure is this referring to?\n\n\nComments on prior work:\n\np 1: authors write: \"vanilla backpropagation (VBP)\" \"was proposed around 1987 Rumelhart et al. (1985).\" \n\nNot true. A main problem with the 1985 paper is that it does not cite the inventors of backpropagation. The VBP that everybody is using now is the one published by  Linnainmaa in 1970, extending Kelley's work of 1960. The first to publish the application of VBP to NNs was Werbos in 1982. Please correct. \n\np 1: authors write: \"Almost at the same time, biologically inspired convolutional networks was also introduced as well using VBP LeCun et al. (1989).\"\n\nHere one must cite the person who really invented this biologically inspired convolutional architecture (but did not apply backprop to it): Fukushima (1979). He is cited later, but in a misleading way. Please correct.\n\np 1: authors write: \"Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012).\" \n\nNot true. Deep Learning was introduced by Ivakhnenko and Lapa in 1965: the first working method for learning in multilayer perceptrons of arbitrary depth. Please correct. (The term \"deep learning\" was introduced to ML in 1986 by Dechter for something else.)\n\np1: authors write: \"Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015).\" \n\nHighway networks were published half a year earlier than resnets, and reached many hundreds of layers before resnets. Please correct.\n\n\nGeneral recommendation: Clear rejection for now. But perhaps the author want to resubmit this to another conference, taking into account the reviewer comments.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning","abstract":"In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.","pdf":"/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf","TL;DR":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.","paperhash":"anonymous|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkpXqwUTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper18/Authors"],"keywords":["Iterative temporal differencing","feedback alignment","spike-time dependent plasticity","vanilla backpropagation","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739528101,"tcdate":1508436516724,"number":18,"cdate":1509739525441,"id":"BkpXqwUTZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkpXqwUTZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning","abstract":"In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability.\nVanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL).\nThis work shows that a differentiable activation function is not necessary any more for error backpropagation. \nThe derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA).\nUsing FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures.\nWe don't claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.","pdf":"/pdf/775ad0b287f599d4744d54c1a7e2189ee66e40be.pdf","TL;DR":"Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning.","paperhash":"anonymous|iterative_temporal_differencing_with_fixed_random_feedback_alignment_support_spiketime_dependent_plasticity_in_vanilla_backpropagation_for_deep_learning","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative temporal differencing with fixed random feedback alignment support spike-time dependent plasticity in vanilla backpropagation for deep learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkpXqwUTZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper18/Authors"],"keywords":["Iterative temporal differencing","feedback alignment","spike-time dependent plasticity","vanilla backpropagation","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}