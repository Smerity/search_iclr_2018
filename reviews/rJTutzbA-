{"notes":[{"tddate":null,"ddate":null,"tmdate":1515171195931,"tcdate":1515171195931,"number":4,"cdate":1515171195931,"id":"SkEtTX6Xz","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Comment","forum":"rJTutzbA-","replyto":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference/Paper890/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper890/Authors"],"content":{"title":"list of changes made to the manuscript","comment":"We group the list of changes made to the manuscript based on suggestions of reviewers:\n\nAnonReviewer 3:\n- Added a paragraph on accelerated and fast methods for finite sums and their implications in the deep learning context. (in related work)\n\nAnonReviewer 2:\n- Included reference on Acceleration for simple non-smooth problems. (in page 1)\n- Included reference on Accelerated SVRG and other suggested references. (in related work)\n- Fixed citations for pytorch/download links and fixed typos.\n\nAnonReviewer 1:\n- Added a paragraph on entropic sgd and path normalized sgd and their complimentary nature compared to this work's message (in related work section).\n\nOther changes:\n- In the related work: background about Stochastic Heavy Ball, adding references addressing reviewer feedback.\n- Removed statement on generalization/batch size. (page 2)\n- Fixed minor typos. (page 3)\n- Added comment about NAG lower bound conjecture. (page 4, below proposition 3)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1513785649856,"tcdate":1513785649856,"number":3,"cdate":1513785649856,"id":"BJqEtWdMf","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Comment","forum":"rJTutzbA-","replyto":"Sy2Sc4CWz","signatures":["ICLR.cc/2018/Conference/Paper890/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper890/Authors"],"content":{"title":"Rebuttal","comment":"Thanks for the references, we have included them in the paper and added a paragraph in Section 6 providing detailed comparison and key differences that we summarize below: \n \nASDCA, Katyusha, accelerated SVRG: these methods are \"offline\" stochastic algorithms that is they require  multiple passes over the data and require multiple rounds of full gradient computation (over the entire training data). In contrast, ASGD is a single pass algorithm and requires gradient computation only a single data point at a time step. In the context of deep learning, this is a critical difference, as computing gradient over entire training data can be extremely slow. See Frostig, Ge, Kakade, Sidford ``Competing with the ERM in a single pass\" (https://arxiv.org/pdf/1412.6606.pdf) for a more detailed discussion on online vs offline stochastic methods. \n\nMoreover, the rate of convergence of the ASDCA depend on \\sqrt{\\kappa n} while the method studied in this paper has \\sqrt{\\kappa \\tilde{kappa}} dependence where \\tilde{kappa} can be much smaller than n. \n\n\n\n\n\n\n\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1513785518522,"tcdate":1513785518522,"number":2,"cdate":1513785518522,"id":"SyL2ub_fM","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Comment","forum":"rJTutzbA-","replyto":"Sk0uMIqef","signatures":["ICLR.cc/2018/Conference/Paper890/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper890/Authors"],"content":{"title":"Rebuttal","comment":"Thanks for your comments. \n\nWe have cited Entropy SGD and Path SGD papers and discuss the differences in Section 6 (related works). However, both the methods are complementary to our method. \n\nEntropy SGD adds a local strong convexity term to the objective function to improve generalization. However, currently we do not understand convergence rates or generalization performance of the technique rigorously, even for convex problems. The paper proposes to use SGD to optimize the altered objective function and mentions that one can use SGD+momentum as well (below algorithm box on page 6). Naturally, one can use the ASGD method as well to optimize the proposed objective function in the paper. \n\nPath SGD uses a modified SGD like update to ensure invariance to the scale of the data. Here again, the main goal is orthogonal to our work and one can easily use ASGD method in the same framework. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1513785422941,"tcdate":1513785422941,"number":1,"cdate":1513785422941,"id":"rkv8dZ_fz","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Comment","forum":"rJTutzbA-","replyto":"Sy3aR8wxz","signatures":["ICLR.cc/2018/Conference/Paper890/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper890/Authors"],"content":{"title":"Rebuttal","comment":"Thanks a lot for insightful comments.  We have updated the paper taking into account several of your comments. We will make more updates according to your suggestions. \n\n\nPaper organization: we will try to better organize the paper to highlight the contributions. \nProposition 3's importance: yes, your assessment is spot on.\n\nMinor comment 1,2: Thanks for pointing the minor mistake, we have updated the corresponding lines. Papers such as Accelerated SVRG, Recht et al. are offline stochastic accelerated methods. The paper of Richtarik (arXiv:1706.01108) deals with solving consistent linear systems in the offline setting; (arXiv:1710.10737) is certainly relevant and we will add more detailed comparison with this line of work. \nMinor comment 3, 5: thanks for pointing out  the typos. They are fixed. \nMinor comment 4: Actually, the problem is a discrete problem where one observes one hot vectors in 2-dimensions, each of the vectors can occur with probability 1/2. So this is the reason why the Hessian does not carry an added factor of 2.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1515642527798,"tcdate":1513142852251,"number":3,"cdate":1513142852251,"id":"Sy2Sc4CWz","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Review","forum":"rJTutzbA-","replyto":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference/Paper890/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Accept","rating":"8: Top 50% of accepted papers, clear accept","review":"I only got access to the paper after the review deadline; and did not have a chance to read it until now. Hence the lateness and brevity.\n\nThe paper is reasonably well written, and tackles an important problem. I did not check the mathematics. \n\nBesides the missing literature mentioned by other reviewers (all directly relevant to the current paper), the authors should also comment on the availability of accelerated methods inn the finite sum / ERM setting. There, the questions this paper is asking are resolved, and properly modified stochastic methods exist which offer acceleration over SGD (and not through minibatching). This paper does not comment on these developments. Look at accelerated SDCA (APPROX, ASDCA), accelerated SVRG (Katyusha) and so on.\n\nProvided these changes are made, I am happy to suggest acceptance.\n\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1515642527836,"tcdate":1511838326313,"number":2,"cdate":1511838326313,"id":"Sk0uMIqef","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Review","forum":"rJTutzbA-","replyto":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference/Paper890/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper, accept","rating":"7: Good paper, accept","review":"I wonder how the ASGD compares to other optimization schemes applicable to DL, like Entropy-SGD, which is yet another algorithm that provably improves over SGD. This question is also valid when it comes to other optimization schemes that are designed for deep learning problems. For instance, Entropy-SGD and Path-SGD should be mentioned and compared with. As a consequence, the literature analysis is insufficient. \n\nAuthors provided necessary clarifications. I am raising my score.\n\n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1515642527873,"tcdate":1511644867830,"number":1,"cdate":1511644867830,"id":"Sy3aR8wxz","invitation":"ICLR.cc/2018/Conference/-/Paper890/Official_Review","forum":"rJTutzbA-","replyto":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference/Paper890/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice idea, Like the paper","rating":"7: Good paper, accept","review":"I like the idea of the paper. Momentum and accelerations are proved to be very useful both in deterministic and stochastic optimization. It is natural that it is understood better in the deterministic case. However, this comes quite naturally, as deterministic case is a bit easier ;) Indeed, just recently people start looking an accelerating in stochastic formulations. There is already accelerated SVRG, Jain et al 2017, or even Richtarik et al (arXiv: 1706.01108, arXiv:1710.10737).\n\nI would somehow split the contributions into two parts:\n1) Theoretical contribution: Proposition 3 (+ proofs in appendix)\n2) Experimental comparison.\n\nI like the experimental part (it is written clearly, and all experiments are described in a lot of detail).\n\nI really like the Proposition 3 as this is the most important contribution of the paper. (Indeed, Algorithms 1 and 2 are for reference and Algorithm 3 was basically described in Jain, right?). \n\nSignificance: I think that this paper is important because it shows that the classical HB method cannot achieve acceleration in a stochastic regime.\n\nClarity: I was easy to read the paper and understand it.\n\nFew minor comments:\n1. Page 1, Paragraph 1: It is not known only for smooth problems, it is also true for simple non-smooth (see e.g. https://link.springer.com/article/10.1007/s10107-012-0629-5)\n2. In abstract : Line 6 - not completely true, there is accelerated SVRG method, i.e. the gradient is not exact there, also see Recht (https://arxiv.org/pdf/1701.03863.pdf) or Richtarik et al (arXiv: 1706.01108, arXiv:1710.10737) for some examples where acceleration can be proved when you do not have an exact gradient.\n3. Page 2, block \"4\" missing \".\" in \"SGD We validate\"....\n4. Section 2. I think you are missing 1/2 in the definition of the function. Otherwise, you would have a constant \"2\" in the Hessian, i.e. H= 2 E[xx^T]. So please define the function as  f_i(w) = 1/2 (y - <w,x_i>)^2. The same applies to Section 3.\n5. Page 6, last line, .... was downloaded from \"pre\". I know it is a link, but when printed, it looks weird. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1515186202368,"tcdate":1509136757090,"number":890,"cdate":1510092362740,"id":"rJTutzbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJTutzbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the insufficiency of existing momentum schemes for Stochastic Optimization","abstract":"Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Theoretically, these ```fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there are simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching.\n\nFurthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Accelerated Gradient Descent. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD.\n","pdf":"/pdf/22dccb2e6934953ab64176a9d8bc0cbba6676a38.pdf","TL;DR":"Existing momentum/acceleration schemes such as heavy ball method and Nesterov's acceleration employed with stochastic gradients do not improve over vanilla stochastic gradient descent, especially when employed with small batch sizes.","paperhash":"anonymous|on_the_insufficiency_of_existing_momentum_schemes_for_stochastic_optimization","_bibtex":"@article{\n  anonymous2018on,\n  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJTutzbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper890/Authors"],"keywords":["Stochastic Gradient Descent","Deep Learning","Momentum","Acceleration","Heavy Ball","Nesterov Acceleration","Stochastic Optimization","SGD","Accelerated Stochastic Gradient Descent"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}