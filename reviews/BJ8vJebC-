{"notes":[{"tddate":null,"ddate":null,"tmdate":1512439293564,"tcdate":1512439293564,"number":1,"cdate":1512439293564,"id":"rJIbAd7-z","invitation":"ICLR.cc/2018/Conference/-/Paper543/Public_Comment","forum":"BJ8vJebC-","replyto":"BJ8vJebC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Better suited to NLP conference","comment":"The paper points out the lack of robustness of character based models and explores a few, very basic solutions, none of which are effective. While starting a discussion around this problem is valuable, the paper provides no actually working solutions, and the solutions explored are very basic from a machine learning point of view. This publication is better suited to a traditional NLP venue such as ACL/EMNLP."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Synthetic and Natural Noise Both Break Neural Machine Translation","abstract":"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ","pdf":"/pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf","TL;DR":"CharNMT is brittle","paperhash":"anonymous|synthetic_and_natural_noise_both_break_neural_machine_translation","_bibtex":"@article{\n  anonymous2018synthetic,\n  title={Synthetic and Natural Noise Both Break Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8vJebC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper543/Authors"],"keywords":["neural machine translation","characters","noise","adversarial examples","robust training"]}},{"tddate":null,"ddate":null,"tmdate":1512222682477,"tcdate":1511977464510,"number":3,"cdate":1511977464510,"id":"SkeZfu2xG","invitation":"ICLR.cc/2018/Conference/-/Paper543/Official_Review","forum":"BJ8vJebC-","replyto":"BJ8vJebC-","signatures":["ICLR.cc/2018/Conference/Paper543/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting study of the impact of noisy text on MT quality","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust.\n\nOverall the paper is a clearly written, well described report of several experiments. It shows convincingly that standard NMT models completely break down on both natural \"noise\" and various types of input perturbations. It then tests how the addition of noise in the input helps robustify the charCNN model somewhat. The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations.\n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community. The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps) but the impact may be. I wonder if you could put this in the context of \"training with input noise\", which has been studied in Neural Network for a while (at least since the 1990s). I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise. Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet.\n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \"noisy\" training set and adding that to clean data? Or introducing noise \"on-line\" as part of the training? If fixed, what sizes were tried? More information on the experimental design would help.\n\nTable 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the \"Rand\" line for German, or the Swap/Mid/Rand lines for Czech. It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance.\n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Synthetic and Natural Noise Both Break Neural Machine Translation","abstract":"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ","pdf":"/pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf","TL;DR":"CharNMT is brittle","paperhash":"anonymous|synthetic_and_natural_noise_both_break_neural_machine_translation","_bibtex":"@article{\n  anonymous2018synthetic,\n  title={Synthetic and Natural Noise Both Break Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8vJebC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper543/Authors"],"keywords":["neural machine translation","characters","noise","adversarial examples","robust training"]}},{"tddate":null,"ddate":null,"tmdate":1512222682515,"tcdate":1511821126158,"number":2,"cdate":1511821126158,"id":"SkABkz5gM","invitation":"ICLR.cc/2018/Conference/-/Paper543/Official_Review","forum":"BJ8vJebC-","replyto":"BJ8vJebC-","signatures":["ICLR.cc/2018/Conference/Paper543/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good work on an under-studied problem","rating":"7: Good paper, accept","review":"This paper investigates the impact of character-level noise on various flavours of neural machine translation. It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that using averaged unigram character embeddings as word representations on the source side. The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables. They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data. Unfortunately, they are not able to show any types of synthetic noise helping address natural noise. However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise.\n\nThis is a thorough exploration of a mostly under-studied problem. The paper is well-written and easy to follow. The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty. The inclusion of so many character-based systems is very nice, but it is the conclusion of natural sources of noise that really makes the paper work. Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others. In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT. The proposed meanChar architecture doesn’t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn’t extremely satisfying. Furthermore, the use of these replacement tables means that even when the noise is natural, it’s still kind of artificial. Finally, this paper doesn’t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference.\n\nRegarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial. First of all, errors learned from the noisy data sources are constrained to exist within a word. This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words. Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data. This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur). I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.\n\nAlso, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6). I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner. On a related note, the line in the abstract stating that “... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise” implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren’t given the chance.\n\nSection 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an ICLR audience. From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies.\n\nI have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Synthetic and Natural Noise Both Break Neural Machine Translation","abstract":"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ","pdf":"/pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf","TL;DR":"CharNMT is brittle","paperhash":"anonymous|synthetic_and_natural_noise_both_break_neural_machine_translation","_bibtex":"@article{\n  anonymous2018synthetic,\n  title={Synthetic and Natural Noise Both Break Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8vJebC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper543/Authors"],"keywords":["neural machine translation","characters","noise","adversarial examples","robust training"]}},{"tddate":null,"ddate":null,"tmdate":1512222682557,"tcdate":1511818075683,"number":1,"cdate":1511818075683,"id":"BkVD7bqlf","invitation":"ICLR.cc/2018/Conference/-/Paper543/Official_Review","forum":"BJ8vJebC-","replyto":"BJ8vJebC-","signatures":["ICLR.cc/2018/Conference/Paper543/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice empirical paper on robustness of NMT","rating":"7: Good paper, accept","review":"This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. The results are not surprising:\n\n* NMT is terrible with noise.\n\n* But it improves on each noise type when it is trained on that noise type.\n\nWhat I like about this paper is that:\n\n1) The experiments are very carefully designed and thorough.\n\n2) This problem might actually matter. Out of curiosity, I ran the example (Table 4) through Google Translate, and the result was gibberish. But as the paper shows, it’s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow. So this paper could have real-world impact.\n\n3) Most importantly, it shows that NMT’s handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different. So solving the problem of natural noise is not so simple… it’s a *real* problem. Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale. So these methods could be applied in the real world. (It would be excellent if an outcome of this paper was that commercial MT providers answered it’s call to provide more realistic noise by actually providing examples.)\n\nThere are no fancy new methods or state-of-the-art numbers in this paper. But it’s careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Synthetic and Natural Noise Both Break Neural Machine Translation","abstract":"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ","pdf":"/pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf","TL;DR":"CharNMT is brittle","paperhash":"anonymous|synthetic_and_natural_noise_both_break_neural_machine_translation","_bibtex":"@article{\n  anonymous2018synthetic,\n  title={Synthetic and Natural Noise Both Break Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8vJebC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper543/Authors"],"keywords":["neural machine translation","characters","noise","adversarial examples","robust training"]}},{"tddate":null,"ddate":null,"tmdate":1509739245116,"tcdate":1509125981837,"number":543,"cdate":1509739242464,"id":"BJ8vJebC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ8vJebC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Synthetic and Natural Noise Both Break Neural Machine Translation","abstract":"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.  Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise. ","pdf":"/pdf/119672f6a6a0b3fe32fdf19ddee2a270ddebfd20.pdf","TL;DR":"CharNMT is brittle","paperhash":"anonymous|synthetic_and_natural_noise_both_break_neural_machine_translation","_bibtex":"@article{\n  anonymous2018synthetic,\n  title={Synthetic and Natural Noise Both Break Neural Machine Translation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ8vJebC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper543/Authors"],"keywords":["neural machine translation","characters","noise","adversarial examples","robust training"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}