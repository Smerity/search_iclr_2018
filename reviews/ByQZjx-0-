{"notes":[{"tddate":null,"ddate":null,"tmdate":1512453557142,"tcdate":1512453557142,"number":4,"cdate":1512453557142,"id":"H1ThH37-f","invitation":"ICLR.cc/2018/Conference/-/Paper619/Public_Comment","forum":"ByQZjx-0-","replyto":"rJaD7VugM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comparison with baseline model","comment":"Doesn't the result of Sanity Check with Ablation Study section imply that REINFORCE successfully learned a competitive architecture from all the possible ones? I think the resulting performance being not much better than that of the baseline is because of the chosen search space. If they adapted the search space of \"Learning Transferable ...\" by Zoph et. al., they would be able to achieve a comparable performance given they used PPO instead, since they achieved the performance comparable to that of NAS by Zoph & Le. I think it's bit too harsh to give 4 for a paper that reduced the computation cost of NAS to 1/100. SMASH achieved much higher score from the reviewers, but they achieved a similar performance. It relies on parameter sharing assumption as well, and they demonstrated that the assumption is valid in their case and therefore reasonable to assume for similar cases. Since the author of ENAS cited SMASH paper, I don't think they have to mention the assumption. You claim that many of the details of the controller are obscure, but we, a third party, didn't experience much difficulty in implementing this algorithm for CNN part after asking a few questions below. So, I'd argue that just a few are obscure, which happens among successful papers as well. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512152503754,"tcdate":1512126689821,"number":3,"cdate":1512126689821,"id":"By51thRez","invitation":"ICLR.cc/2018/Conference/-/Paper619/Public_Comment","forum":"ByQZjx-0-","replyto":"ByQZjx-0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Did you try updating omega and theta alternately for each iteration rather than for each epoch?","comment":"After an epoch of omega updates, theta updates were performed. Did you alternate more frequently to see what happens?\n\nWhat if you replace the reward with loss function or some combination of loss function and classification accuracy?\n\nIf you apply ENAS longer, say for 3 days, will the resulting performance be better? Did you find the performance to converge around the 300 epochs? \n\nAfter the 300 epochs, you sampled many architectures and determined the one with the highest reward to be one trained from scratch. How many architecture samples did you find to be enough (for convergence)? "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222702385,"tcdate":1511805699214,"number":3,"cdate":1511805699214,"id":"Bkj-7CYef","invitation":"ICLR.cc/2018/Conference/-/Paper619/Official_Review","forum":"ByQZjx-0-","replyto":"ByQZjx-0-","signatures":["ICLR.cc/2018/Conference/Paper619/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Improving Neural Architecture Search by Parameter Sharing","rating":"5: Marginally below acceptance threshold","review":"In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing. In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks. In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric. Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice. In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time.\n\nThe authors present two ENAS models: one for CNNs, and another for RNNs. Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections. However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space. This is a limitation, as the model space is not as flexible as one would desire in a discovery task. Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections. Thus, they are essentially learning the skip connections while using a human-selected model. \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections. Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections. Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal.\n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process. Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea. It is also impressive how much faster their model performs on tasks without sacrificing much performance. The main limitation is that the best architectures as currently described are less about discovery and more about human input -- finding a more efficient search path would be an important next step.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222702429,"tcdate":1511699300968,"number":2,"cdate":1511699300968,"id":"rJaD7VugM","invitation":"ICLR.cc/2018/Conference/-/Paper619/Official_Review","forum":"ByQZjx-0-","replyto":"ByQZjx-0-","signatures":["ICLR.cc/2018/Conference/Paper619/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice idea, but the paper suffers from unclear presentation and the empirical results are not convincing enough.","rating":"4: Ok but not good enough - rejection","review":"Summary: \nThe paper presents a method for learning certain aspects of a neural network architecture, specifically the number of output maps in certain connections and the existence of skip connections. The method is relatively efficient, since it searches in a space of similar architectures, and uses weights sharing between the tested models to avoid optimization of each model from scratch. Results are presented for image classification on Cifar 10 and for language modeling.\n\nPage 3: “for each channel, we only predict C/S binary masks”   -> this seems to be a mistake. Probably “for each operation, we only predict C/S binary masks” is the right wording\nPage 4: Stabilizing Stochastic Skip Connections: it seems that the suggested configuration does not enable an identity path, which was found very beneficial in (He. et al., 2016). Identity path does not exist since layers are concatenated and go through 1*1 conv, which does not enable plain identity unless learned by the 1*1 conv.\nPage 5: \n-\tThe last paragraph in section 4.2 is not clear to me. What does a compilation failure mean in this context and why does it occur? And: if each layer is connected to all its previous layers by skip connections, what remains to be learned w.r.t the model structure? Isn’t the pattern of skip connection the thing we would like to learn?\n-\tSome  details of the policy LSTM network are also not clear to me:\no\tHow is the integer mask (output of the B channel steps) encoded? Using 1-hot encoding over 2^{C/S} output neurons? Or maybe C/S output neurons, used for sampling the C/S bits of the mask? this should be reported in some detail.\no\tHow is the mask converted to an input embedding for the next step? Is it by linear multiplication with a matrix? Something more complicated? And are there different matrices used/trained for each mask embedding (one for 1*1 conv, one for 3*3 conv, etc..)?\no\tWhat is the motivation for using equation 5 for the sampling of skip connection flags? What is the motivation for averaging the winning anchors as the average embedding for the next stage (to let it ‘know’ who is connected to the previous?). Is anchor j also added to the average?\no\tHow is equation 5 normalized? That is: the probability is stated to be proportional to an exponent of an inner product, but it is not clear what the constant is and how sampling is done.\n\nPage 6: \n-\t  Section 4.4: what is the fixed policy used for generating models in the stage of training the shared W parameters? (this is answered at page 7\nExperiments:\n-\tThe accuracy figures obtained are impressive, but I’m not convinced the ENAS learning is the important ingredient in obtaining them (rather than a very good baseline)\n-\tSpecifically, in the Cifar -10 example it does not seem that the networks chooses the number of maps in a way which is diverse or different from layer to layer. Therefore we do not have any evidence that the LSTM controller has learnt any interesting rule regarding block type, or relation between block type width and layer index. All we see is that the model does not chose too many maps, thus avoid significant overfit. The relevant baseline here is a model with 64 or 96 maps on each block, each layer.Such a model is likely to do as well as the ENAS model, and can be obtained easily with slight parameter tuning of a single parameter.\n-\t Similarly, I’m not convinced the  skip connection pattern found for Cifar-10 is superior to standard denseNet or Resnet pattern. The found configuration was not compared to these baselines. So again we do not have evidence showing the merit of keeping and tuning many parameters with the RINFORCE\n-\tThe experiments with Penn Treebank are described with too less details: for example, what exactly is the task considered (in terms on input-output mapping), what is the dataset size, etc..\n-\tAlso, for the Penn treebank experiments no baseline is given, so it is not possible to understand if the structure learning here is beneficial. Comparison of the results to an architecture with all skip connections, and with a single skip connection per layer is required to estimate if useful structure is being learnt.\n\nOverall:\n-\tPro: the method gives high accuracy results \n-\tCons: \no\tIt is not clear if the ENAS search is responsible to the results, or just the strong baseline. The advantage of ENAS over plain hyper parameter choosing was not sufficiently established.\no\tThe controller was not presented in a clear enough manner. Many of its details stay obscure.\no\tThe method does not seem to be general. It seems to be limited to choosing a specific set of parameters in very specific scenario (scenario which enable parameter sharing between model. The conditions for this to happen seems to be rather strict, and where not elaborated).\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222702474,"tcdate":1511662733420,"number":1,"cdate":1511662733420,"id":"BkrqNswgf","invitation":"ICLR.cc/2018/Conference/-/Paper619/Official_Review","forum":"ByQZjx-0-","replyto":"ByQZjx-0-","signatures":["ICLR.cc/2018/Conference/Paper619/AnonReviewer1"],"readers":["everyone"],"content":{"title":"an incremental work, but the results are not bad","rating":"6: Marginally above acceptance threshold","review":"In the paper titled \"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model\", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks. The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture. The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback) --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly.\n\nOverall, the paper is well-written. Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly. \n ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511270914689,"tcdate":1511270914689,"number":2,"cdate":1511270914689,"id":"BJi-csZgM","invitation":"ICLR.cc/2018/Conference/-/Paper619/Official_Comment","forum":"ByQZjx-0-","replyto":"rJpVtZbgG","signatures":["ICLR.cc/2018/Conference/Paper619/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper619/Authors"],"content":{"title":"Sharing mask embeddings between operations","comment":"Thanks for the question.\n\nThe input to each time step of the controller RNN at time step t is the embedding of the decision sampled from time step t-1. As there are 2^(C/S)-1 possible decisions at each time step, the embedding matrix has 2^(C/S)-1 rows, which are shared among all 6 operations (1x1_conv, 3x3_conv, 5x5_conv, 7x7_conv, max_p, and avg_p). So to answer your second question, the mask indices from the same embedding matrix are shared among these 6 operations.\n\nMeanwhile, the skip_anchor is treated differently. The input it provides to the next step (which decides the mask for a 1x1_conv) is the mean of the previous anchor steps that get sampled. We describe this in the paragraph right below Equation (5) in the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511229748725,"tcdate":1511229748725,"number":2,"cdate":1511229748725,"id":"rJpVtZbgG","invitation":"ICLR.cc/2018/Conference/-/Paper619/Public_Comment","forum":"ByQZjx-0-","replyto":"ByQZjx-0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"What is the input to each timestep of the controller RNN?","comment":"In Figure 2 of ENAS, what exactly is the input (besides the RNN hidden state) to each timestep of the RNN?\n\nDoes the mask index a row of a different input embedding matrix (1 of 256 rows in 1 of 7 possible 256x64 matrices) (256 is from 2^(C/S)-1) (7 is from the 7 possible layer components) depending on whether the mask from previous timestep output was used to mask a 1x1_conv, 3x3_conv, 5x5_conv, 7x7_conv, max_p, avg_p, or skip_anchor? <--Is this row embedding method the only/correct input to the RNN or is it actually something else?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510149657195,"tcdate":1510149657195,"number":1,"cdate":1510149657195,"id":"rJWmCYxyM","invitation":"ICLR.cc/2018/Conference/-/Paper619/Official_Comment","forum":"ByQZjx-0-","replyto":"r1SrSDy1f","signatures":["ICLR.cc/2018/Conference/Paper619/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper619/Authors"],"content":{"title":"M=1 doesn't work for training policy","comment":"1. We went with REINFORCE for the ease of implementation. We have not tried other methods, such as PPO, TRPO, etc. but we will look into this soon.\n\n2. We tried, and we needed at least M=10 to training the policy π. We suspect this is because the gradient estimated with REINFORCE has a high variance. M=1 works for training omega because every update for omega has its gradient computed on a batch of training example, leading to a smaller variance."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510073660871,"tcdate":1510073660871,"number":1,"cdate":1510073660871,"id":"r1SrSDy1f","invitation":"ICLR.cc/2018/Conference/-/Paper619/Public_Comment","forum":"ByQZjx-0-","replyto":"ByQZjx-0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Is M=1 OK for training policy, too?","comment":"1) Was there any empirical result that made you choose REINFORCE rule instead of using PPO loss function (as in \"Learning Transferable Architectures for Scalable Image Recognition\") for updating theta, which was stated to work better in Zoph et. al. 2017?\n\n2) In the section \"training shared parameter omega,\" it says M=1 is sufficient. Does this apply to \"Training the Policy π\" as well?\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739197987,"tcdate":1509128954972,"number":619,"cdate":1509739195325,"id":"ByQZjx-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByQZjx-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model","abstract":"We propose an approach for automatic model designing, which is significantly faster and less expensive than previous methods. In our method, which we name Efficient Neural Architecture Search (ENAS), a controller learns to discover neural architectures by searching for an optimal path within a larger predetermined model. The parameters of the predetermined model are trained to minimize a canonical loss function, such as the cross entropy, on the training dataset. The controller learns the path with policy gradient to maximize the expected reward on the validation set. In our experiments, ENAS achieves comparable test accuracy while being 10x faster and requiring 100x less resources than NAS. On the CIFAR-10 dataset, ENAS can design novel architectures that achieve the test error of 3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017). On the Penn Treebank dataset, ENAS also discovers a novel architecture, which achieves the test perplexity of 64.6 compared to 62.4 by standard NAS.","pdf":"/pdf/faf198766f34fb01ae8a4471d71aa526e8e92291.pdf","paperhash":"anonymous|faster_discovery_of_neural_architectures_by_searching_for_paths_in_a_large_model","_bibtex":"@article{\n  anonymous2018faster,\n  title={Faster Discovery of Neural Architectures by Searching for Paths in a Large Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQZjx-0-}\n}","keywords":["neural architecture search"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper619/Authors"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}