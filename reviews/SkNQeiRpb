{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222536911,"tcdate":1511897746000,"number":3,"cdate":1511897746000,"id":"rk5q9Vixz","invitation":"ICLR.cc/2018/Conference/-/Paper101/Official_Review","forum":"SkNQeiRpb","replyto":"SkNQeiRpb","signatures":["ICLR.cc/2018/Conference/Paper101/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Thorough experimental paper improving Netflix RMSE.","rating":"6: Marginally above acceptance threshold","review":"In this paper the authors present a model for more accurate Netflix recommendations (rating predictions, RMSE).  In particular, the authors demonstrate that a deep autoencoder, carefully tuned, can out-perform  more complex RNN-based models that have temporal information.  The authors examine how different non-linear activations, model size, dropout, and a novel technique called \"dense re-feeding\" can together improve DNN-based collaborative filtering.\n\nPros:\n- The accuracy results are impressive and a useful datapoint in how to build a DNN-based recommender.  \n- The dense re-feeding technique seems to be novel with incremental (but meaningful) benefits.  \n\nCons:\n- Experimental results on only one dataset. \n- Difficult to know if the results are generalizable.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Deep AutoEncoders for Recommender Systems","abstract":"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training. We empirically demonstrate that: a) deep autoencoder models generalize much better than the shallow ones, b) non-linear activation functions with negative parts are crucial for training deep models, and c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting. We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering. The new algorithm significantly speeds up training and improves model performance. Our code is publicly available.","pdf":"/pdf/26410a13f4bfc8964d2b92664161e00e4401b6f1.pdf","TL;DR":"This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set.","paperhash":"anonymous|training_deep_autoencoders_for_recommender_systems","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Deep AutoEncoders for Recommender Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkNQeiRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper101/Authors"],"keywords":["autoencoder","recommendations","collaborative filtering","selu"]}},{"tddate":null,"ddate":null,"tmdate":1512222536950,"tcdate":1511839271663,"number":2,"cdate":1511839271663,"id":"r1eVIIqgM","invitation":"ICLR.cc/2018/Conference/-/Paper101/Official_Review","forum":"SkNQeiRpb","replyto":"SkNQeiRpb","signatures":["ICLR.cc/2018/Conference/Paper101/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is more like a technical report rather than a research paper","rating":"3: Clear rejection","review":"This paper proposed to use deep AE to do rating prediction tasks in recommender systems.\nSome of the conclusions of the paper, e.g. deep models perform bettern than shallow ones, the non-linear activation\nfunction is important, dropout is necessary to prevent overfitting, are well known, and hence is of less novelty.\nThe proposed re-feeding algorithm to overcome natural sparseness of CF is interesting, however, I don't think it is enough to support being accepted by ICLR. \nSome reference about rating prediction are missing, such as \"A neural autoregressive approach to collaborative filtering, ICML2016\". And it would be better to show the performance of the model on implicit rating data, since it is more desirable in practice, since many industry applications have only implicit rating (e.g. whether the user watches the movie or not.).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Deep AutoEncoders for Recommender Systems","abstract":"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training. We empirically demonstrate that: a) deep autoencoder models generalize much better than the shallow ones, b) non-linear activation functions with negative parts are crucial for training deep models, and c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting. We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering. The new algorithm significantly speeds up training and improves model performance. Our code is publicly available.","pdf":"/pdf/26410a13f4bfc8964d2b92664161e00e4401b6f1.pdf","TL;DR":"This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set.","paperhash":"anonymous|training_deep_autoencoders_for_recommender_systems","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Deep AutoEncoders for Recommender Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkNQeiRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper101/Authors"],"keywords":["autoencoder","recommendations","collaborative filtering","selu"]}},{"tddate":null,"ddate":null,"tmdate":1512222536991,"tcdate":1511591834704,"number":1,"cdate":1511591834704,"id":"S17oyqIgM","invitation":"ICLR.cc/2018/Conference/-/Paper101/Official_Review","forum":"SkNQeiRpb","replyto":"SkNQeiRpb","signatures":["ICLR.cc/2018/Conference/Paper101/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lacks novelty and please no more rating predictions","rating":"4: Ok but not good enough - rejection","review":"This paper presents a deep autoencoder model for rating prediction. The autoencoder takes the user’s rating over all the items as input and tries to predict the observed ratings in the output with mean squared error. A few techniques are applied to make the training feasible without layer-wise pre-training: 1) SELU activation. 2) dropout with high probability. 3) dense output re-feeding. On the Netflix prize dataset, the proposed deep autoencoder outperforms other state-of-the-art approaches. \n\nOverall, the paper is easy to follow. However, I have three major concerns regarding the paper that makes me decide to reject it.\n\n1. Lack of novelty. The paper is essentially a deeper version of the U-AutoRec (Sedhain et al. 2015) with a few recently emerged innovations in deep learning. The dense output re-feeding is not something particularly novel, it is more or less a data-imputation procedure with expectation-maximization — in fact if the authors intend to seek explanation for this output re-feeding technique, EM might be one of the interpretations. And similar technique (more theoretically grounded) has been applied in image imputation for variational autoencoder (Rezende et al. 2014, Stochastic Backpropagation and Approximate Inference in Deep Generative Models). \n\n2. The experimental setup is also worth questioning. Using a time-split dataset is of course more challenging. However, the underlying assumption of autoencoders (or more generally, latent factor models like matrix factorization) is the all the ratings are exchangeable (conditionally independent given the latent representations), i.e., autoencoders/MF are not capable of inferring the temporal information from the data, Thus it is not even a head-to-head comparison with a temporal model (e.g., RNN in Wu et al. 2017). Of course you can still apply a static autoencoder to time-split data, but what ends up happening is the model will use its capacity to try to explain the temporal signal in the data — a deeper model certainly has more extra capacity to do so. I would suggest the authors comparing on a non-time-split dataset with other static models, like I(U)-AutoRec/MF/CF-NADE (Zheng et al. 2016)/etc.  \n\n3. Training deep models on recommender systems data is impressive. However, I would like to suggest we, as a community, start to step away from the task of rating predictions as much as we can, especially in more machine-learning-oriented venues (NIPS, ICML, ICLR, etc.) where the reviewers might be less aware of the shift in recommender systems research. (The task of rating predictions was made popular mostly due to the Netflix prize, yet even Netflix itself has already moved on from ratings.) Training (and evaluating) with RMSE on the observed ratings assumes all the missing ratings are missing at random, which is clearly far from realistic for recommender systems (see Marlin et al. 2007, Collaborative Filtering and the Missing at Random Assumption). In fact, understanding why some of the ratings are missing presents a unique challenge for the recommender systems. See, e.g., Steck 2010, Training and testing of recommender systems on data missing not at random, Liang et al. 2016, Modeling user exposure in recommendation, Schnabel et al. 2016, Recommendations as Treatments: Debiasing Learning and Evaluation. A model with good RMSE in a lot of cases does not translate to good recommendations (Cremonesi et al. 2010, Performance of recommender algorithms on top-n recommendation tasks\n). As a first step, at least start to use all the 0’s in the form of implicit feedback and focus on ranking-based metrics other than RMSE. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Deep AutoEncoders for Recommender Systems","abstract":"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training. We empirically demonstrate that: a) deep autoencoder models generalize much better than the shallow ones, b) non-linear activation functions with negative parts are crucial for training deep models, and c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting. We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering. The new algorithm significantly speeds up training and improves model performance. Our code is publicly available.","pdf":"/pdf/26410a13f4bfc8964d2b92664161e00e4401b6f1.pdf","TL;DR":"This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set.","paperhash":"anonymous|training_deep_autoencoders_for_recommender_systems","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Deep AutoEncoders for Recommender Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkNQeiRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper101/Authors"],"keywords":["autoencoder","recommendations","collaborative filtering","selu"]}},{"tddate":null,"ddate":null,"tmdate":1511087201931,"tcdate":1511087201931,"number":1,"cdate":1511087201931,"id":"HkcDh0A1M","invitation":"ICLR.cc/2018/Conference/-/Paper101/Public_Comment","forum":"SkNQeiRpb","replyto":"SkNQeiRpb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"As far as I know, baselines such as PMF (without biased term) performs very poorly in rating prediction task. Biased matrix factorization is a very solid rating prediction baseline.","comment":"It can be easily verified by using open source toolkit, such as librec, MyMediaLite, etc."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Deep AutoEncoders for Recommender Systems","abstract":"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training. We empirically demonstrate that: a) deep autoencoder models generalize much better than the shallow ones, b) non-linear activation functions with negative parts are crucial for training deep models, and c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting. We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering. The new algorithm significantly speeds up training and improves model performance. Our code is publicly available.","pdf":"/pdf/26410a13f4bfc8964d2b92664161e00e4401b6f1.pdf","TL;DR":"This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set.","paperhash":"anonymous|training_deep_autoencoders_for_recommender_systems","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Deep AutoEncoders for Recommender Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkNQeiRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper101/Authors"],"keywords":["autoencoder","recommendations","collaborative filtering","selu"]}},{"tddate":null,"ddate":null,"tmdate":1509739483946,"tcdate":1508974619614,"number":101,"cdate":1509739481292,"id":"SkNQeiRpb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkNQeiRpb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Training Deep AutoEncoders for Recommender Systems","abstract":"This paper proposes a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. Our model is based on deep autoencoder with 6 layers and is trained end-to-end without any layer-wise pre-training. We empirically demonstrate that: a) deep autoencoder models generalize much better than the shallow ones, b) non-linear activation functions with negative parts are crucial for training deep models, and c) heavy use of regularization techniques such as dropout is necessary to prevent over-fitting. We also propose a new training algorithm based on iterative output re-feeding to overcome natural sparseness of collaborate filtering. The new algorithm significantly speeds up training and improves model performance. Our code is publicly available.","pdf":"/pdf/26410a13f4bfc8964d2b92664161e00e4401b6f1.pdf","TL;DR":"This paper demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set.","paperhash":"anonymous|training_deep_autoencoders_for_recommender_systems","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Deep AutoEncoders for Recommender Systems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkNQeiRpb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper101/Authors"],"keywords":["autoencoder","recommendations","collaborative filtering","selu"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}