{"notes":[{"tddate":null,"ddate":null,"tmdate":1512362400858,"tcdate":1512360095373,"number":3,"cdate":1512360095373,"id":"ByDj_rG-z","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Comment","forum":"r1kP7vlRb","replyto":"HJ2pirpxG","signatures":["ICLR.cc/2018/Conference/Paper289/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper289/Authors"],"content":{"title":"Reply","comment":"Thanks for the review.\nYour first paragraph of the review well summarizes our paper. Our paper is seemingly well understood by you.\n\nQ. How are the action-state values of different length aggregated?\nA. We simply add the Q values of different scales. To balance the importance of different scales, we also introduce hyper parameter alpha.\n\nQ. Why are the future subsequences that do not contain y_{t+1} ignored?\nA2. In some setting such as Go or Atari games, the final state of the agent is important (e.g. win or lose), and future states affect the Q-value a lot. So, it is important to see further future state after the certain action at t to estimate Q-value in those setting. In our setting, however, the importance of states (or subsequences) does not depend on the timesteps. The partial reward functions treat every subsequences at a time step equally. So, we think the subsequences that contain y_{t+1} are enough samples (and they should depend on q-value of y_{t+1} a lot because y_{t_1} itself is in the subsequences) to estimate q-value. \nIn equation (4), the subsequences that do not contain y_{t+1} are not ignored."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512371998191,"tcdate":1512359827562,"number":2,"cdate":1512359827562,"id":"r13qwSzbf","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Comment","forum":"r1kP7vlRb","replyto":"SkY1f6Hlf","signatures":["ICLR.cc/2018/Conference/Paper289/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper289/Authors"],"content":{"title":"Reply","comment":"Thanks for the review.\n\nFrom the title and the first paragraph of your review, we assume that you might not get our paper, maybe due to our poor writing. We are not sure how you understand our paper, so we firstly try to correct your misunderstandings.\n\nThis paper is introducing the two techniques to learn better reward function, partial reward function and expert-based reward function training, rather than introducing new RL approach. From your review, it can be assumed that you think our paper argues about q-learning, but our paper uses policy-based RL approach (it has been firstly done by Ranzato et al. and it is not our novelty) and does not argue about q-learning at all. A policy (or a sequence generator) is learned by a policy gradient, and Q-function is NOT learned by a policy gradient. In REINFORCE, Q-value is estimated by Monte-Carlo samplings. I think the first paragraph of reviewer3 well summarizes our paper. We would appreciate if you could tell us which parts of our paper actually caused your misunderstandings so that we can revise these parts.\n\nQ. Explain about equation 7 specifically.\nA. The motivation of equation 7 is, when the produced fake sequence is not quite different from the true sequence (for example, only one token in the sequence of length 20 is changed), we thought it would be effective to decrease the weight of the objective function, binary cross entropy (BCE), because this fake sequence is actually not so bad sequence. The benefit of decreasing the weight for such sequence is that the learned reward function would become easier to be maximized by a policy gradient, because learned reward function would return some reward to a generated sequence that has some mistakes. In our paper, we describe it as “smooth\" reward function.\nThe parameter \\tau in quality function directly affects the weight of BCE. When \\tau is large, the fake sequence that is little edited from expert one get a large value of quality function, resulting in making (1 - q) / (1 + q) lower than 1, and it decreases the weight of the second term in the right hand side of equation (7). On the other hand, when \\tau is small, the fake sequence that is little edited from expert one gets a near 0 value of quality function, resulting in (1 - q) / (1 + q) ~= 1, and equation (7) becomes the conventional BCE.\nThe term (1 - q) / (1 + q) is heuristic and there is no theoretical background for it, but it enables to control the strictness of the learned reward function by changing the parameter \\tau (“strict” means that only realistic sequence gets the reward close to 1, and others get the reward close to 0. A strict reward function is accurate, but it is considered to be difficult to maximize by a policy gradient because this reward function might be binary-like peaky function). In the experiment, we show that when the partial reward function has long scale, easing the conventional BCE by using \\tau=1.5 is effective.\n\nPlease give us more specific parts that you are still confused, and we are willing to give answers.\n\nBest,"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512359154067,"tcdate":1512359154067,"number":1,"cdate":1512359154067,"id":"S1clBHfZG","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Comment","forum":"r1kP7vlRb","replyto":"H104OpbgM","signatures":["ICLR.cc/2018/Conference/Paper289/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper289/Authors"],"content":{"title":"Reply","comment":"Thank you for the review. I am glad that you enjoyed reading our paper.\nAbout the mistakes of English in the introduction part, we will get native check and revise it."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222614968,"tcdate":1512033219594,"number":3,"cdate":1512033219594,"id":"HJ2pirpxG","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Review","forum":"r1kP7vlRb","replyto":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference/Paper289/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A novel contribution to sequence generation","rating":"7: Good paper, accept","review":"This paper considers the problem of improving sequence generation by learning better metrics. Specifically, it focuses on addressing the exposure bias problem, where traditional methods such as SeqGAN uses GAN framework and reinforcement learning. Different from these work, this paper does not use GAN framework. Instead, it proposed an expert-based reward function training, which trains the reward function (the discriminator) from data that are generated by randomly modifying parts of the expert trajectories. Furthermore, it also introduces partial reward function that measures the quality of the subsequences of different lengths in the generated data. This is similar to the idea of hierarchical RL, which divide the problem into potential subtasks, which could alleviate the difficulty of reinforcement learning from sparse rewards. The idea of the paper is novel. However, there are a few points to be clarified.\n\nIn Section 3.2 and in (4) and (5), the authors explains how the action value Q_{D_i} is modeled and estimated for the partial reward function D_i of length L_{D_i}. But the authors do not explain how the rewards (or action value functions) of different lengths are aggregated together to update the model using policy gradient. Is it a simple sum of all of them?\n\nIt is not clear why the future subsequences that do not contain y_{t+1} are ignored for estimating the action value function Q in (4) and (5). The authors stated that it is for reducing the computation complexity. But it is not clear why specifically dropping the sequences that do not contain y_{t+1}. Please clarify more on this point.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222615010,"tcdate":1511539169294,"number":2,"cdate":1511539169294,"id":"SkY1f6Hlf","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Review","forum":"r1kP7vlRb","replyto":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference/Paper289/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper introduces an RL approach to generating time series data without the difficult training of GANs. Unfortunately, the paper is too poorly written to be clear or effective.","rating":"3: Clear rejection","review":"This paper describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value.  Local and global reward functions are learned from existing data sequences and then the Q-function learned from a policy gradient.\n\nUnfortunately, this description is a little vague, because the paper's details are quite difficult to understand.  Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled.  Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained.\n\nThis paper could be interesting, but substantial editing is needed before it is sufficient for publication.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222615050,"tcdate":1511278646169,"number":1,"cdate":1511278646169,"id":"H104OpbgM","invitation":"ICLR.cc/2018/Conference/-/Paper289/Official_Review","forum":"r1kP7vlRb","replyto":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference/Paper289/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Using IRL techniques instead of GANs for sequence generation","rating":"7: Good paper, accept","review":"This article is a follow-up from recent publications (especially the one on \"seqGAN\" by Yu et al. @ AAAI 2017) which tends to assimilate Generative Adversarial Networks as an Inverse Reinforcement Learning task in order to obtain a better stability.\nThe adversarial learning is replaced here by a combination of policy gradient and a learned reward function.\n\nIf we except the introduction which is tainted with a few typos and English mistakes, the paper is clear and well written. The experiments made on both synthetic and real text data seems solid.\nBeing not expert in GANs I found it pleasant to read and instructive.\n\n\n\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1509739383476,"tcdate":1509090134984,"number":289,"cdate":1509739380822,"id":"r1kP7vlRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1kP7vlRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Toward learning better metrics for sequence generation training with policy gradient","abstract":"Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.","pdf":"/pdf/3360c542d1569cadfade4aecfbca660ef745fae3.pdf","TL;DR":"This paper aims to learn a better metric for unsupervised learning, such as text generation, and shows a significant improvement over SeqGAN.","paperhash":"anonymous|toward_learning_better_metrics_for_sequence_generation_training_with_policy_gradient","_bibtex":"@article{\n  anonymous2018toward,\n  title={Toward learning better metrics for sequence generation training with policy gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1kP7vlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper289/Authors"],"keywords":["sequence generation","reinforcement learning","unsupervised learning","RNN"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}