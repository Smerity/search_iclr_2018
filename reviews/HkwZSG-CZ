{"notes":[{"tddate":null,"ddate":null,"tmdate":1515799729979,"tcdate":1515799729979,"number":10,"cdate":1515799729979,"id":"B18hETI4f","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"HJ985q0fz","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/AnonReviewer2"],"content":{"title":"post-response","comment":"The authors have added some important experiments whose results support their claim, and I do believe that the current version of the paper makes a stronger case. I am still not satisfied that the rank explanation fully captures what is going on there, although it certainly correlates with better results, but this paper will provide an important data point for future research and I am raising my score from 5 to 7."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515082514501,"tcdate":1515082514501,"number":9,"cdate":1515082514501,"id":"Hk9G7RsmM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"Hk_Hu9Rfz","signatures":["~Gábor_Melis1"],"readers":["everyone"],"writers":["~Gábor_Melis1"],"content":{"title":"Re: response","comment":"Thank you for the update that improved an already very good paper.\n\nInformal Rating: 8\nConfidence: 4\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514983141139,"tcdate":1514983141139,"number":8,"cdate":1514983141139,"id":"S1pkkLc7G","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["~Stephen_Merity1"],"readers":["everyone"],"writers":["~Stephen_Merity1"],"content":{"title":"Important paper that analyses and addresses a fundamental issue with large vocabularies","comment":"I would strongly recommend this paper be accepted for publication. This paper uncovers a fundamental issue with large vocabularies and goes beyond just analyzing the issue by proposing a helpful method of addressing this. Whilst I was already excited by the initial version of this paper, the follow up work that has been done by the authors is even more informative. Understanding and considering the rank bottlenecks of our models seems an important consideration for future models.\nIf I can answer any follow up questions in support of this paper I would be happy to.\n\nInformal Rating: 8\nConfidence: 5\n(work directly in this field and have recreated many aspects of the results since publication)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514216217775,"tcdate":1514216017778,"number":8,"cdate":1514216017778,"id":"HJ985q0fz","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"r1zYOdPgz","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thank you for the valuable feedback.\n\n[[Rank and meanings]] It could be possible that A is low-rank for a natural language as it is hard to rule out this possibility rigorously, but we hypothesize that A is high-rank. Our hypothesis is supported by our intuitive reasoning and empirical experiments. Empirically, we give three more pieces of evidences supporting our hypothesis that the rank is the key bottleneck of Softmax and MoS improves the performance by solving the rank bottleneck (section 3.2):\n    - Before the rank saturates to the full rank, using more mixture components in MoS continues to increase the rank of the log-probability matrix. Further, when the rank increases, the perplexity also decreases. \n    - MoS has a similar generalization gap compared to Softmax, which rules out the concern that the improvement actually comes from some unexpected regularization effects of MoS.\n    - In character-level language modeling, since the largest possible rank is upper bounded by the limited vocabulary size, Softmax does not suffer from the rank bottleneck. In this case, Softmax and MoS have almost the same performance, which matches our analysis.\nWe agree that linking semantic meanings to bases lacks rigor and this would be better described as intuitions. We have made corresponding changes in the paper.\n\n[[Computation vs. Capacity]]: It is true that MoS involves a larger amount of computation compared to the standard Softmax. However, [Collins et al] suggests the capacity of neural language models is mostly related to the number of parameters, rather than computation. Moreover, powerful models often require a larger amount of computation. For example, the attention based seq2seq model involves much more computation compared to the vanilla seq2seq.\n\n[[Large-scale experiment]]: We have added a “large-scale language modeling experiment” using the 1B Word Dataset (section 3.1), where MoS significantly outperforms the baseline model with a large margin. This indicates that MoS consistently outperforms Softmax, regardless of the scale of the dataset. Also, note that PTB and WT2 are two de-facto benchmarks widely used in previous work on language modeling. None of the following papers had experiments on datasets larger than WT2: Zoph & Le ICLR 2017, Zilly et al ICML 2017, Inan et al ICLR 2017, Grave et al ICLR 2017, Merity et al ICLR 2017.\n\n[[Varying the number of mixtures]]: Thanks for the suggestion. We performed this experiment, whose result is summarized in the second bullet point of section 3.2 (updated version). As expected, the number of mixture components is positively correlated with the empirical rank. More importantly, before the rank saturates to the full rank, MoS with a higher rank leads to a better performance (lower perplexity). \n\n------------------------------------------------------------------------------------------------------------------------\n[Collins et al] Capacity and Trainability in Recurrent Neural Networks\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514216069890,"tcdate":1514215798640,"number":7,"cdate":1514215798640,"id":"rkkKFc0GG","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"SyTCJyqeM","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thanks for your valuable comments.\n\n[[Large-scale experiment]]: We’ve added a “large-scale language modeling experiment” using the 1B Word Dataset (section 3.1), where MoS significantly outperforms the baseline model by a large margin. This indicates that MoS consistently outperforms Softmax, regardless of the scale of the dataset. Also, note that PTB and WT2 are two de-facto benchmarks widely used in previous work on language modeling. None of the following papers had experiments on datasets larger than WT2: Zoph & Le ICLR 2017, Zilly et al ICML 2017, Inan et al ICLR 2017, Grave et al ICLR 2017, Merity et al ICLR 2017.\n\n[[Character-level LM]]: Firstly, note that the largest possible rank of the log-probability matrix is upper bounded by the vocabulary size. In character-level LM, the vocabulary size is usually much smaller than the embedding size. In this case, Softmax does not suffer from the rank bottleneck problem, and we expect MoS and Softmax to achieve similar performance in practice. To verify our expectation, we perform character-level LM experiment on the text8 dataset, where MoS and Softmax indeed achieve almost the same performance (section 3.2 & appendix C.2 in the updated version). \n\n[[Training time]]: We have added the training time analysis for MoS and provided empirical numbers in the updated versions of the paper (section 3.3 & Appendix C.3). In general, computational wall time of MoS is actually sub-linear w.r.t. the number of mixture components. In most settings, we observe a two to three times slowdown compared to Softmax when using up to 15 components for MoS. We believe such additional computational cost is acceptable for the following reasons:\n    - MoS is highly parallelizable, meaning that using more machines can always speed up the computation almost linearly.\n    - The field of deep learning systems (both hardware and software) is making rapid progress. It might be possible to further optimize MoS on GPUs for fast computation. More developed hardware systems would also further reduce the computational cost.\n    - Historically, important techniques sometimes come with an additional computational cost, e.g., LSTM, attention, deep ResNets. We believe that with MoS, the extra cost is reasonable and the gain is substantial.\n\n[[Application to MT/ASR]]: We believe this is best left to future research, as performing rigorous experiments and careful comparison for such a real-world applications is non-trivial. And we believe language modeling is of its own importance already.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514216111551,"tcdate":1514215633034,"number":6,"cdate":1514215633034,"id":"SktCu50MM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"B1v_izpxM","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thank you for the valuable comments.\n\n[[Claim of SOTA]]: We believe the 2M difference in the number of parameters is negligible compared to the number of parameters we use (i.e., 35M). In fact, we ran MoS in another setting with 31M parameters and got 63.59 on WT2 without finetuning, compared to 63.33 obtained by our best-performing model.\n\n[[Training time]]: Thanks for the suggestion and we have added the training time analysis for MoS and provided empirical numbers in the updated versions of the paper (section 3.3 & Appendix C.3). In general, computational wall time of MoS is actually sub-linear w.r.t. the number of mixture components. In most settings, we observe a two to three times slowdown compared to Softmax when using up to 15 components for MoS. We believe such additional computational cost is acceptable for the following reasons:\n    - MoS is highly parallelizable, meaning that using more machines can always speed up the computation almost linearly.\n    - The field of deep learning systems (both hardware and software) is making rapid progress. It might be possible to further optimize MoS on GPUs for fast computation. More developed hardware systems would also further reduce the computational cost.\n    - Historically, important techniques sometimes come with an additional computational cost, e.g., LSTM, attention, deep ResNets. We believe that with MoS, the extra cost is reasonable and the gain is substantial.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514215487750,"tcdate":1514215487750,"number":5,"cdate":1514215487750,"id":"Hk_Hu9Rfz","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"HkXiuNE-f","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thank you for your valuable feedback. We believe that, on large datasets where explicit regularization techniques like dropout are not crucial, the improvement on training perplexity does give information about whether MoS has an unintended regularization effect that can improve the performance.\n\nThus, in our updated version, we conduct an experiment on the 1B Word dataset, where no dropout or other regularization technique is used. As described in the third bullet point of section 3.2, in this regularization free setting, MoS and Softmax have the same generalization gap (i.e., the gap between training and test error), and performance improvement is fully reflected on the training perplexity. Hence, the superiority of MoS is not caused by some unexpected regularization but improved expressiveness.\n\nWe also provide additional evidence in section 3.2 (updated version) to support our theory that achieving a higher rank is the key to the excellence of MoS. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514386130231,"tcdate":1514215405088,"number":4,"cdate":1514215405088,"id":"BySeO9CGz","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"Update to the paper","comment":"To provide faithful answers to the reviews and comments, we have conducted a series of additional experiments and updated the paper accordingly. The core changes are summarized as follows:\n    (1) we add a “large-scale language modeling experiment” using the 1B Word Dataset (section 3.1)\n    (2) we give three more pieces of “evidence supporting our theory” that the rank is the key bottleneck of Softmax and MoS improves the performance by breaking the rank bottleneck (section 3.2):\n        - Empirically, before the rank saturates to the full rank, using more mixture components in MoS continues to increase the rank of the log-probability matrix. Further, the higher the rank is, the lower the perplexity that can be achieved. \n        - MoS has a similar generalization gap compared to Softmax, which rules out the concern that the improvement actually comes from some unexpected regularization effects of MoS.\n        - In character-level language modeling, since the largest possible rank is upper bounded by the limited vocabulary size, Softmax does not suffer from the rank bottleneck. In this case, Softmax and MoS have almost the same performance, which matches our analysis.\n    (3) we perform “training time analysis” for MoS and provide empirical training time comparison (section 3.3 & Appendix C.3) \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512558763029,"tcdate":1512556535127,"number":7,"cdate":1512556535127,"id":"HkkWuBrZG","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"SyTCJyqeM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: enwik8","comment":"It's true that regularization doesn't play as important a role with large datasets as it does with small datasets, but enwik8 is character based and it's unclear whether this paper's arguments would apply. Sticking to word level corpora, I'd much sooner recommend Wikitext-103 than the Billion Word corpus which has issues.\n\nFurthermore, language modelling improvements are interesting in their own right without having to validate them via MT or speech recognition."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512489590014,"tcdate":1512487066818,"number":6,"cdate":1512487066818,"id":"HkXiuNE-f","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["~Gábor_Melis1"],"readers":["everyone"],"writers":["~Gábor_Melis1"],"content":{"title":"Training fit?","comment":"How much does training perplexity improve with MoS compared to the baseline model?\n\nIf it does improve substantially, that gives more credence to the rank argument.\nIf it doesn't, then we might be observing unexpected regularization effects."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642518121,"tcdate":1512020847262,"number":3,"cdate":1512020847262,"id":"B1v_izpxM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Review","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer1"],"readers":["everyone"],"content":{"title":"nice idea","rating":"8: Top 50% of accepted papers, clear accept","review":"Language models are important components to many NLP tasks. The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state. This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes. The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.\n\nPros:\n--The paper is very well written and easy to follow. The ideas build up on each other in an intuitive way.\n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know.\n--The maths is very rigorous.\n--The experiment section is thorough.\n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters). In Table 2 the baselines have a lower capacity. This is an unfair comparison\n--I suspect the proposed approach is slower than the baselines. There is no mention of computational cost. Reporting that would help interpret the numbers. \n\nThe SOTA claim might not hold if baselines are given the same capacity. But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515922500745,"tcdate":1511808981035,"number":2,"cdate":1511808981035,"id":"SyTCJyqeM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Review","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting. ","rating":"7: Good paper, accept","review":"The authors has addressed my concerns, so I raised my rating. \n\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.\n\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. \n\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning. Results on this data might be more convincing.\n\nThe results of MOS is very good, but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15. This will make it less usable, I think it's necessary to provide the training time comparison.\n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER. \n\n[1] Melis, Gábor, Chris Dyer, and Phil Blunsom. \"On the state of the art of evaluation in neural language models.\" arXiv preprint arXiv:1707.05589 (2017).\n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342\n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515799750799,"tcdate":1511651450084,"number":1,"cdate":1511651450084,"id":"r1zYOdPgz","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Review","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors identify an issue with current prediction models and propose a solution in the form of a mixture of softmax. The idea is interesting, but would require more justification and experimental validation.","rating":"7: Good paper, accept","review":"The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language. As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15). This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores.\n\nThe question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention. This makes a lot of the questions asked in this paper extremely relevant to the field. However, it is unclear that the rank of the logit matrix is the right quantity to consider. For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices. Further, the first two \"observations\" in Section 2.2 would be more accurately described as \"intuitions\" of the authors. As they write themselves \"there is no evidence showing that semantic meanings are fully linearly correlated.\" Why then try to link \"meanings\" to basis vectors for the rows of A?\n\nTo be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant.\n\nUnfortunately, this is only tested on relatively small data sets, up to 2M tokens and a vocabulary of size 30K for language modeling. They do constitute a good starting place to test a model, but given the importance of regularization on those specific tasks, it is difficult to predict how the MoS would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting.\n\nAnother important missing experiment would consist in varying the number of mixture components (this could very well be done on WikiText2). This could help validate the hypothesis: how does the estimated rank vary with the number of components? How about the performance and pairwise KL divergence? \n\nThis paper offers a promising direction for language modeling research, but would require more justification, or at least a more developed experimental section.\n\nPros:\n- Important starting question\n- Thought-provoking approach\n- Experimental gains on small data sets\n\nCons:\n- The link between the intuition and reality of the gains is not obvious\n- Experiments limited to small data sets, some obvious questions remain","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511561223626,"tcdate":1511560358741,"number":3,"cdate":1511560358741,"id":"Hk1nEGUxM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"B1VaeCHeM","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"We haven’t tried larger batch sizes since it does not fit into the memory. We did try training the baseline models with batch size 20 (compared to 40 in the original paper) on Penn Treebank, and the performance degrades a little bit (from 58.95 to 59.10), which indicates that using small batch sizes does not improve the baseline model. The AWD-LSTM paper also confirmed that “relatively large batch sizes (e.g., 40-80) performed better than smaller sizes (e.g., 10-20) for NT-ASGD”. Thus it is likely that MoS will perform even better with larger batch sizes. \n\nOn Penn Treebank, we also tried using lr=20 while keeping batch_size=40 for Softmax, since lr=20 is used for MoS. It turned out this significantly worsened the perplexity of Softmax from 58.95 to 61.39. Combined with the previous analysis, it suggests the performance improvement of MoS does not come from a better choice of learning rate and/or batch size. \n\nMoreover, in our preliminary experiments, we compared MoS with Softmax using the PyTorch demo code (https://github.com/pytorch/examples/tree/master/word_language_model). MoS (nhid=1059, dropout=0.55, n_softmax=20) obtains a perplexity of 68.04, compared to 72.30 obtained by Softmax (nhid=1500, dropout=0.65). In this experiment, the batch sizes and learning rates are the same and we are again seeing clear gains. Note that we reduced nhid to obtain comparable model sizes, and reduced dropout since the hidden unit size is smaller in our case.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511542972037,"tcdate":1511542972037,"number":5,"cdate":1511542972037,"id":"B1VaeCHeM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"BJt665wkf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"thanks for your response","comment":"Thank you for your extensive response.\n\nI fully agree with the fact that the baseline model's performance using MoS hyper-parameters is worse than MoS is a necessary condition. I wrongly assumed, that you also find that to be sufficient condition. I concluded that based on this sentence from your paper 'On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the performance, which rules out hyper-parameters as the main source of improvement'. It is probably just an unfortunate sentence and you may want to paraphrase it.\n\nI found your arguments very persuasive. My only concern is that you use very small batch sizes that make training procedure very slow (is it more than a week on a single GPU for WT2?). Have you tried to train with batch sizes comparable with state-of-the-art model that you compare with?\n\nThank you again for making your statements more clear to me."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510612416891,"tcdate":1510612416891,"number":2,"cdate":1510612416891,"id":"BJt665wkf","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"SyRk1cPkM","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thanks for your comments. We believe our comparison is fair and MoS is indeed better than the baseline, with the following reasons:\n\nFirstly, the hyper-parameters for MoS are chosen by trial and error (i.e., graduate student descent) rather than an extensive hyper-parameter search such as the one used by [Melis et al]. The baseline AWD-LSTM uses a similar strategy in searching for hyper-parameters. Therefore, both MoS and the baseline are well tuned and comparable.\n\nSecondly, the baseline was the previous state-of-the-art (SOTA) (before our paper is released). It is usually true that one would not expect hyper-parameter tuning alone to substantially improve SOTA results on widely-studied benchmarks.\n\nThirdly, since MoS introduces another hidden layer, the number of parameters would significantly increase if we kept the embedding size and hidden size the same as the baseline, which would lead to an unfair comparison [Collins et al]. Thus, we have to trim the network size and modify related hyper-parameters accordingly. \n\nFourthly, the fact that the baseline with MoS hyper-parameters is worse than MoS is just a necessary condition of our argument that MoS is better than the baseline. (And we do not claim it is sufficient; sufficiency is proved by comparison with SOTA).\n\n[Melis et al] On the State of the Art of Evaluation in Neural Language Models\n[Collins et al] Capacity and Trainability in Recurrent Neural Networks\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510608678514,"tcdate":1510608614470,"number":4,"cdate":1510608614470,"id":"SyRk1cPkM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Worse performance of the baseline (AWD-LSTM) under MoS hyperparameters","comment":"In ablation studies, during comparison between MoS and the baseline (AWD-LSTM), the authors use the hyperparameters tuned on MoS for the evaluation of AWD-LSTM. It is claimed that the worse performance of AWD-LSTM under these hyperparameters implies that the better performance of MoS is not due to hyperparamters. I think a fairer comparison would be to search for the best hyperparameters individually for each task. Can the authors elaborate on the motivation for this approach? Also, was an evaluation of MoS performed using the AWD-LSTM hyperparameters provided by Merity et al 2017, and only tuning the MoS specific hyperparameters on top of that? I believe this would be a fairer comparison (if finding the best hyperparameters for each model individually using a comparable extensive grid search is too expensive)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509570350727,"tcdate":1509570350727,"number":2,"cdate":1509570350727,"id":"B1D4D3wRb","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"rJFe7vDCW","signatures":["~Aaron_Jaech1"],"readers":["everyone"],"writers":["~Aaron_Jaech1"],"content":{"title":"thanks for your response","comment":"Thanks for your response. Just so it's clear, I want to reiterate that I really liked your paper and thought it was an original and worthwhile contribution. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"ddate":null,"tddate":1511560373508,"tmdate":1511560386920,"tcdate":1509548784644,"number":1,"cdate":1509548784644,"id":"rJFe7vDCW","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"S10GwzSCb","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thank you for the comments. We will include the related papers in our later version. For now, we will summarize the difference between the mentioned work and ours as follows:\n\n1. As discussed in Section 2.3, there is a tradeoff between expressiveness and generalization. Ngram models are expressive but do not generalize well due to data sparsity (or curse of dimensionality). Hutchinson et al. attempted to improve generalization using sparse plus low-rank Ngram models. By contrast, neural language models with standard Softmax generalize well but do not have enough expressiveness (as shown in Sections 2.1 and 2.2). This motivates our high-rank approach that improves expressiveness without sacrificing generalization. In a nutshell, the difference is that we aim to improve expressiveness while Hutchinson et al. aimed to improve generalization, and such a difference is a result of the intrinsic difference between neural and Ngram language models.\n\n2. There are two key differences between the MoD model (Neubig and Dyer) and ours. \n        - Firstly, the motivations are totally different. Neubig and Dyer proposed to hybridize Ngram and neural language models to unify and benefit from both. In comparison, we identify the Softmax bottleneck problem using a matrix factorization formulation, and hence motivate our approach to break the bottleneck.\n        - Secondly, our approach is end-to-end and achieves a good tradeoff between generalization and expressiveness (Cf. Section 2.3). In comparison, MoD might not generalize well since Ngram models, which have poor generalization, are included in the mixture. Moreover, the Ngram parameters are fixed in MoD, which limits its expressiveness.\nDespite the differences, note that it is possible to combine our work with theirs to further improve language modeling.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509398294355,"tcdate":1509398294355,"number":1,"cdate":1509398294355,"id":"S10GwzSCb","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["~Aaron_Jaech1"],"readers":["everyone"],"writers":["~Aaron_Jaech1"],"content":{"title":"prior work on the low-rank problem","comment":"This seems like an interesting idea and a great result!\n\nWhen I was reading what you wrote about potential easy fixes, I was reminded of the work from Brian Hutchinson where he shows how back-off smoothing is a low-rank approximation of the \"A\" matrix. (See, \"Low Rank Language Models for Small Training Sets\" by Hutchinson, Ostendorf, and Fazel.)  In later work by the same authors they use a sparse plus low-rank model to remedy the deficiencies of the low-rank approximation.  (See, \"A Sparse Plus Low Rank Maximum Entropy Language Model.\") That line of work ended up being impractical for large datasets and the solution that you have come up with seems much more promising. \n\nAnother paper that I think is related to your Mixture of Softmaxes is the Mixture of Distributions model from Neubig and Dyer's paper \"Generalizing and Hybridizing Count-based and Neural Language Models.\" They are talking about mixing n-gram distributions with a softmax distribution. Their model is another way of \"breaking the softmax bottleneck\" although they don't motivate it as such. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514215272425,"tcdate":1509135614799,"number":831,"cdate":1509739075167,"id":"HkwZSG-CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.","pdf":"/pdf/8e353063db5c4aa24abe6df39919ff209b1c4d93.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]},"nonreaders":[],"replyCount":20,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}