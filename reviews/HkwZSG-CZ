{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222783048,"tcdate":1512020847262,"number":3,"cdate":1512020847262,"id":"B1v_izpxM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Review","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer1"],"readers":["everyone"],"content":{"title":"nice idea","rating":"8: Top 50% of accepted papers, clear accept","review":"Language models are important components to many NLP tasks. The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state. This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes. The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.\n\nPros:\n--The paper is very well written and easy to follow. The ideas build up on each other in an intuitive way.\n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know.\n--The maths is very rigorous.\n--The experiment section is thorough.\n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters). In Table 2 the baselines have a lower capacity. This is an unfair comparison\n--I suspect the proposed approach is slower than the baselines. There is no mention of computational cost. Reporting that would help interpret the numbers. \n\nThe SOTA claim might not hold if baselines are given the same capacity. But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222783089,"tcdate":1511808981035,"number":2,"cdate":1511808981035,"id":"SyTCJyqeM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Review","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting. ","rating":"6: Marginally above acceptance threshold","review":"The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.\n\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. \n\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning. Results on this data might be more convincing.\n\nThe results of MOS is very good, but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15. This will make it less usable, I think it's necessary to provide the training time comparison.\n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER. \n\n[1] Melis, Gábor, Chris Dyer, and Phil Blunsom. \"On the state of the art of evaluation in neural language models.\" arXiv preprint arXiv:1707.05589 (2017).\n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342\n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222783133,"tcdate":1511651450084,"number":1,"cdate":1511651450084,"id":"r1zYOdPgz","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Review","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference/Paper831/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors identify an issue with current prediction models and propose a solution in the form of a mixture of softmax. The idea is interesting, but would require more justification and experimental validation.","rating":"5: Marginally below acceptance threshold","review":"The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language. As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15). This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores.\n\nThe question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention. This makes a lot of the questions asked in this paper extremely relevant to the field. However, it is unclear that the rank of the logit matrix is the right quantity to consider. For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices. Further, the first two \"observations\" in Section 2.2 would be more accurately described as \"intuitions\" of the authors. As they write themselves \"there is no evidence showing that semantic meanings are fully linearly correlated.\" Why then try to link \"meanings\" to basis vectors for the rows of A?\n\nTo be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant.\n\nUnfortunately, this is only tested on relatively small data sets, up to 2M tokens and a vocabulary of size 30K for language modeling. They do constitute a good starting place to test a model, but given the importance of regularization on those specific tasks, it is difficult to predict how the MoS would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting.\n\nAnother important missing experiment would consist in varying the number of mixture components (this could very well be done on WikiText2). This could help validate the hypothesis: how does the estimated rank vary with the number of components? How about the performance and pairwise KL divergence? \n\nThis paper offers a promising direction for language modeling research, but would require more justification, or at least a more developed experimental section.\n\nPros:\n- Important starting question\n- Thought-provoking approach\n- Experimental gains on small data sets\n\nCons:\n- The link between the intuition and reality of the gains is not obvious\n- Experiments limited to small data sets, some obvious questions remain","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511561223626,"tcdate":1511560358741,"number":3,"cdate":1511560358741,"id":"Hk1nEGUxM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"B1VaeCHeM","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"We haven’t tried larger batch sizes since it does not fit into the memory. We did try training the baseline models with batch size 20 (compared to 40 in the original paper) on Penn Treebank, and the performance degrades a little bit (from 58.95 to 59.10), which indicates that using small batch sizes does not improve the baseline model. The AWD-LSTM paper also confirmed that “relatively large batch sizes (e.g., 40-80) performed better than smaller sizes (e.g., 10-20) for NT-ASGD”. Thus it is likely that MoS will perform even better with larger batch sizes. \n\nOn Penn Treebank, we also tried using lr=20 while keeping batch_size=40 for Softmax, since lr=20 is used for MoS. It turned out this significantly worsened the perplexity of Softmax from 58.95 to 61.39. Combined with the previous analysis, it suggests the performance improvement of MoS does not come from a better choice of learning rate and/or batch size. \n\nMoreover, in our preliminary experiments, we compared MoS with Softmax using the PyTorch demo code (https://github.com/pytorch/examples/tree/master/word_language_model). MoS (nhid=1059, dropout=0.55, n_softmax=20) obtains a perplexity of 68.04, compared to 72.30 obtained by Softmax (nhid=1500, dropout=0.65). In this experiment, the batch sizes and learning rates are the same and we are again seeing clear gains. Note that we reduced nhid to obtain comparable model sizes, and reduced dropout since the hidden unit size is smaller in our case.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511542972037,"tcdate":1511542972037,"number":5,"cdate":1511542972037,"id":"B1VaeCHeM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"BJt665wkf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"thanks for your response","comment":"Thank you for your extensive response.\n\nI fully agree with the fact that the baseline model's performance using MoS hyper-parameters is worse than MoS is a necessary condition. I wrongly assumed, that you also find that to be sufficient condition. I concluded that based on this sentence from your paper 'On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the performance, which rules out hyper-parameters as the main source of improvement'. It is probably just an unfortunate sentence and you may want to paraphrase it.\n\nI found your arguments very persuasive. My only concern is that you use very small batch sizes that make training procedure very slow (is it more than a week on a single GPU for WT2?). Have you tried to train with batch sizes comparable with state-of-the-art model that you compare with?\n\nThank you again for making your statements more clear to me."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510612416891,"tcdate":1510612416891,"number":2,"cdate":1510612416891,"id":"BJt665wkf","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"SyRk1cPkM","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thanks for your comments. We believe our comparison is fair and MoS is indeed better than the baseline, with the following reasons:\n\nFirstly, the hyper-parameters for MoS are chosen by trial and error (i.e., graduate student descent) rather than an extensive hyper-parameter search such as the one used by [Melis et al]. The baseline AWD-LSTM uses a similar strategy in searching for hyper-parameters. Therefore, both MoS and the baseline are well tuned and comparable.\n\nSecondly, the baseline was the previous state-of-the-art (SOTA) (before our paper is released). It is usually true that one would not expect hyper-parameter tuning alone to substantially improve SOTA results on widely-studied benchmarks.\n\nThirdly, since MoS introduces another hidden layer, the number of parameters would significantly increase if we kept the embedding size and hidden size the same as the baseline, which would lead to an unfair comparison [Collins et al]. Thus, we have to trim the network size and modify related hyper-parameters accordingly. \n\nFourthly, the fact that the baseline with MoS hyper-parameters is worse than MoS is just a necessary condition of our argument that MoS is better than the baseline. (And we do not claim it is sufficient; sufficiency is proved by comparison with SOTA).\n\n[Melis et al] On the State of the Art of Evaluation in Neural Language Models\n[Collins et al] Capacity and Trainability in Recurrent Neural Networks\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510608678514,"tcdate":1510608614470,"number":4,"cdate":1510608614470,"id":"SyRk1cPkM","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Worse performance of the baseline (AWD-LSTM) under MoS hyperparameters","comment":"In ablation studies, during comparison between MoS and the baseline (AWD-LSTM), the authors use the hyperparameters tuned on MoS for the evaluation of AWD-LSTM. It is claimed that the worse performance of AWD-LSTM under these hyperparameters implies that the better performance of MoS is not due to hyperparamters. I think a fairer comparison would be to search for the best hyperparameters individually for each task. Can the authors elaborate on the motivation for this approach? Also, was an evaluation of MoS performed using the AWD-LSTM hyperparameters provided by Merity et al 2017, and only tuning the MoS specific hyperparameters on top of that? I believe this would be a fairer comparison (if finding the best hyperparameters for each model individually using a comparable extensive grid search is too expensive)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509570350727,"tcdate":1509570350727,"number":2,"cdate":1509570350727,"id":"B1D4D3wRb","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"rJFe7vDCW","signatures":["~Aaron_Jaech1"],"readers":["everyone"],"writers":["~Aaron_Jaech1"],"content":{"title":"thanks for your response","comment":"Thanks for your response. Just so it's clear, I want to reiterate that I really liked your paper and thought it was an original and worthwhile contribution. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"ddate":null,"tddate":1511560373508,"tmdate":1511560386920,"tcdate":1509548784644,"number":1,"cdate":1509548784644,"id":"rJFe7vDCW","invitation":"ICLR.cc/2018/Conference/-/Paper831/Official_Comment","forum":"HkwZSG-CZ","replyto":"S10GwzSCb","signatures":["ICLR.cc/2018/Conference/Paper831/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper831/Authors"],"content":{"title":"response","comment":"Thank you for the comments. We will include the related papers in our later version. For now, we will summarize the difference between the mentioned work and ours as follows:\n\n1. As discussed in Section 2.3, there is a tradeoff between expressiveness and generalization. Ngram models are expressive but do not generalize well due to data sparsity (or curse of dimensionality). Hutchinson et al. attempted to improve generalization using sparse plus low-rank Ngram models. By contrast, neural language models with standard Softmax generalize well but do not have enough expressiveness (as shown in Sections 2.1 and 2.2). This motivates our high-rank approach that improves expressiveness without sacrificing generalization. In a nutshell, the difference is that we aim to improve expressiveness while Hutchinson et al. aimed to improve generalization, and such a difference is a result of the intrinsic difference between neural and Ngram language models.\n\n2. There are two key differences between the MoD model (Neubig and Dyer) and ours. \n        - Firstly, the motivations are totally different. Neubig and Dyer proposed to hybridize Ngram and neural language models to unify and benefit from both. In comparison, we identify the Softmax bottleneck problem using a matrix factorization formulation, and hence motivate our approach to break the bottleneck.\n        - Secondly, our approach is end-to-end and achieves a good tradeoff between generalization and expressiveness (Cf. Section 2.3). In comparison, MoD might not generalize well since Ngram models, which have poor generalization, are included in the mixture. Moreover, the Ngram parameters are fixed in MoD, which limits its expressiveness.\nDespite the differences, note that it is possible to combine our work with theirs to further improve language modeling.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509398294355,"tcdate":1509398294355,"number":1,"cdate":1509398294355,"id":"S10GwzSCb","invitation":"ICLR.cc/2018/Conference/-/Paper831/Public_Comment","forum":"HkwZSG-CZ","replyto":"HkwZSG-CZ","signatures":["~Aaron_Jaech1"],"readers":["everyone"],"writers":["~Aaron_Jaech1"],"content":{"title":"prior work on the low-rank problem","comment":"This seems like an interesting idea and a great result!\n\nWhen I was reading what you wrote about potential easy fixes, I was reminded of the work from Brian Hutchinson where he shows how back-off smoothing is a low-rank approximation of the \"A\" matrix. (See, \"Low Rank Language Models for Small Training Sets\" by Hutchinson, Ostendorf, and Fazel.)  In later work by the same authors they use a sparse plus low-rank model to remedy the deficiencies of the low-rank approximation.  (See, \"A Sparse Plus Low Rank Maximum Entropy Language Model.\") That line of work ended up being impractical for large datasets and the solution that you have come up with seems much more promising. \n\nAnother paper that I think is related to your Mixture of Softmaxes is the Mixture of Distributions model from Neubig and Dyer's paper \"Generalizing and Hybridizing Count-based and Neural Language Models.\" They are talking about mixing n-gram distributions with a softmax distribution. Their model is another way of \"breaking the softmax bottleneck\" although they don't motivate it as such. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739077830,"tcdate":1509135614799,"number":831,"cdate":1509739075167,"id":"HkwZSG-CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkwZSG-CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Breaking the Softmax Bottleneck: A High-Rank RNN Language Model","abstract":"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.","pdf":"/pdf/abc7e52827abecebc6862a2bd2ebe6e84da28fb8.pdf","paperhash":"anonymous|breaking_the_softmax_bottleneck_a_highrank_rnn_language_model","_bibtex":"@article{\n  anonymous2018breaking,\n  title={Breaking the Softmax Bottleneck: A High-Rank RNN Language Model},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwZSG-CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper831/Authors"],"keywords":[]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}