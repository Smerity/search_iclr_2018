{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222587293,"tcdate":1511840051119,"number":3,"cdate":1511840051119,"id":"SJoNKL5xM","invitation":"ICLR.cc/2018/Conference/-/Paper192/Official_Review","forum":"BkgYL010b","replyto":"BkgYL010b","signatures":["ICLR.cc/2018/Conference/Paper192/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Original ideas, but some decisions are perplexing.","rating":"6: Marginally above acceptance threshold","review":"This paper provides a new method for learning representations of prepositions. The basic idea is to count word pairs which co-occur with a preposition, rather than single words which co-occur, as in standard word vector models such as word2vec. This seems to work quite well, and I speculate that it is because prepositions often function to indicate grammatical relations between different arguments, rather than being content-bearing words themselves. The paper counts up these word-pair co-occurrences in a tensor, then applies a tensor decomposition and low-rank approximation method to produce word and preposition representations. Experiments show that the method helps to find paraphrases of phrasal verbs, as well as improve downstream performance on preposition selection and preposition attachment disambiguation.\n\nThis paper was quite interesting and clearly written for the most part. I enjoyed reading it and the various evaluations that it described that target both the use of prepositions inside phrasal verbs as well as in its role in indicating grammatical relationships between different elements in a sentence.  I think that this work could be quite useful to the field, but that a number of frustrating weaknesses prevent me from recommending it without qualifications.\n\nThe main weaknesses of the paper are in the soundness of some of its qualitative analyses and claims. First, I found the cosine similarity scores in Table 1 largely uninterpretable. The claim is that different prepositions should have representations that are sufficiently distinct from each other. Even if we accept this premise (and why should we? They are of the same syntactic category after all), using the similarity scores to make this argument is not reasonable, as there is no absolute interpretation or calibration of the similarity scores that can be applied across models. Is 0.22 in similarity high or low?\n\nThe other evaluation decision that is confusing is the paraphrase evaluation of the phrasal verbs. This was not done systematically, but a broad general claim that the tensor multiplication models does the best cannot be verified. To me, the word2vec addition paraphrases also look quite good. It seems to me that a human subject experiment to somehow compare the two methods is required.\n\nI wonder whether this approach could be generalized to other classes of words or morphemes. For example, you could imagine that in a morphologically rich language, this method would work well to learn the representation of certain morphemes such as case endings or verbal conjugation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor-Based Preposition Representation","abstract":"Prepositions are among the most frequent words. Good prepositional representation  is of great syntactic and semantic interest  in computational linguistics. Existing methods on preposition representation either treat prepositions as content words (e.g., word2vec and GloVe) or depend heavily on external linguistic resources including syntactic parsing, training task and dataset-specific representations. In this paper we use word-triple counts (one of the words is a preposition) to  capture the preposition's interaction with its head and children. Prepositional  embeddings are derived via tensor decompositions on a large unlabeled corpus.  We reveal a new geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing of phrasal verbs. Furthermore, our prepositional  embeddings are used as simple features to two challenging downstream tasks: preposition selection and prepositional attachment disambiguation. We achieve comparable to or better results than state of the art on  multiple standardized datasets.  ","pdf":"/pdf/28b2f29422c33519f13c3e01e61753002a7c753e.pdf","TL;DR":"This work is about tensor-based method for preposition representation training.","paperhash":"anonymous|tensorbased_preposition_representation","_bibtex":"@article{\n  anonymous2018tensor-based,\n  title={Tensor-Based Preposition Representation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkgYL010b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper192/Authors"],"keywords":["word representation","unsupervised learning","computational linguistics"]}},{"tddate":null,"ddate":null,"tmdate":1512222587329,"tcdate":1511820185897,"number":2,"cdate":1511820185897,"id":"BkGojWcgG","invitation":"ICLR.cc/2018/Conference/-/Paper192/Official_Review","forum":"BkgYL010b","replyto":"BkgYL010b","signatures":["ICLR.cc/2018/Conference/Paper192/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Title","rating":"4: Ok but not good enough - rejection","review":"This paper introduces a method for learning representations for prepositions. They first take co-occurrence counts counts of pairs of words in a local window of each preposition, and then factorize the matrix to find low dimensional word representations. The main difference from previous work is restricting the context to be close to a preposition. The authors report improved paraphrasing of phrasal verbs, and state-of-the-art accuracy in correcting grammatical errors involving prepositions, and good results on prepositional phrase attachment. \n\n- The paper frequently overclaims. For one example, we’re told that “Preposition selection [is] a major area of study in both syntactic and semantic computational linguistics”, but at best it’s quite a specialized niche. The paper would be much improved if it was generally toned down.\n- The authors claim their method is “vastly” better at paraphrasing phrasal verbs than baselines, based on qualitative comparison. However, I couldn’t find any details on how the phrasal verbs were chosen, or what (if any) held out data was used for tuning the models. Even assuming this is a meaningful task, surely the natural baseline would be to treat these phrasal verbs as non-compositional (e.g. extend the vocab with words like “sparked_off”)  and train Word2Vec.\n- The other experiments are lacking important details. For example, we’re just told some values that hyperparameters were fixed at for both tasks - how were these chosen (including for the baselines)? Was the model architecture tuned based on the proposed representations? Were the word representations fixed, or fine tuned during training? \n- Despite the author’s expectations that their representations will be ‘widely used’, I am struggling to think of cases where they would be useful, outside of the very specific tasks involving prepositions that they use. That is because almost all tasks require good representations for all words, not just prepositions. The authors should add more justification for the where/how these representations will be useful.\n\nOverall, I think the technical contributions of the paper are quite limited, and the experiments are not well enough described for publication.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor-Based Preposition Representation","abstract":"Prepositions are among the most frequent words. Good prepositional representation  is of great syntactic and semantic interest  in computational linguistics. Existing methods on preposition representation either treat prepositions as content words (e.g., word2vec and GloVe) or depend heavily on external linguistic resources including syntactic parsing, training task and dataset-specific representations. In this paper we use word-triple counts (one of the words is a preposition) to  capture the preposition's interaction with its head and children. Prepositional  embeddings are derived via tensor decompositions on a large unlabeled corpus.  We reveal a new geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing of phrasal verbs. Furthermore, our prepositional  embeddings are used as simple features to two challenging downstream tasks: preposition selection and prepositional attachment disambiguation. We achieve comparable to or better results than state of the art on  multiple standardized datasets.  ","pdf":"/pdf/28b2f29422c33519f13c3e01e61753002a7c753e.pdf","TL;DR":"This work is about tensor-based method for preposition representation training.","paperhash":"anonymous|tensorbased_preposition_representation","_bibtex":"@article{\n  anonymous2018tensor-based,\n  title={Tensor-Based Preposition Representation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkgYL010b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper192/Authors"],"keywords":["word representation","unsupervised learning","computational linguistics"]}},{"tddate":null,"ddate":null,"tmdate":1512222587369,"tcdate":1511746717057,"number":1,"cdate":1511746717057,"id":"BJrsn1KlM","invitation":"ICLR.cc/2018/Conference/-/Paper192/Official_Review","forum":"BkgYL010b","replyto":"BkgYL010b","signatures":["ICLR.cc/2018/Conference/Paper192/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A nice contribution to linguistic representation learning but may lack modeling originality for ICLR","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to learn vector representations of prepositions by learning them as tensor decompositions of a triple of a left word (maybe head), the preposition, and right word (maybe dependent). This is an interesting idea with linguistic validity, and practically possible because of the commonness and promiscuity of prepositions, reflecting their primary grammatical and relational roles (as function words not content words). The resulting representations are show to be useful – they produce SOTA results on preposition selection by a decent margin (on a practically useful and recently studied tasks that is arguably the task that best reflects on the quality of the learned representations) and good (but not quite SOTA) results, without further linguistic features beyond POS tags, on the much more studied task of preposition attachment disambiguation. Overall, I really liked this paper.\n\nDespite this enthusiasm, I am doubtful whether this is a good paper for ICLR. And I write this as a card-carrying computational linguist. This is partly because of the writing. It is very hard not to see the content of the introduction as addressing a linguistics/computational linguistics audience rather than the mainstream of the ICLR audience (you get this impression rather strongly from the start of each of the first 3 paragraphs of the introduction...). More profoundly, this impression comes from the nature of the investigation and results. While this paper makes a contribution to representation learning in suggesting a good way to learn a representation for prepositions, it does not make any contributions to methods of representation learning. Indeed, it is basically an application of the orthogonalized alternating least squares method of Sharan and Valiant (2017) and more generally of the tensor decomposition ideas of numerous papers of Anandkumar and colleagues. There aren't any new technical ideas here.\n\nWhile the learned representations are successful for the two main performance tasks discussed above, the ancillary evidence provided from the paraphrasing of prepositional phrase seems highly uncompelling to me. That is, the task seems a completely valid one – one would like to be able to show that \"sparked off\" is a synonym of \"provoked\", but the actually results provided on this task seem quite uncompelling. Among other things, the example used in the text in section 3 seems bad to me. It isn't really the case that \"split off something\" means \"divided something\".  (\"Sally split off a sliver of wood\" does not mean \"Sally divided a sliver of wood\".  \"separated\" would be much closer. Indeed, of the examples in Table 2, the first 3 look bad, the fourth isn't generally  true but valid in certain contexts, the 5th is again wrong and only the 6th is really good. Similar remarks for the many more examples in the supplementary materials.\n\nThe most intriguing question is the one raised in the first paragraph of the conclusion: While prepositions are natural for modeling via word triples and indeed their high frequency and small number of types makes this quite practical, the kind of concerns raised here are also applicable to a whole bunch of word types, and it would be natural to want to extend the method to them.  E.g., we would also like to learn synonymy with light verb like \"take note\" or \"pay attention\" means roughly \"notice\" or \"observe\"; or the widely studied SVO triples like <rock,sank,ship> would also seem to cry out for a tensor decomposition. It would be interesting to think about what further might be done here.\n\n\nMinor comments:\n - Abstract: saying that word2vec and GloVe treat prepositions as \"content words\" seems slightly wrong; really they treat them just as \"words\" since all words are treated the same – though one can argue that most words are content words and the method of modeling word meaning is generally much more appropriate for content words.\n - p.2: \"folklore within the NLP community\". I'm not sure whether this is true or not; while pairwise counts have been the method of choice in recent word vector learning methods, it wasn't true of older methods (Collobert and Weston or Bengio's NPLM) and n-gram counts for n > 2 are widespread in pre-neural NLP. At any rate, it is rather unconvincing when the only evidence you cite is the paper by Sharan and Valiant, where AFAIK, neither author has ever published an NLP paper or attended  an NLP conference....\n - p.5 FEC should be FCE; Ng et al. (2014) should be (Ng et al., 2014)\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Tensor-Based Preposition Representation","abstract":"Prepositions are among the most frequent words. Good prepositional representation  is of great syntactic and semantic interest  in computational linguistics. Existing methods on preposition representation either treat prepositions as content words (e.g., word2vec and GloVe) or depend heavily on external linguistic resources including syntactic parsing, training task and dataset-specific representations. In this paper we use word-triple counts (one of the words is a preposition) to  capture the preposition's interaction with its head and children. Prepositional  embeddings are derived via tensor decompositions on a large unlabeled corpus.  We reveal a new geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing of phrasal verbs. Furthermore, our prepositional  embeddings are used as simple features to two challenging downstream tasks: preposition selection and prepositional attachment disambiguation. We achieve comparable to or better results than state of the art on  multiple standardized datasets.  ","pdf":"/pdf/28b2f29422c33519f13c3e01e61753002a7c753e.pdf","TL;DR":"This work is about tensor-based method for preposition representation training.","paperhash":"anonymous|tensorbased_preposition_representation","_bibtex":"@article{\n  anonymous2018tensor-based,\n  title={Tensor-Based Preposition Representation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkgYL010b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper192/Authors"],"keywords":["word representation","unsupervised learning","computational linguistics"]}},{"tddate":null,"ddate":null,"tmdate":1509739435901,"tcdate":1509054072122,"number":192,"cdate":1509739433242,"id":"BkgYL010b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkgYL010b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Tensor-Based Preposition Representation","abstract":"Prepositions are among the most frequent words. Good prepositional representation  is of great syntactic and semantic interest  in computational linguistics. Existing methods on preposition representation either treat prepositions as content words (e.g., word2vec and GloVe) or depend heavily on external linguistic resources including syntactic parsing, training task and dataset-specific representations. In this paper we use word-triple counts (one of the words is a preposition) to  capture the preposition's interaction with its head and children. Prepositional  embeddings are derived via tensor decompositions on a large unlabeled corpus.  We reveal a new geometry involving Hadamard products and empirically demonstrate its utility in paraphrasing of phrasal verbs. Furthermore, our prepositional  embeddings are used as simple features to two challenging downstream tasks: preposition selection and prepositional attachment disambiguation. We achieve comparable to or better results than state of the art on  multiple standardized datasets.  ","pdf":"/pdf/28b2f29422c33519f13c3e01e61753002a7c753e.pdf","TL;DR":"This work is about tensor-based method for preposition representation training.","paperhash":"anonymous|tensorbased_preposition_representation","_bibtex":"@article{\n  anonymous2018tensor-based,\n  title={Tensor-Based Preposition Representation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkgYL010b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper192/Authors"],"keywords":["word representation","unsupervised learning","computational linguistics"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}