{"notes":[{"tddate":null,"ddate":null,"tmdate":1515132852110,"tcdate":1515132328109,"number":4,"cdate":1515132328109,"id":"Sye2r5h7z","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Comment","forum":"rJJzTyWCZ","replyto":"SJU2A_Yyf","signatures":["ICLR.cc/2018/Conference/Paper525/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper525/Authors"],"content":{"title":"Response","comment":"Thank you for your valuable review!\n1. Please see our comment about the attention baseline in the top thread. \n2. Indeed,  the statement about informativeness is not rigorous. With further experiments, we find that the results should be explained by a distributional mismatch instead of informativeness. Specifically, when the training set contains both the human-designed data and automatically generated data, the accuracy on automatically generated data increases if we have a higher proportion of automatically generated data in the training set. Please see Table 7 for more details. We restructured Section 4 and removed the informativeness section. \n3. However, we believe human-designed data is a much better test bed for general cloze test with the following reasons: Human-designed data is different from automatically generated data since it leads to a larger gap between the model’s performance and the human performance. The model's performance and human's performance on the human-designed data are 0.484 and 0.860 respectively, leading to a gap of 0.376. The performance gap on the automatically-generated data is at most 0.185 since the model's performance reaches 0.815. Similarly, on Children’s Book Test where the questions are generated, the human performance is between 0.708 to 0.828 on four categories and the language model can nearly achieve human performance on the preposition and verb categories. Hence human-designed data is a good test base because of the larger gap between performances of the model and the human, although the distributional mismatch problem makes it difficult to be the best training source for out-of-domain cloze test such as automatically generated cloze test.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515132745981,"tcdate":1515132233563,"number":3,"cdate":1515132233563,"id":"HJMLr93Xz","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Comment","forum":"rJJzTyWCZ","replyto":"BJAUOGclz","signatures":["ICLR.cc/2018/Conference/Paper525/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper525/Authors"],"content":{"title":"Response","comment":"Thank you for your valuable review!\ni) Please see our comment about the attention baseline in the top thread. \nii) The error rates for each kind of questions are added in Figure 1. \niii) The questions in CLOTH dataset require a wider span when compared to automatically generated questions. We added more comparisons about human-designed data and automatically generated data in Section 4.1. \niv) The margin 15.3% results from training on a large external dataset. Specifically, the 1-billion-word dataset is more than 40 times larger than our dataset. However, in practice, it requires too many computational resources to train models on such a large dataset. Hence, it is valuable to compare models that do not use external data. When we do not use external data, the margin between the best model and the human performance is 27.7%, which is still a large margin.\nv) Accuracies on all categories are improved if we train the LM on the 1-billion-word corpus. It shows that a large amount of data is necessary to learn complex language regularities. Please see Figure 1 for more details. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515131963579,"tcdate":1515131963579,"number":2,"cdate":1515131963579,"id":"H1zBE92QM","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Comment","forum":"rJJzTyWCZ","replyto":"BynNGX9eG","signatures":["ICLR.cc/2018/Conference/Paper525/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper525/Authors"],"content":{"title":"Response","comment":"Thank you for your valuable review! Please see our comment about the attention baseline in the top thread. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515132495050,"tcdate":1515131612974,"number":1,"cdate":1515131612974,"id":"SJ3RG937f","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Comment","forum":"rJJzTyWCZ","replyto":"rJJzTyWCZ","signatures":["ICLR.cc/2018/Conference/Paper525/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper525/Authors"],"content":{"title":"Attention Baselines","comment":"Since all three reviewers suggested employing stronger baselines, specifically attention models, we will first clarify here:\n\n1. We tested machine comprehension models (with attention) when we started working on the task but found that they do not significantly outperform the LSTM baseline. Specifically, the Stanford Attentive Reader achieves an accuracy of 0.487 on CLOTH while an LSTM based method has an accuracy of 0.484. We also implemented position-aware attention model [Zhang et al. 2017] to enable the model to use the distance information. It achieves an accuracy of 0.485. We have updated these results in the paper. \n2. In fact, LSTM based language model is capable of modeling statistical regularities of language. Hill et al. 2015 show language models outperform memory networks and nearly achieves human performance on the verbs or prepositions questions of Children’s Book Test. A concurrent work also shows that language model is very good at modeling complex language regularities when trained on a large amount of data, although they use the LM to extract features instead of directly using it for prediction (Please see ICLR submission  “Deep contextualized word representations” ). Specifically, by replacing word vectors with hidden representations of LM, they achieve state-of-the-art results on six language tasks including textual entailment, question answering, semantic role labeling, coreference resolution, named entity extraction, sentiment analysis. Reasoning also benefits from LM features, e.g., the F1 on reading comprehension (SQuAD) improves from 81.1 to 85.3.\n3. We hypothesize the attention models’ unexpected performance is due to the difficulty to learn to comprehend longer contexts when the majority of the training data only requires understanding short-term information. Specifically, there are 23.2% of questions that require a long-term context. Note that although the cloze test was previously introduced for evaluating reasoning abilities in the machine comprehension task, CLOTH does NOT focus on reasoning. We mentioned the difference in the related work section: “Our dataset focuses on evaluating language proficiency including knowledge in vocabulary, reasoning and grammar while the focus of reading comprehension is reasoning.” We have updated the paper to emphasize this point in the introduction. \n\nReference:\nZhang, Y., Zhong, V., Chen, D., Angeli, G., & Manning, C. D. (2017). Position-aware Attention and Supervised Data Improve Slot Filling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 35-45).\nHill, F., Bordes, A., Chopra, S., & Weston, J. (2015). The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations. arXiv preprint arXiv:1511.02301.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515857381550,"tcdate":1511825972117,"number":3,"cdate":1511825972117,"id":"BynNGX9eG","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Review","forum":"rJJzTyWCZ","replyto":"rJJzTyWCZ","signatures":["ICLR.cc/2018/Conference/Paper525/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This is an interesting dataset but the baselines are not very compelling.","rating":"4: Ok but not good enough - rejection","review":"This paper collects a cloze-style fill-in-the-missing-word dataset constructed manually by English teachers to test English proficiency.  Experiments are given which are claimed to show that  this dataset is difficult for machines relative to human performance.  The dataset seems interesting but I find the empirical evaluations unconvincing.  The models used to evaluate machine difficulty are basic language models.  The problems are multiple choice with at most four choices per question.  This allows multiple choice reading comprehension architectures to be used.   A window of words around the blank could be used as the \"question\".  A simple reading comprehension baseline is to encode the question (a window around the blank) and use the question vector to compute an attention over the passage.  One can then compute a question-specific representation of the passage and score each candidate answer by the inner product of the question-specific sentence representation and the vector representation of the candidate answer.  See \"A thorough examination of the CNN/Daily Mail reading comprehension task\" by Chen, Bolton and Manning.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515642462205,"tcdate":1511823446759,"number":2,"cdate":1511823446759,"id":"BJAUOGclz","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Review","forum":"rJJzTyWCZ","replyto":"rJJzTyWCZ","signatures":["ICLR.cc/2018/Conference/Paper525/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not convinced","rating":"4: Ok but not good enough - rejection","review":"1) this paper introduces a new cloze dataset, \"CLOTH\", which is designed by teachers. The authors claim that this cloze dataset is a more challenging dataset since CLOTH requires a deeper language understanding and wider attention span. I think this dataset is useful for demonstrating the robustness of current RC models. However, I still have the following questions which lead me to reject this paper.\n\n2) I have the questions as follows:\ni) The major flaw of this paper is about the baselines in experiments. I don't think the language model is a robust baseline for this paper.  When a wider span is used for selecting answers, the attention-based model should be a reasonable baseline instead of pure LM. \nii) the author also should provide the error rates for each kind of questions (grammar questions or long-term reasoning). \niii) the author claim that this CLOTH dataset requires wider span for getting the correct answer, however, there are only 22.4 of the entire data need long-term reasoning. More importantly, there are 26.5% questions are about grammar. These problems can be easily solved by LM. \niv) I would not consider 16% percent of accuracy is a \"significant margin\" between human and pure LM-based methods. LM-based methods should not be considered as RC model.\nv) what kind accuracy is improved if you use 1-billion corpus trained LM? Are these improvements mostly in grammar? I did not see why larger training corpus for LM could help a lot about reasoning since reasoning is only related to question document.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515642462244,"tcdate":1510735534025,"number":1,"cdate":1510735534025,"id":"SJU2A_Yyf","invitation":"ICLR.cc/2018/Conference/-/Paper525/Official_Review","forum":"rJJzTyWCZ","replyto":"rJJzTyWCZ","signatures":["ICLR.cc/2018/Conference/Paper525/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Promising dataset; needs better experiments to analyze","rating":"7: Good paper, accept","review":"This paper presents a new dataset for cloze style question-answering. The paper starts with a very valid premise that many of the automatically generated cloze datasets for testing reading comprehension suffer from many shortcomings. The paper collects data from a novel source: reading comprehension data for English exams in China. The authors collect data for middle school and high school exams and clean it to obtain passages and corresponding questions and candidate answers for each question.\n\nThe rest of the paper is about analyzing this data and performance of various models on this dataset. \n\n1) The authors divide the questions into various types based on the type of reasoning needed to answer the question, noticeably short-term reasoning and long-term reasoning. \n2) The authors then show that human performance on this dataset is much higher than the performance of LSTM-based and language model-based baselines; this is in contrast to existing cloze style datasets where neural models achieve close to human performance. \n3) The authors hypothesize that this is partially explained by the fact that neural models do not make use of long-distance information. The authors verify their claim by running human eval where they show annotators only 1 sentence near the empty slot and find that the human performance is basically matched by a language model trained on 1 billion words. This part is very cool.\n4) The authors then hypothesize that human-generated data provides more information. They even train an informativeness prediction network to (re-)weight randomly generated examples which can then be used to train a reading comprehension model.\n\nPros of this work:\n1) This work contributes a nice dataset that addresses a real problem faced by automatically generated datasets.\n2) The breakdown of characteristics of questions is quite nice as well.\n3) The paper is clear, well-written, and is easy to read.\n\nCons:\n1) Overall, some of the claims made by the paper are not fully supported by the experiments. E.g., the paper claims that neural approaches are much worse than humans on CLOTH data -- however, they do not use state-of-the-art neural reading comprehension techniques but only a standard LSTM baseline. It might be the case that the best available neural techniques are still much worse than humans on CLOTH data, but that remains to be seen. \n2) Informativeness prediction: The authors claim that the human-generated data provides more information than automatically/randomly generated data by showing that the models trained on the former achieve better performance than the latter on test data generated by humans. The claim here is problematic for two reasons:\n   a) The notion of \"informativeness\" is not clearly defined. What does it mean here exactly?\n   b) The claim does not seem fully justified by the experiments -- the results could just as well be explained by distributional mismatch without appealing to the amount of information per se. The authors should show comparisons when evaluating on randomly generated data.\n\nOverall, this paper contributes a useful dataset; the analysis can be improved in some places.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1515131298778,"tcdate":1509125382687,"number":525,"cdate":1509739252505,"id":"rJJzTyWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJJzTyWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Large-scale Cloze Test Dataset Designed by Teachers","abstract":"Cloze test is widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-designed cloze test dataset CLOTH in which the questions were used in middle-school and high-school language exams. With the missing blanks carefully created by teachers and candidate choices purposely designed to be confusing, CLOTH requires a deeper language understanding and a wider attention span than previous automatically generated cloze datasets. We show humans outperform dedicated designed baseline models by a significant margin, even when the model is trained on sufficiently large external data. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending a long-term context to be the key bottleneck. In addition, we find that human-designed data leads to a larger gap between the model's performance and human performance when compared to automatically generated data. ","pdf":"/pdf/c467d55ffc4d0ceb8d88467df0d56f660a005dfe.pdf","TL;DR":"A cloze test dataset designed by teachers to assess language proficiency","paperhash":"anonymous|largescale_cloze_test_dataset_designed_by_teachers","_bibtex":"@article{\n  anonymous2018large-scale,\n  title={Large-scale Cloze Test Dataset Designed by Teachers},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJJzTyWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper525/Authors"],"keywords":["dataset","human-designed","language understanding"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}