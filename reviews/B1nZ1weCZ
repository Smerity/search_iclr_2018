{"notes":[{"tddate":null,"ddate":null,"tmdate":1515493827537,"tcdate":1515493827537,"number":5,"cdate":1515493827537,"id":"rJj6KMfVG","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Comment","forum":"B1nZ1weCZ","replyto":"Sy6Q3DTXz","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper284/AnonReviewer3"],"content":{"title":"Comments on rebuttal for reviewer 3","comment":"I will slightly increase my score and I will not argue against the paper because the paper contains interesting material. Nevertheless, in my opinion, the active strategy heavily relies on the knowledge of target scores. The paper should have contained a precise description of the DUA4C algorithm --not only experimental results--. For instance, when a target score is doubled and becomes unfeasible, how the algorithm behaves ? Why is there no degradation in this case ?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515188639770,"tcdate":1515187237412,"number":4,"cdate":1515187237412,"id":"Sy6Q3DTXz","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Comment","forum":"B1nZ1weCZ","replyto":"r1XoHKtlf","signatures":["ICLR.cc/2018/Conference/Paper284/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper284/Authors"],"content":{"title":"Rebuttal for Reviewer 3","comment":"Thank you for the reviews. We address your comments below:\n\n> In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption.\nWe believe that the Doubling Paradigm is an important part of the paper and thus, as requested by the reviewer, we have added additional results for the DUA4C agent. \n\nApart from MT1, we now show results on another 6 task instance (MT2), one 8 task instance (MT4) and one 12 task instance (MT5). \nIn all the cases, the DUA4C agent outperforms the BA3C agent and is able to perform well on all the MTIs. \nWe are still running the DUA4C agent on the 21 task instance and will be able to add the results on the same in the camera-ready version of the paper. These results have increased the quality of our work and we hope the reviewer raises his score in the light of these new experiments.\n\n\n> Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks. The gap is so large that I am not convinced on the fairness of the comparison. For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge.\n\nAs stated in Section 4.1, we do believe that the lackluster performance of BA3C agent is due to the uniform sampling of the tasks. The DUA4C agent is not provided with the baselines either and it is nevertheless able to beat the BA3C agent by a margin on all the MTIs. The experiments with DUA4C verify our claim that it is indeed the probability distribution over the tasks that causes the huge improvement in our agents.\n\n\n> I do not see how the single output layer is defined.\n\nAs stated in Section 3, the single output layer is a superset of all the actions in different tasks. Take an MTI with Pong and Breakout. Pong has valid actions as up, down, and no-op(do nothing). Breakout has valid actions as left, right and no-op. The single output layer will have valid actions as up, down, left, right and no-op. While playing an episode of Pong, if the agent chooses left or right(non-valid actions for Pong), it would be treated as a no-op action. \nIn all our experiments, since we deal with Atari Games, we set the output layer as all the possible 18 actions in ALE with non-valid actions as a no-op.\nYou can now see how not providing the identity of the task makes learning hard. The agent on seeing a frame is supposed to figure out what is the valid action subset first and thus, learning is harder.\n\n\n> As said in the general comments, in my opinion Section 6 should be developed and more experiments should be done with the DUA4C algorithm.\n\nWe have hopefully addressed the issue of developing DUA4C further with the new experiments.\n\n\n> Section 7.1. It is not clear why degradation does not happen. It seems to be only an experimental fact.\n\nWhile we do agree that we haven’t provided with a theoretical explanation of why degradation doesn’t happen, Section 7.1 does provide with an intuition for why the algorithm is able to prevent catastrophic forgetting. We reiterate: Catastrophic Forgetting in our agents is avoided due to the way in which we sample the tasks. The probability of a task getting sampled in our agents is higher for those tasks on which the agent is currently bad at. Once the agent becomes good on a task, if degradation has happened on a task which was previously good, the agent will switch back to the other task and will thus ensure that it trains more on the degraded task.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515188603985,"tcdate":1515186750127,"number":3,"cdate":1515186750127,"id":"ryLScD6QM","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Comment","forum":"B1nZ1weCZ","replyto":"BkTVPAYeG","signatures":["ICLR.cc/2018/Conference/Paper284/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper284/Authors"],"content":{"title":"Rebuttal for Reviewer 2","comment":"Thanks for reviewing the paper, the comments and questions! We believe addressing these questions will increase the quality of the work, and we will certainly do that.\n\n> Comparison only to a very basic baseline (i.e. uniform sampling). Couldn't comparisons be made, in some way, to other multitask work?\n\nWe do make a direct comparison to another multi-task work. As stated in Section 5, the tasks in MT4 (8 task instance) are exactly the same as those used in Actor Mimic Networks (Parisotto et al., 2015). AMNs achieve a q_am of 0.79 while all of our agents achieve a q_am greater than 0.9.\n\n\n> The assumption … future work section).\n\nBefore we go ahead, we would like to reiterate that we see the baselines as target scores that we want to achieve on the tasks. As we have shown in Appendix G, it’s not necessary to take them from published works, a human being could try solving a task and set his score as the target as well. Our algorithm is robust to target scores as well as seen in the same Appendix, i.e you could choose (reasonably) bigger targets as well.\n\nWe however also believe that the Doubling Paradigm is an important part of the paper and thus, as requested by the reviewer, we have added additional results for the DUA4C agent. Apart from MT1, we now show results on another 6 task instance (MT2), one 8 task instance (MT4) and one 12 task instance (MT5). We are still running the DUA4C agent on the 21 task instance and will be able to add the results on the same in the camera-ready version of the paper. In all the cases, the DUA4C agent outperforms the BA3C agent and is able to perform well on all the MTIs. These results have increased the quality of our work and we thank the reviewer again for raising these requests.\n\n\n> Can the authors show that a trained network (via their multitask approached) learns significantly faster on a brand new game (that's similar to games already trained on), compared to learning from scratch?\n\nThe work we have presented focuses specifically on Multi-task learning only and not transfer learning and thus, we didn’t show results on transfer learning. While we haven’t shown explicit results on transfer learning, we STRONGLY believe that it will indeed be the case that the MTAs will learn faster on a new similar game. This is attributed to the fact that all the agents in our work learn task agnostic features (as shown in Section 7) and having learned these features beforehand will speed up training on a similar new task. All in all, we are currently designating transfer learning as future work.\n\n\n> How does the performance improve/degrade (or the variance), on the same set of tasks, if the different multitask instances (MT_i) formed a supersets hierarchy, ie if MT_2 contained all the tasks/games in MT_1, could training on MT_2 help average performance on the games in MT_1 ? Could go either way since the network has to allocate resources to learn other games too.  But is there a pattern?\n\nWe do have a supersets hierarchy in the MTIs we’ve chosen. Note that MT1 is a subset of MT5. We see that it is indeed the case that the network has allocated resources to learn other games too. For an A5C agent trained on MT5, the q_am for just the MT1 tasks is 0.697. For the A5C agent trained on MT1, the q_am is 0.799. Please note that the size of the network is same in both the cases. Clearly, the network has allocated some of its representational power to learn the other games. We, however, do not claim this to be a pattern and this forms an interesting direction for further work. We thank the reviewer for this question. This provides further insight into how the network is allocating its resources for multi-tasking.\n\n\n> 'Figure 7.2' in section 7.2 refers to Figure 5.\n\nWe apologize for the typo. We’ve fixed it in the revision.\n\n\n> Can you motivate/discuss better why not providing the identity of a game as an input is an advantage? Why not explore both possibilities? what are the pros/cons? (section 3)\n\nNot providing the identity of the game is clearly not an advantage. This is because the agent now has to figure out the subset of actions which make sense for the task (if the actions not valid for a task are chosen, it is treated as a no-op action). It makes the setup harder to solve. The motivation behind doing this is that in real-world problems, the identity of the tasks might not be provided. We point out that in spite of not providing the identity of the tasks, the agents perform quite well on the MTIs."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515186533473,"tcdate":1515186533473,"number":2,"cdate":1515186533473,"id":"rkpwKvT7M","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Comment","forum":"B1nZ1weCZ","replyto":"HkFWypcgf","signatures":["ICLR.cc/2018/Conference/Paper284/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper284/Authors"],"content":{"title":"Rebuttal for Reviewer 1","comment":"Thank you for the positive reviews. We address your comments below:\n\n> The choice of distinct of Atari games to multitask learn may be almost adversarial.\n\nWe agree with the reviewer that the choice of tasks in our paper could be adversarial because the state spaces are very different visually. This was intentional (we point it out in the caption of Fig 1) with the purpose of raising the standard of the results and strengthens the work presented because, in spite of the state spaces being so visually different, the agents are able to perform very well on all the tasks as the results show.\n\n\n> How efficient would the algorithm be for visually similar tasks?\n\nAs we claim in the introduction to Section 7, an ideal MTA performs well due to learning task-agnostic abstract features which help it generalize across multiple tasks. In the case where tasks have visually similar state spaces, finding such features is clearly easier. We thus believe solving visually similar tasks are easier. \nApplying the framework to environments apart from Atari has currently been left as future work because of time and computational constraints."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642425022,"tcdate":1511866112597,"number":3,"cdate":1511866112597,"id":"HkFWypcgf","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Review","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Improved multitask deep reinforcement learning with active learning","rating":"7: Good paper, accept","review":"In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games. A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works.\n\nAlthough the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper. Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper. The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs.\n\nAlthough performance is still reduced compared to single task learning in some cases, this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale.\n\nI wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642425062,"tcdate":1511806772929,"number":2,"cdate":1511806772929,"id":"BkTVPAYeG","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Review","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An active learning approach to multitask RL with significant potential","rating":"7: Good paper, accept","review":"\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL. The domain is video games.   All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).\n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL\n\n- a number of approaches developed for the basic idea\n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)\n\n- paper is overall well written/clear\n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling)\nCouldn't comparisons be made, in some way, to other multitask work?\n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!).\n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling).\n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section).\n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space). Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper.\n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features. Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting.\n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game\n    (that's similar to games already trained on), compared to learning from\n    scratch?\n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ? Could go either way since the network\n   has to allocate resources to learn other games too.  But is there a pattern?\n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5.\n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage? Why not explore both\n  possibilities? what are the pros/cons? (section 3)\n\n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642425112,"tcdate":1511785883297,"number":1,"cdate":1511785883297,"id":"r1XoHKtlf","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Review","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference/Paper284/AnonReviewer3"],"readers":["everyone"],"content":{"title":"New online algorithms for learning multiple sequential problems","rating":"5: Marginally below acceptance threshold","review":"The paper present online algorithms for learning multiple sequential problems. The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm. Experimental results are given on different multi-task instances. The contributions are interesting and experimental results seem promising. But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix (ten sections in the Appendix and 28 pages). Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known. This is a very strong assumption. In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption. Therefore, I am not convinced that the paper is ready for publication at ICLR'18.\n* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks. The gap is so large that I am not convinced on the fairness of the comparison. For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge.\n* I do not see how the single output layer is defined.\n* As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm.\n* Section 7.1. It is not clear why degradation does not happen. It seems to be only an experimental fact.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511666296672,"tcdate":1511666296672,"number":3,"cdate":1511666296672,"id":"rybYG3Pgz","invitation":"ICLR.cc/2018/Conference/-/Paper284/Public_Comment","forum":"B1nZ1weCZ","replyto":"H1kXk_weG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reply to author(s)","comment":"Thank you for the confirmation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511649173635,"tcdate":1511649046742,"number":1,"cdate":1511649046742,"id":"H1kXk_weG","invitation":"ICLR.cc/2018/Conference/-/Paper284/Official_Comment","forum":"B1nZ1weCZ","replyto":"H1KJtmmez","signatures":["ICLR.cc/2018/Conference/Paper284/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper284/Authors"],"content":{"title":"Regarding the discrepancy in plots","comment":"We thank the reader for pointing out the error.\n\nThere indeed is a discrepancy in the STA3C scores on Page 7 and 8. We checked our results again and we found that the STA3C scores in Figure 3 on Page 7 are correct (they can be verified in Appendix J of the paper). We'll make the corrections in Figure 4 and update the paper once we're allowed to.\n\nAs explained in the paper, the STA3C scores are of no importance during the learning of the DUA4C agent. Thus, we checked our results for the final values of the performance metrics (since they do depend on the STA3C agent scores) in case they needed to be changed as well. We found that the performance metrics (p_am, q_am, etc.) were all calculated using the correct baseline scores and thus they are ALL CORRECT. That is the results reports in Table 2 are correct.\n\nWhile we agree that DUA4C does better on Demon Attack only (and is nearly equal to STA3C on Seaquest), the purpose of a Multi-tasking network is to do reasonably well on ALL the tasks, which might come at the cost of not doing better than the baseline on some tasks. We had included this discussion in the paper in the performance metrics section where we motivated the new performance metric q_am.\n\nFinally, no we didn't use a different set of parameters for the A3C scores. We're not sure of how the error crept only into the plot. We apologize again for the mistake in Figure 4."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511368928760,"tcdate":1511368928760,"number":1,"cdate":1511368928760,"id":"H1KJtmmez","invitation":"ICLR.cc/2018/Conference/-/Paper284/Public_Comment","forum":"B1nZ1weCZ","replyto":"B1nZ1weCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Difference in baseline scores","comment":"There seems to be a difference in the baseline scores reported between page 7 and page 8 for A3C/STA3C scores. If I use the baseline scores from page 7 and compare them against DUA4C, I think DUA4C does better in Demon Attack only?\n\nHave you used a different set of parameters to compute the A3C scores for the DUA4C experiments?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515218766225,"tcdate":1509089027548,"number":284,"cdate":1509739383536,"id":"B1nZ1weCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1nZ1weCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Multi-Task by Active Sampling","abstract":"One of the long-standing challenges in Artificial Intelligence for learning goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential problems has been in the form of distillation based learning wherein a student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large expert networks which require extensive data and computation time for training.\nIn this work, we propose an efficient multi-task learning framework which solves multiple goal-directed tasks in an on-line setup without the need for expert supervision. Our work uses active learning principles to achieve multi-task learning by sampling the harder tasks more than the easier ones. We propose three distinct models under our active sampling framework. An adaptive method with extremely competitive multi-tasking performance. A UCB-based meta-learner which casts the problem of picking the next task to train on as a multi-armed bandit problem. A meta-learning method that casts the next-task picking problem as a full Reinforcement Learning problem and uses actor-critic methods for optimizing the multi-tasking performance directly. We demonstrate results in the Atari 2600 domain on seven multi-tasking instances: three 6-task instances, one 8-task instance, two 12-task instances and one 21-task instance.","pdf":"/pdf/11d06848dc03d237cbcfd3bf14782c7b3fd17bb2.pdf","TL;DR":"Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially","paperhash":"anonymous|learning_to_multitask_by_active_sampling","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Multi-Task by Active Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1nZ1weCZ}\n}","keywords":["Deep Reinforcement Learning"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper284/Authors"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}