{"notes":[{"tddate":null,"ddate":null,"tmdate":1514425976124,"tcdate":1514425976124,"number":4,"cdate":1514425976124,"id":"B1etRT-7f","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Comment","forum":"S17mtzbRb","replyto":"BkH22ZFxG","signatures":["ICLR.cc/2018/Conference/Paper883/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper883/Authors"],"content":{"title":"Response to AnonReviewer1","comment":"Q. Have you compared with other choices of similarity measure, e.g., cosine similarity?\nA. We performed additional experiments on MNIST strokes sequences dataset and on the DBPedia dataset using the cosine similarity on weights, cosine similarity on activations, and orthogonality regularization on the weights.  All of these improved the quality of clusterization in terms of AMI and NMI, however, the regularizations we propose in this paper perform the best (see the results table in the general response section above).  We hypothesize that the reason for this is that the proposed method involves non-linear relationships between the change in the weights and the corresponding loss, whereas, for example, cosine similarity does not.\n\nQ. In the binary classification experiments, it is very strange to almost randomly group several different classes of images into the same category...\nA. Our goal was to look at the case where labeled classes are composed from different “types” (sub-classes) of objects. In a sense, this is a hierarchical classification, where only the labels of the first level is accessible to the network.  However, we agree that additional experiments using a proper hierarchical dataset would also be informative, and we will include them.\n\nQ. I am curious how the proposed method compares to other competitors in terms of the original classification setting.\nA. We compared the classification accuracy on the binary classification task and there was no effect (or a negligible effect) of the proposed method on the classification accuracy. \n\nQ. What will happen for the multi-layer loss if the network architecture is very large such that you can not use large batch size, e.g., less than 10?\nA: Since the batches are usually formed randomly, chances are there will be samples with the same label (this is the always the case in the batch of 3 or more) and different underlying groups. Since the training is repeated for many batches, this should not be an issue. \n\nQ. In drawing figure 2 and 3, if the nonlinear activation function is not ReLU, how would you exam the same behavior? \nA: Actually, we experimented with different activation functions (ReLU, sigmoid, tanh) and they all had the same behaviour. We will mention this in the paper.\n\nQ. Have you tried multi-class classification for the case “without proposed loss component” and does the similar pattern still happen or not?\nA. We have not tried this. We expect the same behaviour as it was general for such diverse architectures and tasks, and the binary classification is just a particular case of a multi-class classification. We will, however, perform additional experiments to verify it empirically. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1514425936707,"tcdate":1514425936707,"number":3,"cdate":1514425936707,"id":"HyYLRp-7G","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Comment","forum":"S17mtzbRb","replyto":"HyDv0CYgz","signatures":["ICLR.cc/2018/Conference/Paper883/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper883/Authors"],"content":{"title":"Response to AnonReviewer2","comment":"Q. Why is softmax applied to weight vectors and ReLU activations to convert them to  probability distributions, as opposed to, for example, using dot products or other simpler alternatives to measure similarity?\n\nA. We chose the proposed measure based on an intuition that its non-linear nature would be better suitable in this case as opposed to, for example, dot product.  In the general response section above, we report additional experiments on the MNIST strokes sequences and on DBPedia using the W^TW regularization, as well as cosine similarity on weights and cosine similarity on activations.  All of these improved the quality of clusterization in terms of AMI and NMI, however, the regularizations we propose in the paper perform the best.  Interestingly, the general approach of forcing apart weights/activation improves the quality of the clustering regardless of the particular similarity measure used.\n\nQ. The objective of this paper seems to be to produce representations that are\neasy to separate into clusters. This topic has a wealth of previous work. Of\nparticular relevance are methods such as t-SNE [1], parametric t-SNE [2], and\nDEC [3]. \nA. Note that the goal of the proposed method is quite different from the objective of t-SNE, which is pure dimensionality reduction. Our goal, in contrast, is to learn a representation suitable for clustering in the absence of exhaustive labels that would allow the model to learn this representation explicitly from supervision while solving a classification task.  Essentially, we are trying to address a different task, even though it also relates to recovering latent structure in the data.\n\nQ. Disentangling usually refers to disentangling factors of variation. This paper seems  to be about learning separable representations.\nA. Thank you for catching that, we were actually aware of this, and have planned to change the title to avoid the confusion. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1514425889182,"tcdate":1514425889182,"number":2,"cdate":1514425889182,"id":"BytXAaW7f","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Comment","forum":"S17mtzbRb","replyto":"r1qbYHogz","signatures":["ICLR.cc/2018/Conference/Paper883/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper883/Authors"],"content":{"title":"Response to AnonReviewer3 ","comment":"Most of the comments in this review seem to stem from the reviewer’s issues with presentation and writing clarity, rather than the substantive proposal in this paper.  While it seems that the other reviewers found the presentation to be clear, we will make an earnest attempt to address the concerns with the presentation flow raised in this review.\n\nQ. What are Adjusted Mutual Information (AMI) and Normalized Mutual Information  (NMI)? How are they calculated? Or at least, the mutual information between what and what are they measuring?\nA. MI-based measures we use to evaluate clustering solutions are quite standard, and we give a reference to the paper that explains them in detail.  However, we would be happy to include definitions in the camera-ready version of the paper.\n\nQ. “this loss component may help to learn a better representation only if the input to the target layer still contains the information about latent characteristics of the input data\" What does this mean? The representation always contains such information, that is relevant to the task at hand…\nA. As we explain in the introduction, for example, if the target task is binary classification, the representation learned by the network may only contain the information relevant to that task. As a result, clustering the data according to other latent characteristics may be impossible, since they might not be captured in this representation.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1514426715004,"tcdate":1514425790539,"number":1,"cdate":1514425790539,"id":"BkUT6TW7G","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Comment","forum":"S17mtzbRb","replyto":"S17mtzbRb","signatures":["ICLR.cc/2018/Conference/Paper883/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper883/Authors"],"content":{"title":"General response","comment":"We thank the reviewers, particularly reviewers #1 and #2, for their detailed and constructive comments.\n\nTo reiterate, in this paper, our goal was to learn a representation suitable for clustering in the absence of exhaustive labels that would allow the model to learn this representation explicitly from supervision while solving a classification task.  We proposed two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure.\n\nThe main concerns raised by the reviewers related to the choice of the similarity measure in the proposed regularization method, and its relative value compared to simpler methods such as dot products or cosine similarity on weights or activations.\nWe performed additional experiments on MNIST strokes sequences and on DBPedia using additional baseline methods mentioned by the reviewers, including cosine similarity on weights, cosine similarity on activations, and orthogonality regularization on the weights.  Please see the results in the table below. \n\nThere are two main takeaways from these experiments:\n 1. The proposed general approach of forcing apart weights/activation improves the quality of the clustering regardless of the particular similarity measure used.\n 2. While in all of these experiments, the quality of clusterization in terms of AMI and NMI was improved, the regularizations we proposed in the paper consistently performed the best.  \n\n+--------------------------------------------------------------------+\n|                                  | MNIST            | DBPedia          |\n+                                  +----------------------------------------+\n|                                  | AMI   | NMI   | AMI    | NMI   |\n+--------------------------------------------------------------------+\n| W^TW                      | 0.523 | 0.530 | 0.350 | 0.366 |\n+--------------------------------------------------------------------+\n| Weights cosine      | 0.520 | 0.527 | 0.385 | 0.412 |\n+--------------------------------------------------------------------+\n| Activations cosine | 0.516 | 0.550 | 0.384 | 0.449 |\n+--------------------------------------------------------------------+\n| Proposed                | 0.544 | 0.553 | 0.529 | 0.533 |\n+--------------------------------------------------------------------+"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1515642526821,"tcdate":1511901441828,"number":3,"cdate":1511901441828,"id":"r1qbYHogz","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Review","forum":"S17mtzbRb","replyto":"S17mtzbRb","signatures":["ICLR.cc/2018/Conference/Paper883/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Desperately seeking neural network representations to be more useful for clustering tasks","rating":"4: Ok but not good enough - rejection","review":"The paper proposes techniques for encouraging neural network representations to be more useful for clustering tasks. The paper contains some interesting experimental results, but unfortunately lacks concise motivation and description of the method and quality of writing.\n\nIntroduction:\nThe introduction is supposed to present the problem and the 'chain of events' that led to this present work, but does not do that. The first paragraph contains a too length explanation that in a classification task, representations are only concerned about being helpful for this task, and not any other task. The paragraph starting with 'Consider the case...', describes in detailed some specific neural network architecture, and what will happen in this architecture during training. The main problem with this paragraph is that it does not belong in the introduction. Indeed, other parts of the introduction have no relation to this paragraph, and the first part of the text that related to this paragraph appears suddenly in Section 3. The fact that this paragraph is two thirds of the introduction text, this is very peculiar.\n\nFurthermore, the introduction does not present the problem well: \n1) What does is a better representation for a clustering task?\n2) Why is that important?\n\nMethod:\nThere are a few problematic statements in this part:\n\"The first loss component L_single works on a single layer and does not affect the other layers in the network\". This is not exactly true, because it affect the layer it's related to, which affect upper layers through their feedforward input or bottom layer through the backward pass. \n\"Recall from the example in the introduction that we want to force the model to produce divergent representations for the samples that belong to the same class, but are in fact substantively different from each other\". It is not clear why this is a corollary of the example in the introduction (that should be moved to the method part). \n\"this loss component may help to learn a better representation only if the input to the target layer still contains the information about latent characteristics of the input data\". What does this mean? The representation always contains such information, that is relevant to the task at hand...\nAnd others. The main problem is that the work is poorly explained: starting from the task at hand, through the intuition behind the idea how to solve it. \n\nThe experiments parts contains results that show that the proposed method is superior by a substantial margin over the baseline approaches. However, the evaluation metrics and procedure are poorly explained; What are Adjusted Mutual Information (AMI) and Normalized Mutual Information  (NMI)? How are they calculated? Or at least, the mutual information between what and what are they measuring?\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1515642526861,"tcdate":1511808607105,"number":2,"cdate":1511808607105,"id":"HyDv0CYgz","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Review","forum":"S17mtzbRb","replyto":"S17mtzbRb","signatures":["ICLR.cc/2018/Conference/Paper883/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Needs comparison to other baselines","rating":"5: Marginally below acceptance threshold","review":"Summary\nThis paper proposes two regularizers that are intended to make the\nrepresentations learned in the penultimate layer of a classifier more conforming\nto inherent structure in the data, rather than just the class structure enforced\nby the classifier. One regularizer encourages the weights feeding into the\npenultimate layer to be dissimilar and the other encourages the activations\nacross samples (even if they belong to the same class) to be dissimilar.\n\nPros\n- The proposed regularizers are able to separate out the classes inherent in the\n  data, even if this information is not provided through class labels. This is\nvalidated on several datasets using visualizations as well as quantitative\nmetrics based on mutual information.\n\nCons\n- It is not explained why it makes sense to first convert the weight vectors\n  into probability distributions by applying the softmax function, and then\nmeasuring distances using KL divergence between the probability distributions.\nIt should be explained more clearly if there is there a natural interpretation\nof the weight vectors as probability distributions. Otherwise it is not obvious\nwhy the distance between the weight vectors is measured the way it is.\n\n- Similarly, the ReLU activations are also first converted into probability\n  distributions by applying a softmax. It should be explained why the model does\nthis, as opposed to simply using dot products to measure similarity.\n\n- The model is not compared to simpler alternatives such as adding an\n  orthogonality regularization on the weights, i.e., computing W^TW and making\nthe diagonals close to 1 and all other terms 0. Similar regularizers can be\napplied for activation vectors as well.\n\n- The objective of this paper seems to be to produce representations that are\n  easy to separate into clusters. This topic has a wealth of previous work. Of\nparticular relevance are methods such as t-SNE [1], parametric t-SNE [2], and\nDEC [3]. The losses introduced in this paper are fairly straight-forward.\nTherefore it would be good to compare to these baselines to show that a simple\nloss function is sufficient to achieve the objective.\n\n- Disentangling usually refers to disentangling factors of variation, for\n  example, lighting, pose, and object identity which affect the appearance of a\ndata point. This is different from separability, which is the property of a\nrepresentation that makes the presence of clusters evident. This paper seems to\nbe about learning separable representations, whereas the title suggests that it\nis about disentangled ones. \n\nQuality\nThe design choices made in the paper (such as the choice of distance function)\nis not well explained. Also, given that the modifications introduced are quite\nsimple, it can be improved by doing more thorough comparisons to other\nbaselines.\n\nClarity\nThe paper is easy to follow.\n\nOriginality\nThe novel aspect of the paper is the way distance is measured by converting the\nweights (and activations) to probability distributions and using KL divergence\nto measure distance. However, it is not explained what motivated this choice.\n\nSignificance\nThe objective of this model is to produce representations that are separable, which\nis of general interest. However, given the wealth of previous work done in\nclustering, this paper would only be impactful if it compares to other hard\nbaselines and shows clear advantages.\n\n[1] van der Maaten, Laurens and Hinton, Geoffrey. Visualizing\ndata using t-SNE. JMLR, 2008.\n\n[2] van der Maaten, Laurens. Learning a parametric embedding\nby preserving local structure. In International Conference\non Artificial Intelligence and Statistics, 2009.\n\n[3] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for\nclustering analysis. ICML 2016.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1515642526901,"tcdate":1511754924659,"number":1,"cdate":1511754924659,"id":"BkH22ZFxG","invitation":"ICLR.cc/2018/Conference/-/Paper883/Official_Review","forum":"S17mtzbRb","replyto":"S17mtzbRb","signatures":["ICLR.cc/2018/Conference/Paper883/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper proposes two regularization terms to encourage learning disentangled representations.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes two regularization terms to encourage learning disentangled representations. One term is applied to weight parameters of a layer just like weight decay. The other is applied to the activations of the target layer (e.g., the penultimate layer). The core part of both regularization terms is a compound hinge loss of which the input is the KL divergence between two softmax-normalized input arguments. Experiments demonstrate the proposed regularization terms are helpful in learning representations which significantly facilitate clustering performance.\n\nPros:\n(1) This paper is clearly written and easy to follow.\n\n(2) Authors proposed multiple variants of the regularization term which cover both supervised and unsupervised settings.\n\n(3) Authors did a variety of classification experiments ranging from time serials, image and text data.\n\nCons:\n(1) The design choice of the compound hinge loss is a bit arbitrary. KL divergence is a natural similarity measure for probability distribution. However, it seems that authors use softmax to force the weights or the activations of neural networks to be probability distributions just for the purpose of using KL divergence. Have you compared with other choices of similarity measure, e.g., cosine similarity? I think the comparison as an additional experiment would help explain the design choice of the proposed function.\n\n(2) In the binary classification experiments, it is very strange to almost randomly group several different classes of images into the same category. I would suggest authors look into datasets where the class hierarchy is already provided, e.g., ImageNet or a combination of several fine-grained image classification datasets.\n\nAdditionally, I have the following questions:\n(1) I am curious how the proposed method compares to other competitors in terms of the original classification setting, e.g., 10-class classification accuracy on CIFAR10. \n(2) What will happen for the multi-layer loss if the network architecture is very large such that you can not use large batch size, e.g., less than 10? \n\n(3) In drawing figure 2 and 3, if the nonlinear activation function is not ReLU, how would you exam the same behavior? Have you tried multi-class classification for the case “without proposed loss component” and does the similar pattern still happen or not?\n\nSome typos:\n(1) In introduction, “when the cosine between the vectors 1” should be “when the cosine between the vectors is 1”.\n\n(2) In section 4.3, “we used the DBPedia ontology dataset dataset” should be “we used the DBPedia ontology dataset”. \n\nI would like to hear authors’ feedback on the issues I raised.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]}},{"tddate":null,"ddate":null,"tmdate":1509739049563,"tcdate":1509136666677,"number":883,"cdate":1509739046906,"id":"S17mtzbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S17mtzbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels","abstract":"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods.","pdf":"/pdf/1127adbc6a9f6d01b20b784518adc30170f05d58.pdf","TL;DR":"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task.","paperhash":"anonymous|forced_apart_discovering_disentangled_representations_without_exhaustive_labels","_bibtex":"@article{\n  anonymous2018forced,\n  title={Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S17mtzbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper883/Authors"],"keywords":["learning representation","clustering","loss"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}