{"notes":[{"tddate":null,"ddate":null,"tmdate":1513946408003,"tcdate":1513946408003,"number":4,"cdate":1513946408003,"id":"SJlNadqfM","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Comment","forum":"S1GDXzb0b","replyto":"B1V5vHb1M","signatures":["ICLR.cc/2018/Conference/Paper805/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper805/Authors"],"content":{"title":"Alternative simple baseline ","comment":"Thank you for illustrating an alternative model-based method. We believe this is a useful baseline method that we should compare our method against. We agree that since our experiments are simple in nature, this proposed alternative method might perform equally well compared to the proposed method. \n\nHowever, the main difference between the suggested method and the proposed method, is that, the suggested method attempts to learn the inverse dynamics of the system (eg. given two locations (a,b) in space of the end-effector, find the torque value for moving a robotic arm from a to b) which might be difficult to learn in a general setting. Our proposed method learns the forward dynamics (eg. given current locations 'a' of the end-effector and torque values, find the next location), which might have a well-defined equation in mechanics in most general cases. However, we do agree that for the simple experimental evaluations that we have performed both, inverse and forward dynamics might be of equal difficulty.\n\nNo, in our case we manually specify the location of the flappy-bird as \\phi(s_t). It is challenging to directly learn dynamics model between raw video streams as has been already pointed out in the comment, and it is a limitation of the current proposed method. One method can be to automatically learn \\phi(s_t) in case of high dimensional inputs, using action prediction from consecutive states, using ideas in prior methods, like Pathak. et al. ICML 2017.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1513946255726,"tcdate":1513946174007,"number":3,"cdate":1513946174007,"id":"ry8Bn_5Mz","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Comment","forum":"S1GDXzb0b","replyto":"SymLN__gM","signatures":["ICLR.cc/2018/Conference/Paper805/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper805/Authors"],"content":{"title":"Rebuttal","comment":"We thank the reviewer for the overall constructive comments.\n\nQ : It does not cite or discuss a very important piece of related work: \"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation\" (Liu et al., 2017)\nA: Thank you for pointing out the relevant prior work using observations only. We added citation to this work in the introduction section of the paper. Our main contribution in this work is to show that the proposed method uses a combination of model-based and model-free methods for acceleration in imitation learning from observations alone. Although the mentioned prior work is similar to the proposed method, it's main focus is on transferring learned tasks on expert observations in a source domain to a novel target domain.\n\nQ: The empirical results are unconvincing - it seems like in all problems they use there is a straightforward mapping from state feature differences to actions, as pointed out in an anonymous comment.\nA : We agree that our experiments are simple in nature, with easy to learn dynamics model, which is a drawback of the current evaluation scheme. However, the main contribution of this paper is to present the novel idea that combination of proposed model-based and model-free training has the advantage of accelerated training for imitation learning from observation alone, which can be illustrated by these simple setups. In the future, we plan to build upon the current idea on complex dynamics model setup as well for future work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1513946023384,"tcdate":1513946023384,"number":2,"cdate":1513946023384,"id":"rJynoO9zM","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Comment","forum":"S1GDXzb0b","replyto":"rJfWQ9OeG","signatures":["ICLR.cc/2018/Conference/Paper805/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper805/Authors"],"content":{"title":"Rebuttal","comment":"Thank you for the overall encouraging review. We address some of the concerns in the following,\n\nQ : Not clear that method converges on all problems. \nA: Yes it does not converge on all dynamics models. Currently, the main drawback of the method is that it cannot model complex dynamics models like raw video transitions as mentioned in the anonymous comment also.\n\nQ : Not clear that the method is able to extract the state from video — authors had to extract position manually\nA: Learning the useful state representations from raw video is a challenging problem. In literature, Pathak. et al. ICML 2017 proposes to use a feature extractor \\phi, which learns to predict the action given the current and next state. However, in our case, we simplify the assumption by manually specifying parts of the state that depends on the actions. This is a limitation of the proposed method but we hope to address this issue in the future versions using methods in literature, such as Pathak. et al. ICML 2017.\n\nThe overall approach and algorithms are described fairly clearly. Some minor typos here and there.\nA : We changed the typos and reordered the figures.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1513945923694,"tcdate":1513945923694,"number":1,"cdate":1513945923694,"id":"HkhHjuqMf","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Comment","forum":"S1GDXzb0b","replyto":"HJO3Kl0ef","signatures":["ICLR.cc/2018/Conference/Paper805/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper805/Authors"],"content":{"title":"Rebuttal","comment":"We thank the reviewer for the overall constructive comments. We address some of the raised concerns below,\n\nQ: The algorithm is based on many heuristics that are not well motivated.\n \nA: We make simplifying assumption that the observation trajectories sampled from the expert(\\tau_e) and the trajectories taken by the imitating agent(\\tau_RL) occupy similar distribution in the observation space. Hence, we can replace the state transition model learned from expert data for reward estimation during reinforcement learning for imitating the expert. However, this is just a heuristic approach for {s_t, a_t, s_t+1} data collection to train the environment dynamics model. Our core idea is to reuse the expert observation trajectory (\\tau_e), that was used to train p(s_t+1|s_t) to train a model based policy obtaining policy parameter gradients from the dynamics model.\n\nQ :The algorithm is only optimizing the one step error function for imitation learning but not the long term behavior. \n\nWe do agree that the proposed model-based policy(\\pi_mb) as well as the state transition model does not take into account long-term reward maximization and suffers from compounding error problems due to single step reward optimization. Thank you very much for pointing out the limitation of the proposed method. Reflecting your comment, we gave clearer explanations so that the objective of the paper is more easily understood by the readers. Due to this reason, the proposed method's performance is always upper bounded by behavioral cloning's performance.\n\nQ : It is unclear why a model-based and model-free policy need to be used. Is the model-based policy used at any time in the algorithm? If it is just used as final result, why train it iteratively? Why can we not just also use the model-based policy for data collection?\n\nA: The iteration is necessary because of the following reason. The proposed algorithm collects a certain amount of system dynamics data, (s_t,a_t,s_{t+1}), by the model-free part. Then, we train a system dynamics model using the data and then train the action policy using the system dynamics model in the model-based part. This is one cycle. Then, we repeat the cycle again from the model-free part. With each iteration, we collect additional system dynamics data, which results in a more precise system dynamics model. This differentiable version of the dynamics model enables the direct policy parameter update by allowing gradient computation with respect to the actions. To explain the system flow more clearly, we modified the explanation in Sec 2.4. \n\nThe model-based policy may also be used to sample the data for training the dynamics model by a bootstrapping method, where the trajectories sampled from model-based policy will be used for training the dynamics model, which in turn will be used to train the policy network. However, such a method might be unstable because the triplets will be sampled from a continuously varying distribution which are trained by the same sampled trajectories. We need to compare the performance of the proposed method with this technique to comment about which method is more advantageous, but it is out of the scope of the current version of the paper.\n\nQ : Other algorithms (such as GAIL) could be used in the same setup (no action observations). Comparisons to other imitation learning approaches are needed.\n\nA: Since the original GAIL algorithm uses both expert states and action for imitation learning, we did not compare against this method. However, since learning in GAIL is model-free in nature, our model-based method might be faster than model-free counter-parts, which is the main claim of the paper, and shown by simple experimental setups. We understand that it is possible to setup GAIL (and other prior methods) with only expert observations and we hope to compare against these methods in the future.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1515642513763,"tcdate":1512077743690,"number":3,"cdate":1512077743690,"id":"HJO3Kl0ef","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Review","forum":"S1GDXzb0b","replyto":"S1GDXzb0b","signatures":["ICLR.cc/2018/Conference/Paper805/AnonReviewer3"],"readers":["everyone"],"content":{"title":"In summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated. Also the results are not convincing. Clear reject.","rating":"3: Clear rejection","review":"The paper presents a model-based imitation learning framework which learns the state transition distribution of the expert. A model-based policy is learned that should matches the expert transition dynamics. The approach can be used for imitation learning when the actions of the expert are not observed, but only the state transitions (which is an important special case).  \n\nPros:\n- The paper concentrates on an interesting special case of imitation learning\n\nCons:\n- The paper is written very confusingly and hard to understand. The algorithm needs to be better motivated and explained and the paper needs proof reading.\n- The algorithm is based on many heuristics that are not well motivated. \n- The algorithm is only optimizing the one step error function for imitation learning but not the long term behavior. It heavily relies on the learned transition dynamics of the expert p(s_t+1|s_t). This transition model will be wrong if we go away from the expert's trajectories. Hence, I do not see why we should use p(s_t+1|s_t) to define the reward function. It does not prevent the single step \nerrors of the policy to accumulate (which is the main goal of inverse reinforcement learning)\n- The results are not convincing\n- Other algorithms (such as GAIL) could be used in the same setup (no action observations). Comparisons to other imitation learning approaches are needed.\n\nIn summary, this is a poorly written paper that seems to rely on a lot of heuristics that are not well motivated. Also the results are not convincing. Clear reject.\n\n\nMore detailed comments\n- It is unclear why a model-based and model-free policy need to be used. Is the model-based policy used at any time in the algorithm? If it is just used as final result, why train it iteratively? Why can we not just also use the model-based policy for data collection?\n- It is unclear why the heuristic reward function makes sense. First of all, the defined reward is stochastic as \\hat{s}_t+1 is a sample from the next state from the expert's transition model. Why do not we use the mean of the transition model here, then it would not be stochastic any more. Second, a much simpler reward could be used that essentially does the same thing. Instead of requiring a learned dynamics model f_E for predicting the next state, we can just use the experienced next state s_t+1. Note that the reward function for time step t can depend on s_t+1 in an MDP.  \n- The objective that is optimized (Eq. 4) is not well defined. A function is not an objective function if we can only optimize part of it for theta while keeping theta fixed for the other part. It is unclear which objective the real algorithm optimizes\n- There are quite a few confusions in terms of notation. Sometimes, a stochastic transition model p(s_t+1|s_t, a_t) is used and sometimes a deterministic model f_E(s,a). It is unclear how they relate. \n- Many other imitation learning techniques could be used in this setup including max-entropy inverse RL [1], IRL by distribution matching [2] and the approach given in [3] and GAIL. A comparison to at least a subset of these methods is needed\n\n[1] B. Ziebart et al, Maximum Entropy Inverse Reinforcement Learning, AAAI 2008\n[2] Arenz, O.; Abdulsamad, H.; Neumann, G. (2016). Optimal Control and Inverse Optimal Control by Distribution Matching, Proceedings of the International Conference on Intelligent Robots and Systems (IROS)\n[3] P Englert, A Paraschos, J Peters, MP Deisenroth, Model-based Imitation Learning by Probabilistic Trajectory Matching, IEEE International Conference on Robotics and Automation","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1516043290317,"tcdate":1511723769634,"number":2,"cdate":1511723769634,"id":"rJfWQ9OeG","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Review","forum":"S1GDXzb0b","replyto":"S1GDXzb0b","signatures":["ICLR.cc/2018/Conference/Paper805/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting argument for model-based imitation learning","rating":"7: Good paper, accept","review":"Model-Based Imitation Learning from State Trajectories\n\nSIGNIFICANCE AND ORIGINALITY:\n\nThe authors propose a model-based method for accelerating the learning of a policy\nby observing only the state transitions of an expert trace.\nThis is an important problem in many fields such as robotics where\nfinding a feasible policy is hard using pure RL methods.\n\nThe authors propose a unique two step method to find a high-quality model-based policy.\n\nFirst: To create the environment model for the model-based learner, \n they need a source of state transitions with actions ( St, At,xa St+1 ).\nTo generate these samples, they first employ a model-free algorithm.\nThe model-free algorithm is trained to try to duplicate the expert state at each trajectory.\nIn continuous domains, the state is not unique … so they build a soft next state predictor\nthat gives a probability over next states favoring those demonstrated by the expert.\nSince the transitions were generated by the agent acting in the environment,\nthese transitions have both states and actions ( St, At, St+1 ).\nThese are added to a pool.\n\nThe authors argue that the policy found by this model-free learner is\nnot highly accurate or guaranteed to converge, but presumably is good at\ngenerating transitions relevant to the expert’s policy.\n(Perhaps slowly reducing the \\sigma in the reward would improve accuracy?)\nI guess if expert trace data is sparse, the model-free learner can generate a lot \nof transitions which enable it to create accurate dynamics models which in turn\nallow it to extract more information out of sparse expert traces?\n\nSecond: They then train a model based agent using the collected transitions ( St, At, St+1 ).\nThey formulate the problem as a maximum likelihood problem with two terms: \nan action dynamics model which is learned from local exploration using the learner’s own actions and outcomes\nand expert policy model in terms of the actions learned above \nthat maximizes the probability of the observed expert’s trajectory.\nThis is a nice clean formulation that integrates the two processes.\nI thought the comparison to an encoder - decoder network was interesting.\n\nThe authors do a good job of positioning the work in the context of recent work in IML.\n\nIt looks like the authors extract position information from flappy bird frames, \nso the algorithm is only using images for obstacle reasoning?\n\n\nQUALITY\n\nThe propose model is described fairly completely and evaluated on \na “reaching\" problem and the \"flappy bird” game domain.\nThe evaluation framework is described in enough detail to replicate the results.\n\nInterestingly, the assisted method starts off much higher in the “reacher” task.\nPresumably this task is easy to observe the correct actions.\n\nThe flappy bird test shows off the difference between unassisted learning (DQN),\nmodel free learning with the heuristic reward (DQN+reward prediction) \nand model based learning. \n\nInterestingly, DQN + heuristic reward approaches expert performance\nwhile behavioral cloning never achieves expert performance level even though it has actions.\n\nWhy does the model-based method only run to 600 steps and stopped before convergence??\nDoes it not converge to expert level?? If so, this would be useful to know.\n\nThere are minor grammatical mistakes that can be corrected.\n\nAfter equation 5, the authors suggest categorical loss for discrete problems, \nbut cross-entropy loss might work better. Maybe this is what they meant.\n\n\nCLARITY\n\nThe overall approach and algorithms are described fairly clearly. Some minor typos here and there.\n\nAlgorithm 1 does not make clear the relationship between the model learned in step 2 and the algorithms in steps 4 to 6.\n\nI would reverse the order of a few things to align with a right to left ordering principle. \nIn Figure 1, put the model free transition generator on the left and the model-based sample consumer on the right.\nIn Figure 3, put the “reacher” test on the left and the “flappy bird” on the right.\n\n\nPROS AND CONS\n\nInteresting idea for learning quickly from small numbers of samples of expert state trajectories. \n\nNot clear that method converges on all problems. \n\nNot clear that the method is able to extract the state from video — authors had to extract position manually\n(this point is more about their deep architecture than the imitation framework they describe -\nthough perhaps a key argument for the authors is the ability to work with small numbers of \nexpert samples and still be able to train deep methods ) ??\n\n\nPOST REVIEW SUBMISSION:\n\nThe authors make a number of clarifying comments to improve the text and add the reference suggested by another reviewer. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1515642513847,"tcdate":1511715915298,"number":1,"cdate":1511715915298,"id":"SymLN__gM","invitation":"ICLR.cc/2018/Conference/-/Paper805/Official_Review","forum":"S1GDXzb0b","replyto":"S1GDXzb0b","signatures":["ICLR.cc/2018/Conference/Paper805/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Addresses an important problem, but misses existing research and results are unconvincing","rating":"4: Ok but not good enough - rejection","review":"The problem addressed here is imitation learning when no action information is available, which is an important problem in robotics for instance. The main idea of the proposed method is to produce a policy that matches the states observed in the expert trajectories, and this is achieved via a somewhat complex mix of model-free and model-based learning.\n\nMy main issues with the paper are:\n- It does not cite or discuss a very important piece of related work: \"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation\" (Liu et al., 2017)\n- The empirical results are unconvincing - it seems like in all problems they use there is a straightforward mapping from state feature differences to actions, as pointed out in an anonymous comment.\n\nAdditionally, it would have been nice to show empirically how helpful the model-based component of their approach is.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1510197132281,"tcdate":1510197132281,"number":1,"cdate":1510197132281,"id":"B1V5vHb1M","invitation":"ICLR.cc/2018/Conference/-/Paper805/Public_Comment","forum":"S1GDXzb0b","replyto":"S1GDXzb0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Simple baseline","comment":"I think there is an important baseline missing, namely:\n- Train a model to predict the action a_t using (phi(s_t), phi(s_{t+1})) as input, using samples obtained from the model-free policy. \n- Use this model to predict actions performed in the expert trajectories. \n- Train a standard imitation learner using the expert state trajectories together with the predicted actions as targets.\n\nThis should perform similarly to behavior cloning using true actions if the learned action predictor is accurate, which I think should be the case due to the way states are represented. For example, if I understand correctly in the 2D Obstacle Avoider task the action is simply a_t = phi(s_{t+1}) - phi(s_t). In Flappy Bird, the action is 1 if phi(s_{t+1})-phi(s_t) > 0 and -1 otherwise. It should be possible to learn both of these action predictors using very few samples. \n\nThis baseline would have more trouble if the inputs were videos and the hardcoded phi(s_t) was not provided, because the classifier would receive a high-dimensional input and would need more samples with known actions to fit its parameters. Did you try your method on Flappy Bird using only video, without providing the phi(s_t) as input?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]}},{"tddate":null,"ddate":null,"tmdate":1513948109741,"tcdate":1509135193966,"number":805,"cdate":1509739089247,"id":"S1GDXzb0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1GDXzb0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Model-based imitation learning from state trajectories","abstract":"Imitation learning from demonstrations usually relies on learning a policy from trajectories of optimal states and actions. However, in real life expert demonstrations, often the action information is missing and only state trajectories are available. We present a model-based imitation learning method that can learn environment-specific optimal actions only from expert state trajectories. Our proposed method starts with a model-free reinforcement learning algorithm with a heuristic reward signal to sample environment dynamics, which is then used to train the state-transition probability. Subsequently, we learn the optimal actions from expert state trajectories by supervised learning, while back-propagating the error gradients through the modeled environment dynamics. Experimental evaluations show that our proposed method successfully achieves performance similar to (state, action) trajectory-based traditional imitation learning methods even in the absence of action information, with much fewer iterations compared to conventional model-free reinforcement learning methods. We also demonstrate that our method can learn to act from only video demonstrations of expert agent for simple games and can learn to achieve desired performance in less number of iterations.","pdf":"/pdf/79200e781e071a6601153638fb62456de8394b15.pdf","TL;DR":"Learning to imitate an expert in the absence of optimal actions learning a dynamics model while exploring the environment.","paperhash":"anonymous|modelbased_imitation_learning_from_state_trajectories","_bibtex":"@article{\n  anonymous2018model-based,\n  title={Model-based imitation learning from state trajectories},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1GDXzb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper805/Authors"],"keywords":["Model based reinforcement learning","Imitation learning","dynamics model"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}