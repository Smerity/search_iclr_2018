{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222682954,"tcdate":1512102793433,"number":2,"cdate":1512102793433,"id":"BJbqjU0eM","invitation":"ICLR.cc/2018/Conference/-/Paper547/Official_Review","forum":"Hk5elxbRW","replyto":"Hk5elxbRW","signatures":["ICLR.cc/2018/Conference/Paper547/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper is well written and the contribution is sound","rating":"7: Good paper, accept","review":"This paper made some efforts in smoothing the top-k losses proposed in Lapin et al. (2015).  A family of smooth surrogate loss es was proposed, with the help of which the top-k error may be minimized directly. The properties of the smooth surrogate losses were studied and the computational algorithms for SVM with these losses function were also proposed. \n\nPros:\n1, The paper is well presented and is easy to follow.\n2, The contribution made in this paper is sound, and the mathematical analysis seems to be correct. \n3, The experimental results look convincing. \n\nCons:\nSome statements in this paper are not clear to me. For example, the authors mentioned sparse or non-sparse loss functions. This statement, in my view, could be misleading without further explanation (the non-sparse loss was mentioned in the abstract).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Smooth Loss Functions for Deep Top-k Classification","abstract":"Human labeling of data constitutes a long and expensive process. As a consequence, many classification tasks entail incomplete annotation and incorrect labels, while being built on a restricted amount of data. In order to handle the ambiguity and the label noise, the performance of machine learning models is usually assessed with top-$k$ error metrics rather than top-$1$. Theoretical results suggest that to minimize this error, various loss functions, including cross-entropy, are equally optimal choices of learning objectives in the limit of infinite data. However, the choice of loss function becomes crucial in the context of limited and noisy data. Besides, our empirical evidence suggests that the loss function must be smooth and non-sparse to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\\\"i}ve algorithm would require $\\mathcal{O}(\\binom{C}{k})$ operations, where $C$ is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k C)$. Furthermore, we present a novel and error-bounded approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size. Our investigation reveals that our loss provides on-par performance with cross-entropy for $k=1$, and is more robust to noise and overfitting for $k=5$.","pdf":"/pdf/69f52d077dcb96f952df52c7e262be399ac7b18e.pdf","TL;DR":"Smooth Loss Function for Top-k Error Minimization","paperhash":"anonymous|smooth_loss_functions_for_deep_topk_classification","_bibtex":"@article{\n  anonymous2018smooth,\n  title={Smooth Loss Functions for Deep Top-k Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk5elxbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper547/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222682994,"tcdate":1511694999279,"number":1,"cdate":1511694999279,"id":"HykoG7Oef","invitation":"ICLR.cc/2018/Conference/-/Paper547/Official_Review","forum":"Hk5elxbRW","replyto":"Hk5elxbRW","signatures":["ICLR.cc/2018/Conference/Paper547/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Promising extension of SVM's top-k loss to deep models","rating":"6: Marginally above acceptance threshold","review":"The paper is clear and well written. The proposed approach seems to be of interest and to produce interesting results. As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better.\n\nThe paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models. As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps.\n\nI have two main concerns with the presentation.\n\nA/ In addition to the main contribution, the paper devotes a significant amount of space to explaining how to compute the smoothed loss. This can be done by evaluating elementary symmetric polynomials at well-chosen values.\n\nThe paper argues that classical methods for such evaluations (e.g., using the usual recurrence relation or more advanced methods that compensate for numerical errors) are not enough when using single precision floating point arithmetic. The paper also advances that GPU parallelization must be used to be able to efficiently train the network.\n\nThose claims are not substantiated, however, and the method proposed by the paper seems to add substantial complexity without really proving that it is useful.\n\nThe paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value. I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch? I believe the paper could justify this approach better by providing a bit more insights as to why it is required. For instance:\n\n- What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials?\n- How do those compare with CE and L_{5, 1} with the proposed method?\n- Are numerical instabilities making this completely unfeasible? This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss.\n\n\nB/ No mention is made of the object detection problem, although multiple of the motivating examples in Figure 1 consider cases that would fall naturally into the object detection framework. Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting.\n\nAdditional comments:\n\n- Figure 2b: this visualization is confusing. This is presented in the same figure and paragraph as the CIFAR results, but instead uses a single synthetic data point in dimension 5, and k=1. This is not convincing. An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Smooth Loss Functions for Deep Top-k Classification","abstract":"Human labeling of data constitutes a long and expensive process. As a consequence, many classification tasks entail incomplete annotation and incorrect labels, while being built on a restricted amount of data. In order to handle the ambiguity and the label noise, the performance of machine learning models is usually assessed with top-$k$ error metrics rather than top-$1$. Theoretical results suggest that to minimize this error, various loss functions, including cross-entropy, are equally optimal choices of learning objectives in the limit of infinite data. However, the choice of loss function becomes crucial in the context of limited and noisy data. Besides, our empirical evidence suggests that the loss function must be smooth and non-sparse to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\\\"i}ve algorithm would require $\\mathcal{O}(\\binom{C}{k})$ operations, where $C$ is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k C)$. Furthermore, we present a novel and error-bounded approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size. Our investigation reveals that our loss provides on-par performance with cross-entropy for $k=1$, and is more robust to noise and overfitting for $k=5$.","pdf":"/pdf/69f52d077dcb96f952df52c7e262be399ac7b18e.pdf","TL;DR":"Smooth Loss Function for Top-k Error Minimization","paperhash":"anonymous|smooth_loss_functions_for_deep_topk_classification","_bibtex":"@article{\n  anonymous2018smooth,\n  title={Smooth Loss Functions for Deep Top-k Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk5elxbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper547/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739242914,"tcdate":1509126129772,"number":547,"cdate":1509739240251,"id":"Hk5elxbRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk5elxbRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Smooth Loss Functions for Deep Top-k Classification","abstract":"Human labeling of data constitutes a long and expensive process. As a consequence, many classification tasks entail incomplete annotation and incorrect labels, while being built on a restricted amount of data. In order to handle the ambiguity and the label noise, the performance of machine learning models is usually assessed with top-$k$ error metrics rather than top-$1$. Theoretical results suggest that to minimize this error, various loss functions, including cross-entropy, are equally optimal choices of learning objectives in the limit of infinite data. However, the choice of loss function becomes crucial in the context of limited and noisy data. Besides, our empirical evidence suggests that the loss function must be smooth and non-sparse to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-$k$ optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\\\"i}ve algorithm would require $\\mathcal{O}(\\binom{C}{k})$ operations, where $C$ is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of $\\mathcal{O}(k C)$. Furthermore, we present a novel and error-bounded approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size. Our investigation reveals that our loss provides on-par performance with cross-entropy for $k=1$, and is more robust to noise and overfitting for $k=5$.","pdf":"/pdf/69f52d077dcb96f952df52c7e262be399ac7b18e.pdf","TL;DR":"Smooth Loss Function for Top-k Error Minimization","paperhash":"anonymous|smooth_loss_functions_for_deep_topk_classification","_bibtex":"@article{\n  anonymous2018smooth,\n  title={Smooth Loss Functions for Deep Top-k Classification},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk5elxbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper547/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}