{"notes":[{"tddate":null,"ddate":null,"tmdate":1515085735821,"tcdate":1515085735821,"number":4,"cdate":1515085735821,"id":"BJghy1hmz","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Comment","forum":"HyRnez-RW","replyto":"HyRnez-RW","signatures":["ICLR.cc/2018/Conference/Paper774/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper774/Authors"],"content":{"title":"Revision of the draft based on reviewer comments","comment":"We thank for the reviewers for their valuable feedback and have made the following main improvements to the paper:\n\n-Content:\n1. Speed comparison (Figure 4 - right): We compare the speed of our approach to a vanilla bi-LSTM on a GPU. Because of our approach is trivially parallelizable, it gets relatively much faster compared to the LSTM as the document length increases (reaching ~45x speedup for a truncation limit of 10K tokens).\n2. Oracle statistics for truncation limit (Figure 3 - right): To justify our choice of truncation limit, we plot the oracle accuracy for various truncation thresholds.\n3. Analysis of each submodel. We provide:\n    a. Figure 3 - left: A quantitative analysis showing the performance of the top K results of each submodel\n    b. Table 3: A table of examples showing predictions of each submodel, and cases where our aggregation model (level 3) is able to do more than other submodels.\n\n-Writing:\n1. Introduction: We have clarified the novelty of our approach:\n    a. Multi-loss formulation, which to our knowledge has not been used in question answering, before. Empirically, this factors to a 10pt difference in the dev EM, as demonstrated in Table 2.\n    b. Aggregating multiple mentions of candidates at the representation level. We found this strategy to allow us to obtain high accuracy with simpler models when the multiple-mention assumption holds. Empirically, removing the aggregation level drops accuracy by 5.5 points in dev EM as shown in Table 2.\n    c. Unlike existing approaches, our model is trivially parallelizable in that we can process all the O(nl) spans in the document in parallel, allowing it to handle larger documents.\n\n-Complexity: We have clarified that O(nl) is similar to O(n) since l = maximum span length and is restricted to 5, and is therefore not quadratic.\n\n-Additional citations: Kadlec et al. 2016 (attention sum reader network), Seo et al. 2017, Xiong et al. 2017\n\n-Analysis: Sample of predictions have been provided and analysed in Section 4.4.\n\n-Experimental Results (Leaderboard): We believe we have addressed AnonReviewer3’s concerns (see the responses below), and also added a clarifying footnote to the paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1513032002470,"tcdate":1513032002470,"number":2,"cdate":1513032002470,"id":"By5HYFnZM","invitation":"ICLR.cc/2018/Conference/-/Paper774/Public_Comment","forum":"HyRnez-RW","replyto":"H1whTGi-f","signatures":["~Mandar_Joshi1"],"readers":["everyone"],"writers":["~Mandar_Joshi1"],"content":{"title":"TriviaQA leaderboard","comment":"I'm the administrator for the TriviaQA leaderboard on Codalab. I second Chris' comment. The leaderboard allows private submissions. In addition, the date field for each entry on the leaderboard refers to the date of submission (and not the date it was made public)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1512938927096,"tcdate":1512938927096,"number":1,"cdate":1512938927096,"id":"H1whTGi-f","invitation":"ICLR.cc/2018/Conference/-/Paper774/Public_Comment","forum":"HyRnez-RW","replyto":"rJa0zH9xf","signatures":["~Christopher_Clark1"],"readers":["everyone"],"writers":["~Christopher_Clark1"],"content":{"title":"TriviaQA leaderboard","comment":"I am the author of the \"chrisc\" submission on the TriviaQA Leaderboard.\n\nI just wanted to comment and confirm that our result on the leaderboard was not made public until after the ICLR deadline. The date listed on the leader board reflects the time we uploaded our test results, but we did not make that result public until after we had finished writing the paper and completed the rest of our evaluations, which occurred shortly after the ICLR submission deadline."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1512706972946,"tcdate":1512706972946,"number":3,"cdate":1512706972946,"id":"rJHomqP-M","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Comment","forum":"HyRnez-RW","replyto":"rkKuj7zgz","signatures":["ICLR.cc/2018/Conference/Paper774/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper774/Authors"],"content":{"title":"Addressing AnonReviewer2 Concerns","comment":"Thank you for your comments! We will revise the paper based on your feedback but we would like to clarify some aspects beforehand:\n\nSuccess of the model:\nOur model benefits from both multi-mention reasoning and more document context. The ablation in Table 2 shows that without the Level 3 multi-mention aggregation, model performance drops from 52.18% to 46.52%. The only purpose of the level 3 model is to do aggregation across multiple mentions, and therefore this shows that multi-mention reasoning helps our model significantly. More document context allows the upper bound of the dev EM under our approach to be 92%, compared to the 83% in the baseline method from Joshi et. al. (2017). We can make this more clear in our revision. \n\nComputational efficiency:\nWe only allow spans up to a length l (where l=5). Therefore our computational complexity is O(nl) and not O(n^2). Moreover, our method trivially parallelizes across the length of the document, unlike recurrent network based approaches.\n\nNegation:\nOur model does not handle negation specifically but negation is not typically a key aspect of existing reading comprehension tasks (as it is in sentiment analysis for instance).\n\nAttention:\nWe will clarify the attention section and cite (Seo et al. 2017, Xiong et al. 2017). Unlike their approach, we use the word embeddings as input to the attention, not the LSTM states as they do.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1512706822244,"tcdate":1512706719713,"number":2,"cdate":1512706719713,"id":"SkOozcw-f","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Comment","forum":"HyRnez-RW","replyto":"rJa0zH9xf","signatures":["ICLR.cc/2018/Conference/Paper774/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper774/Authors"],"content":{"title":"Addressing AnonReviewer3 Concerns","comment":"Thank you for your comments! We will revise our paper based on your feedback, particularly discussing more the contribution of each submodel and addressing your detailed comments. However, we would like to clarify some main points  below:\n\nThe “chrisc” leaderboard submission:\nTriviaQA allows someone to submit privately and then make their result public later. Therefore while the web result for the chrisc model might have been submitted earlier, it was not publicly visible before the ICLR Oct 27 deadline and therefore was not included in our table. None of the other ICLR submissions we are aware of report this result either e.g. (https://openreview.net/forum?id=rJl3yM-Ab, https://openreview.net/forum?id=HJRV1ZZAW, https://openreview.net/pdf?id=B1twdMCab ) The paper itself was posted  on arXiv on 29th Oct (https://arxiv.org/abs/1710.10723) and only contained Web (and not Wikipedia) results. We would also like to point out that the chrisc model involves a two-stage pipeline, many layers of recurrent neural nets and is procedurally more involved than ours.\n\nPruning spans:\nWe did try pruning spans, based on the levels, i.e. level 1 considered all spans in the document (up to truncation) and level 2 considered the top K spans from level 1, and so on. However, we found this decreased the accuracy by ~4-5 points. This could be attributed to the lower levels pruning away good candidates, because they did not have access to more information, such as sentence context, and attention with the question. We will revise the paper to include these results.\n\nUsing biLSTMs:\nRunning a biLSTM over the entire document of length n to obtain span representations is not parallelizable over the document length and therefore would be much slower (unlike our approach which trivially parallelizes the attention computation over the O(nl) spans). \n\nTruncation stats:\nTruncating documents to contain at most 6000 tokens gives us an upper bound of 92% on dev EM in the Wikipedia domain (avg number of sentences being 141). 87% of documents in the Wikipedia dataset are fully covered under this truncation limit. \n\nWe do not make any claims about the expressive power of BOW models vs RNNs. Our model performance can be attributed to the scalability of BOW architectures which can take advantage of longer documents, which RNN architectures are not well suited for. Furthermore, our other contributions (multi-loss + multi-mention learning) significantly boost the performance of these simple architectures as explored in the ablations in Table 2. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1512706625842,"tcdate":1512706625842,"number":1,"cdate":1512706625842,"id":"r15HG5DZM","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Comment","forum":"HyRnez-RW","replyto":"B1jA_O5xM","signatures":["ICLR.cc/2018/Conference/Paper774/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper774/Authors"],"content":{"title":"Addressing AnonReviewer1 Concerns","comment":"Thank you very much for your comments, we will revise the paper over the next few weeks based on your feedback. Below, we make some clarifications.\n\nPrimary contributions of our work:\nOur work presents a novel and more scalable approach to question answering that is considerably different than the existing literature that is dominated by monolithic LSTM-based architectures. The takeaway messages regarding our approach are:\n\n1. Multi-loss formulation, which to our knowledge has not been used in question answering, before. Empirically, this factors to a 10pt difference in the dev EM, as demonstrated in Table 2.\n2. Aggregating multiple mentions of candidates at the representation level. We found this strategy to allow us to obtain high accuracy with simpler models when the multiple-mention assumption holds. Empirically, removing the aggregation level drops accuracy by 5.5 points in dev EM as shown in Table 2.\n3. Unlike existing approaches, our model is trivially parallelizable in that we can process all the O(nl) spans in the document in parallel, allowing it to handle larger documents.\n\nWe believe that since our approach can scale to much longer document spans with only 1 GPU attests to its scalability. We also provide asymptotic analysis of our runtime complexity.\n\nCascades:\nWe would like to point out in contrast to previous approaches that aggregate answers (e.g. Attention Sum Reader Network, which we will add a reference for), our method aggregates positions at the *representation* level (by adding vector representations of mentions), not at the score level. While we agree that there could be more complex ways of realizing cascades, we chose the simplest approach that would show the efficacy of such an idea. More sophisticated ways to combine information may not be compatible with the trivial parallelizability in our model. \n\nSQuAD dataset:\nWhile it could run on SQuAD, our model was specifically designed for a case that is different from SQuAD. In SQuAD, the evidence only consists of a paragraph (avg length 122 tokens, TriviaQA evidence is more than 20X longer) so scalability is not a concern. Furthermore, answers to SQuAD questions are almost always unique spans in the passage, hence many of our intuitions of multi-mention learning might not be relevant for this task.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515642506356,"tcdate":1511848147417,"number":3,"cdate":1511848147417,"id":"B1jA_O5xM","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Review","forum":"HyRnez-RW","replyto":"HyRnez-RW","signatures":["ICLR.cc/2018/Conference/Paper774/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Official review","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long).\n\nOverall, I think it is a nice demonstration that non-recurrent models can work so well, but I also don’t find the results strikingly surprising. It is also a bit hard to get the main takeaway messages. It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547). But all the other components seem to have been demonstrated previously in other papers. \n\nAn important feature of this model is it is easier to parallelize and speed up the training/testing processes. However, I don’t see any demonstration of this in the experiments section.\n\nAlso, I am a bit disappointed by how “cascades” are actually implemented. I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on). The proposed model just simply sums up all the occurrences of candidate answers throughout the full document. 3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity. \n\nI am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515642506404,"tcdate":1511834325052,"number":2,"cdate":1511834325052,"id":"rJa0zH9xf","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Review","forum":"HyRnez-RW","replyto":"HyRnez-RW","signatures":["ICLR.cc/2018/Conference/Paper774/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches. The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence. The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline. It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs. \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction. On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning. As such, I am borderline on its acceptance.\n\n* The TriviaQA leaderboard shows a submission from 9/24/17 (by \"chrisc\") that has significantly higher EM/F1 scores than the proposed model. Why is this result not compared to in Table 1? \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017? This may allow you to avoid truncation altogether. Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient.\n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions?\n- What is the average number of sentences per document? It's hard to get an idea of how reasonable the chosen truncation thresholds are without this.\n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased. Did the authors try truncating after more words (e.g., 10k)?\n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works.\n- \"Krasner\" misspelled multiple times as \"Kramer\"","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515642506445,"tcdate":1511304049520,"number":1,"cdate":1511304049520,"id":"rkKuj7zgz","invitation":"ICLR.cc/2018/Conference/-/Paper774/Official_Review","forum":"HyRnez-RW","replyto":"HyRnez-RW","signatures":["ICLR.cc/2018/Conference/Paper774/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Intuitive model for scaling question answering ","rating":"7: Good paper, accept","review":"The authors present a scalable model for questioning answering that is able to train on long documents. On the TriviaQA dataset, the proposed model achieves state of the art results on both domains (wikipedia and web). The formulation of the model is straight-forward, however I am skeptical about whether the results prove the premise of the paper (e.g. multi-mention reasoning is necessary). Furthermore, I am slightly unconvinced about the authors' claim of efficiency. Nevertheless, I think this work is important given its performance on the task.\n\n1. Why is this model successful? Multi-mention reasoning or more document context?\nI am not convinced of the necessity of multi-mention reasoning, which the authors use as motivation, as shown in the examples in the paper. For example, in Figure 1, the answer is solely obtained using the second last passage. The other mentions provide signal, but does not provide conclusive evidence. Perhaps I am mistaken, but it seems to me that the proposed model cannot seem to handle negation, can the authors confirm/deny this? I am also skeptical about the computation efficiency of a model that scores all spans in a document (which is O(N^2), where N is the document length). Can you show some analysis of your model results that confirm/deny this hypothesis?\n\n2. Why is the computational complexity not a function of the number of spans?\nIt seems like the derivations presents several equations that score a given span. Perhaps I am mistaken, but there seems to be n^2 spans in the document that one has to score. Shouldn't the computational complexity then be at least O(n^2), which makes it actually much slower than, say, SQuAD models that do greedy decoding O(2n + nm)?\n\nSome minor notes\n- 3.3.1 seems like an attention computation in which the attention context over the question and span is computed using the question. Explicitly mentioning this may help the reading grasp the formulation.\n- Same for 3.4, which seems like the biattention (Seo 2017) or coattention (Xiong 2017) from previous squad work.\n- The sentence \"We define ... to be the embeddings of the l words of the sentence that contains s.\" is not very clear. Do you mean that the sentence contains l words? It could be interpreted that the span has l words.\n- There is a typo in your 3.7 \"level 1 complexity\": there is an extra O inside the big O notation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]}},{"tddate":null,"ddate":null,"tmdate":1515086735752,"tcdate":1509134518091,"number":774,"cdate":1509739107729,"id":"HyRnez-RW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyRnez-RW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multi-Mention Learning for Reading Comprehension with Neural Cascades","abstract":"Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.","pdf":"/pdf/532f22003ab8306b6f9a786fd8389c3f580a0980.pdf","TL;DR":"We propose neural cascades, a simple and trivially parallelizable approach to reading comprehension, consisting only of feed-forward nets and attention that achieves state-of-the-art performance on the TriviaQA dataset.","paperhash":"anonymous|multimention_learning_for_reading_comprehension_with_neural_cascades","_bibtex":"@article{\n  anonymous2018multi-mention,\n  title={Multi-Mention Learning for Reading Comprehension with Neural Cascades},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRnez-RW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper774/Authors"],"keywords":["reading comprehension","multi-loss","question answering","scalable","TriviaQA","feed-forward","latent variable","attention"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}