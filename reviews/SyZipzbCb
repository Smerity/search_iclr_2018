{"notes":[{"tddate":null,"ddate":null,"tmdate":1512272562745,"tcdate":1512272562745,"number":3,"cdate":1512272562745,"id":"ryjnMgWZz","invitation":"ICLR.cc/2018/Conference/-/Paper1023/Public_Comment","forum":"SyZipzbCb","replyto":"HJkIEozeM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"New questions","comment":"I suppose it should have been questions not to my comment but to the authors of the paper?\n\nAlso I'd like to refresh my question to authors - do you plan to release a videos showing parkour and robotic hand training results? It's almost a standard for RL research and lack of them can cause some unnecessary suspects and questions. \n\nIn addition videos can show quality of trained policies. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222542801,"tcdate":1511817988525,"number":3,"cdate":1511817988525,"id":"Bk3bXW5gM","invitation":"ICLR.cc/2018/Conference/-/Paper1023/Official_Review","forum":"SyZipzbCb","replyto":"SyZipzbCb","signatures":["ICLR.cc/2018/Conference/Paper1023/AnonReviewer3"],"readers":["everyone"],"content":{"title":"the proposed method is simple","rating":"5: Marginally below acceptance threshold","review":"\nComment: The paper proposes a simple extension to DDPG that uses a distributional Bellman operator for critic updates, and introduces two simple modifications which are the use of N-step returns and parallelizing evaluations. The method is evaluated on a wide variety of many control and robotic talks. \n\nIn general, the paper is well written and organised. However I have some following major concerns regarding the quality of the paper:\n\n- The proposal, D4PG, is quite straightforward which is simply use the idea of distributional value function by Bellemare et al. (previously used in DQN). Two modifications are also simple and well-known techniques. It would be nicer if the description in Section 3 is less straightforward by giving more justifications and analysis why and how distributional updates are necessary in the context of policy search methods like DDPG. \n\n- A positive side of the paper is a large set of evaluations on many different control and robotic tasks. For many tasks, D4PG performs better than the variant that does not use distributional updates (D3PG), however by not much. There are some tasks showing no-difference. On the other hand, the choice of N=5 in comparisons is hard to understand and lacks further experimental justifications. Different setting and new performance metrics (e.g. data efficiency, number of episodes in total) might also reveal more properties of the proposed methods.\n\n\n\n* Other minor comments:\n\n- Algorithm 1 consists of two parts but there are connection between them. It might be confused for ones who are not familiar with the actor-critic framework.\n\n- It would be nicer if all expectation operators in Section 3 comes with corresponding distributions. \n\n- page 2, second paragraph: typos in \"hence my require less samples to learn\"\n\n- it might be better if the reference on arXiv should be changed to relevant publication conferences with archival proceeding: work by Marc G. Bellemare at ICML 2017","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222542843,"tcdate":1511809673374,"number":2,"cdate":1511809673374,"id":"r1Wcz1clz","invitation":"ICLR.cc/2018/Conference/-/Paper1023/Official_Review","forum":"SyZipzbCb","replyto":"SyZipzbCb","signatures":["ICLR.cc/2018/Conference/Paper1023/AnonReviewer1"],"readers":["everyone"],"content":{"title":"good evaluation, but lacking in originality","rating":"6: Marginally above acceptance threshold","review":"The paper investigates a number of additions to DDPG algorithm and their effect on performance. The additions investigated are distributional Bellman updates, N-step returns, and prioritized experience replay.\n\nThe paper does a good job of analyzing these effects on a wide range of continuous control tasks, from the standard benchmark suite, to hand manipulation, to complex terrain locomotion and I believe these results are valuable to the community.\n\nHowever, I have a concern about the soundness of using N-step returns in DDPG setting. When a sequence of length N is sampled from the replay buffer and used to calculate N-step return, this sequence is generated according a particular policy. As a result, experience is non-stationary - for the same state-action pair, early iterations of the algorithm will produce structurally different (not just due to stochasticity) N-step returns because the policy to generate those N steps has changed between algorithm iterations. So it seems to me the authors are using off-policy updates where strictly on-policy updates should be used. I would like some clarification from the authors on this point, and if it is indeed the case to bring attention to this point in the final manuscript.\n\nIt would also be useful to evaluate the effect of N for values other than 1 and 5, especially given the significance this addition has on performance. I can believe N-step returns are useful, possibly due to effectively enlarging simulation timestep, but it would be good to know at which point it becomes detrimental.\n\nI also believe \"Distributional Policy Gradients\" is an overly broad title for this submission as this work still relies on off-policy updates and does not tackle the problem of marrying distributional updates with on-policy methods. \"Distributional DDPG\" or \"Distributional Actor-Critic\" or variant perhaps could be more fair title choices?\n\nAside from these concerns, lack of originality of contributions makes it difficult to highly recommend the paper. Nonetheless, I do believe the experimental evaluation if well-conducted and would be of interest to the ICLR community. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222542887,"tcdate":1511759778522,"number":1,"cdate":1511759778522,"id":"Byqj1QtlM","invitation":"ICLR.cc/2018/Conference/-/Paper1023/Official_Review","forum":"SyZipzbCb","replyto":"SyZipzbCb","signatures":["ICLR.cc/2018/Conference/Paper1023/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Thorough investigation of distributional policy gradients for continuous problems","rating":"9: Top 15% of accepted papers, strong accept","review":"A DeepRL algorithm is presented that represents distributions over Q values, as applied to DDPG,\nand in conjunction with distributed evaluation across multiple actors, prioritized experience replay, and \nN-step look-aheads. The algorithm is called Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG.\nSOTA results are generated for a number of challenging continuous domain learning problems,\nas compared to benchmarks that include DDPG and PPO, in terms of wall-clock time, and also (most often) in terms\nof sample efficiency.\n\npros/cons\n+ the paper provides a thorough investigation of the distributional approach, as applied to difficult continuous\n  action problems, and in conjunction with a set of other improvements (with ablation tests)\n- the story is a bit mixed in terms of the benefits, as compared to the non-distributional approach, D3PG\n- it is not clear which of the baselines are covered in detail in the cited paper:\n  \"Anonymous. Distributed prioritized experience replay. In submission, 2017.\", \n   i.e., should readers assume that D3PG already exists and is attributable to this other submission?\n\nOverall, I believe that the community will find this to be interesting work.\n\nIs a video of the results available?\n\nIt seems that the distributional model often does not make much of a difference, \nas compared to D3PG non-prioritized.  However, sometimes it does make a big difference, i.e., 3D parkour; acrobot.\nDo the examples where it yields the largest payoff share a particular characteristic?\n\nThe benefit of the distributional models is quite different between the 1-step and 5-step versions. Any ideas why?\n\nOccasionally, D4PG with N=1 fails very badly, e.g., fish, manipulator (bring ball), swimmer.\nWhy would that be? Shouldn't it do at least as well as D3PG in general?\n\nHow many atoms are used for the categorical representation?\nAs many as [Bellemare et al.], i.e., 51 ?\nHow much \"resolution\" is necessary here in order to gain most of the benefits of the distributional representation?\n\nAs far as I understand, V_min and V_max are not the global values, but are specific to the current distribution.\nHence the need for the projection. Is that correct?\n\nWould increasing the exploration noise result in a larger benefit for the distributional approach?\n\nFigure 2: DDPG performs suprisingly poorly in most examples. Any comments on this,\nor is DDPG best avoided in normal circumstances for continuous problems? :-)\n\nIs the humanoid stand so easy because of large (or unlimited) torque limits?\n\nThe wall-clock times are for a cluster with K=32 cores for Figure 1?\n\n\"we utilize a network architecture as specified in Figure 1 which processes the terrain info in order to reduce its dimensionality\"\nFigure 1 provides no information about the reduced dimensionality of the terrain representation, unless I am somehow failing to see this.\n\n\"the full critic architecture is completed by attaching a critic head as defined in Section A\"\nI could find no further documenation in the paper with regard to the \"head\" or a separate critic for the \"head\".\nIt is not clear to me why multiple critics are needed.\n\nDo you have an intuition as to why prioritized replay might be reducing performance in many cases?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1511334983534,"tcdate":1511334983534,"number":2,"cdate":1511334983534,"id":"HJkIEozeM","invitation":"ICLR.cc/2018/Conference/-/Paper1023/Public_Comment","forum":"SyZipzbCb","replyto":"HJt1T-2R-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Experimental setup questions","comment":"1) I had seen some convergence issues when I implemented something similar. Did you face anything similar? How important was the power of the neural approximator and the size of the distribution support set (in case of multinomial distribution)?\n\n2) Does the extra 'distributed' improve speed or quality of convergence?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1509854432737,"tcdate":1509854432737,"number":1,"cdate":1509854432737,"id":"HJt1T-2R-","invitation":"ICLR.cc/2018/Conference/-/Paper1023/Public_Comment","forum":"SyZipzbCb","replyto":"SyZipzbCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Experimental result and set-up","comment":"Hi,\n\nCan you clarify a few questions about experimental set-up and results? In section 4 Result you've describe a few important design choices:\n \n1) You've chosen a fixed Gaussian noise for exploration and made a statement that Ornstein-Uhlenbeck didn't add to performance in your experiments. which contradicts a known results for DDPG. Can you provide a comparison plots and describe an experimental set-up supporting this statement for D4PG?\n2) You've chosen the same learning rate for actor and critic, what is a bit different from common practice for DDPG, when a critic usually has leaning rate an order of magnitude higher the same for actor. What is a justification of such a choice? Can provide some experimental results showing performance of a few different choices of learning rates for actor and critic?\n\nIn 4.3, Parkour section you showed results of the evaluation of different variants of D4PG and D3PG and comparison vs PPO. But it's unclear what a performance level do they correspond:\n\n3) Can you provide a few videos of the final performance for best variants of D4PG and PPO for Walker2d and Humanoid?\n4) You compare with PPO in wall time and number of actors steps. But they are not the only possible metrics for comparison. Can you provide some insights on:\n        a) Maximum rewards: What is a maximum performance D4PG vs PPO? How maximum rewards achieved with this 2           algorithm can be compared if perform longer training?\n        b) Stability: PPO is known to be very stable algorithm, while DDPG is much more sensitive to the hyperparameters choice. How a stability of D4PG training can be compared to PPO?\n        c) Scalability: How well D4PG is scaling with increasing  number of parallel workers in comparison with PPO?\n5) Do you plan to release your implementation for D4PG?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092382449,"tcdate":1509137831262,"number":1023,"cdate":1510092360678,"id":"SyZipzbCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyZipzbCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Distributional Policy Gradients","abstract":"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.","pdf":"/pdf/e3cba47803f92fe6868f4b2605bcdf1b60118f21.pdf","TL;DR":"We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.","paperhash":"anonymous|distributional_policy_gradients","_bibtex":"@article{\n  anonymous2018distributional,\n  title={Distributional Policy Gradients},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyZipzbCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1023/Authors"],"keywords":["policy gradient","continuous control","actor critic","reinforcement learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}