{"notes":[{"tddate":null,"ddate":null,"tmdate":1515747948910,"tcdate":1515747948910,"number":6,"cdate":1515747948910,"id":"HJH_9xLNG","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Comment","forum":"rkHVZWZAZ","replyto":"SJqjT_ZGf","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper650/AnonReviewer2"],"content":{"title":"Final thoughts","comment":"Thanks to the authors for their response. As I mentioned in the initial review, I think the method is definitely promising and provides improvements. My comments were more on claims like \"Reactor significantly outperforms Rainbow\" which is not evident from the results in the paper (a point also noted by Reviewer 3). These claims could be made more specific, with appropriate caveats, or additional experiments could be performed to help substantiate the claims better. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1515616634584,"tcdate":1515616634584,"number":5,"cdate":1515616634584,"id":"SyQtte4NM","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Comment","forum":"rkHVZWZAZ","replyto":"HJgzROWzf","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper650/AnonReviewer3"],"content":{"title":"Final thoughts","comment":"Thanks!\n\nI can definitely imagine it was hard to make a proper comparison to Rainbow within such a short timeframe. I still think such a comparison would be quite valuable, to better evaluate the impact of their respective unique components. I'm afraid we are back to a situation where it's not clear what works best -- I guess that's the curse of the Atari benchmark.\n\nI appreciate the many improvements to the paper (though I lack time to look at them thoroughly), in particular the Appendix section on the comparisons with Rainbow. I admit I had read your paper as a DQN extension, while it makes more sense to see it as an A3C extension. I'll change my score to acceptance.\n\nNB: I disagree with the statement that \"In the human-starts evaluation Reactor significantly outperforms Rainbow at 200M steps\". It has slightly higher median normalized score, but lower Elo score. I don't think we can draw a solid conclusion from this (like claiming that \"Reactor generalizes better to these unseen starting states\").\n\nAlso if you can fix this typo in a final version, it looks like you added a \"i=1\" in eq. 12's sum, but forgot its upper bound."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1515175318330,"tcdate":1515164009021,"number":4,"cdate":1515164009021,"id":"B1W_WMpQz","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Comment","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper650/Authors"],"content":{"title":"New revision.","comment":"We have just added a new revision addressing the reviewer comments, which we much appreciate."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1513359402776,"tcdate":1513356807715,"number":3,"cdate":1513356807715,"id":"HJgzROWzf","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Comment","forum":"rkHVZWZAZ","replyto":"SJRs56Ylz","signatures":["ICLR.cc/2018/Conference/Paper650/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper650/Authors"],"content":{"title":"Authors' response to AnonReviewer3","comment":" Thank you very much for your helpful review.\n\n>> There is almost no discussion on the differences between Reactor and Rainbow\n>> I assume that this submission was mostly written before Rainbow came out, and that comparisons to Rainbow were hastily added just before the ICLR deadline\n\nAdmittedly, the comparisons with Rainbow were less detailed than we would have liked. Please note that Rainbow was put on Arxiv only three weeks before the ICLR submission deadline. However we have already included experimental comparisons with Rainbow, both in the form of presenting the learning curves and final evaluations. We will add a more in-depth comparison with Rainbow and discussion of related work in the appendix.\n\n>> I believe a comparison between Reactor and Rainbow needs to either have them both parallelized or none of them.\n\nRainbow works on GPUs, Reactor works on CPUs. A single GPU is not equivalent to a single CPU. Parallelizing Rainbow is out of the scope of this work. First, because this was not the focus of our work. Second, because it would be a non-trivial task potentially worth publication on its own. More generally, the same parallelization argument would also apply to comparisons between A3C and DQN.\n\n>> Rainbow uses the traditional feedforward DQN architecture while Reactor uses a recurrent network. It is not clear to which extent this has an impact on the results.\n\n\nThere are many differences between Rainbow and Reactor: 1) LSTM vs frame stacking, 2) actor-critic vs value-based algorithm 3) beta-LOO vs Q-learning, 4) Retrace vs n-step learning, 5) sequence prioritization vs transition prioritization, 6) entropy bonus vs noisy networks. Reactor is not an incremental improvement of Rainbow and is a completely different algorithm. This makes it impractical to compare on a component-by-component basis. For the most important contributions we performed an ablation study within Reactor’s framework, but naturally we can not ablate every architectural choice that we have made.\n\n>> Rainbow was stopped at 200M steps, at which point it seems to be overall superior to Reactor at 200M steps.\n\nThis is not correct. In the human-starts evaluation Reactor significantly outperforms Rainbow at 200M steps. In the no-op-starts evaluation Rainbow significantly outperforms Reactor at 200M steps. Both Reactor and Rainbow were trained with 30 random no-op-starts. Their evaluation with 30 random human starts shows how well each algorithm generalizes to new initial conditions. We would argue that the issues of generalization here are similar to those seen between training and testing error in supervised learning. We thus show that Reactor generalizes better to these unseen starting states.\n\n>> (network architecture): a simple diagram in the appendix would make it much easier to understand\n\nWe will add the diagram to the supplementary material.\n\n>> again a visual illustration of the partitioning scheme would in my opinion help clarify the approach\n\nWe will add an illustration to the supplementary material. We will also correct all other typos mentioned in the review. Thank you for taking note of them.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1513359450131,"tcdate":1513356753191,"number":2,"cdate":1513356753191,"id":"SJtRa_WMM","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Comment","forum":"rkHVZWZAZ","replyto":"r1MU1AtlG","signatures":["ICLR.cc/2018/Conference/Paper650/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper650/Authors"],"content":{"title":"Authors response to AnonReviewer1","comment":"Thank you very much for your review and recognising novelty of our contributions.\n\n>> I found the replay buffer not as clear as the other parts of the paper.\n\nWe will do our best to clarify the description, most likely in the appendix given space limitations.\n\n>> Methods not clearly labeled. For example, what is the difference between Reactor and Reactor 500M?\n\nWe will clarify the labels. `Reactor 500M` denotes the performance of Reactor at 500 million training steps. \n\n>> but no PPO in the evaluation\n\nThe PPO paper did not present results at 200M frames but at 40M frames, and their results seem to be weaker than ACER on 40M frames: ACER was better than PPO on 28/49 games tested. For the purpose of comparison to other algorithms, we chose to evaluate all algorithms at (at least) 200M frames, and Reactor is much better than ACER on 200M frames. Unfortunately, we don’t know how PPO perform at 200M frames, so a direct comparison is impossible.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1513359385489,"tcdate":1513356706132,"number":1,"cdate":1513356706132,"id":"SJqjT_ZGf","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Comment","forum":"rkHVZWZAZ","replyto":"rksMwz9xG","signatures":["ICLR.cc/2018/Conference/Paper650/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper650/Authors"],"content":{"title":"Author response to AnonReviewer2","comment":"We were happy to see that the reviewer recognised the novelty both the introduced ideas (prioritization, distributional Retrace and the beta-LOO policy gradient algorithm) and integration of the ideas into a single agent architecture.\n\n>> Reactor is still less sample-efficient than Rainbow, with significantly lower scores after 200M frames\n\nThis is not correct. In the human-starts evaluation Reactor significantly outperforms Rainbow at 200M steps. In the no-op-starts evaluation Rainbow significantly outperforms Reactor at 200M steps. Both Reactor and Rainbow were trained with 30 random no-op-starts. Their evaluation with 30 random human starts shows how well each algorithm generalizes to new initial conditions. We would argue that the issues of generalization here are similar to those seen between training and testing error in supervised learning. We thus show that Reactor generalizes better to these unseen starting states.\n\n>> While the Reactor trains much faster, it does use more parallel compute, so the comparison with Rainbow on wall clock time is not entirely fair.\n\nThe reviewer is right in the sense that Reactor executes more floating point operations per second, but it trains much shorter in wall time resulting in an overall similar number of computations executed. We make no claim that Reactor uses overall less computational operations to train an agent. Nevertheless, we believe that having a fast algorithm in terms of wall time is important because of the potential to shorten experimentation time. The measure is still informative, as one may choose Reactor over Rainbow when multiple CPU machines are available (as opposed to a single GPU machine).\n\n>>  Empirical comparisons are restricted to the Atari domain.\n\nWe focused on Atari domain to facilitate the comparison to the prior work.\n\n>> Since the paper introduces a few new ideas like prioritized sequence replay, it would help if a more detailed analysis was performed on the impact of these individual schemes\n\nThe paper already contains the ablation study comparing relative importances of individual components. Since the number of novel contributions is large (beta-LOO, distributional retrace, prioritized sequence replay), it is difficult to explore all possible configurations of the components.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1515642485612,"tcdate":1511823122752,"number":3,"cdate":1511823122752,"id":"rksMwz9xG","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Review","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice integration of recent deep RL advances with decent empirical results","rating":"7: Good paper, accept","review":"This paper presents a new reinforcement learning architecture called Reactor by combining various improvements in\ndeep reinforcement learning algorithms and architectures into a single model. The main contributions of the paper\nare to achieve a better bias-variance trade-off in policy gradient updates, multi-step off-policy updates with\ndistributional RL, and prioritized experience replay for transition sequences. The different modules are integrated\nwell and the empirical results are very promising. The experiments (though limited to Atari) are well carried out and\nthe evaluation is performed on both sample efficiency and training time.\n\nPros:\n1. Nice integration of several recent improvements in deep RL, along with a few novel tricks to improve training.\n2. The empirical results on 57 Atari games are impressive, in terms of final scores as well as real-time training speed.\n\nCons:\n1. Reactor is still less sample-efficient than Rainbow, with significantly lower scores after 200M frames. While the\nreactor trains much faster, it does use more parallel compute, so the comparison with Rainbow on wall clock time is\n not entirely fair. Would a distributed version of Rainbow perform better in this respect?\n2. Empirical comparisons are restricted to the Atari domain. The conclusions of the paper will be much stronger if\nresults are also shown on other environments like Mujoco/Vizdoom/Deepmind Lab.\n3. Since the paper introduces a few new ideas like prioritized sequence replay, it would help if a more detailed analysis\n was performed on the impact of these individual schemes, even if in a model simpler than the Reactor. For instance, one could investigate the impact of prioritized sequence replay in models like multi-step DQN or recurrent DQN. This will help us understand  the impact of each of these ideas in a more comprehensive fashion.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1515642485657,"tcdate":1511804746530,"number":2,"cdate":1511804746530,"id":"r1MU1AtlG","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Review","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting work with several contributions and large experiments with some but not all recent approaches","rating":"7: Good paper, accept","review":"This paper proposes a novel reinforcement learning algorithm containing several contributions made by the authors: 1) a policy gradient algorithm that uses value function estimates to improve the policy gradient, 2) a distributed multi-step off-policy algorithm to estimate the value function, 3) an experience replay buffer mechanism that can handle sequences and (4) a distributed architecture, where threads are dedicated to either learning or interracting with the environment. Most contributions consist in improvements to handle multi-step trajectories instead of single step transitions. The resulting algorithm is evaluated on the ATARI domain and shown to outperform other similar algorithms, both in terms of score and training time. Ablation studies are also performed to study the interest of the 4 contributions. \n\nI find the paper interesting. It is also well written and reasonably clear. The experiments are large, although I was disappointed that PPO was not included in the evaluation, as this algorithm also trains much faster than other algorithms.\n\nquality\n+ several contributions\n+ impressive experiments\n\nclarity\n- I found the replay buffer not as clear as the other parts of the paper.\n. run time comparison: source of the code for the baseline methods?\n+ ablation study showing the merits of the different contributions\n- Methods not clearly labeled. For example, what is the difference between Reactor and Reactor 500M?\n\noriginality\n+ 4 contributions\n\nsignificance\n+ important problem, very active area of research\n+ comparison to very recent algorithms\n- but no PPO in the evaluation","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1515642485697,"tcdate":1511803558125,"number":1,"cdate":1511803558125,"id":"SJRs56Ylz","invitation":"ICLR.cc/2018/Conference/-/Paper650/Official_Review","forum":"rkHVZWZAZ","replyto":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference/Paper650/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A promising \"Rainbow-like\" combination of deep RL techniques, unfortunately with no proper comparison to Rainbow","rating":"7: Good paper, accept","review":"This paper proposes a novel reinforcement learning algorithm (« The Reactor ») based on the combination of several improvements to DQN: a distributional version of Retrace, a policy gradient update rule called beta-LOO aiming at variance reduction, a variant of prioritized experience replay for sequences, and a parallel training architecture. Experiments on Atari games show a significant improvement over prioritized dueling networks in particular, and competitive performance compared to Rainbow, at a fraction of the training time.\n\nThere are definitely several interesting and meaningful contributions in this submission, and I like the motivations behind them. They are not groundbreaking (essentially extending existing techniques) but are still very relevant to current RL research.\n\nUnfortunately I also see it as a step back in terms of comparison to other algorithms. The recent Rainbow paper finally established a long overdue clear benchmark on Atari. We have seen with the « Deep Reinforcement Learning that Matters » paper how important (and difficult) it is to properly compare algorithms on deep RL problems. I assume that this submission was mostly written before Rainbow came out, and that comparisons to Rainbow were hastily added just before the ICLR deadline: this would explain why they are quite limited, but in my opinion it remains a major issue, which is the main reason why I am advocating for rejection.\n\nMore precisely, focusing on the comparison to Rainbow which is the main competitor here, my concerns are the following:\n- There is almost no discussion on the differences between Reactor and Rainbow (actually the paper lacks a « related work » section). In particular Rainbow also uses a version of distributional multi-step, which as far as I can tell may not be as well motivated (from a mathematical point of view) as the one in this submission (since it does not correct for the « off-policyness » of the replay data), but still seems to work well on Atari.\n- Rainbow is not distributed. This was a deliberate choice by its authors to focus on algorithmic comparisons. However, it seems to me that it could benefit from a parallel training scheme like Reactor’s. I believe a comparison between Reactor and Rainbow needs to either have them both parallelized or none of them (especially for a comparison on time efficiency like in Fig. 2)\n- Rainbow uses the traditional feedforward DQN architecture while Reactor uses a recurrent network. It is not clear to which extent this has an impact on the results.\n- Rainbow was stopped at 200M steps, at which point it seems to be overall superior to Reactor at 200M steps. The results as presented here emphasize the superiority of Reactor at 500M steps, but a proper comparison would require Rainbow results at 500M steps as well.\n\nIn addition, although I found most of the paper to be clear enough, some parts were confusing to me, in particular:\n- « multi-step distributional Bellman operator » in 3.2: not clear exactly what the target distribution is. If I understand correctly this is the same as the Rainbow extension, but this link is not mentioned.\n- 3.4.1 (network architecture): a simple diagram in the appendix would make it much easier to understand (Table 3 is still hard to read because it is not clear which layers are connected together)\n- 3.3 (prioritized sequence replay): again a visual illustration of the partitioning scheme would in my opinion help clarify the approach\n\nA few minor points to conclude:\n- In eq. 6, 7 and the rest of this section, A does not depend (directly) on theta so it should probably be removed to avoid confusion. Note also that using the letter A may not be best since A is used to denote an action in 3.1.\n- In 3.1: « Let us assume that for the chosen action A we have access to an estimate R(A) of Qπ(A) » => « unbiased estimate »\n- In last equation of p.5 it is not clear what q_i^n is\n- There is a lambda missing on p.6 in the equation showing that alphas are non-negative on average, just before the min\n- In the equation above eq. 12 there is a sum over « i=1 »\n- That same equation ends with some h_z_i that are not defined\n- In Fig. 2 (left) for Reactor we see one worker using large batches and another one using many threads. This is confusing.\n- 3.3 mentions sequences of length 32 but 3.4 says length 33.\n- 3.3 says tree operations are in O(n ln(n)) but it should be O(ln(n))\n- At very end of 3.3 it is not clear what « total variation » is.\n- In 3.4 please specify the frequency at which the learner thread downloads shared parameters and uploads updates\n- Caption of Fig. 3 talks about « changing the number of workers » for the left plot while it is in the right plot\n- The explanation on what the variants of Reactor (ND and 500M) mean comes after results are shown in Fig. 2.\n- Section 4 starts with Fig. 3 without explaining what the task is, how performance is measured, etc. It also claims that Distributional Retrace helps while this is not the case in Fig. 3 (I realize it is explained afterwards, but it is confusing when reading the sentence « We can also see... »). Finally it says priorization is the most important component while the beta-LOO ablation seems to perform just the same.\n- Footnote 3 should say it is 200M observations except for Reactor 500M\n- End of 4.1: « The algorithms that we compare Reactor against are » => missing ACER, A3C and Rainbow\n- There are two references for « Sample efficient actor-critic with experience replay »\n- I do not see the added benefit of the Elo computation. It seems to convey essentially the same information as average rank.\n\nAnd a few typos:\n- Just above 2.1.3: « increasing » => increasingly\n- In 3.1: « where V is a baseline that depend » => depends\n- p.7: « hight » => high, and « to all other sequences » => of all other sequences\n- Double parentheses in Bellemare citation at beginning of section 4\n- Several typos in appendix (too many to list)\n\nNote: I did not have time to carefully read Appendix 6.3 (contextual priority tree)\n\nEdit after revision: bumped score from 5 to 7 because (1) authors did many improvements to the paper, and (2) their explanations shed light on some of my concerns","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]}},{"tddate":null,"ddate":null,"tmdate":1515163844955,"tcdate":1509130540849,"number":650,"cdate":1509739178196,"id":"rkHVZWZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkHVZWZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning","abstract":"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.","pdf":"/pdf/9d8c49e3cf838870f12237c90e675848b4533929.pdf","TL;DR":"Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.","paperhash":"anonymous|the_reactor_a_fast_and_sampleefficient_actorcritic_agent_for_reinforcement_learning","_bibtex":"@article{\n  anonymous2018the,\n  title={The Reactor: A fast and sample-efficient Actor-Critic agent for  Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkHVZWZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper650/Authors"],"keywords":["reinforcement learning","policy gradient","distributional reinforcement learning","distributed computing"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}