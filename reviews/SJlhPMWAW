{"notes":[{"tddate":null,"ddate":null,"tmdate":1512366848319,"tcdate":1512366848319,"number":3,"cdate":1512366848319,"id":"B1ubmvfZM","invitation":"ICLR.cc/2018/Conference/-/Paper864/Official_Review","forum":"SJlhPMWAW","replyto":"SJlhPMWAW","signatures":["ICLR.cc/2018/Conference/Paper864/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting topic, but paper isn't ready yet","rating":"4: Ok but not good enough - rejection","review":"This paper studies the problem of learning to generate graphs using deep learning methods. The main challenges of generating graphs as opposed to text or images are said to be the following:\n(a) Graphs are discrete structures, and incrementally constructing them would lead to non-differentiability (I don't agree with this; see below)\n(b) It's not clear how to linearize the construction of graphs due to their symmetries. Based on this motivation, the paper decides to generate a graph in \"one shot\", directly  outputting node and edge existence probabilities, and node attribute vectors.\n\nA graph is represented by a soft adjacency matrix A (entries are probability of existence of an edge), an edge attribute tensor E (entries are probability of each edge being one of d_e discrete types), and a node attribute matrix F, which has a node vector for each  potential node. A cross entropy loss is developed to measure the loss between generated A, E, and F and corresponding targets.\n\nThe main issue with training models in this formulation is the alignment of the generated graph to the ground truth graph. To handle this, the paper proposes to use a simple graph  matching algorithm (Max Pooling Matching) to align nodes and edges. A downside to the algorithm is that it has complexity O(k^4) for graphs with k nodes, but the authors argue that this is not a problem when generating small graphs. Once the best correspondence is found, it is treated as constant and gradients are propagated appropriately.\n\nExperimentally, generative models of chemical graphs are trained on two datasets. Qualitative results and ELBO values are reported as the dimensionality of the embeddings is varied. No baseline results are presented. A further small set of experiments evaluates the quality of the matching algorithm on a synthetic setup.\n\nStrengths:\n- Generating graphs is an interesting problem, and the proposed approach seems like an easy-to-implement, mostly reasonable way of approaching the problem.\n\n- The exposition is clear (although a bit more detail on MPM matching would be appreciated)\n\nHowever, there are some significant weaknesses. First, the motivation for one-shot graph construction is not very strong:\n\n- I don't understand why the non-differentiability argued in (a) above is an issue. If training uses a maximum likelihood objective, then we should be able to decompose the generation of a graph into a sequence of decisions and maximize the sum of the logprobs of the conditionals. People do this all the time with sequence data and non-differentiability is not an issue.\n\n- I also don't agree that the one shot graph construction sidesteps the issue of how to linearize the construction of a graph. Even after doing so, the authors need to solve a matching problem to resolve the alignment issue. I see this as equivalent to choosing an order in which to linearize the order of nodes and edges in the graph.\n\nSecond, the experiments are quite weak. No baselines are presented to back up the claims motivating the formulation. I don't know how to interpret whether the results are good or bad. I would have at least liked to see a comparison to a method that generated SMILES format in an autoregressive manner (similar to previous work on chemical graph generation), and would ideally have liked to see an attempt at solving the alignment problem within an autoregressive formulation (e.g., by greedily constructing the alignment as the graph was generated). If one is willing to spend O(k^4) computation to solve the alignment problem, then there seem like many possibilities that could be easily applied to the autoregressive formulation. The authors might also be interested in a concurrent ICLR submission that approaches the problem from an autoregressive angle (https://openreview.net/pdf?id=Hy1d-ebAb). \n\nFinally, I would have expected to see a discussion and comparison to \"Learning Graphical State Transitions\" (Johnson, 2017). Please also don't make statements like \"To the best of our knowledge, we are the first to address graph generation using deep learning.\" This is very clearly not true. Even disregarding Johnson (2017), which the authors claim to be unaware of, I would consider approaches that generate SMILES format (like Gomez-Bombarelli et al) to be doing graph generation using deep learning.\n\nOverall, the paper is about an interesting subject, but in my opinion the execution isn't strong enough to warrant publication at this point.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders","abstract":"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with non-differentiability of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of conditional molecule generation. ","pdf":"/pdf/17aea562efcfef028ce455b5b97bb3ab0b75a721.pdf","TL;DR":"We demonstate an autoencoder for graphs for the first time.","paperhash":"anonymous|graphvae_towards_generation_of_small_graphs_using_variational_autoencoders","_bibtex":"@article{\n  anonymous2018graphvae:,\n  title={GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJlhPMWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper864/Authors"],"keywords":["graph","generative model","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512201354952,"tcdate":1512201069121,"number":1,"cdate":1512201069121,"id":"BkVdoCJZG","invitation":"ICLR.cc/2018/Conference/-/Paper864/Public_Comment","forum":"SJlhPMWAW","replyto":"SJlhPMWAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reference to previous work on matching via the Hungarian algorithm","comment":"I would like to point the authors to a relevant paper that similarly solves a one-to-one matching problem on unordered sets via the Hungarian algorithm within an end-to-end model:\n\nR. Stewart, M. Andriluka, A.Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016\n\nI think it would be fair to cite and discuss their work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders","abstract":"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with non-differentiability of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of conditional molecule generation. ","pdf":"/pdf/17aea562efcfef028ce455b5b97bb3ab0b75a721.pdf","TL;DR":"We demonstate an autoencoder for graphs for the first time.","paperhash":"anonymous|graphvae_towards_generation_of_small_graphs_using_variational_autoencoders","_bibtex":"@article{\n  anonymous2018graphvae:,\n  title={GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJlhPMWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper864/Authors"],"keywords":["graph","generative model","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512222797953,"tcdate":1512145886672,"number":2,"cdate":1512145886672,"id":"ByvkN-k-G","invitation":"ICLR.cc/2018/Conference/-/Paper864/Official_Review","forum":"SJlhPMWAW","replyto":"SJlhPMWAW","signatures":["ICLR.cc/2018/Conference/Paper864/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting step in generative ANN architectures  for graphs ","rating":"7: Good paper, accept","review":"The authors propose a variational auto encoder architecture to generate graphs.  \n\nPros:\n- the formulation of the problem as the modeling of a probabilistic graph is of interest \n- some of the main issues with graph generation are acknowledged (e.g. the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix)\n- notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs \n\nCons: \n- the work is quite preliminary\n- many crucial elements  in graph generation are not dealt with: \n a) the adjacency matrix and the label tensors are not independent of each other, the notion of a graph is in itself a way to represent the 'relational links' between the various components\n b) the boundaries between a feasible and an infeasible graph are sharp: one edge or one label can be sufficient for acting the transition independently of the graph size, this makes it a difficult task for a continuous model. The authors acknowledge this but do not offer ways to tackle the issue\n c) conditioning on the label histogram should make the problem easy: one is giving away the number of nodes and the label identities after all; however even in this setup the approach fails more often than not \n d) the graph matching procedure proposed is a rough patch for a much deeper problem\n- the evaluation should include a measure of the capacity of the architecture to :\n a) reconstruct perfectly the input\n b) denoise perturbations over node labels and additional/missing  edges  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders","abstract":"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with non-differentiability of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of conditional molecule generation. ","pdf":"/pdf/17aea562efcfef028ce455b5b97bb3ab0b75a721.pdf","TL;DR":"We demonstate an autoencoder for graphs for the first time.","paperhash":"anonymous|graphvae_towards_generation_of_small_graphs_using_variational_autoencoders","_bibtex":"@article{\n  anonymous2018graphvae:,\n  title={GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJlhPMWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper864/Authors"],"keywords":["graph","generative model","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1512222797995,"tcdate":1511596393189,"number":1,"cdate":1511596393189,"id":"rJZdWiUxz","invitation":"ICLR.cc/2018/Conference/-/Paper864/Official_Review","forum":"SJlhPMWAW","replyto":"SJlhPMWAW","signatures":["ICLR.cc/2018/Conference/Paper864/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting Graph Generator","rating":"7: Good paper, accept","review":"This work proposed an interesting graph generator using a variational autoencoder. The work should be interesting to researchers in the various areas. However, the work can only work on small graphs. The search space of small graph generation is usually very small, is there any other traditional methods can work on this problem? Moreover, the notations are a little confusing.  ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders","abstract":"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with non-differentiability of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of conditional molecule generation. ","pdf":"/pdf/17aea562efcfef028ce455b5b97bb3ab0b75a721.pdf","TL;DR":"We demonstate an autoencoder for graphs for the first time.","paperhash":"anonymous|graphvae_towards_generation_of_small_graphs_using_variational_autoencoders","_bibtex":"@article{\n  anonymous2018graphvae:,\n  title={GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJlhPMWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper864/Authors"],"keywords":["graph","generative model","autoencoder"]}},{"tddate":null,"ddate":null,"tmdate":1509739061677,"tcdate":1509136295847,"number":864,"cdate":1509739059018,"id":"SJlhPMWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJlhPMWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders","abstract":"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks only, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with non-differentiability of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of conditional molecule generation. ","pdf":"/pdf/17aea562efcfef028ce455b5b97bb3ab0b75a721.pdf","TL;DR":"We demonstate an autoencoder for graphs for the first time.","paperhash":"anonymous|graphvae_towards_generation_of_small_graphs_using_variational_autoencoders","_bibtex":"@article{\n  anonymous2018graphvae:,\n  title={GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJlhPMWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper864/Authors"],"keywords":["graph","generative model","autoencoder"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}