{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222542417,"tcdate":1511802847465,"number":3,"cdate":1511802847465,"id":"Byw1O6Fgz","invitation":"ICLR.cc/2018/Conference/-/Paper1020/Official_Review","forum":"Hyig0zb0Z","replyto":"Hyig0zb0Z","signatures":["ICLR.cc/2018/Conference/Paper1020/AnonReviewer2"],"readers":["everyone"],"content":{"title":"More work needed","rating":"4: Ok but not good enough - rejection","review":"The paper is interesting, but needs more work, and should provide clear and fair comparisons. Per se, the model is incrementally new, but it is not clear what the strengths are, and the presentations needs to be done more carefully.\n\nIn detail:\n- please fix several typos throughout the manuscript, and have a native speaker (and preferably an ASR expert) proofread the paper\n\nIntroduction\n- please define HMM/GMM model (and other abbreviations that will be introduced later), it cannot be assumed that the reader is familiar with all of them (\"ASG\" is used before it is defined, ...)\n- The standard units that most ASR systems use can be called \"senones\", and they are context dependent sub-phonetic units (see http://ssli.ee.washington.edu/~mhwang/), not phonetic states. Also the units that generate the alignment and the units that are trained on an alignment can be different (I can use a system with 10000 states to write alignments for a system with 3000 states) - this needs to be corrected.\n- When introducing CNNs, please also cite Waibel and TDNNs - they are *the same* as 1-d CNNs, and predate them. They have been extended to 2-d later on (Spatio-temporal TDNNs)\n- The most influential deep learning paper here might be Seide, Li, Yu Interspeech 2011 on CD-DNN-HMMs, rather than overview articles\n- Many papers get rid of the HMM pipeline, I would add https://arxiv.org/abs/1408.2873, which predates Deep Speech\n- What is a \"sequence-level variant of CTC\"? CTC is a sequence training criterion\n- The reason that Deep Speech 2 is better on noisy test sets is not only the fact they trained on more data, but they also trained on \"noisy\" (matched) data\n- how is this an end-to-end approach if you are using an n-gram language model for decoding? \n\nArchitecture\n- MFSC are log Filterbanks ...\n- 1D CNNs would be TDNNs\n- Figure 2: can you plot the various transition types (normalized, un-normalized, ...) in the plots? not sure if it would help, but it might\n- Maybe provide a reference for HMM/GMM and EM (forward backward training)\n- MMI was also widely used in HMM/GMM systems, not just NN systems\n- the \"blank\" states do *not* model \"garbage\" frames, if one wants to interpret them, they might be said to model \"non-stationary\" frames between CTC \"peaks\", but these are different from silence, garbage, noise, ...\n- what is the relationship of the presented ASG criterion to MMI? the form of equation (3) looks like an MMI criterion to me?\n\nExperiments\n- Many of the previous comments still hold, please proofread\n- you say there is no \"complexity\" incrase when using \"logadd\" - how do you measure this? number of operations? is there an implementation of \"logadd\" that is (absolutely) as fast as \"add\"?\n- There is discussion as to what i-vectors model (speaker or environment information) - I would leave out this discussion entirely here, it is enough to mention that other systems use adaptation, and maybe re-run an unadapted baselien for comparsion\n- There are techniques for incremental adaptation and a constrained MLLR (feature adaptation) approaches that are very eficient, if one wnats to get into this\n- it may also be interesting to discuss the role of the language model to see which factors influence system performance\n- some of the other papers might use data augmentation, which would increase noise robustness (did not check, but this might explain some of the results in table 4)\n- I am confused by the references in the caption of Table 3 - surely the Waibel reference is meant to be for TDNNs (and should appear earlier in the paper), while p-norm came later (Povey used it first for ASR, I think) and is related to Maxout\n- can you also compare the training times? \n\nConculsion\n- can you show how your approach is not so computationally expensive as RNN based approaches? either in terms of FLOPS or measured times\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gated ConvNets for Letter-Based ASR","abstract":"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed. Key ingredients for the acoustic model are Gated Linear Units and high dropout. We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n","pdf":"/pdf/fd0e5b0202837238db3b2fbaf15b614696ade0a1.pdf","TL;DR":"A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline.","paperhash":"anonymous|gated_convnets_for_letterbased_asr","_bibtex":"@article{\n  anonymous2018gated,\n  title={Gated ConvNets for Letter-Based ASR},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyig0zb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1020/Authors"],"keywords":["automatic speech recognition","letter-based acoustic model","gated convnets"]}},{"tddate":null,"ddate":null,"tmdate":1512222542464,"tcdate":1511328660787,"number":2,"cdate":1511328660787,"id":"Hyp5jKfxf","invitation":"ICLR.cc/2018/Conference/-/Paper1020/Official_Review","forum":"Hyig0zb0Z","replyto":"Hyig0zb0Z","signatures":["ICLR.cc/2018/Conference/Paper1020/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting- maybe as workshop paper","rating":"6: Marginally above acceptance threshold","review":"The paper describes some interesting work but for a combination of reasons I think it's more like a workshop-track paper.\nThere is not much that's technically new in the paper-- at least not much that's really understandable.   There is some text about a variant of CTC, but it does not explain very clearly what was done or what the motivation was.\nThere are also quite a few misspellings.  \nSince the system is presented without any comparisons to alternatives for any of the individual components, it doesn't really shed any light on the significance of the various modeling decisions that were made.  That limits the value.\nIf rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gated ConvNets for Letter-Based ASR","abstract":"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed. Key ingredients for the acoustic model are Gated Linear Units and high dropout. We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n","pdf":"/pdf/fd0e5b0202837238db3b2fbaf15b614696ade0a1.pdf","TL;DR":"A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline.","paperhash":"anonymous|gated_convnets_for_letterbased_asr","_bibtex":"@article{\n  anonymous2018gated,\n  title={Gated ConvNets for Letter-Based ASR},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyig0zb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1020/Authors"],"keywords":["automatic speech recognition","letter-based acoustic model","gated convnets"]}},{"tddate":null,"ddate":null,"tmdate":1512222542571,"tcdate":1510729547565,"number":1,"cdate":1510729547565,"id":"SkEUwDtkG","invitation":"ICLR.cc/2018/Conference/-/Paper1020/Official_Review","forum":"Hyig0zb0Z","replyto":"Hyig0zb0Z","signatures":["ICLR.cc/2018/Conference/Paper1020/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Minimal novelty","rating":"3: Clear rejection","review":"This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]. It is fair to say that this paper contains almost no novelty.\n\nThis paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach. However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG. Prior work along this line includes [3, 4, 5, 6, 7].\n\nUsing MFSC, or more commonly known as log mel filter bank outputs, has been pretty common since [8]. Having a separate subsection (2.1) discussing this seems unnecessary.\n\nArguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG. It is unclear why discriminative training, such as MMI, sMBR, and lattice-free MMI, is mentioned in section 2.3. Discriminative training is not invented to overcome the lack of manual segmentations, and is equally applicable to the case where we have manual segmentations.\n\nThe authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding. However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters. In addition, ASG requires additional forward-backward computation for computing the partition function (second term in eq 3). There is no reason to believe that ASG can be faster than CTC in both training and decoding.\n\nThe connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. Otherwise, the objective won't be a proper probability distribution.\n\nThe citation style in section 2.4 seems off. Also see [4] for a great description of how beam search is done in CTC.\n\nDetails about training, such as the optimizer, step size, and batch size, are missing. Does no batching (in section 3.2) means a batch size of one utterance?\n\nIn the last paragraph of section 3.2, why is there a huge difference in real-time factors between the clean and other set? Something is wrong unless the authors are using different beam widths in the two settings.\n\nThe paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet. It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet. Similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj. None of the comparison in table 4 is really meaningful, because none of the other systems have parameters as many as 19 layers of convolution. Why does CTC fail when trained without the blanks? Is there a way to fix it besides using ASG? It is also unclear why speaker-adaptive training is not needed. At which layer do the features become speaker invariant? Can the system improve further if speaker-adaptive features are used instead of log mels? This paper would be much stronger if the authors can include these experiments and analyses.\n\n[1] R Collobert, C Puhrsch, G Synnaeve, Wav2letter: an end-to-end convnet-based speech recognition system, 2016\n\n[2] Y Dauphin, A Fan, M Auli, D Grangier, Language modeling with gated convolutional nets, 2017\n\n[3] A Graves and N Jaitly, Towards End-to-End Speech Recognition with Recurrent Neural Networks, 2014\n\n[4] A Maas, Z Xie, D Jurafsky, A Ng, Lexicon-Free Conversational Speech Recognition with Neural Networks, 2015\n\n[5] Y Miao, M Gowayyed, F Metze, EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding, 2015\n\n[6] D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, End-to-end attention-based large vocabulary speech recognition, 2016\n\n[7] W Chan, N Jaitly, Q Le, O Vinyals, Listen, attend and spell, 2015\n\n[8] A Graves, A Mohamed, G Hinton, Speech recognition with deep recurrent neural networks, 2013\n\n[9] H Tang, L Lu, L Kong, K Gimpel, K Livescu, C Dyer, N Smith, S Renals, End-to-End Neural Segmental Models for Speech Recognition, 2017","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gated ConvNets for Letter-Based ASR","abstract":"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed. Key ingredients for the acoustic model are Gated Linear Units and high dropout. We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n","pdf":"/pdf/fd0e5b0202837238db3b2fbaf15b614696ade0a1.pdf","TL;DR":"A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline.","paperhash":"anonymous|gated_convnets_for_letterbased_asr","_bibtex":"@article{\n  anonymous2018gated,\n  title={Gated ConvNets for Letter-Based ASR},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyig0zb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1020/Authors"],"keywords":["automatic speech recognition","letter-based acoustic model","gated convnets"]}},{"tddate":null,"ddate":null,"tmdate":1510609070353,"tcdate":1510609070353,"number":1,"cdate":1510609070353,"id":"rkU2e9DJM","invitation":"ICLR.cc/2018/Conference/-/Paper1020/Public_Comment","forum":"Hyig0zb0Z","replyto":"Hyig0zb0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Notorious comment on missing citations","comment":"The paper seems to completely ignore a set of works on character-based ASR with attention networks:\n  - Chorowski et al. \"Attention-based models for speech recognition.\", 2015\n  - Chan et al. \"Listen, attend and spell\", 2015\n  - Bahdanau et al. \"End-to-end attention-based large vocabulary speech recognition.\", 2016\nand some milestone works with CTC loss function:\n  - Graves and Jaitly \"Towards end-to-end speech recognition with recurrent neural networks.\" , 2014\n  - Zhang et al. \"Towards end-to-end speech recognition with deep convolutional neural networks.\", 2017\n\nThis work uses rather unusual corpus LibriSpeech, therefore the performance is not comparable to works listed above (that benchmark mainly on WSJ). LibriSpeech is great, but the paper lacks comparison to prior work on end-to-end recognition.\n\nMFSC features are presented as an invention in this paper. Such features are usually referred as \"log-mel filterbank\" in the literature."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Gated ConvNets for Letter-Based ASR","abstract":"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed. Key ingredients for the acoustic model are Gated Linear Units and high dropout. We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n","pdf":"/pdf/fd0e5b0202837238db3b2fbaf15b614696ade0a1.pdf","TL;DR":"A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline.","paperhash":"anonymous|gated_convnets_for_letterbased_asr","_bibtex":"@article{\n  anonymous2018gated,\n  title={Gated ConvNets for Letter-Based ASR},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyig0zb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1020/Authors"],"keywords":["automatic speech recognition","letter-based acoustic model","gated convnets"]}},{"tddate":null,"ddate":null,"tmdate":1510092381881,"tcdate":1509137945644,"number":1020,"cdate":1510092360481,"id":"Hyig0zb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hyig0zb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Gated ConvNets for Letter-Based ASR","abstract":"In this paper we introduce a new speech recognition system, leveraging a simple letter-based ConvNet acoustic model. The acoustic model requires only audio transcription for training -- no alignment annotations, nor any forced alignment step is needed. At inference, our decoder takes only a word list and a language model, and is fed with letter scores from the acoustic model -- no phonetic word lexicon is needed. Key ingredients for the acoustic model are Gated Linear Units and high dropout. We show near state-of-the-art results in word error rate on the LibriSpeech corpus with MFSC features, both on the clean and other configurations.\n","pdf":"/pdf/fd0e5b0202837238db3b2fbaf15b614696ade0a1.pdf","TL;DR":"A letter-based ConvNet acoustic model leads to a simple and competitive speech recognition pipeline.","paperhash":"anonymous|gated_convnets_for_letterbased_asr","_bibtex":"@article{\n  anonymous2018gated,\n  title={Gated ConvNets for Letter-Based ASR},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyig0zb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1020/Authors"],"keywords":["automatic speech recognition","letter-based acoustic model","gated convnets"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}