{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642506634,"tcdate":1512021107107,"number":3,"cdate":1512021107107,"id":"Hyi_2MTxz","invitation":"ICLR.cc/2018/Conference/-/Paper776/Official_Review","forum":"Skz_WfbCZ","replyto":"Skz_WfbCZ","signatures":["ICLR.cc/2018/Conference/Paper776/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A simple proof for another generalization bounds on ReLU NNs","rating":"7: Good paper, accept","review":"This paper combines a simple PAC-Bayes argument with a simple perturbation analysis (Lemma 2) to get a margin based generalization error bound for ReLU neural networks (Theorem 1) which depends on the product of the spectral norms of the layer parameters as well as their Frobenius norm. The main contribution of the paper is the simple proof technique to derive Theorem 1, much simpler than the one use in the very interesting work [Bartlett et al. 2017] (appearing at NIPS 2017) which got an analogous bound but with a dependence on the l1-norm of the layers instead of the Frobenius norm. The authors make a useful comparison between these bounds in Section 3 showing that none is dominating the others, but still analyzing their properties in terms of structural properties of the weight matrices.\n\nI enjoyed reading this paper. One could think that it makes a somewhat incremental contribution with respect to the more complete work (both theory and practice) from [Bartlett et al. 2017]. Nevertheless, the simplicity and elegance of the proof as well as the result might be useful for the community to get progress on the theoretical analysis of NNs.\n\nThe paper is well written, though I make some suggestions for the camera ready version below to improve clarity.\n\nI verified most of the math.\n\n== Detailed suggestions ==\n\n1) The authors should specify in the abstract and in the introduction that they are analyzing feedforward neural networks *with ReLU activation functions* so that the current context of the result is more transparent. It is quite unclear how one could generalize the Theorem 1 to arbitrary activation functions phi given the crucial use of the homogeneity of the ReLU at the beginning of p.4. Though the proof of Lemma 2 only appears to be using the 1-Lipschitzness property of phi as well as phi(0) =0. (Unless they can generalize further; I also suggest that they explicitly state in the (interesting) Lemma 2 that it is for the ReLU activations (like they did in Theorem 1)).\n\n2) A footnote (or citation) could be useful to give a hint on how the inequality 1/e beta^(d-1) <= tilde{beta}^(d-1) <= e beta^(d-1) is proven from the property |beta-tilde{beta}|<= 1/d beta (middle of p.4).\n\n3) Equation (3) -- put the missing 2 subscript for the l2 norm of |f_(w+u)(x) - f_w(x)|_2 on the LHS (for clarity).\n\n4) One extra line of derivation would be helpful for the reader to rederive the bound|w|^2/2sigma^2  <= O(...) just above equation (4). I.e. first doing the expansion keeping the beta terms and Frobenius norm sum, and then going directly to the current O(...) term.\n\n5) bottom of p.4: use hat{L}_gamma = 1 instead of L_gamma =1 for more clarity.\n\n6) Top of p.5: the sentence \"Since we need tilde{beta} to satisfy (...)\" is currently awkwardly stated. I suggest instead to say that \"|tilde{beta}- beta| <= 1/d (gamma/2B)^(1/d) is a sufficient condition to have the needed condition |tilde{beta}-beta| <= 1/d beta over this range, thus we can use a cover of size dm^(1/2d).\"\n\n7) Typo below (6): citetbarlett2017...\n\n8) Last paragraph p.5: \"Recalling that W_i is *at most* a hxh matrix\" (as your result do not require constant size layers and covers the rectangular case). \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks","abstract":"We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights.  The generalization bound is derived using a PAC-Bayes analysis.","pdf":"/pdf/99e333610ae8f6605479c5dad603e1dab684c9c4.pdf","paperhash":"anonymous|a_pacbayesian_approach_to_spectrallynormalized_margin_bounds_for_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skz_WfbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper776/Authors"],"keywords":["Neural Networks","Generalization","PAC-Bayes","Sharpness"]}},{"tddate":null,"ddate":null,"tmdate":1515642508179,"tcdate":1511768211680,"number":2,"cdate":1511768211680,"id":"rkn5xHFlf","invitation":"ICLR.cc/2018/Conference/-/Paper776/Official_Review","forum":"Skz_WfbCZ","replyto":"Skz_WfbCZ","signatures":["ICLR.cc/2018/Conference/Paper776/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A PAC-Bayesian generalization bound","rating":"6: Marginally above acceptance threshold","review":"This paper provides a new generalization bound for feed forward networks based on a PAC-Bayesian analysis. The generalization bound depends on the spectral norm of the layers and the Frobenius norm of the weights. The resulting generalization bound is similar (though not comparable) to a recent result of Bartlett et al (2017), however the technique is different since this submission uses PAC-Bayesian analysis. The resulting proof is more simple and streamlined compared to that of Bartlett et al (2017).\n\nThe paper is well presented, the result is explained and compared to other results, and the proofs seem correct. The result is not particularly different from previous ones, but the different proof technique might be a good enough reason to accept this paper. \n\n\n\n\nTypos: Several citations are unparenthesized when they should be. Also, after equation (6) there is a reference command that is not compiled properly.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks","abstract":"We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights.  The generalization bound is derived using a PAC-Bayes analysis.","pdf":"/pdf/99e333610ae8f6605479c5dad603e1dab684c9c4.pdf","paperhash":"anonymous|a_pacbayesian_approach_to_spectrallynormalized_margin_bounds_for_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skz_WfbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper776/Authors"],"keywords":["Neural Networks","Generalization","PAC-Bayes","Sharpness"]}},{"tddate":null,"ddate":null,"tmdate":1515642508217,"tcdate":1510266280087,"number":1,"cdate":1510266280087,"id":"SJehrIf1f","invitation":"ICLR.cc/2018/Conference/-/Paper776/Official_Review","forum":"Skz_WfbCZ","replyto":"Skz_WfbCZ","signatures":["ICLR.cc/2018/Conference/Paper776/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Elegant","rating":"9: Top 15% of accepted papers, strong accept","review":"The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices.  They compare this\nbound with a similar recent bound proved by Bartlett, et al.  While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions.\n\nThe analysis is elegant.  It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent.\n\nVery nice paper.\n\nI guess that networks including convolutional layers are covered by\ntheir analysis.  It feels to me that these tend to be sparse, but that\ntheir analysis still my provides some additional leverage for such\nlayers.  Some explicit discussion of convolutional layers may be\nhelpful.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks","abstract":"We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights.  The generalization bound is derived using a PAC-Bayes analysis.","pdf":"/pdf/99e333610ae8f6605479c5dad603e1dab684c9c4.pdf","paperhash":"anonymous|a_pacbayesian_approach_to_spectrallynormalized_margin_bounds_for_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skz_WfbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper776/Authors"],"keywords":["Neural Networks","Generalization","PAC-Bayes","Sharpness"]}},{"tddate":null,"ddate":null,"tmdate":1515087214026,"tcdate":1509134698181,"number":776,"cdate":1509739106650,"id":"Skz_WfbCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Skz_WfbCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks","abstract":"We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights.  The generalization bound is derived using a PAC-Bayes analysis.","pdf":"/pdf/99e333610ae8f6605479c5dad603e1dab684c9c4.pdf","paperhash":"anonymous|a_pacbayesian_approach_to_spectrallynormalized_margin_bounds_for_neural_networks","_bibtex":"@article{\n  anonymous2018a,\n  title={A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Skz_WfbCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper776/Authors"],"keywords":["Neural Networks","Generalization","PAC-Bayes","Sharpness"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}