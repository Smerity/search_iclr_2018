{"notes":[{"tddate":null,"ddate":null,"tmdate":1512331497545,"tcdate":1512331497545,"number":2,"cdate":1512331497545,"id":"Bk-lFRWWz","invitation":"ICLR.cc/2018/Conference/-/Paper300/Official_Review","forum":"BJgPCveAW","replyto":"BJgPCveAW","signatures":["ICLR.cc/2018/Conference/Paper300/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Sparse networks have fewer parameters without potentially sacrificing performance","rating":"5: Marginally below acceptance threshold","review":"The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers. Numerical experiments show that such sparse networks can have similar performance to fully connected ones. They introduce a concept of “scatter” that correlates with network performance. Although  I found the results useful and potentially promising, I did not find much insight in this paper.\nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer. Once the signals from different windows are intermixed, how do you even define the windows?  \nMinor\nSecond line of Section 2.1: “lesser” -> less or fewer\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Sparse Connectivity Patterns in Neural Networks","abstract":"We propose a novel way of reducing the number of parameters in the storage-hungry fully connected classification layers of a neural network by using pre-defined sparsity, where the majority of connections are absent prior to starting training. Our results indicate that convolutional neural networks can operate without any loss of accuracy at less than 0.5% classification layer connection density, or less than 5% overall network connection density. We also investigate the effects of pre-defining the sparsity of networks with only fully connected layers. Based on our sparsifying technique, we introduce the 'scatter' metric to characterize the quality of a particular connection pattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset on classifying Morse code symbols, which highlights some interesting trends and limits of sparse connection patterns.","pdf":"/pdf/d3745325e6ec60cca4b020f743b11d00d33cffcc.pdf","TL;DR":"Neural networks can be pre-defined to have sparse connectivity without performance degradation.","paperhash":"anonymous|characterizing_sparse_connectivity_patterns_in_neural_networks","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Sparse Connectivity Patterns in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgPCveAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper300/Authors"],"keywords":["Machine learning","Neural networks","Sparse neural networks","Pre-defined sparsity","Scatter","Connectivity patterns","Adjacency matrix","Parameter Reduction","Morse code"]}},{"tddate":null,"ddate":null,"tmdate":1512222617118,"tcdate":1511743377864,"number":1,"cdate":1511743377864,"id":"S1951kYef","invitation":"ICLR.cc/2018/Conference/-/Paper300/Official_Review","forum":"BJgPCveAW","replyto":"BJgPCveAW","signatures":["ICLR.cc/2018/Conference/Paper300/AnonReviewer2"],"readers":["everyone"],"content":{"title":"hard to follow, confused about many points","rating":"4: Ok but not good enough - rejection","review":"This paper examines sparse connection patterns in upper layers of convolutional image classification networks.  Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks.  Heuristics for distributing connections among windows/groups and a measure called \"scatter\" are introduced to construct the connectivity masks, and evaluated experimentally on CIFAR-10 and -100, MNIST and Morse code symbols.\n\nWhile it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions.  I've described many of the points I was confused by in more detailed comments below.\n\n\nDetailed comments and questions:\n\n\nThe distribution of connections in \"windows\" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image.  But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure.  So are do the \"windows\" correspond to spatial windows, and if so, how?  Or are they different (maybe arbitrary) groupings over the feature maps?\n\nAlso a bit confusing is the notation \"conv2\", \"conv3\", etc.  These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example).  But here it seems just to indicate the number of \"CL\" layers: 2.  And p.1 says that the \"CL\" layers are those often referred to as \"FC\" layers, not \"conv\" (though they may be convolutionally applied with spatial 1x1 kernels).\n\nThe heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but I'm not convinced this will work well in all situations, and may even be sub-optimal for the examined datasets.  For example, to distinguish MNIST 1 vs 7 vs 9, it is most important to see the top-left:  whether it is empty, has a horizontal line, or a loop.  So some regions are more important than others, and the top half may be more important than an equally spaced global view.  So the description of how to space connections between windows makes some intuitive sense, but I'm unclear on whether other more general connections might be even better, including some that might not be as easily analyzed with the \"scatter\" metric described.\n\nAnother broader question I have is in the distinction between lower and upper layers (those referred to as \"feature extracting\" and \"classification\" in this paper).  It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret).  So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the \"classification\" function is pushed down to lower layers, as the upper layers are reduced in size.  How would they respond to similar reductions?\n\nI'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think?  I'm also unsure whether the windows are over spatial extent only, or over features.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Characterizing Sparse Connectivity Patterns in Neural Networks","abstract":"We propose a novel way of reducing the number of parameters in the storage-hungry fully connected classification layers of a neural network by using pre-defined sparsity, where the majority of connections are absent prior to starting training. Our results indicate that convolutional neural networks can operate without any loss of accuracy at less than 0.5% classification layer connection density, or less than 5% overall network connection density. We also investigate the effects of pre-defining the sparsity of networks with only fully connected layers. Based on our sparsifying technique, we introduce the 'scatter' metric to characterize the quality of a particular connection pattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset on classifying Morse code symbols, which highlights some interesting trends and limits of sparse connection patterns.","pdf":"/pdf/d3745325e6ec60cca4b020f743b11d00d33cffcc.pdf","TL;DR":"Neural networks can be pre-defined to have sparse connectivity without performance degradation.","paperhash":"anonymous|characterizing_sparse_connectivity_patterns_in_neural_networks","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Sparse Connectivity Patterns in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgPCveAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper300/Authors"],"keywords":["Machine learning","Neural networks","Sparse neural networks","Pre-defined sparsity","Scatter","Connectivity patterns","Adjacency matrix","Parameter Reduction","Morse code"]}},{"tddate":null,"ddate":null,"tmdate":1509739377497,"tcdate":1509092951547,"number":300,"cdate":1509739374837,"id":"BJgPCveAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJgPCveAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Characterizing Sparse Connectivity Patterns in Neural Networks","abstract":"We propose a novel way of reducing the number of parameters in the storage-hungry fully connected classification layers of a neural network by using pre-defined sparsity, where the majority of connections are absent prior to starting training. Our results indicate that convolutional neural networks can operate without any loss of accuracy at less than 0.5% classification layer connection density, or less than 5% overall network connection density. We also investigate the effects of pre-defining the sparsity of networks with only fully connected layers. Based on our sparsifying technique, we introduce the 'scatter' metric to characterize the quality of a particular connection pattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset on classifying Morse code symbols, which highlights some interesting trends and limits of sparse connection patterns.","pdf":"/pdf/d3745325e6ec60cca4b020f743b11d00d33cffcc.pdf","TL;DR":"Neural networks can be pre-defined to have sparse connectivity without performance degradation.","paperhash":"anonymous|characterizing_sparse_connectivity_patterns_in_neural_networks","_bibtex":"@article{\n  anonymous2018characterizing,\n  title={Characterizing Sparse Connectivity Patterns in Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgPCveAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper300/Authors"],"keywords":["Machine learning","Neural networks","Sparse neural networks","Pre-defined sparsity","Scatter","Connectivity patterns","Adjacency matrix","Parameter Reduction","Morse code"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}