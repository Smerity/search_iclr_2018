{"notes":[{"tddate":null,"ddate":null,"tmdate":1515060963244,"tcdate":1514901789949,"number":4,"cdate":1514901789949,"id":"BJ87ZfYmG","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Comment","forum":"Hk__kGbCW","replyto":"B1owoOFlz","signatures":["ICLR.cc/2018/Conference/Paper756/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper756/Authors"],"content":{"title":"Thank you for your reading and comments.","comment":"Dear reviewer,\nThank you for your reading and comments. Below are the responses to your questions of the paper:\n\n(1) [Regarding your first concern on whether dense connection or dense attention actually makes the gain]\n  Please refer to point (1) of our general response to all reviewers. To state again, we conduct ablation studies on text summarization task and the results indicate that both dense connection and dense attention contribute to the improvement.\n\n(2) [Regarding model layer and hidden size]\n  We now add experiments of fixing layer number but varying hidden size on DenseRNN. We mainly focused on the parameters used, so we fixed the hidden size and varied layer number for convenience. As you concerned about this, we build dense models with fixed layer number but different hidden sizes to investigate its effects to performance. Concretely, we build four 4-layer dense models with hidden size of 150, 192, 256 and 320, leading to respectively 43M, 50M, 70M and 92M total model parameters, for the sake of comprehensive study.  \n  We report the results here:\n  +---------------------------- +------------------+------------------+------------------+------------------+------------------+------------------+\n  |       Model                   | Hidden size  |     #Layers    |    #Param     |    ROUGE-1   |   ROUGE-2    |   ROUGE-L     |\n  |  DenseRNN-4L-h150|        150          |           4          |        43M        |        36.15      |        17.12      |        33.57       |\n  |  DenseRNN-4L-h192|        192          |           4          |        50M        |        36.25      |        17.28      |        33.56       |\n  |  DenseRNN-4L-h256|        256          |           4          |        70M        |        36.49      |        17.40      |        33.74       |\n  |  DenseRNN-4L-h320|        320          |           4          |        92M        |        36.56      |        17.57      |        33.78       |\n  +---------------------------- +------------------+------------------+------------------+------------------+------------------+------------------+           \n Compare these results with the results in Table 3, we can see that when fixing layer and varying hidden size to vary the model size, same observations as in the paper can be obtained: dense models show similar or even better performance as baselines when using only half parameters, and outperform baselines under similar number of parameters.\n\n(3) [Regarding your question of why the concatenation of all layers in section 3.2 is not experimented]\n  We respectfully disagree with your claim. Actually, we indeed did the same in experiment as in section 3.2. Section 3.2 describes the dense connection, where \"dense\" means that the attention operation is applied between hidden states in decoder and every layer`s output in encoder, and then all the context vectors are concatenated. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515060931754,"tcdate":1514901755783,"number":3,"cdate":1514901755783,"id":"ryNb-fF7f","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Comment","forum":"Hk__kGbCW","replyto":"B1FEuWcez","signatures":["ICLR.cc/2018/Conference/Paper756/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper756/Authors"],"content":{"title":"We really appreciate your constructive comments","comment":"Dear reviewer,\n\nWe really appreciate your constructive comments and questions of the paper. Here are our responses to your questions:\n\n(1) [Regarding the significance of dense operation in deep neural networks for sequence-to-sequence learning] \n  It is well motivated to explore the potential of leveraging dense connections in sequence-to-sequence learning: 1) Sequence to sequence learning used to leverage single layer model, but recent work finds it necessary to use deep models for the sake of satisfactory, even stat-of-art performances. Tasking three most representative and effective sequence-to-sequence models as example: GNMT (Google Neural Machine Translation) (Wu et al., 2016) has 8 layers; ConvS2S(Convolutional Sequence to Sequence Learning) (Gehring et al., 2016) by Facebook uses 15 layers; The latest STOA sequence-to-sequence learning model â€“ Transformer (Vaswani et al., 2017) - uses 6 layers as its basic model structure. However, each layer contains 4 sub-layer, leading to total 24 layers in both the encoder and decoder. Therefore, it is important to improve deep sequence-to-sequence models for the real-world usage.  2) Dense connections have been shown to be an effective approach in computer vision to improve parameter usage in building deep models. In terms of better performance/parameter usage, it is then a natural question that whether such dense operators help to improve the deep structures in sequence-to-sequence models such as stacked LSTM. 3) However it is not trivial to bring the dense operations to sequence-to-sequence learning: on one hand the model architectures (e.g., LSTM) are quite different with CNN; on the other hand, we need to specially handle the inter connections between encoder and decoder within the model. Out of all these items, we motivate the exploration of leveraging dense connections in sequence-to-sequence learning.\n\n(2) [Regarding the performance issue of deeper model that in Table 3 baseline-6L is worse than baseline-4L]\n  Please refer to point (1) of our general response to all reviewers.\n\n(3) [Regarding the number of parameters increase with the depth of the network]\n    When we do concatenation, since the information flow is enhanced, we do not need to use the hidden size as big as in stacked RNN so we decrease the hidden size much and thus decrease the total parameters.\n  We have shown the number of parameters of all the dense models in all our tables (Table 1, Table 2 and Table 3) in the experiments section. It shows our dense models use less parameter size.\n\n(4) [Smaller baseline models to match the smaller dense networks]\n  Thanks for your suggestion and we have conducted such new experiment. To be more concrete, on WMT English->German translation, to match the model size of our previously reported DenseRNN-6L with 117M parameters (test set BLEU score of 24.00 shown in Table 2), we additionally run a 2-layer baseline stack LSTM model with 133M parameters.  The test set BLEU of such a smaller baseline model is 23.56, worse than our DenseRNN-6L-117M (24.00).\n\n(5) [Regarding paper writing]\n  Thanks for your comments on the paper writing. We have fixed some typos and will further carefully polish the paper and improve the writing.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515060990828,"tcdate":1514901711984,"number":2,"cdate":1514901711984,"id":"HydCeMt7f","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Comment","forum":"Hk__kGbCW","replyto":"Bkr5fKdlf","signatures":["ICLR.cc/2018/Conference/Paper756/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper756/Authors"],"content":{"title":"New baseline method exploration and clarifications for training curve 'jump'","comment":"Dear reviewer,\nThank you for your comments and questions. We would like to make several points for the sake of clarifications:\n\n(1) [Regarding the contribution about dense connections and dense attention]\n  Please refer to our general response, point 2.\n\n(2) [Regarding deeper and narrower baseline models]\n  To verify the effects of deep and narrow models V.S. shallow and fat models with roughly the same model capacity, we build another deeper and narrower baseline model with 22 layers, hidden size of 256, and 92M parameters in total (referred as baseline-22L), to compare with the baseline-4L (with 4 layers, hidden size of 512 and 96M parameters in total) we originally reported in the paper. The performance of baseline-22L is 35.71, 16.69 and 33.16 on ROUGE-1, ROUGE-2 and ROUGE-L respectively, which is similar to the shallow and fat baseline-4L with scores of 35.91, 16.68 and 33.01. It shows that baseline models obtain similar performance when having similar number of parameters, whether they are shallow and fat, or deep and narrow. \n\n(3) [Regarding the training time curve] \n  The \"jump\" is indeed caused by the learning rate annealing. Here what we want to specially make clear is that we make fair comparisons: the learning rate anneal strategy is the same for all the experiments including both baseline methods and our proposed methods: the model is evaluated on valid data set every 10000 steps and if the valid performance does not increase for 8 consecutive check points (in another word, 80000 steps) the learning rate will decay to 1/10.  Thus the \"early jumps\" indicate that the model has converged here under current learning rate and has to be fine-tuned. Therefore, \"early jumps\" not only indicate learning rate anneal, but more importantly, also indicates the speed of model convergence.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515060891513,"tcdate":1514901658832,"number":1,"cdate":1514901658832,"id":"SyXslfFmz","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Comment","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper756/Authors"],"content":{"title":"Towards results on text summarization and ablation study","comment":"Dear Reviewers,\n\nWe thank all of your constructive review comments which help us improve the paper. Here are some general points for some of your common issues:\n\n(1) In the original version of the paper, due to limited time and resources, we apologize for not fully exploring the potential of different hyper parameter values of text summarization task, leading to a not satisfactory baseline method performance (e.g., the 6-6 layer baseline model performs worse than the 4-4 model).  Therefore we re-run all the experiments for this task with a comprehensive fine-tuning strategist for hyper-parameter values and achieve better performance for baseline models and our proposed models. Please refer to Table 3 for latest detailed results on text summarization.\n\n(2) We regret for not investigating different contributions of dense connection in RNN layers and dense attention. In the latest paper version, we conduct such ablation studies using text summarization as an example and list the results in Table 3.  Overall speaking, we show that both dense connections and dense attention make contribution to the performance improvement and please check the analysis in the main text under Table 3 (section 4.2, paragraph 3).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642503685,"tcdate":1511819312571,"number":3,"cdate":1511819312571,"id":"B1FEuWcez","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Review","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review of \"Densely connected recurrent neural network for sequence-to-sequence learning\"","rating":"4: Ok but not good enough - rejection","review":"This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se). Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs. The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.  \n\nOverall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak. The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?) for which the reason could be a multitude of issues probably related to hyper-parameter tuning. What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation? Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2. Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642503721,"tcdate":1511783267394,"number":2,"cdate":1511783267394,"id":"B1owoOFlz","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Review","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/AnonReviewer3"],"readers":["everyone"],"content":{"title":"incremental to prior work","rating":"6: Marginally above acceptance threshold","review":"This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer. In addition, attention context is computed for each layer, then, combined together as a single context. Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters.\n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable. However I have some concerns to this paper.\n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from. I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms.\n\n- It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers. Do you have experiments by fixing the number of layers and varying the hidden size?\n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642503756,"tcdate":1511719564566,"number":1,"cdate":1511719564566,"id":"Bkr5fKdlf","invitation":"ICLR.cc/2018/Conference/-/Paper756/Official_Review","forum":"Hk__kGbCW","replyto":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference/Paper756/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Dense skip-connections between layers improve recurrent networks","rating":"5: Marginally below acceptance threshold","review":"The article proposes to use dense skip-connections on the \"vertical\" (between-layers) connections of recurrent networks. Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to.\n\nThe experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks.\n\nI am not convinced by the presented results for the following reasons:\n1) the paper introduces two concepts - the dense skip-connections and the multi-head attention. Experiments only show their joint impact, yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attention?\n2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines. What happens for deeper and narrower baselines that have a similar number of parameters?\n3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the \"jumps\" in the training curves as signs of learning rate anneal). Maybe this is what helps? I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated?\n\nQuality:\nGood\n\nClarity:\nThe paper is clearly written.\n\nOriginality:\nThe addition of dense connections to recurrent networks is trivial.\n\n\nPros&cons\n+ the proposed additions (dense skip connections) and multi-head attentions yield performance improvements\n- the impact of the two contributions is not disentangled in the paper\n- the two contributions are fairly obvious","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514902479236,"tcdate":1509134192205,"number":756,"cdate":1509739118237,"id":"Hk__kGbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk__kGbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING","abstract":"Deep neural networks based sequence-to-sequence learning has achieved remarkable progress in applications like machine translation and text summarization. However, sequence-to-sequence models suffer from severe inefficiency in training process, requiring huge amount of training time as well as memory usage. In this work, inspired by densely connected layers in modern convolutional neural network, we introduce densely connected sequence-to-sequence learning mechanism to tackle this challenge. In this mechanism, multiple layers of representations from stacked recurrent neural networks are concatenated to enhance feature reuse. Furthermore, a densely connected attention model is elaborately leveraged to improve information flow with more efficient parameter usage via multi-branch structure and local sparsity. We show that such a densely connected mechanism significantly reduces training time and memory usage for sequence-to-sequence learning. In particular, in WMT-14 English-French translation task with a subset of 12M training data, it takes half of training time and model parameters to achieve similar BLEU as typical stacked LSTM models.","pdf":"/pdf/2af5d0859d9f07a818436b83ef48aff7f4dd0aaa.pdf","paperhash":"anonymous|densely_connected_recurrent_neural_network_for_sequencetosequence_learning","_bibtex":"@article{\n  anonymous2018densely,\n  title={DENSELY CONNECTED RECURRENT NEURAL NETWORK FOR SEQUENCE-TO-SEQUENCE LEARNING},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk__kGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper756/Authors"],"keywords":[]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}