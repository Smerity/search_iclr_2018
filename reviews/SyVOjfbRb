{"notes":[{"tddate":null,"ddate":null,"tmdate":1512373033481,"tcdate":1512372680459,"number":3,"cdate":1512372680459,"id":"SJl0YdfWM","invitation":"ICLR.cc/2018/Conference/-/Paper939/Official_Review","forum":"SyVOjfbRb","replyto":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference/Paper939/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A simple application of LSH but logically disordered","rating":"4: Ok but not good enough - rejection","review":"The main contribution of this work is just a combination of LSH schemes and SGD updates. Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation. The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound.  Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity).\n\nStrengths:  a sound approach; a simple and straightforward idea that is shown to work well in evaluations.\n\nWeaknesses: \n1. The phrase of \"computational chicken-and-egg loop\" in the title and also in the main body is misleading and not accurate. The so-called \"chicken-and-egg” issue concerns the causality dilemma: two causally related things, which comes the first. In the paper, the authors concerned \"more accurate gradients\" and \"faster convergence\"; their causality is very clear (the first leads to the second), and there is no causality dilemma. Even from a computational perspective, \"SDG schemes aim for computational efficiency\" and \"stochastic makes the convergence slow down\" are not a causality dilemma.  The reason behind is that the latter is the cost of the first one, just the old saying that \"there is no such thing as a free lunch\". Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by \"twisted\" and unnatural logics. \n \n2. The depth is so limited. Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided. The theorems (Theorems 1~3) are trivial, with loose relations with LSH.\n\t \n3. The LSH schemes are not correctly referred to. Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS. It is not in depth to assume \"any known LSH scheme\" in Alg. 2. Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity).\n\n4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase \"computational chicken-and-egg loop\", the organization and presentation of the whole manuscript are poor.\n\n5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]}},{"tddate":null,"ddate":null,"tmdate":1512222823533,"tcdate":1511829233618,"number":2,"cdate":1511829233618,"id":"rkcg14qlz","invitation":"ICLR.cc/2018/Conference/-/Paper939/Official_Review","forum":"SyVOjfbRb","replyto":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference/Paper939/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Another application of LSH sampling","rating":"4: Ok but not good enough - rejection","review":"  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss.\nThe basic form of SGD selects an example uniformly.   Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient.\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015].\n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger.\n\n  The paper uses LSH structures, computed over the set of examples,\n to quickly sample examples with large inner products with the current parameter vector \\theta.   Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity.\n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings).\n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited. The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.  There are theorems,  but they are trivial, straightforward applications of importance sampling. \n\n The paper is not well written. The presentation is much more complex that need be. References to classic weighted sampling are \n\n  The application is limited to certain loss functions for which we can compute LSH structures.  This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]}},{"tddate":null,"ddate":null,"tmdate":1512222823579,"tcdate":1511740906904,"number":1,"cdate":1511740906904,"id":"HkmgURdlf","invitation":"ICLR.cc/2018/Conference/-/Paper939/Official_Review","forum":"SyVOjfbRb","replyto":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference/Paper939/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Creative Paper Worth Sharing","rating":"8: Top 50% of accepted papers, clear accept","review":"Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. I found the paper relatively creative and generally well-founded and well-argued.\n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. \n\nExperiments: appreciated the wall clock timings.\n\nSGD comparison: “fixed learning rate.” Didn't see how the initial (well constant here) step size was tuned? Why not use the more standard 1/t decay?\n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective? Legend backwards?\n\nWhy were so many of the chosen datasets have so few training examples?\n\nPaper is mostly very clearly written, though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]}},{"tddate":null,"ddate":null,"tmdate":1510092384694,"tcdate":1509137261557,"number":939,"cdate":1510092362165,"id":"SyVOjfbRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyVOjfbRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION","abstract":"Stochastic Gradient Descent or SGD is the most popular algorithm for large-scale optimization. In SGD, the gradient is estimated by uniform sampling with sample size one. There have been several results that show better gradient estimates, using weighted non-uniform sampling, which leads to faster convergence. Unfortunately, the per-iteration cost of maintaining this adaptive distribution is costlier than the exact gradient computation itself, which create a chicken-and-egg loop making the fast convergence useless. In this paper, we break this chicken-and-egg loop and provide the first demonstration of a sampling scheme, which leads to superior gradient estimation, while keeping the sampling cost per iteration similar to the uniform sampling. Such a scheme is possible due to recent advances in Locality Sensitive Hashing (LSH) literature. As a consequence, we improve the running time of all existing gradient descent algorithms.","pdf":"/pdf/4a6a47622cca390bf07da0e029487cfe84e80f5d.pdf","TL;DR":"We improve the running of all existing gradient descent algorithms.","paperhash":"anonymous|lshsampling_breaks_the_computational_chickenandegg_loop_in_adaptive_stochastic_gradient_estimation","_bibtex":"@article{\n  anonymous2018lsh-sampling,\n  title={LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyVOjfbRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper939/Authors"],"keywords":["Stochastic Gradient Descent","Optimization","Sampling","Estimation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}