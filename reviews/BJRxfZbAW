{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222710273,"tcdate":1511965863385,"number":2,"cdate":1511965863385,"id":"BkJ3NH2lM","invitation":"ICLR.cc/2018/Conference/-/Paper658/Official_Review","forum":"BJRxfZbAW","replyto":"BJRxfZbAW","signatures":["ICLR.cc/2018/Conference/Paper658/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A mixture of Neural Statisticians, lacks quantitative evaluation","rating":"4: Ok but not good enough - rejection","review":"This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185). The generative model defines the process that produces the dataset. This model is first a mixture over contexts followed by i.i.d. generation of the dataset with possibly some unobserved random variable. This corresponds to a mixture of Neural Statisicians. The authors suggest that such a model could help with disentangling factors of variation in data. In the experiments they only consider training the model with the context selection variable and the data variables observed.\n\nUnfortunately there is minimal quantitative evaluation (visualizing 264 MNIST samples is not enough). The only quantitative evaluation is in Table 1, and it seems the model is not able to generalize reliably to all rotations and all digits. Clearly, we can't expect perfect performance, but there are some troubling results: 5.2 accuracy on non-rotated 0s, 0.0 accuracy on non-rotated 6s. Every digit has at least one rotation that is not well classified, so this section could use more discussion and analysis. For example, how would this metric classify VAE samples with contexts corresponding only to digit type (no rotations)? How would this metric classify vanilla VAE samples that are hand labeled? Moreover, the context selection variable \"a\" should be considered part of the dataset, and as such the paper should report how \"a\" was selected.\n\nThis model is a relatively simple extension of the Neural Statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation. I do think the idea is well-motivated, and represents a promising way to incorporate prior knowledge of concepts into our training of VAEs. Still, the paper as it stands is not complete, and I encourage the authors to followup with more thorough quantitative empirical evaluations.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Context-Aware Learner","abstract":"One important aspect of generalization in machine learning involves reasoning about previously seen data in new settings. Such reasoning requires learning disentangled representations of data which are interpretable in isolation, but can also be combined in a new, unseen scenario. To this end, we introduce the context-aware learner, a model based on the variational autoencoding framework, which can learn such representations across data sets exhibiting a number of distinct contexts. Moreover, it is successfully able to combine these representations to generate data not seen at training time. The model enjoys an exponential increase in representational ability for a linear increase in context count. We demonstrate that the theory readily extends to a meta-learning setting such as this, and describe a fully unsupervised model in complete generality. Finally, we validate our approach using an adaptation with weak supervision.","pdf":"/pdf/012015f2dd7c7c2ddda23a8015bfb38251b06480.pdf","paperhash":"anonymous|the_contextaware_learner","_bibtex":"@article{\n  anonymous2018the,\n  title={The Context-Aware Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRxfZbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper658/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222710307,"tcdate":1511820024448,"number":1,"cdate":1511820024448,"id":"BkzesZcxG","invitation":"ICLR.cc/2018/Conference/-/Paper658/Official_Review","forum":"BJRxfZbAW","replyto":"BJRxfZbAW","signatures":["ICLR.cc/2018/Conference/Paper658/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors propose an interesting disentangling version of the neural statistician with richer discrete structures.","rating":"6: Marginally above acceptance threshold","review":"The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features. This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets.\n\nA particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a, which leads to a disentangled representation.\nThis is also not dissimilar to ideas used in 'Bayesian Representation Learning With Oracle Constraints' Karaletsos et al 2016 where similar contextual features c are learned to disentangle representations over observations and implicit supervision.\n\nThe authors provide a clean variational inference algorithm to learn their model. However, a key problem is the following: the nature of the discrete variables being used makes them hard to be inferred with variational inference. The authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach. In fact, it would be interesting to study which level of these variables could be analytically collapsed (such as done in the Semi-Supervised learning work by Kingma et al 2014) and which ones can be sampled effectively using a form of reparametrization.\n\nThis also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived,  the empirical evaluation is on the weak side and the rich properties of the model are not really shown off. It would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy settings.\nCan the model recover richer structure that was imposed during data generation? How limiting is the learning of a?\nHow does the likelihood of the model behave under the circumstances?\nThe experiments do not really convey how well this all will work in practice.\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The Context-Aware Learner","abstract":"One important aspect of generalization in machine learning involves reasoning about previously seen data in new settings. Such reasoning requires learning disentangled representations of data which are interpretable in isolation, but can also be combined in a new, unseen scenario. To this end, we introduce the context-aware learner, a model based on the variational autoencoding framework, which can learn such representations across data sets exhibiting a number of distinct contexts. Moreover, it is successfully able to combine these representations to generate data not seen at training time. The model enjoys an exponential increase in representational ability for a linear increase in context count. We demonstrate that the theory readily extends to a meta-learning setting such as this, and describe a fully unsupervised model in complete generality. Finally, we validate our approach using an adaptation with weak supervision.","pdf":"/pdf/012015f2dd7c7c2ddda23a8015bfb38251b06480.pdf","paperhash":"anonymous|the_contextaware_learner","_bibtex":"@article{\n  anonymous2018the,\n  title={The Context-Aware Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRxfZbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper658/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739176436,"tcdate":1509130742112,"number":658,"cdate":1509739173765,"id":"BJRxfZbAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJRxfZbAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The Context-Aware Learner","abstract":"One important aspect of generalization in machine learning involves reasoning about previously seen data in new settings. Such reasoning requires learning disentangled representations of data which are interpretable in isolation, but can also be combined in a new, unseen scenario. To this end, we introduce the context-aware learner, a model based on the variational autoencoding framework, which can learn such representations across data sets exhibiting a number of distinct contexts. Moreover, it is successfully able to combine these representations to generate data not seen at training time. The model enjoys an exponential increase in representational ability for a linear increase in context count. We demonstrate that the theory readily extends to a meta-learning setting such as this, and describe a fully unsupervised model in complete generality. Finally, we validate our approach using an adaptation with weak supervision.","pdf":"/pdf/012015f2dd7c7c2ddda23a8015bfb38251b06480.pdf","paperhash":"anonymous|the_contextaware_learner","_bibtex":"@article{\n  anonymous2018the,\n  title={The Context-Aware Learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRxfZbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper658/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}