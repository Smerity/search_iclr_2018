{"notes":[{"tddate":null,"ddate":null,"tmdate":1515004933805,"tcdate":1515004933805,"number":4,"cdate":1515004933805,"id":"SJ0-Es9XG","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Comment","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper158/Authors"],"content":{"title":"paper update","comment":"We have updated the paper with the following:\n- added few sentences on the complexity of action space being shifted into the MDP\n- added equation for DDPG update\n- added more related work from Pazis and Lagoudakis\n- added action space dimensionality for each environment in experiments\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515004749836,"tcdate":1515004749836,"number":3,"cdate":1515004749836,"id":"rkI8QoqXz","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Comment","forum":"r1SuFjkRW","replyto":"H13MV5tkM","signatures":["ICLR.cc/2018/Conference/Paper158/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper158/Authors"],"content":{"title":"rebutal","comment":"Thank you for your thoughtful review.\n\nWe were not actually aware of Pazis and Lagoudakis's work on this subject (we did cite them, but for another one of there papers, not the paper we believe you are referencing.). We have updated the text to include a section on this work. As per the differences, we are using neural network function approximators. Naively applying this decomposition increases the time dependences in the MDP, as such when using function approximators error accumulates. While attempting to train like this does work, it is incredibly unstable and hyperparameter sensitive. Our second contribution is thus a modified way of training these networks by training the hierarchy of MDP together -- using the upper to bootstrap the lower. This, unlike the original PL-like algorithm, is much more stable as it reduces function overestimation approximator error. With these improvements we are able to train on more complex tasks than originally explored.\n\nAs per your question on action ordering: we have an experiment (section 4.4). We found on that problem at least that there was little to no change in performance given different action orders.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515004697973,"tcdate":1515004697973,"number":2,"cdate":1515004697973,"id":"S1fQmo5mz","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Comment","forum":"r1SuFjkRW","replyto":"rkaH5MsgM","signatures":["ICLR.cc/2018/Conference/Paper158/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper158/Authors"],"content":{"title":"rebuttal","comment":"Thank you for your thoughtful review. We will try to address your concerns, as follows:\n\n# Clarity\nWe agreed on both fronts, and we have updated the text. \n\n# Quality\n2. Huge action dimensions would be an interesting application but are outside the scope of our focus: continuous control for robotics tasks. Theoretically, our algorithm scales linearly in terms of compute but exponentially in terms of the MDP (although in practice there is a lot of independence between actions which again makes it closer to linear scaling). We would expect that as N grows, learning the lower MDP will become harder and harder due to the increased temporal dependencies. For very large N, we would almost surely expect that one would need a logarithmic hierarchy or a technique similar to [1].\n3. As a baseline while developing this work (not included in this paper), we used algorithms that were not sequence based. This algorithm predicted Q values independently for each action dimension. While the algorithm worked, the lack of action-to-action conditioning greatly restricted the functional form of our model and resulted in sub-par performance.\nThe sequence version allows these previously missing action-to-action interactions while keeping maxation tractable. By putting action dimensions in a sequence, we are able to easily condition results on the previous action dimensions. The fact that ordering does not matter is a good thing for us and allows this technique to work! It is possible to construct some set based interaction scheme that has a similar ability to preserve conditioning, but we are not aware of any such constructions that support explicit maxation while retaining this interaction action conditioning.\n\n# Significance\nWe do not see training multiple networks as a limitation as long as sample complexity does not suffer (as we have shown in regard to DDPG). In the robotics settings, the compute cost is often much, much smaller in comparison to the hardware / robot cost. The run time is no different than say running a single RNN over the action dimensions, and in terms of memory, these models are also quite small ~ order 0.1 - 1Mb per action dimension depending on network sizes. Additionally, not all of the components need to be separate. In a tasks involving vision, for example, one could use a common feature extractor.\n\n[1]Dulac-Arnold, Gabriel, et al. \"Deep reinforcement learning in large discrete action spaces.\" arXiv preprint arXiv:1512.07679(2015).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515004354120,"tcdate":1515004354120,"number":1,"cdate":1515004354120,"id":"S15a-iqQG","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Comment","forum":"r1SuFjkRW","replyto":"S1m6NXjlM","signatures":["ICLR.cc/2018/Conference/Paper158/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper158/Authors"],"content":{"title":"rebuttal","comment":"Thank you for your thoughtful review. We will try to address your concerns bellow:\n\n# Hierarchy\nWe agree that, in theory, the lower MDP should be sufficient. In practice, as we pointed out in the paper, Q learning with (deep) function approximators is unstable and sensitive to hyperparameters. To our knowledge, this phenomenon is not thoroughly understood. There have been many papers, however, describing potential failure points, proposing a solution, and showing improvement however -- examples include Double DQN [1], Dueling networks [2], all improvements from rainbow networks [3], and many more. We see our 2-layer training in a similar vein to these works. In particular, the training procedure described here is closely related to Double DQN in theory and implementation.\nThe issue we seek to address is the failure in the \"Bellman backup\" through time due to repeated function approximator error. When working with long MDP, learning associations between states and action sequences have been shown to be hard [4]. [4] shows that this effect is so impactful that by lowering the control frequency (increasing the frame-skip count) actually increased performance in some tasks. Additionally, in the policy gradient algorithms, increasing the frequency of states has been shown to increase gradient variance with the number of timesteps [5].\nInitially we explored just the lower MDP and achieved reasonable performance, but the resulting algorithm was incredibly sensitive to hyperparameter and quite unstable, partially due to Q value overestimation. Our hierarchy is a way to address this instability. It does so in a similar manner to that employed in Double DQN; the use of two networks to combat overestimation. Still, solving the root instability of Q-learning with function approximators is an open question and something that interests us greatly.\n\n# Exponential problems\nThank you for this observation. This is true and it is worth calling more attention to it, which we have done in the text (now updated). The exponential action space does turn into a exponential MDP. Luckily for us though, many problems do not actually require full search of this exponential space. Early in this work, we hypothesized that the space was largely independent between action dimensions. This fact is often exploited in policy gradient approaches as the policy distribution is often parameterized as a diagonal covariance normal[6, 7]. We tested this independence hypothesis in the Q learning settings (using a novel Q learning algorithm, not included in this paper) where the Q values were the sum of terms computed from each action dimension independently) and found that we were able achieve reasonable performance, though not state of the art. In general, we don't expect to be able to search the full exponential space. Early in training the interactions will mostly be linear / independent due to the nature of these neural networks at initialization. As training progresses, we do expect to be able to capture some interaction relationships. In our experiments, adding in this conditioning does increase performance of the final algorithm.\n\n\n[1] Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep Reinforcement Learning with Double Q-Learning.\" AAAI. 2016.\n[2] Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" arXiv preprint arXiv:1511.06581(2015).\n[3] Hessel, Matteo, et al. \"Rainbow: Combining Improvements in Deep Reinforcement Learning.\" arXiv preprint arXiv:1710.02298 (2017).\n[4] Braylan, Alex, et al. \"Frame skip is a powerful parameter for learning to play atari.\" Space 1600 (2000): 1800.\n[5] Salimans, Tim, et al. \"Evolution strategies as a scalable alternative to reinforcement learning.\" arXiv preprint arXiv:1703.03864 (2017).\n[6]Schulman, John, et al. \"Trust region policy optimization.\" Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 2015.\n[7]Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International Conference on Machine Learning. 2016.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642400325,"tcdate":1511892154920,"number":3,"cdate":1511892154920,"id":"S1m6NXjlM","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Review","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"The paper describes a new RL technique for high dimensional action spaces.  It discretizes each dimension of the action space, but to avoid an exponential blowup, it selects the action for each dimension in sequence.  This is an interesting approach.  The paper reformulates the MDP with a high dimensional action space into an equivalent MDP with more time steps (one per dimension) that each selects the action in one dimension.  This makes sense.\n\nWhile I do like very much the model, I am perplex about the training technique.  The lower MDP is precisely the new proposed model with unidimensional actions and therefore it should be sufficient.  However, the paper also describes an upper MDP that seems to be superfluous.  The two MDPs are mathematically equivalent, but their Q-values are obtained differently (TD-0 for the upper MDP and Q-learning for the lower MDP) and yet the paper tries to minimize the Euclidean distance between them.  This is really puzzling since the different training algorithms suggest that the Q-values should be different while minimizing the Euclidean distance between them tries to make them equal.  The paper suggests that divergence occurs without the upper MDP.  This is really suspicious. The approach feels like a band-aid solution to cover a problem that the authors could not identify.  While the empirical results are good, I don't think the paper should be published until the authors figure out a principled way of training.\n\nThe proposed approach reformulates the MDP with high dimensional actions into an equivalent one with uni dimensional actions.  There is a catch.  This approach effectively hides the exponential action space into the state space which becomes exponential.  Since u contains all the actions of the previous dimensions, we are effectively increasing the state space by an exponential factor.  The paper should discuss this and explain what are the consequences in practice.  In the end, the MDP does not become simpler.\n\nOverall, this is an interesting paper with a good idea, but the training technique is not mature enough for publication.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642400369,"tcdate":1511889477514,"number":2,"cdate":1511889477514,"id":"rkaH5MsgM","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Review","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A useful method to handle continuous action spaces; however comes with additional cost of training as many networks as the number of actions","rating":"5: Marginally below acceptance threshold","review":"Originality\n--------------\nWhen the action space is N-dimensional, computing argmax could be problematic. The paper proposes to address the problem by creating N MDPs with 1-D actions. \n\nClarity\n---------\n1) Explicitly writing down DDPG will be helpful\n2) The number of actions in each of the domains will also be useful\n\nQuality\n----------\n1) The paper reports experimental results on order of actions as well as binning, and the results confirm with what one would expect from intuition. \n2) It will be important to talk about the case when the action dimension N is very large, what happens in that case? Does the proposed method would work in such a scenario? A discussion is needed.\n3) Given that the ordering of actions does not matter, what is the real take away of looking at them as 'sequence' (which has not temporal structure because action order could be arbitrary)?\n\n\nSignificance\n----------------\nWhile the proposed method seems a reasonable approach to handle the argmax problem, it still requires training multiple networks for Q^i (i=1,..N) for Q^L, which is a limitation. Further, since the actions could be arbitrary, it is unclear where 'sequence' approach helps. These limit the understand and hence significance.\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642400410,"tcdate":1510741011803,"number":1,"cdate":1510741011803,"id":"H13MV5tkM","invitation":"ICLR.cc/2018/Conference/-/Paper158/Official_Review","forum":"r1SuFjkRW","replyto":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference/Paper158/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper presents a correct, reasonably effective, but not groundbreaking approach to Q value approximation in MDPs with high-dimensional action spaces","rating":"7: Good paper, accept","review":"The paper presents Sequential Deep Q-Networks (SDQNs), which select actions from discretized high-dimensional action spaces.  This is done by introducing another, undiscounted MDP in which each action dimension is chosen sequentially by an agent.  By training a Q network to best choose these action dimensions, and loosely enforcing equality between the original and new MDPs at points where they are equivalent, the new MDP can be successfully navigated, resulting in good action selection for the original MDP.  This is experimentally compared against DDPG in several domains.  There are no theoretical results.\n\nThis work is correct and clearly written.  Experiments do demonstrate improved effectiveness in the chosen domains, and the authors do a nice job of illustrating the range of performance by their approach (which has low variance in some domains, but high variance in others).  Because of the clarity of the paper, the effectiveness of the approach, and the high quality experiments, I encourage acceptance.\n\nIt doesn't strike me as world-changing, however.  The MDP-within-an-MDP approach is quite similar to the Pazis and Lagoudakis MDP decomposition for the same problem (work which is appropriately cited, but maybe too briefly compared against).  In other words, it strikes me as merely being P&L plus networks, dampening my enthusiasm.\n\nMy one question for the authors is how much the order of action dimension selection matters.  This seems probably quite important practically, but is undiscussed.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515004847616,"tcdate":1509042541346,"number":158,"cdate":1509739450544,"id":"r1SuFjkRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1SuFjkRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discrete Sequential Prediction of Continuous Actions for Deep RL","abstract":"It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.","pdf":"/pdf/2657e56090af97cd72a725e25af9a509cee17281.pdf","TL;DR":"A method to do Q-learning on continuous action spaces by predicting a sequence of discretized 1-D actions.","paperhash":"anonymous|discrete_sequential_prediction_of_continuous_actions_for_deep_rl","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Sequential Prediction of Continuous Actions for Deep RL},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SuFjkRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper158/Authors"],"keywords":["Reinforcement learning","continuous control","deep learning"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}