{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222709709,"tcdate":1511882878455,"number":3,"cdate":1511882878455,"id":"Bk8FeZjgf","invitation":"ICLR.cc/2018/Conference/-/Paper655/Official_Review","forum":"B1Z3W-b0W","replyto":"B1Z3W-b0W","signatures":["ICLR.cc/2018/Conference/Paper655/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Learning-to-learn is applied to the inference step in VAE models to improve speed and accuracy","rating":"5: Marginally below acceptance threshold","review":"Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two.  In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients.  The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate.\n\nAlthough probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context.  From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016).  Indeed the paper perhaps anticipates this perspective and preemptively offers that \"variational inference is a qualitatively different optimization problem\" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work.  But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient.  That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients.\n\nBeyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE.  While these results are enlightening, most of the conclusions are not entirely unexpected.  For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result.  It would certainly seem strange if this were not the case.  And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance.  Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016).\n\nIn terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison?  For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration.  In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained.\n\n\nOther minor comment:\n* In Fig. 5(a), it seems like the performance of the standard inference model is still improving but the iterative inference model has mostly saturated.\n* A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer","abstract":"Inference models, which replace an optimization-based inference procedure with a learned model, have been fundamental in advancing Bayesian deep learning, the most notable example being variational auto-encoders (VAEs). In this paper, we propose \\textit{iterative inference models}, which learn how to optimize a variational lower bound through repeatedly encoding gradients. Our approach generalizes VAEs under certain conditions, and by viewing VAEs in the context of iterative inference, we provide further insight into several recent empirical findings. We demonstrate the inference optimization capabilities of iterative inference models, explore unique aspects of these models, and show that they outperform standard inference models on typical benchmark data sets.","pdf":"/pdf/ee8052f814c1bab929dfb7e1f3ecbcb58810b19d.pdf","TL;DR":"We propose a new class of inference models that iteratively encode gradients to estimate approximate posterior distributions.","paperhash":"anonymous|learning_to_infer","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Z3W-b0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper655/Authors"],"keywords":["Bayesian Deep Learning","Amortized Inference","Variational Auto-Encoders","Learning to Learn"]}},{"tddate":null,"ddate":null,"tmdate":1512222709749,"tcdate":1511830531605,"number":2,"cdate":1511830531605,"id":"Sy3-NV9xG","invitation":"ICLR.cc/2018/Conference/-/Paper655/Official_Review","forum":"B1Z3W-b0W","replyto":"B1Z3W-b0W","signatures":["ICLR.cc/2018/Conference/Paper655/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A nice application of learning-to-learn to address some limitations of purely feedforward amortized inference in VAEs.","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution. The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm.\n\nThis approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family). This source of error is often ignored in the literature, although there are some exceptions that may be worth mentioning:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class).\n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation.\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs. They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables.\n\nI think this is good and potentially important work, although I do have some questions/concerns about the results in Table 1 (see below). \n\n\nSome more specific comments:\n\nFigure 2: I think this might be clearer if you unrolled a couple of iterations in (a) and (c).\n\n(Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset. A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm.\n\nSection 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn’t quite match the formulation of Rezende&Mohamed (2014). First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance. Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1. An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter.\n\nWhy not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}?\n\nTable 1: These results are strange in a few ways:\n* The gap between the standard and iterative inference network seems very small (0.3 nats at most). This is much smaller than the gap in Figure 5(a).\n* The MNIST results are suspiciously good overall, given that it’s ultimately a Gaussian approximation and simple fully connected architecture. I’ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don’t think I’ve ever seen a number better than ~87 nats.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer","abstract":"Inference models, which replace an optimization-based inference procedure with a learned model, have been fundamental in advancing Bayesian deep learning, the most notable example being variational auto-encoders (VAEs). In this paper, we propose \\textit{iterative inference models}, which learn how to optimize a variational lower bound through repeatedly encoding gradients. Our approach generalizes VAEs under certain conditions, and by viewing VAEs in the context of iterative inference, we provide further insight into several recent empirical findings. We demonstrate the inference optimization capabilities of iterative inference models, explore unique aspects of these models, and show that they outperform standard inference models on typical benchmark data sets.","pdf":"/pdf/ee8052f814c1bab929dfb7e1f3ecbcb58810b19d.pdf","TL;DR":"We propose a new class of inference models that iteratively encode gradients to estimate approximate posterior distributions.","paperhash":"anonymous|learning_to_infer","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Z3W-b0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper655/Authors"],"keywords":["Bayesian Deep Learning","Amortized Inference","Variational Auto-Encoders","Learning to Learn"]}},{"tddate":null,"ddate":null,"tmdate":1512222709786,"tcdate":1511776576011,"number":1,"cdate":1511776576011,"id":"rJuH-vKeG","invitation":"ICLR.cc/2018/Conference/-/Paper655/Official_Review","forum":"B1Z3W-b0W","replyto":"B1Z3W-b0W","signatures":["ICLR.cc/2018/Conference/Paper655/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting approach, but the advantages of the approach are unclear ","rating":"5: Marginally below acceptance threshold","review":"This paper proposes an iterative inference scheme for latent variable models that use inference networks. Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al. The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network).\n\nMy main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach. It appears that the iterative method should result in \"direct improvement with additional samples and inference iterations\". I am supposing this is at the test time. It is not clear exactly when this will be useful. \n\nI believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al. 2014 (they used this method to perform data imputation).\n\nThe paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE.\n\nIn summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to Infer","abstract":"Inference models, which replace an optimization-based inference procedure with a learned model, have been fundamental in advancing Bayesian deep learning, the most notable example being variational auto-encoders (VAEs). In this paper, we propose \\textit{iterative inference models}, which learn how to optimize a variational lower bound through repeatedly encoding gradients. Our approach generalizes VAEs under certain conditions, and by viewing VAEs in the context of iterative inference, we provide further insight into several recent empirical findings. We demonstrate the inference optimization capabilities of iterative inference models, explore unique aspects of these models, and show that they outperform standard inference models on typical benchmark data sets.","pdf":"/pdf/ee8052f814c1bab929dfb7e1f3ecbcb58810b19d.pdf","TL;DR":"We propose a new class of inference models that iteratively encode gradients to estimate approximate posterior distributions.","paperhash":"anonymous|learning_to_infer","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Z3W-b0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper655/Authors"],"keywords":["Bayesian Deep Learning","Amortized Inference","Variational Auto-Encoders","Learning to Learn"]}},{"tddate":null,"ddate":null,"tmdate":1509739178096,"tcdate":1509130664799,"number":655,"cdate":1509739175441,"id":"B1Z3W-b0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1Z3W-b0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to Infer","abstract":"Inference models, which replace an optimization-based inference procedure with a learned model, have been fundamental in advancing Bayesian deep learning, the most notable example being variational auto-encoders (VAEs). In this paper, we propose \\textit{iterative inference models}, which learn how to optimize a variational lower bound through repeatedly encoding gradients. Our approach generalizes VAEs under certain conditions, and by viewing VAEs in the context of iterative inference, we provide further insight into several recent empirical findings. We demonstrate the inference optimization capabilities of iterative inference models, explore unique aspects of these models, and show that they outperform standard inference models on typical benchmark data sets.","pdf":"/pdf/ee8052f814c1bab929dfb7e1f3ecbcb58810b19d.pdf","TL;DR":"We propose a new class of inference models that iteratively encode gradients to estimate approximate posterior distributions.","paperhash":"anonymous|learning_to_infer","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to Infer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Z3W-b0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper655/Authors"],"keywords":["Bayesian Deep Learning","Amortized Inference","Variational Auto-Encoders","Learning to Learn"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}