{"notes":[{"tddate":null,"ddate":null,"tmdate":1516025030822,"tcdate":1516025030822,"number":9,"cdate":1516025030822,"id":"rk10EE5Vf","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Generalization analysis","comment":"A generalization analysis, along with a curve showing precision vs warp range, with warps longer than those seen during training was added to the appendix."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516024962125,"tcdate":1516024962125,"number":8,"cdate":1516024962125,"id":"HyqtEE54z","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"Hyb2BDI4G","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Added analysis in the supplementary","comment":"The generalization analysis, along with a curve showing precision vs warp range, with warps longer than those seen during training was added. We hope this is what you had in mind."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515775401495,"tcdate":1515775401495,"number":7,"cdate":1515775401495,"id":"Hyb2BDI4G","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"ry_xKKQEM","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/AnonReviewer2"],"content":{"title":"Reply","comment":"Thanks for the answers. These results are very interesting and important. \n\nI urge the authors to include this extended analysis with any other discussions (of plots etc.) in the supplementary material. Since they have already done these experiments, it will be helpful for any future readers.\n\nI am willing to upgrade my rating if the authors add this analysis.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515587824465,"tcdate":1515587824465,"number":6,"cdate":1515587824465,"id":"ry_xKKQEM","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"S12wnhz4f","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Answers","comment":"Losses are computed on the evaluation set for all setups. We chose not to plot both the train and evaluation curves as both exhibit very similar trends. The difference between the architectures is not an effect of overfitting.\n\nFor generalizing to longer warpings than those of the training set, all networks display reasonably good (but not perfect) generalization. Even with warps 10 times longer than the training set warps, the networks still have decent accuracy, decreasing from 100% to around 75%. We tested this today, by training the architectures with max_warp=50, and evaluating on sequences with warps from 100 to 500.\n\nInterestingly, plain RNNs and gated RNNs display a different pattern: overall, gated RNNs perform better, but their generalization performance decreases faster with warps 8x-10x max_warp, while plain RNN never have perfect accuracy (always below 80% even within the training set range) but have a flatter performance when going beyond the training set max_warp; the two curves cross at about 9x max_warp, at 75%-80% accuracy."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515535459587,"tcdate":1515535459587,"number":5,"cdate":1515535459587,"id":"S12wnhz4f","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"HyuqsxtXG","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/AnonReviewer2"],"content":{"title":"Questions","comment":"I'm happy to see the added experiments. One question: In Fig. 1, are the reported losses computed on the test set? In any case, I think it would be interesting to include results for both training and test losses for these experiments.\n\nAnother question: what happens if the networks are tested on a test set with maximum_warping higher than that used during training?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515018969833,"tcdate":1515018969833,"number":4,"cdate":1515018969833,"id":"HyGJjR9QG","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Addition of experiments validating the core hypothesis","comment":"In response to the reviewers insightful comments, two experiments precisely testing the invariance properties of simple recurrent, leaky and gated networks have been added, which validate the theoretical claim."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1514896532290,"tcdate":1514896479450,"number":3,"cdate":1514896479450,"id":"ryPwnxFXf","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"SkrzwsKeG","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Answer to reviewer 3","comment":"Thank you for your insightful review.  The lack of an experiment to test the main theoretical result of the paper was indeed a major flaw of the original version. We have updated the paper accordingly, by adding experiments with pure warpings/pure paddings to the main track of the paper (pages 6-8), and moving less central experiments to the appendix. We hope this experiment is more or less what you had in mind. The results are in line with the theoretical derivations: plain RNNs cannot account for warpings, leaky RNNs can\naccount for uniform time scalings but not irregular warpings, and gated RNNs can adapt to irregular warpings."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1514896366302,"tcdate":1514896366302,"number":2,"cdate":1514896366302,"id":"HJUl3etXf","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"HkIzPXqxM","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Answer to Reviewer 2","comment":"Thank you for your constructive comments! As for larger-scale\nexperiments, we had to make choices and decided to focus on the setup suggested by Reviewer 3, namely, to center the experiments specifically on the theoretical claims of Section 1. This is now included in the text, while MNIST, pMNIST and next-step prediction have been moved to the appendix instead. We are aware this is not what you suggested, but as pointed out by Reviewer 3, we think this is more in line with the central claim of this work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1514896272537,"tcdate":1514896272537,"number":1,"cdate":1514896272537,"id":"HyuqsxtXG","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Comment","forum":"SJcKhk-Ab","replyto":"Sk2_qmcxf","signatures":["ICLR.cc/2018/Conference/Paper519/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper519/Authors"],"content":{"title":"Answer to Reviewer 1","comment":"Thank you for the constructive review of our paper. We've noted your\nremarks on the limited benefits of the method for next-step prediction,\nalong with those of reviewer 3. Consequently, we have revised the paper to include an experiment on pure warping/padding that specifically tests the main theoretical claim made in the paper. Accordingly, MNIST, pMNIST and next-step prediction have been moved to the appendix instead."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642460053,"tcdate":1511828083733,"number":3,"cdate":1511828083733,"id":"Sk2_qmcxf","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Review","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A simple and important insight about LSTMs","rating":"7: Good paper, accept","review":"Summary:\nThis paper shows that incorporating invariance to time transformations in recurrent networks naturally results in a gating mechanism used by LSTMs and their variants. This is then used to develop a simple bias initialization scheme for the gates when the range of temporal dependencies relevant for a problem can be estimated or are known. Experiments demonstrate that the proposed initialization speeds up learning on synthetic tasks, although benefits for next-step prediction tasks are limited.\n\nQuality and significance:\nThe core insight of the paper is the link between recurrent network design and its effect on how the network reacts to time transformations. This insight is simple, elegant and valuable in my opinion. \n\nIt is becoming increasingly apparent recently that the benefits of the gating and cell mechanisms introduced by the LSTM, now also used in feedforward networks, go beyond avoiding vanishing gradients. The particular structural elements also induce certain inductive biases which make learning or generalization easier in many cases. Understanding the link between model architecture and behavior is very useful for the field in general, and this paper contributes to this knowledge. In light of this, I think it is reasonable to ignore the fact that the proposed initialization does not provide benefits on Penn Treebank and text8. The real value of the paper is in providing an alternative way of thinking about LSTMs that is theoretically sound and intuitive. \n\nClarity:\nThe paper is well-written in general and easy to understand. A minor complaint is that there are an unnecessarily large number of paragraph breaks, especially on pages 3 and 4, which make reading slightly jarring.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515642460093,"tcdate":1511827213935,"number":2,"cdate":1511827213935,"id":"HkIzPXqxM","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Review","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting development to gated RNN","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper provides an interesting theoretical explanation on why gated RNN architectures such as LSTM and GRU work well in practice. The paper shows how \"gate values appear as time contraction or time dilation coefficients\". The authors also point out the connection between the gate biases and the range of time dependencies captured in the network. From that, they develop a simple yet effective initialization method which performs well on different datasets.\n\nPros:\n- The idea is interesting, it well explain the success of gated RNNs.\n- Writing: The paper is well written and easy to read. \n\nCons:\nExperiments: only small datasets were used in the experiments, it would be more convincing if the author could use larger datasets. One suggestion to make the experiment more complete is to gradually increase the initial value of the biases to see how it affect the performance. To use 'chrono initialization', one need to estimate the range of time dependency which could be difficult in practice. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1515779588168,"tcdate":1511794445283,"number":1,"cdate":1511794445283,"id":"SkrzwsKeG","invitation":"ICLR.cc/2018/Conference/-/Paper519/Official_Review","forum":"SJcKhk-Ab","replyto":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference/Paper519/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Cool theoretical contribution with rather unrelated experiments","rating":"8: Top 50% of accepted papers, clear accept","review":"tl;dr: \n - The paper has a really cool theoretical contribution. \n - The experiments do not directly test whether the theoretical insight holds in practice, but instead a derivate method is tested on various benchmarks. \n\nI must say that this paper has cleared up quite a few things for me. I have always been a skeptic wrt LSTM, since I myself did not fully understand when to prefer them over vanilla RNNs for reasons other than “they empirically work much better in many domains.” and “they are less prone to vanishing gradients”. \n\nSection 1 is a bliss: it provides a very useful candidate explanation under which conditions vanilla RNNs fail (or at least, do not efficiently generalise) in contrast to gated cells. I am sincerely happy about the write up and will point many people to it.\n\nThe major problem with the paper, in my eyes, is the lack of experiments specific to test the hypothesis. Obviously, quite a bit of effort has gone into the experimental section. The focus however is comparison to the state of the art in terms of raw performance.  \n\nThat leaves me asking: are gated RNNs superior to vanilla RNNs if the data is warped?\nWell, I don’t know now. I only can say that there is reason to believe so. \n\nI *really* do encourage the authors to go back to the experiments and see if they can come up with an experiment to test the main hypothesis of the paper. E.g. one could make synthetic warpings, apply it to any data set and test if things work out as expected. Such a result would in my opinion be of much more use than the tiny increment in performance that is the main output of the paper as of now, and which will be stomped by some other trick in the months to come. It would be a shame if such a nice theoretical insight got under the carpet because of that. E.g. today we hold [Pascanu 2013] dear not because of the proposed method, but because of the theoretical analysis.\n\nSome minor points.\n- The authors could make use of less footnotes, and try to incorporate them into the text or appendix.\n- A table of results would be nice.\n- Some choices of the experimental section seem arbitrary, e.g. the use of optimiser and to not use clipping of gradients. In general, the evaluation of the hyper parameters is not rigorous.\n- “abruplty” -> “abruptly” on page 5, 2nd paragraph\n\n### References\n[Pascanu 2013] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. \"On the difficulty of training recurrent neural networks.\" International Conference on Machine Learning. 2013.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1516024678703,"tcdate":1509125250112,"number":519,"cdate":1509739255776,"id":"SJcKhk-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJcKhk-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Can recurrent neural networks warp time?","abstract":"Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use \\emph{ad hoc} gating mechanisms.  Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.\n\t\nWe prove that learnable gates in a recurrent model formally provide \\emph{quasi-invariance to general time transformations} in the input data. We recover part of the LSTM architecture from a simple axiomatic approach.\n\t\nThis result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new \\emph{chrono initialization} is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n","pdf":"/pdf/9b623cf5f927971858fb6181a94815a5e58d131a.pdf","TL;DR":"Proves that gating mechanisms provide invariance to time transformations. Introduces and tests a new initialization for LSTMs from this insight.","paperhash":"anonymous|can_recurrent_neural_networks_warp_time","_bibtex":"@article{\n  anonymous2018can,\n  title={Can recurrent neural networks warp time?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJcKhk-Ab}\n}","keywords":["RNN"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper519/Authors"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}