{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222577528,"tcdate":1511813384183,"number":3,"cdate":1511813384183,"id":"B1xzWeqgG","invitation":"ICLR.cc/2018/Conference/-/Paper153/Official_Review","forum":"B1tExikAW","replyto":"B1tExikAW","signatures":["ICLR.cc/2018/Conference/Paper153/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Too simplistic scenario","rating":"4: Ok but not good enough - rejection","review":"This paper is concerned with both security and machine learning. \nAssuming that data is encoded, transmited, and decoded using a VAE,\nthe paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified.\nThe objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input; 2) make minimal change in the middle so that the attack is not detectable. \n\nThis paper is concerned with both security and machine learning, but there is no clear contributions to either field. From the machine learning perspective, the proposed \"attacking\" method is standard without any technical novelty. From the security perspective, the scenarios are too simplistic. The encoding-decoding mechanism being attacked is too simple without any security enhancement. This is an unrealistic scenario. For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them. Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community. \n\nI am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas. For example, what are the necessary and sufficient conditions for an attacking method to be undetectable? Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient? Is there any reference for them? Why do we require the correspondence between the classification confidence of tranformed and original data? Would it be enough to match the DISTRIBUTION of the confidence? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LatentPoison -- Adversarial Attacks On The Latent Space","abstract":"Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.","pdf":"/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf","TL;DR":"Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs","paperhash":"anonymous|latentpoison_adversarial_attacks_on_the_latent_space","_bibtex":"@article{\n  anonymous2018latentpoison,\n  title={LatentPoison -- Adversarial Attacks On The Latent Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1tExikAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper153/Authors"],"keywords":["adversarial attacks","security","auto-encoder"]}},{"tddate":null,"ddate":null,"tmdate":1512222577567,"tcdate":1511798902129,"number":2,"cdate":1511798902129,"id":"HkCuu2YxG","invitation":"ICLR.cc/2018/Conference/-/Paper153/Official_Review","forum":"B1tExikAW","replyto":"B1tExikAW","signatures":["ICLR.cc/2018/Conference/Paper153/AnonReviewer1"],"readers":["everyone"],"content":{"title":"VAE are not a compression scheme","rating":"3: Clear rejection","review":"This paper misses the point of what VAEs (or GANs, in general) are used for. The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples. The use of these techniques for compressing is still unclear and their quality today is too low. So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense. \n\nBut letâ€™s assume that at some point they can be used as the authors propose. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless? I think this will make the first attack useless. \n\nThe other two attacks require that the foe is inserted in the middle of the training of the VAE. This is even less doable, because the encoder and decoder are not train remotely. They are train of the same machine or cluster in a controlled manner by the person that would use the system. Once it is train it will give away the decoder and keep the encoder for sending information.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LatentPoison -- Adversarial Attacks On The Latent Space","abstract":"Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.","pdf":"/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf","TL;DR":"Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs","paperhash":"anonymous|latentpoison_adversarial_attacks_on_the_latent_space","_bibtex":"@article{\n  anonymous2018latentpoison,\n  title={LatentPoison -- Adversarial Attacks On The Latent Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1tExikAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper153/Authors"],"keywords":["adversarial attacks","security","auto-encoder"]}},{"tddate":null,"ddate":null,"tmdate":1512222577638,"tcdate":1511771902361,"number":1,"cdate":1511771902361,"id":"H1I-1LYxf","invitation":"ICLR.cc/2018/Conference/-/Paper153/Official_Review","forum":"B1tExikAW","replyto":"B1tExikAW","signatures":["ICLR.cc/2018/Conference/Paper153/AnonReviewer2"],"readers":["everyone"],"content":{"title":"LatentPoison -- Adversarial Attacks On The Latent Space","rating":"5: Marginally below acceptance threshold","review":"The idea is clearly stated (but lacks some details) and I enjoyed reading the paper. \n\nI understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better. From the adversary's standpoint, it would be easier to manipulate inputs than latent variables. On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation.\n\nIn Section 3.1, the attack methods #2 and #3 should be detailed more. I could not imagine how VAE and T are trained simultaneously.\n\nIn Section 3.2, the authors listed a couple of loss functions. How were these loss functions are combined? The final optimization problem that is used for training of the propose VAE should be formally defined. Also, the detailed specification of the VAE should be detailed.\n\nFrom figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner, however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines. For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks? Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LatentPoison -- Adversarial Attacks On The Latent Space","abstract":"Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.","pdf":"/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf","TL;DR":"Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs","paperhash":"anonymous|latentpoison_adversarial_attacks_on_the_latent_space","_bibtex":"@article{\n  anonymous2018latentpoison,\n  title={LatentPoison -- Adversarial Attacks On The Latent Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1tExikAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper153/Authors"],"keywords":["adversarial attacks","security","auto-encoder"]}},{"tddate":null,"ddate":null,"tmdate":1509739457612,"tcdate":1509040176646,"number":153,"cdate":1509739454954,"id":"B1tExikAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1tExikAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LatentPoison -- Adversarial Attacks On The Latent Space","abstract":"Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.","pdf":"/pdf/78593296e77084bcc2bfd794c91ee48c76bd59e3.pdf","TL;DR":"Adversarial attacks on the latent space of variational autoencoders to change the semantic meaning of inputs","paperhash":"anonymous|latentpoison_adversarial_attacks_on_the_latent_space","_bibtex":"@article{\n  anonymous2018latentpoison,\n  title={LatentPoison -- Adversarial Attacks On The Latent Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1tExikAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper153/Authors"],"keywords":["adversarial attacks","security","auto-encoder"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}