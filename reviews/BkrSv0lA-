{"notes":[{"tddate":null,"ddate":null,"tmdate":1513744372595,"tcdate":1513744372595,"number":3,"cdate":1513744372595,"id":"rkaeuvwGM","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Comment","forum":"BkrSv0lA-","replyto":"B1h7tYSlM","signatures":["ICLR.cc/2018/Conference/Paper452/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper452/Authors"],"content":{"title":"RE: AnonReviewer3's review","comment":"Thanks for your review and suggestions.\n\n1. \"compare their computational complexity\"\n\n- For space (assuming that the weight values are stored in 32 bits for full-precision networks), the memory required by ternarized networks are 16 times smaller than the the full-precision network; while m-bit networks are 32/m times smaller.\n- For time, consider the product WX between a rxs weight matrix W and sxn input matrix X. For full-precision networks, the cost of WX is (M+A)rsn, where M and A are the computation costs of 32-bit floating-point multiplication and addition respectively. With the proposed ternarization, WX is computed by steps 3-5 in Algorithm 3. For illustration, we use the approximate solver (Algorithm 2) to compute the scaling parameter \\alpha and ternarized value b (invoked in Step 3 of Algorithm 3). With fixed b, computing \\alpha takes 2rs multiplications and 2rs additions. With fixed \\alpha, computing b takes rs comparisons. Assume that alternating minimization is run for k steps (empirically, k<=10), the computation cost of ternarization using Algorithm 2 is k(2M+2A+U)rs, where U is the computation cost of 32-bit floating-point comparison. Moreover, Steps 4 and 5 of Algorithm 3 take sn multiplications and rsn additions respectively. Thus the total cost for the product is Arsn + Msn + k(2M+2A+U)rs, and the speedup ratio is S = ((M+A) rsn)/(Arsn + Msn + k(2M+2A+U)rs), which is approximately (M+A)/A (some terms can be omitted as usually r >> 1 and n >>1, and k is very small). Following (Hubara et al, 2016), we consider the implication on power in 45nm technology (with A = 0.9J, and M=3.7J). Substituting into the ratio above, the energy reduction is then approximately 5. For the other ternarization algorithms such as TWN and TTQ, they also need at least sn multiplications and rsn additions for the product of WX, and thus the computation cost is similar to the proposed LAT_a.\n- Details and complexity analysis for the other models will be provided in the final version of the paper.\n\n2. \"discussion on the results in Table 2\"\n\n- The quantized LSTM performs better than full-precision network because deep networks often have larger-than-needed capacities, and so are less affected by the limited expressiveness of quantized weights. Besides, low-bit quantization acts as regularization, and so contributes positively to the performance. We will add the discussion in the final version of the paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513744177891,"tcdate":1513744177891,"number":2,"cdate":1513744177891,"id":"rJ5VPPDMf","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Comment","forum":"BkrSv0lA-","replyto":"Sy_vAgOgz","signatures":["ICLR.cc/2018/Conference/Paper452/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper452/Authors"],"content":{"title":"RE: AnonReviewer2's review","comment":"Thanks for your review and suggestions.\n\n1. \"the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides. Consequently, there is not a great degree of novelty in terms of the proposed method\"\n\n- Please see our reply to reviewer 1 above.\n\n2. \"the results are only slightly better than those of previous methods\"\n\n- The testing errors on these data sets are often only a few percent, and so the improvements may appear small. To have a clearer comparison, we added the percentage degradation of classification error as compared to the full-precision network in Table 1 (https://www.dropbox.com/s/miquko7qhff9kns/iclr2018_rebuttal.pdf?dl=0). \n- As can be seen, among the weight-ternarized networks, the proposed LAT and its variants achieve much smaller performance degradation on all four data sets. Existing methods often have large degradation, while ours has <3% degradation on MNIST and <1% on the other three data sets. On CIFAR-100, the proposed LAT and its variants achieve even better results than the full-precision network.\n- For recurrent networks, we similarly added the percentage degradation of cross-entropy in Table 2. As can be seen, the proposed weight ternarization is the only method that performs even better than the full-precision counterpart on all three data sets. On the Linux Kernel and Penn Treebank data sets, the proposed LAT and its variants even have >5% performance gain. On the War and Peace dataset, the proposed LAT and its variants are the only methods that achieve significantly better results than the full-precision network. \n\n3. \"it can't be simply stated that the algorithm converges\"\n\n- On the theory side, we can only show convergence of the objective value. We will clarify this in the final version of the paper. \n- Empirically, the quantized weight also converges, as can be seen from the convergence of $\\alpha$ in Figure 1(b).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513743968332,"tcdate":1513743968332,"number":1,"cdate":1513743968332,"id":"S1_DIvDfM","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Comment","forum":"BkrSv0lA-","replyto":"Bk4fUyieG","signatures":["ICLR.cc/2018/Conference/Paper452/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper452/Authors"],"content":{"title":"RE: AnonReviewer1's review","comment":"Thanks for your review and suggestions.\n\n1. \"it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to ternarization or m-bit\"\n\n- While the idea of extending from 1-bit (binarization) to more bits is straightforward, the difficulty and novelty are in the mathematical derivations. In Hou et al. (2017), the optimal closed-form solution for loss-aware binarization can be derived easily. However, for ternarization, the optimal \\alpha and b (in Proposition 3.2) cannot be easily solved. A straightforward solution would require combinatorial search. Instead, we proposed an exact solver (Algorithm 1) which relies only on sorting. This can be further simplified to an efficient alternating minimization procedure (Algorithm 2). The same situation applies to m-bit quantization. \n\n2. \"there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b). Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary\"\n\n- While analogous extension ideas have been proposed, their weight solutions obtained are not rigorously derived. Specifically, ternary-connect (Lin et al., 2016b) performs simple stochastic quantization, but does not relate that to any quality measure (e.g., the loss, or distance between the quantized and full-precision weights). In TWN (Li & Liu, 2016b), obtaining the theoretical optimal solution is time-consuming and so they used a heuristic instead. In this paper, we explicitly consider the quantization effect to the loss (as in Hou et al (2017)). However, the resultant optimization problem is much more difficult than theirs as explained above.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515642450812,"tcdate":1511876107983,"number":3,"cdate":1511876107983,"id":"Bk4fUyieG","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Review","forum":"BkrSv0lA-","replyto":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference/Paper452/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novelty","rating":"6: Marginally above acceptance threshold","review":"This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments.\n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm.  They extend the scheme to allow the use of different scaling parameters and to m-bit quantization. Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods. \n\nThe experiments are complete and the writing is good.\n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b). Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515642450849,"tcdate":1511685728551,"number":2,"cdate":1511685728551,"id":"Sy_vAgOgz","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Review","forum":"BkrSv0lA-","replyto":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference/Paper452/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A new method for weight quantization. A step in the right direction, with interesting results, but not a huge level of novelty. ","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights). \n\nThe paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose. The experiments are very clearly presented and solidly designed.\n\nIn fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides. Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.\n\nFinally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm. However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$). This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515642450885,"tcdate":1511524644071,"number":1,"cdate":1511524644071,"id":"B1h7tYSlM","invitation":"ICLR.cc/2018/Conference/-/Paper452/Official_Review","forum":"BkrSv0lA-","replyto":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference/Paper452/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Ternarization-based network compression via loss-aware minimization","rating":"8: Top 50% of accepted papers, clear accept","review":"In this paper, the authors propose a method of compressing network by means of weight ternarization. The network weights ternatization is formulated in the form of loss-aware quantization, which originally proposed by Hou et al. (2017).\n\nTo this reviewerâ€™s understanding, the proposed method can be regarded as the extension of the previous work of LAB and TWN, which can be the main contribution of the work.\n\nWhile the proposed method achieved promising results compared to the competing methods, it is still necessary to compare their computational complexity, which is one of the main concerns in network compression.\n\nIt would be appreciated to have discussion on the results in Table 2, which tells that the performance of quantized networks is better than the full-precision network.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]}},{"tddate":null,"ddate":null,"tmdate":1509739296860,"tcdate":1509119805195,"number":452,"cdate":1509739294198,"id":"BkrSv0lA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkrSv0lA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Loss-aware Weight Quantization of Deep Networks","abstract":"The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization (with possibly different scaling parameters for the positive and negative weights) and arbitrary m-bit quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.","pdf":"/pdf/f73e5823f858a7bc10b3d4e647d805fcceaf34a7.pdf","TL;DR":"A loss-aware weight quantization algorithm that directly considers its effect on the loss is proposed.","paperhash":"anonymous|lossaware_weight_quantization_of_deep_networks","_bibtex":"@article{\n  anonymous2018loss-aware,\n  title={Loss-aware Weight Quantization of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkrSv0lA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper452/Authors"],"keywords":["deep learning","network quantization"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}