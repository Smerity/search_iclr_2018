{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222827376,"tcdate":1511854588608,"number":3,"cdate":1511854588608,"id":"BJBWMqqlf","invitation":"ICLR.cc/2018/Conference/-/Paper952/Official_Review","forum":"ryUlhzWCZ","replyto":"ryUlhzWCZ","signatures":["ICLR.cc/2018/Conference/Paper952/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper proposes a new theoretically-motivated method for combining reinforcement learning and imitation learning for acquiring policies that are as good as or superior to the expert. The method assumes access to an expert value function (which could be trained using expert roll-outs) and uses the value function to shape the reward function and allow for truncated-horizon policy search. The algorithm can gracefully handle suboptimal demonstrations/value functions, since the demonstrations are only used for reward shaping, and the experiments demonstrate faster convergence and better performance compared to RL and AggreVaTeD on a range of simulated control domains. The paper is well-written and easy to understand.\n\nMy main feedback is with regard to the experiments:\nI appreciate that the experiments used 25 random seeds! This provides a convincing evaluation.\nIt would be nice to see experimental results on even higher dimensional domains such as the ant, humanoid, or vision-based tasks, since the experiments seem to suggest that the benefit of the proposed method is diminished in the swimmer and hopper domains compared to the simpler settings.\nSince the method uses demonstrations, it would be nice to see three additional comparisons: (a) training with supervised learning on the expert roll-outs, (b) initializing THOR and AggreVaTeD (k=1) with a policy trained with supervised learning, and (c) initializing TRPO with a policy trained with supervised learning. There doesn't seem to be any reason not to initialize in such a way, when expert demonstrations are available, and such an initialization should likely provide a significant speed boost in training for all methods.\nHow many demonstrations were used for training the value function in each domain? I did not see this information in the paper.\n\nWith regard to the method and discussion:\nThe paper discusses the connection between the proposed method and short-horizon imitation and long-horizon RL, describing the method as a midway point. It would also be interesting to see a discussion of the relation to inverse RL, which considers long-term outcomes from expert demonstrations. For example, MacGlashn & Littman propose a midway point between imitation and inverse RL [1].\nTheoretically, would it make sense to anneal k from small to large? (to learn the most effectively from the smallest amount of experience)\n\n[1] https://www.ijcai.org/Proceedings/15/Papers/519.pdf\n\n\nMinor feedback:\n- The RHS of the first inequality in the proof of Thm 3.3 seems to have an error in the indexing of i and exponent, which differs from the line before and line after","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION","abstract":"Combination of Reinforcement Learning and Imitation Learning brings the best from both: we can quickly learn by imitating near-optimal oracles that achieve good performance on the task, while also explore and exploit to improve what we have learned. In this paper, we propose a novel way to combine imitation and reinforcement via the idea of reward shaping using such an oracle. We theoretically study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner’s planning horizon as function of its accuracy. When the oracle is sub-optimal, to ensure to find a policy that can outperform the oracle, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximizing the total reshaped reward over a finite planning horizon. We experimentally demonstrate a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.","pdf":"/pdf/cf252bd5d6a654b7acfc39280eb7771c52fdf486.pdf","TL;DR":"Combining Imitation Learning and Reinforcement Learning to learn to outperform the expert","paperhash":"anonymous|truncated_horizon_policy_search_deep_combination_of_reinforcement_and_imitation","_bibtex":"@article{\n  anonymous2018truncated,\n  title={TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryUlhzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper952/Authors"],"keywords":["Imitation Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222827427,"tcdate":1511844102770,"number":2,"cdate":1511844102770,"id":"H1JzYwcxM","invitation":"ICLR.cc/2018/Conference/-/Paper952/Official_Review","forum":"ryUlhzWCZ","replyto":"ryUlhzWCZ","signatures":["ICLR.cc/2018/Conference/Paper952/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The main idea has been studied before, but not properly discussed.","rating":"5: Marginally below acceptance threshold","review":"=== SUMMARY ===\n\nThe paper considers a combination of Reinforcement Learning (RL) and Imitation Learning (IL), in the infinite horizon discounted MDP setting.\nThe IL part is in the form of an oracle that returns a value function V^e, which is an approximation of the optimal value function. The paper defines a new cost (or reward) function based on V^e, through shaping (Eq. 1). It is known that shaping does not change the optimal policy.\n\nA key aspect of this paper is to consider a truncated horizon problem (say horizon k) with the reshaped cost function, instead of an infinite horizon MDP.\nFor this truncated problem, one can write the (dis)advantage function as a k-step sum of reward plus the value returned by the oracle at the k-th step (cf. Eq. 5).\nTheorem 3.3 shows that the value of the optimal policy of the truncated MDP w.r.t. the original MDP is only O(gamma^k eps) worse than the optimal policy of the original problem (gamma is the discount factor and eps is the error between V^e and V*).\n\nThis suggests two things: \n1) Having an oracle that is accurate (small eps) leads to good performance. If oracle is the same as the optimal value function, we do not need to plan more than a single step ahead.\n2) By planning for k steps ahead, one can decrease the error in the oracle geometrically fast. In the limit of k —> inf, the error in the oracle does not matter.\n\nBased on this insight, the paper suggests an actor-critic-like algorithm called THOR (Truncated HORizon policy search) that minimizes the total cost over a truncated horizon with a modified cost function.\n\nThrough a series of experiments on several benchmark problems (inverted pendulum, swimmer, etc.), the paper shows the effect of planning horizon k.\n\n\n\n=== EVALUATION & COMMENTS ===\n\nI like the main idea of this paper. The paper is also well-written. But one of the main ideas of this paper (truncating the planning horizon and replacing it with approximation of the optimal value function) is not new and has been studied before, but has not been properly cited and discussed.\n\nThere are a few papers that discuss truncated planning. Most closely is the following paper:\n\nFarahmand, Nikovski, Igarashi, and Konaka, “Truncated Approximate Dynamic Programming With Task-Dependent Terminal Value,” AAAI, 2016.\n\nThe motivation of AAAI 2016 paper is different from this work. The goal there is to speedup the computation of finite, but large, horizon problem with a truncated horizon planning. The setting there is not the combination of RL and IL, but multi-task RL. An approximation of optimal value function for each task is learned off-line and then used as the terminal cost. \nThe important point is that the learned function there plays the same role as the value provided by the oracle V^e in this work. They both are used to shorten the planning horizon. That paper theoretically shows the effect of various error terms, including terms related to the approximation in the planning process (this paper does not do that).\n\nNonetheless, the resulting algorithms are quite different. The result of this work is an actor-critic type of algorithm. AAAI 2016 paper is an approximate dynamic programming type of algorithm.\n\nThere are some other papers that have ideas similar to this work in relation to truncating the horizon. For example, the multi-step lookahead policies and the use of approximate value function as the terminal cost in the following paper:\n\nBertsekas, “Dynamic Programming and Suboptimal Control: A Survey from ADP to MPC,” European Journal of Control, 2005.\n\nThe use of learned value function to truncate the rollout trajectory in a classification-based approximate policy iteration method has been studied by\n\nGabillon, Lazaric, Ghavamzadeh, and Scherrer, “Classification-based Policy Iteration with a Critic,” ICML, 2011.\n\nOr in the context of Monte Carlo Tree Search planning, the following paper is relevant:\n\nSilver et al., “Mastering the game of Go with deep neural networks and tree search,” Nature, 2016.\n\nTheir “value network” has a similar role to V^e. It provides an estimate of the states at the truncated horizon to shorten the planning depth.\n\nNote that even though these aforementioned papers are not about IL, this paper’s stringent requirement of having access to V^e essentially make it similar to those papers.\n\n\nIn short, a significant part of this work’s novelty has been explored before. Even though not being completely novel is totally acceptable, it is important that the paper better position itself compared to the prior art.\n\n\nAside this main issue, there are some other comments:\n\n\n- Theorem 3.1 is not stated clearly and may suggest more than what is actually shown in the proof. The problem is that it is not clear about the fact the choice of eps is not arbitrary.\nThe proof works only for eps that is larger than 0.5. With the construction of the proof, if eps is smaller than 0.5, there would not be any error, i.e., J(\\hat{pi}^*) = J(pi^*).\n\nThe theorem basically states that if the error is very large (half of the range of value function), the agent does not not perform well. Is this an interesting case?\n\n\n- In addition to the papers I mentioned earlier, there are some results suggesting that shorter horizons might be beneficial and/or sufficient under certain conditions. A related work is a theorem in the PhD dissertation of Ng:\n\nAndrew Ng, Shaping and Policy Search in Reinforcement Learning, PhD Dissertation, 2003.\n(Theorem 5 in Appendix 3.B: Learning with a smaller horizon).\n\nIt is shown that if the error between Phi (equivalent to V^e here) and V* is small, one may choose a discount factor gamma’ that is smaller than gamma of the original MDP, and still have some guarantees. As the discount factor has an interpretation of the effective planning horizon, this result is relevant. The result, however, is not directly comparable to this work as the planning horizon appears implicitly in the form of 1/(1-gamma’) instead of k, but I believe it is worth to mention and possibly compare.\n\n- The IL setting in this work is that an oracle provides V^e, which is the same as (Ross & Bagnell, 2014). I believe this setting is relatively restrictive as in many problems we only have access to (state, action) pairs, or sequence thereof, and not the associated value function. For example, if a human is showing how a robot or a car should move, we do not easily have access to V^e (unless the reward function is known and we estimate the value with rollouts; which requires us having a long trajectory). This is not a deal breaker, and I would not consider this as a weakness of the work, but the paper should be more clear and upfront about this.\n\n\n- The use of differential operator nabla instead of gradient of a function (a vector field) in Equations (10), (14), (15) is non-standard.\n\n- Figures are difficult to read, as the colors corresponding to confidence regions of different curves are all mixed up. Maybe it is better to use standard error instead of standard deviation.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION","abstract":"Combination of Reinforcement Learning and Imitation Learning brings the best from both: we can quickly learn by imitating near-optimal oracles that achieve good performance on the task, while also explore and exploit to improve what we have learned. In this paper, we propose a novel way to combine imitation and reinforcement via the idea of reward shaping using such an oracle. We theoretically study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner’s planning horizon as function of its accuracy. When the oracle is sub-optimal, to ensure to find a policy that can outperform the oracle, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximizing the total reshaped reward over a finite planning horizon. We experimentally demonstrate a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.","pdf":"/pdf/cf252bd5d6a654b7acfc39280eb7771c52fdf486.pdf","TL;DR":"Combining Imitation Learning and Reinforcement Learning to learn to outperform the expert","paperhash":"anonymous|truncated_horizon_policy_search_deep_combination_of_reinforcement_and_imitation","_bibtex":"@article{\n  anonymous2018truncated,\n  title={TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryUlhzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper952/Authors"],"keywords":["Imitation Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222827469,"tcdate":1511777748568,"number":1,"cdate":1511777748568,"id":"H16Rrvtlz","invitation":"ICLR.cc/2018/Conference/-/Paper952/Official_Review","forum":"ryUlhzWCZ","replyto":"ryUlhzWCZ","signatures":["ICLR.cc/2018/Conference/Paper952/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lack of rigor and claim (that k=1 does not allow the RL agent to outperform the expert) not justified","rating":"3: Clear rejection","review":"This work proposes to use the value function V^e of some expert policy \\pi^e in order to speed up learning of an RL agent which should eventually do better than the expert. The emphasis is put on using k-steps (with k>1) Bellman updates using bootstrapping from V^e. \n\nIt is claimed that the case k=1 does not allow the agent to outperform the expert policy, whereas k>1 does (Section 3.1, paragraph before Lemma 3.2).\n\nI disagree with this claim. Indeed a policy gradient algorithm (similar to (10)) with a 1-step advantage c(s,a) + gamma V^e(s_{t+1}) - V^e(s_t) will converge (say in the tabular case, or in the case you consider of a rich enough policy space \\Pi) to the greedy policy with respect to V^e, which is strictly better than V^e (if V^e is not optimal). So you don’t need to use k>1 to improve the expert policy. Now it’s true that this will not converge to the optimal policy (since you keep bootstrapping with V^e instead of the current value function), but neither the k-step advantage will. \n\nSo I don’t see any fundamental difference between k=1 and k>1. The only difference being that the k-step bootstrapping will implement a k-step Bellman operator which contracts faster (as gamma^k) when k is large. But the best choice of k has to be discussed in light of a bias-variance discussion, which is missing here. So I find that the main motivation for this work is not well supported. \n\nAlgorithmic suggestion:\nInstead of bootstrapping with V^e, why not bootstrap with min(V^e, V), where V is your current approximation of the value function. In that way you would benefit from (1) fast initialization with V^e at the beginning of learning, (2) continual improvement once you’ve reached the performance of the expert. \n\nOther comments:\nRequiring that we know the value function of the expert on the whole state space is a very strong assumption that we do not usually make in Imitation learning. Instead we assume we have trajectories from expert (from which we can compute value function along those trajectories only). Generalization of the value function to other states is a hard problem in RL and is the topic of important research.\n\nThe overall writing lacks rigor and the contribution is poor. Indeed the lower bound (Theorem 3.1) is not novel (btw, the constant hidden in the \\Omega notation is 1/(1-gamma)). Theorems 3.2 and 3.3 are not novel either. Please read [Bertsekas and Tsitsiklis, 96] as an introduction to dynamic programming with approximation.\n\nThe writing could be improved, and there are many typos, such as:\n- J is not defined (Equation (2))\n- Why do you call A a disadvantage function whereas this quantity is usually called an advantage?\n- You are considering a finite (ie, k) horizon setting, so the value function depend on time. For example the value functions defined in (11) depend on time. \n- All derivations in Section 4, before subsection 4.1 are very approximate and lack rigor.\n- Last sentence of Proof of theorem 3.1. I don’t understand H -> 2H epsilon. H is fixed, right? Also your example does not seem to be a discounted problem.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION","abstract":"Combination of Reinforcement Learning and Imitation Learning brings the best from both: we can quickly learn by imitating near-optimal oracles that achieve good performance on the task, while also explore and exploit to improve what we have learned. In this paper, we propose a novel way to combine imitation and reinforcement via the idea of reward shaping using such an oracle. We theoretically study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner’s planning horizon as function of its accuracy. When the oracle is sub-optimal, to ensure to find a policy that can outperform the oracle, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximizing the total reshaped reward over a finite planning horizon. We experimentally demonstrate a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.","pdf":"/pdf/cf252bd5d6a654b7acfc39280eb7771c52fdf486.pdf","TL;DR":"Combining Imitation Learning and Reinforcement Learning to learn to outperform the expert","paperhash":"anonymous|truncated_horizon_policy_search_deep_combination_of_reinforcement_and_imitation","_bibtex":"@article{\n  anonymous2018truncated,\n  title={TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryUlhzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper952/Authors"],"keywords":["Imitation Learning","Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092383566,"tcdate":1509137425973,"number":952,"cdate":1510092361927,"id":"ryUlhzWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryUlhzWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION","abstract":"Combination of Reinforcement Learning and Imitation Learning brings the best from both: we can quickly learn by imitating near-optimal oracles that achieve good performance on the task, while also explore and exploit to improve what we have learned. In this paper, we propose a novel way to combine imitation and reinforcement via the idea of reward shaping using such an oracle. We theoretically study the effectiveness of the near-optimal cost-to-go oracle on the planning horizon and demonstrate that the cost-to-go oracle shortens the learner’s planning horizon as function of its accuracy. When the oracle is sub-optimal, to ensure to find a policy that can outperform the oracle, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximizing the total reshaped reward over a finite planning horizon. We experimentally demonstrate a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal.","pdf":"/pdf/cf252bd5d6a654b7acfc39280eb7771c52fdf486.pdf","TL;DR":"Combining Imitation Learning and Reinforcement Learning to learn to outperform the expert","paperhash":"anonymous|truncated_horizon_policy_search_deep_combination_of_reinforcement_and_imitation","_bibtex":"@article{\n  anonymous2018truncated,\n  title={TRUNCATED HORIZON POLICY SEARCH: DEEP COMBINATION OF REINFORCEMENT AND IMITATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryUlhzWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper952/Authors"],"keywords":["Imitation Learning","Reinforcement Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}