{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222720869,"tcdate":1511819980046,"number":3,"cdate":1511819980046,"id":"ByNRqb5lz","invitation":"ICLR.cc/2018/Conference/-/Paper693/Official_Review","forum":"HJWGdbbCW","replyto":"HJWGdbbCW","signatures":["ICLR.cc/2018/Conference/Paper693/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Minor contributions leading to relatively poor results on real robot","rating":"5: Marginally below acceptance threshold","review":"Given that imitation learning is often used to initialize reinforcement learning, the authors should consider using a more descriptive title for this paper. \n\nThe main contribution of the paper is to use a mixture of the reinforcement learning reward and the imitation learning signal from GAIL. This approach is generally fairly straightforward, but seems to be effective in simulation, and boils down to equation 2. It would be interesting to discuss and evaluate the effects of changing lambda overtime. \n\nThe second contribution can be seen as a list of four techniques for getting the most out of using the simulation environment and the state information that it provides. A list of these types of techniques could be useful for students training networks on simulations, although each point on the list is fairly straightforward. This part also leads to an ablation study to determine the effects of the proposed techniques. The results plot should include error bars.\n\nThe earlier parts of the experiment were evaluated on three additional tasks. Although these tasks are all variations of putting things into a box, they do add some variability to the experiments. It was also great seeing the robot learning multiple strategies for the table clearing task. \n\nThe third part is transferring the simulation policy to the real robot. This section and the additional supplementary material are fairly short and should be expanded upon. It seems as though the transfer mainly depends on learning from randomized domains to achieve more robust policies that can then be applied to the real domain. The transfer learning is a crucial step of the pipeline. Unfortunately the results are not great. A 64% success rate for lifting the block and 35% for stacking are both fairly low. The lifting success is somewhat higher for the stacking at 80% with repeated attempts. The authors need to discuss these results. What is the cause of these low success rates? Is it the transfer learning or due to an earlier step in the pipeline? How do these success rates change with the variance in the training scenarios?\n\nHow much of the shape variability is being accounted for by the natural adaptability of the hand? If you give the robot images with one set of objects, but the actual task is performed  using objects of different shapes and sizes, how much does the performance decrease?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement and Imitation Learning for Diverse Visuomotor Skills","abstract":"We propose a general deep reinforcement learning method and apply it to robot manipulation tasks. Our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved. We train visuomotor policies end-to-end to learn a direct mapping from RGB camera inputs to joint velocities. Our experiments indicate that our reinforcement and imitation approach can solve contact-rich robot manipulation tasks that neither the state-of-the-art reinforcement nor imitation learning method can solve alone. We also illustrate that these policies achieved zero-shot sim2real transfer by training with large visual and dynamics variations.","pdf":"/pdf/c39982c4ecfdad60fa6d9794ddff7574bb1cb21d.pdf","TL;DR":"combine reinforcement learning and imitation learning to solve complex robot manipulation tasks from pixels","paperhash":"anonymous|reinforcement_and_imitation_learning_for_diverse_visuomotor_skills","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement and Imitation Learning for Diverse Visuomotor Skills},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJWGdbbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper693/Authors"],"keywords":["reinforcement learning","imitation learning","robotics","visuomotor skills"]}},{"tddate":null,"ddate":null,"tmdate":1512222720908,"tcdate":1511727619443,"number":2,"cdate":1511727619443,"id":"B1oZGo_ez","invitation":"ICLR.cc/2018/Conference/-/Paper693/Official_Review","forum":"HJWGdbbCW","replyto":"HJWGdbbCW","signatures":["ICLR.cc/2018/Conference/Paper693/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice results, but approach only works in simulation","rating":"4: Ok but not good enough - rejection","review":"This paper claims to present a \"general deep reinforcement learning\" method that addresses the issues of real-world robotics: data constraints, safety, and lack of state information, and exploration by using demonstrations. However, this paper actually addresses these problems by training in a simulator, and only transferring 2 of the 6 tasks to the real world. The real world results are  lackluster. However, the simulated results are nice.\n\nThe method in the paper is as follows: the environment reward is augmented by a reward function learned from human demonstrations using GAIL on full state (except for the arm). Then, an actor-critic method is used where the critic gets full state information, while the actor needs to learn from an image. However, the actor's convolutional layers are additionally trained to detect the object positions. \n\nStrengths:\n+ The simulated tasks are novel and difficult (sorting, clearing a table)\n+ Resetting to demonstration states is a nice way to provide curriculum\n\nLimitations:\n+ The results make me worries that the simulation environments have been hyper-tailored to the method, as the real environments looks very similar, and should transfer. \n+ Each part of the method is not particularly novel. Combining IRL and RL has been done before (as the authors point out in the related work), side-training perception module to predict full state has been done before (\"End-to-end visuomotor learning\"), diversity of training conditions has been done before (Domain randomization).\n+ Requiring hand-specified clusters of states for both selecting starting states and defining a reward functions requires domain knowledge. Why can't they be clustered using a clustering method?\n+ Because the method needs simulation to learn a policy, it is limited to tasks that can be simulated somewhat accurately (e.g. ones with simple dynamics). As shown by the poor transfer of the stacking task, block stacking with foam blocks is not a such task.\n\n\nQuestions:\n+ How many demonstrations do you use per task?\n+ What are the \"relative\" positions included in the \"object-centric\" state input? \n\nMisleading parts of the paper:\n+ The introduction of the paper primes the reader to expect a method that can work on a real system. However, this method only gets 64% accuracy on a simple block lifting task, 35% on a stacking task.\n+ \"Appendix C. \"We define functions on the underlying physical state to determine the stage of a stateâ€¦The definition of stages also gives rise to a convenient way of specifying the reward functions without hand-engineering a shaping reward. \"-> You are literally hand engineering a shaping reward. The main text misleadingly refers to \"sparse reward\", which usually refers to a single reward upon task completion.\n\nIn conclusion, I find that the work lacks significance because the results are dependent on a list of hacks that are only possible in simulation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement and Imitation Learning for Diverse Visuomotor Skills","abstract":"We propose a general deep reinforcement learning method and apply it to robot manipulation tasks. Our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved. We train visuomotor policies end-to-end to learn a direct mapping from RGB camera inputs to joint velocities. Our experiments indicate that our reinforcement and imitation approach can solve contact-rich robot manipulation tasks that neither the state-of-the-art reinforcement nor imitation learning method can solve alone. We also illustrate that these policies achieved zero-shot sim2real transfer by training with large visual and dynamics variations.","pdf":"/pdf/c39982c4ecfdad60fa6d9794ddff7574bb1cb21d.pdf","TL;DR":"combine reinforcement learning and imitation learning to solve complex robot manipulation tasks from pixels","paperhash":"anonymous|reinforcement_and_imitation_learning_for_diverse_visuomotor_skills","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement and Imitation Learning for Diverse Visuomotor Skills},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJWGdbbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper693/Authors"],"keywords":["reinforcement learning","imitation learning","robotics","visuomotor skills"]}},{"tddate":null,"ddate":null,"tmdate":1512222720951,"tcdate":1511649000358,"number":1,"cdate":1511649000358,"id":"HJge1dvgz","invitation":"ICLR.cc/2018/Conference/-/Paper693/Official_Review","forum":"HJWGdbbCW","replyto":"HJWGdbbCW","signatures":["ICLR.cc/2018/Conference/Paper693/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good experimental work, but some pretty severe over-statements and claims","rating":"6: Marginally above acceptance threshold","review":"Paper summary: The authors propose a number of tricks to enable training policies for pick and place style tasks using a combination of GAIL-based imitation learning and hand-specified rewards, as well as use of unobserved state information during training and hand-designed curricula. The results demonstrate manipulation policies for stacking blocks and moving objects, as well as preliminary results for zero-shot transfer from simulation to a real robot for a picking task and an attempt at a stacking task.\n\nReview summary: The paper proposes a limited but interesting contribution that will be especially of interest to practitioners, but the scope of the contribution is somewhat incremental in light of recent work, and the results, while interesting, could certainly be better. In the balance, I think the paper should be accepted, because it will be of value to practitioners, and I appreciate the detail and real-world experiments. However, some of the claims should be revised to better reflect what the paper actually accomplishes: the contribution is a bit limited in places, but that's *OK* -- the authors should just be up-front about it.\n\nPros:\n- Interesting tasks that combine imitation and reinforcement in a logical (but somewhat heuristic) way\n- Good simulated results on a variety of pick-and-place style problems\n- Some initial attempt at real-world transfer that seems promising, but limited\n- Related work is very detailed and I think many will find it to be a very valuable overview\nCons:\n- Some of the claims (detailed below) are a bit excessive in my opinion\n- The paper would be better if it was scoped more narrowly\n- Contribution is a bit incremental and somewhat heuristic\n- The experimental results are difficult to interpret in simulation\n- The real-world experimental results are not great\n- There are a couple of missing citations (but overall related work is great)\n\nDetailed discussion of potential issues and constructive feedback:\n\n> \"Our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved.\"\n>> This claim is a bit peculiar. Picking up and placing objects is certainly not \"unsolved,\" there are many examples. If you want image-based pick and place with demonstrations for example, see Chebotar '17 (not cited). If you want stacking blocks, see Nair '17. While it's true that there is a particular combination of factors that doesn't exactly appear in prior work, the statement the authors make is way too strong. Chebotar '17 shows picking and placing a real-world objective with a much higher success rate than reported here, without simulation. Nair '17 shows a much harder stacking task, but without images -- would that method have worked just as well with image-based distillation? Very likely. Rajeswaran '17 shows tasks that arguably are much harder. Maybe a more honest statement is that this paper proposes some tasks that prior methods don't show, and some prior methods show tasks that the proposed method can't solve. But as-is, this statement misrepresents prior work.\n\n> Previous RL-based robot manipulation policies (Nair et al., 2017; Popov et al., 2017) largely rely on low-level states as input, or use severely limited action spaces that ignore the arm and instead learn Cartesian control of a simple gripper. This limits the ability of these methods to represent and solve more complex tasks (e.g., manipulating arbitrary 3D objects) and to deploy in real environments where the privileged state information is unavailable.\n>> This is a funny statement. Some use images, some don't. There is a ton of prior work on RL-based robot manipulation that does use images. The current paper does use object state information during training, which some prior works manage to avoid. The comments about Cartesian control are a bit peculiar... the proposed method controls fingers, but the hand is simple. Some prior works have simpler grippers (e.g., Nair) and some have much more complex hands (e.g., Rajeswaran). So this one falls somewhere in the middle. That's fine, but again, this statement overclaims a bit.\n\n> To sidestep the constraints of training on real hardware we embrace the sim2real paradigm which\nhas recently shown promising results (James et al., 2017; Rusu et al., 2016a).\n>> Probably should cite Sadeghi et al. and Tobin et al. in regard to randomization, both of which precede James '17.\n\n> we can, during training, exploit privileged information about the true system state\n>> This was done also in Pinto et al. and many of the cited GPS papers\n\n> our policies solve the tasks that the state-of-the-art reinforcement and imitation learning cannot solve\n>> I don't think this statement is justified without much wider comparisons -- the authors don't attempt any comparisons to prior work, such as Chebotar '17 (which arguably is closest in terms of demonstrated behaviors), Nair '17 (which is also close but doesn't use images, though it likely could).\n\n> An alternative strategy for dealing with the data demand is to train in simulation and transfer\n>> Aside from previously mentioned citations, should probably cite Devin \"Towards Adapting Deep Visuomotor Representations\"\n\n> Sec 3.2.1\n>> This method seems a bit heuristic. It's logical, but can you say anything about what this will converge to? GAIL will try to match the demonstration distribution, and RL will try to maximize expected reward. What will this method do?\n\n> Experiments\n>> Would it be possible to indicate some measure of success rate for the simulated experiments? As-is, it's hard to tell how well either the proposed method or the baselines actually work.\n\n> Transfer\n>> My reading of the transfer experiments is that they are basically unsuccessful. Picking up a rectangular object with 80% success rate is not very good. The stacking success rate is too low to be useful. I do appreciate the authors trying out their method on a real robotic platform, but perhaps the more honest assessment of the outcome of these experiments is that the approach didn't work very well, and more research is needed. Again, it's *OK* to say this! Part of the purpose of publishing a paper is to stimulate future research directions. I think the transfer experiments should definitely be kept, but the authors should discuss the limitations to help future work address them, and present the transfer appropriately in the intro.\n\n> Diverse Visuomotor Skills\n>> I think this is a peculiar thing to put in the title. Is the implication that prior work is not diverse? Arguably several prior papers show substantially more diverse skills. It seems that all the skills here are essentially pick and place skills, which is fine (these are interesting skills), but the title seems like a peculiar jab at prior work not being \"diverse\" enough, which is simply misleading.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement and Imitation Learning for Diverse Visuomotor Skills","abstract":"We propose a general deep reinforcement learning method and apply it to robot manipulation tasks. Our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved. We train visuomotor policies end-to-end to learn a direct mapping from RGB camera inputs to joint velocities. Our experiments indicate that our reinforcement and imitation approach can solve contact-rich robot manipulation tasks that neither the state-of-the-art reinforcement nor imitation learning method can solve alone. We also illustrate that these policies achieved zero-shot sim2real transfer by training with large visual and dynamics variations.","pdf":"/pdf/c39982c4ecfdad60fa6d9794ddff7574bb1cb21d.pdf","TL;DR":"combine reinforcement learning and imitation learning to solve complex robot manipulation tasks from pixels","paperhash":"anonymous|reinforcement_and_imitation_learning_for_diverse_visuomotor_skills","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement and Imitation Learning for Diverse Visuomotor Skills},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJWGdbbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper693/Authors"],"keywords":["reinforcement learning","imitation learning","robotics","visuomotor skills"]}},{"tddate":null,"ddate":null,"tmdate":1509739156637,"tcdate":1509132297478,"number":693,"cdate":1509739153925,"id":"HJWGdbbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJWGdbbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Reinforcement and Imitation Learning for Diverse Visuomotor Skills","abstract":"We propose a general deep reinforcement learning method and apply it to robot manipulation tasks. Our approach leverages demonstration data to assist a reinforcement learning agent in learning to solve a wide range of tasks, mainly previously unsolved. We train visuomotor policies end-to-end to learn a direct mapping from RGB camera inputs to joint velocities. Our experiments indicate that our reinforcement and imitation approach can solve contact-rich robot manipulation tasks that neither the state-of-the-art reinforcement nor imitation learning method can solve alone. We also illustrate that these policies achieved zero-shot sim2real transfer by training with large visual and dynamics variations.","pdf":"/pdf/c39982c4ecfdad60fa6d9794ddff7574bb1cb21d.pdf","TL;DR":"combine reinforcement learning and imitation learning to solve complex robot manipulation tasks from pixels","paperhash":"anonymous|reinforcement_and_imitation_learning_for_diverse_visuomotor_skills","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement and Imitation Learning for Diverse Visuomotor Skills},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJWGdbbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper693/Authors"],"keywords":["reinforcement learning","imitation learning","robotics","visuomotor skills"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}