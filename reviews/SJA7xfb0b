{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222745476,"tcdate":1512095548256,"number":2,"cdate":1512095548256,"id":"Hk4rkBRlz","invitation":"ICLR.cc/2018/Conference/-/Paper765/Official_Review","forum":"SJA7xfb0b","replyto":"SJA7xfb0b","signatures":["ICLR.cc/2018/Conference/Paper765/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting mathematically, unclear if a step above other GANs","rating":"6: Marginally above acceptance threshold","review":"Summary: The authors provide another type of GAN--the Sobolev GAN--which is the typical setup of a GAN but using a function class F for which f belongs to F iff \\grad f belongs to L^2(mu). They relate this MMD to the Cramer and Fisher distance and then produce a recipe for training GANs with this sort of function class. In their empirical examples, they show it has similar performance to the WGAN-GP.\n\nOverall, the paper has some interesting mathematical relationships to other MMDs. However, I finished reading the paper wondering why one would want to trust this GAN over any of the other GANs. I may have missed it, but I didn't see any compelling theoretical reason the gradients from this method would prove superior to many of the other GANs in existence today. The authors argue \"from [equation 5] we see that we are comparing CDFs, which are better behaved on discrete distributions,\" but I wasn't sure what exactly to make of this comment.\n\nNits:\n* The \"Stein metric\" is actually called the Stein discrepancy [see Gorham & Mackey (2015) Measuring Sample Quality using Stein's Method].","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sobolev GAN","abstract":"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","pdf":"/pdf/28ff8cf760e392a61fdd147f15d88a3cc373510a.pdf","TL;DR":"We define a new Integral Probability Metric (Sobolev IPM) and show how it can be used for training GANs for text generation and semi-supervised learning.","paperhash":"anonymous|sobolev_gan","_bibtex":"@article{\n  anonymous2018sobolev,\n  title={Sobolev GAN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJA7xfb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper765/Authors"],"keywords":["GAN theory","Integral Probability Metrics","elliptic PDE and diffusion","GAN for discrete sequences","semi-supervised learning."]}},{"tddate":null,"ddate":null,"tmdate":1512222745520,"tcdate":1511773030814,"number":1,"cdate":1511773030814,"id":"HyJ_7LFlG","invitation":"ICLR.cc/2018/Conference/-/Paper765/Official_Review","forum":"SJA7xfb0b","replyto":"SJA7xfb0b","signatures":["ICLR.cc/2018/Conference/Paper765/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper suggests a novel regularization scheme for GANs based on a Sobolev norm, measuring deviations between L2 norms of derivatives. It establishes some nice theoretical properties, and demonstrates effectiveness through simulations. ","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper deals with the increasingly popular GAN approach to constructing generative models.  Following the first formulation of GANs in 2014, it was soon realized that the training dynamics was highly unstable, leading to significant difficulties in achieving stable results. The paper by Arjovsky et al (2017) provided a framework based on the Wasserstein distance, a distance measure between probability distributions belonging to the class of so-called Integral Probability Metrics (IPMs). This approach solved the stability issues of GANs and demonstrated improved empirical results. Several other works were then developed to deal with these stability issues, specifically the Fisher IPM. Both these methods relied on discriminating between distributions P and Q based on computing a function f, belonging to an appropriate function class {\\cal F}, that maximizes the deviation E_{x~P}f(x)-E_{x~Q}f(x). The main issue relates to the choice of the class {\\cal F}. For the Wasserstein distance this was the class of L_1 Lipschitz functions, while for the Fisher distance it was the class of square integrable functions. The present paper introduces a new notion of distance, where {\\cal F} is the defined through the Sobolev norm, based on the L_2 norm of the gradient of f(x), with respect to a measure \\mu(x), where the latter can be freely chosen under certain assumptions. \n\nThe authors prove a theorem related to the properties of the Sobolev norm, and express it in terms of the component-wise conditional distributions. Moreover, they show that the optimal critic f is obtained by solving a PDE subject to zero boundary conditions. They then use their suggested metric in order to develop a GAN algorithm, and present experimental results demonstrating its utility. The Sobolev IPM has two nice features. First, it is based on the component-wise conditional distribution of the CDFs, and, second, its relation to the Laplacian regularizer from manifold learning. Its 1D version also relates to the well-known von Mises Cramer statistics used in hypothesis testing. \n\nThe paper belongs to a class of recent papers attempting to suggest improvements to the original GAN algorithm, relying on the KL divergence. It is well conceived and articulated, and provides an interesting and potentially powerful new direction to improve GANs in practice. However, it is somewhat difficult to follow the paper, and would urge the authors to improve and augment their presentation of the following issues. \n1)\tOne often poses regularization schemes based on optimality criteria. Is there any optimality principle under which the Sobolev IPM is a desired choice? \n2)\tThe authors argue that their approach is especially well suited for discrete sequential data. This issue was not clear to me, and it would be good if the authors could expand on this issue and provide a clearer explanation. \n3)\tHow would the Sobolev norm behave under a change of coordinates or a homeomorphism of the space? Would it make sense to require some invariance in this respect? \n4)\tThe Lagrangian in eq. (9) contains both a Lagrange constraint on the Sobolev norm and a penalty term. Why are both needed? Why do the updates of  \\lambda and p in Algorithm 1 used different schemes (SGD and ADAM, respectively). \n5)\tTable 2, p. 13 â€“ it would be nice to see a comparison to the recently introduced gradient penalty approach, Gulrajani et al., Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.\n6)\tThe integral defining F_p(x) on p. 3 has x as an argument on the LHS and as an integrand of the RHS. Please correct this. Also specify that x=(x_1,\\ldots,x_d).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sobolev GAN","abstract":"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","pdf":"/pdf/28ff8cf760e392a61fdd147f15d88a3cc373510a.pdf","TL;DR":"We define a new Integral Probability Metric (Sobolev IPM) and show how it can be used for training GANs for text generation and semi-supervised learning.","paperhash":"anonymous|sobolev_gan","_bibtex":"@article{\n  anonymous2018sobolev,\n  title={Sobolev GAN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJA7xfb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper765/Authors"],"keywords":["GAN theory","Integral Probability Metrics","elliptic PDE and diffusion","GAN for discrete sequences","semi-supervised learning."]}},{"tddate":null,"ddate":null,"tmdate":1509739115846,"tcdate":1509134374319,"number":765,"cdate":1509739113191,"id":"SJA7xfb0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJA7xfb0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sobolev GAN","abstract":"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure mu. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure mu plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cramer statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.","pdf":"/pdf/28ff8cf760e392a61fdd147f15d88a3cc373510a.pdf","TL;DR":"We define a new Integral Probability Metric (Sobolev IPM) and show how it can be used for training GANs for text generation and semi-supervised learning.","paperhash":"anonymous|sobolev_gan","_bibtex":"@article{\n  anonymous2018sobolev,\n  title={Sobolev GAN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJA7xfb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper765/Authors"],"keywords":["GAN theory","Integral Probability Metrics","elliptic PDE and diffusion","GAN for discrete sequences","semi-supervised learning."]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}