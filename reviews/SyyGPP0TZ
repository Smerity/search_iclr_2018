{"notes":[{"tddate":null,"ddate":null,"tmdate":1512458247693,"tcdate":1512458247693,"number":3,"cdate":1512458247693,"id":"BJlf_TmWG","invitation":"ICLR.cc/2018/Conference/-/Paper94/Official_Review","forum":"SyyGPP0TZ","replyto":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference/Paper94/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper proposes regularization strategies for word LSTM-based language models achieving state-of-the-art results on the PennTree Bank and the Wiki-Text2 LM tasks. A variant of  Average-SGD (ASGD) where the threshold for determining which iterates to average is also proposed, yielding better results than SGD.","rating":"7: Good paper, accept","review":"This is a well-written paper that proposes regularization and optimization strategies for word-based language modeling tasks.   The authors propose the use of DropConnect  on the hidden-hidden connections as a regularization method, in order to take advantage of high-speed LSTM implementations via the cuDNN LSTM libraries from NVIDIA.  The  focus of this work is on the prevention of overfitting on the recurrent connections of the LSTM.  The authors explore a variant of Average-SGD (NT-ASGD) as an optimization strategy which eliminates the need for tuning the average trigger and uses a constant learning rate.  Averaging is triggered when the validation loss worsens or stagnates for a few cycles, leading to two new hyper parameters: logging interval and non-monotone interval.  Other forms of well-know regularization methods were applied to the non-recurrent connections, input, output and embedding matrices.  \n\nAs the authors point out, all the methods used in this paper have been proposed before and theoretical convergence explained. The novelty of this work lies in its successful application to the language modeling task achieving state-of-the-art results.\n\nOn the PTB task, the proposed AWD-LSTM achieves a perplexity of 57.3 vs 58.3 (Melis et al 2017) and almost the same perplexity as Melis et el. on the Wiki-Text2 task (65.8 vs 65.9).  The addition of a cache model provides significant gains on both tasks.   \n\nIt would be useful, if authors had explored the behavior of the  AWD-LSTM algorithm with respect to various hyper parameters  and provided a few insights towards their choices for other large vocabulary language modeling tasks (1 million vocabulary sizes).  \n\nSimilarly, the choice of the average trigger and number of cycles seem arbitrary -  it would have been good to see a graph over a range of values, showing their impact on the model's performance.\n\nA 3-layer LSTM has been used for the experiments  - how was this choice made?  What is the impact of this algorithm if the net was a 2-layer net as is typical in most large-scale LMs?\n\nTable 3 is interesting to see how the cache model helps with rare words  and as such has applications in key word spotting tasks. Were the hyper parameters of the cache tuned to perform better on rare words?  More details on the design of the cache model would have been useful.\n\nYou state that the gains obtained using the cache model were far less than what was obtained in Graves et al 2016 - what do you attribute this to?\n\nAblation analysis in Table 4 is very useful - in particular it shows how lack of regularization of the recurrent connections can lead to maximum degradation in performance.\n\nMost of the results in this paper have been based on one choice of various model parameters. Given the emperical nature of this work, it would have made the paper even clearer if an analysis of their choices were presented.  Overall, it would be beneficial to the MLP community to see this paper accepted in the conference.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]}},{"tddate":null,"ddate":null,"tmdate":1512222823643,"tcdate":1511824021877,"number":2,"cdate":1511824021877,"id":"ByC55Gcxz","invitation":"ICLR.cc/2018/Conference/-/Paper94/Official_Review","forum":"SyyGPP0TZ","replyto":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference/Paper94/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Evaluation of a number of heuristics to improve LSTM language models.","rating":"7: Good paper, accept","review":"Clearly presented paper, including a number of reasonable techniques to improve LSTM-LMs. The proposed techniques are heuristic, but are reasonable and appear to yield improvements in perplexity. Some specific comments follow.\n\nre. \"ASGD\" for Averaged SGD: ASGD usually stands for Asynchronous SGD, have the authors considered an alternative acronym? AvSGD?\n\nre. Optimization criterion on page 2, note that SGD is usually taken to minimizing expected loss, not just empirical loss (Bottou thesis 1991).\n\nIs there any theoretical analysis of convergence for Averaged SGD?\n\nre. paragraph starting with \"To prevent such inefficient data usage, we randomly select the sequence length for the forward and backward pass in two steps\": the explanation is a bit unclear. What is the \"base sequence length\" exactly? Also, re. the motivation above this paragraph, I'm not sure what \"elements\" really refers to, though I can guess.\n\nWhat is the number of training tokens of the datasets used, PTB and WT2?\n\nCan the authors provide more explanation for what \"neural cache models\" are, and how they relate to \"pointer models\"?\n\nWhy do the sections \"Pointer models\", \"Ablation analysis\", and \"AWD-QRNN\" come after the Experiments section?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]}},{"tddate":null,"ddate":null,"tmdate":1512222823685,"tcdate":1511536939006,"number":1,"cdate":1511536939006,"id":"HyQVY2Bxz","invitation":"ICLR.cc/2018/Conference/-/Paper94/Official_Review","forum":"SyyGPP0TZ","replyto":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference/Paper94/AnonReviewer3"],"readers":["everyone"],"content":{"title":"State of the art on small word level language modelling datasets","rating":"7: Good paper, accept","review":"The paper sets a new state of the art on word level language modelling on the Penn Treebank and Wikitext-2 datasets using various optimization and regularization techniques. These already very good results are further improved, by a large margin, using a Neural Cache.\n\nThe paper is well written, easy to follow and the results speak for themselves. One possible criticism is that the experimental methodology does not allow for reliable conclusions to be drawn about contributions of all different techniques, because they seem to have been evaluated at a single hyperparameter setting (that was hand tuned for the full model?).\n\nA variant on the Averaged SGD method is proposed. This so called NT-ASGD optimizer switches to averaging mode based on recent validation losses. I would have liked to see a more thorough assessment of NT-ASGD, especially against well tuned SGD.\n\nI particularly liked Figure 3 which shows how the Neural Cache makes the model much better at handling rare words and UNK (!) at the expense of very common words. Speaking of the Neural Cache, a natural baseline would have been dynamic evaluation.\n\nAll in all, the paper is a solid contribution which deserves to be accepted. It could become even better, were the experiments to tease the various factors apart.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]}},{"tddate":null,"ddate":null,"tmdate":1509739489421,"tcdate":1508960007207,"number":94,"cdate":1509739486763,"id":"SyyGPP0TZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}