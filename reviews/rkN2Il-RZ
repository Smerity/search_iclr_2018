{"notes":[{"tddate":null,"ddate":null,"tmdate":1512354367009,"tcdate":1512354367009,"number":3,"cdate":1512354367009,"id":"H1vrGEM-G","invitation":"ICLR.cc/2018/Conference/-/Paper593/Official_Review","forum":"rkN2Il-RZ","replyto":"rkN2Il-RZ","signatures":["ICLR.cc/2018/Conference/Paper593/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good paper, but some pieces are missing","rating":"6: Marginally above acceptance threshold","review":"Summary\n---\nThis paper proposes a new model called SCAN (Symbol-Concept Association Network) for hierarchical concept learning. It trains one VAE on images then another one on symbols and aligns their latent spaces. This allows for symbol2image and image2symbol inference. But it also allows for generalization to new concepts composed from existing concepts using logical operators. Experiments show that SCAN generates images which correspond to provided concept labels and span the space of concepts which match these labels.\n\nThe model starts with a beta-VAE trained on images (x) from the relevant domain (in this case, simple scenes generated from DeepMind Lab which vary across a few known dimensions). This is complemented by the SCAN model, which is a beta-VAE trained to reconstruct symbols (y; k-hot encoded concepts like {red, suitcase}) with a slightly modified objective. SCAN optimizes the ELBO plus a KL term which pushes the latent distribution of the y VAE toward the latent distribution of the x (image) VAE. This aligns the latent representations so now a symbol can be encoded into a latent distribution z and decoded as an image.\n\nOne nice property of the learned latent representation is that more specific concepts have more specific latent representations. Consider latent distributions z1 and z2 for a more general symbol {red} and a more specific symbol {red, suitcase}. Fewer dimensions of z2 have high variance than dimensions of z1. For example, the latent space could encode red and suitcase in two dimensions (as binary attributes). z1 would have high variance on all dimensions but the one which encodes red and z2 would have high variance on all dimensions but red and suitcase. In the reported experiments some of the dimensions do seem to be interpretable attributes (figure 5 right).\n\nSCAN also pays particular attention to hierarchical concepts. Another very simple model (1d convolution layer) is learned to mimic logical operators. Normally a SCAN encoder takes {red} as input and the decoder reconstructs {red}. Now another model is trained that takes \"{red} AND {suitcase}\" as input and reconstructs {red, suitcase}. The two input concepts {red} and {suitcase} are each encoded by a pre-trained SCAN encoder and then those two distributions are combined into one by a simple 1d convolution module trained to implement the AND operator (or IGNORE/IN COMMON). This allows images of concepts like {small, red, suitcase} to be generated even if small red suitcases are not in the training data.\n\nExperiments provide some basic verification and analysis of the method:\n1) Qualitatively, concept samples are correct and diverse, generating images with all configurations of attributes not specified by the input concept.\n2) As SCAN sees more diverse examples of a concept (e.g. suitcases of all colors instead of just red ones) it starts to generate more diverse image samples of that concept.\n3) SCAN samples/representations are more accurate (generate images of the right concept) and more diverse (far from a uniform prior in a KL sense) than JMVAE and TELBO baselines.\n4) SCAN is also compared to SCAN_U, which uses an image beta-VAE that learned an entangled (Unstructured) representation. SCAN_U performed worse than SCAN\nand baselines.\n5) Concepts expressed as logical combinations of other concepts generalize well for both the SCAN representation and the baseline representations.\n\n\nStrengths\n---\n\nThe idea of concept learning considered here is novel and satisfying. It imposing logical, hierarchical structure on latent representations in a general way. This suggests opportunities for inserting prior information and adds interpretability to the latent space.\n\n\nWeaknesses\n---\n\nI think this paper is missing some important evaluation.\n\nRole/Nature of Disentangled Features not Clear (major):\n\n* Disentangled features seem to be very important for SCAN to work well (SCAN vs SCAN_U). It seems that the only difference between the unstructured (entangled) and the structured (disentangled) visual VAE is the color space of the input (RGB vs HSV). If so, this should be stated more clearly in the main paper. What role did beta-VAE (tuning beta) as opposed to plain VAE play in learning disentangled features?\n\n* What color space was used for the JMVAE and TELBO baselines? Training these with HSV seems especially important for establishing a good comparison, but it would be good to report results for HSV and RGB for all models.\n\n* How specific is the HSV trick to this domain? Would it matter for natural images?\n\n* How would a latent representation learned via supervision perform? (Maybe explicitly align dimensions of z to red/suitcase/small with supervision through some mechanism. c.f. \"Discovering Hidden Factors of Variation in Deep Networks\" by Cheung et al.)\n\nEvaluation of sample complexity (major):\n\n* One of the main benefits of SCAN is that it works with less training data. There should be a more systematic evaluation of this claim. In particular, I would like to see a Number of Examples vs Performance (Accuracy/Diversity) plot for both SCAN and the baselines.\n\nMinor questions/comments/concerns:\n\n* What do the logical operators learn that the hand-specified versions do not?\n\n* Does training SCAN with the structure provided by the logical operators lead to improved performance?\n\n* There seems to be a mistake in figure 5 unless I interpreted it incorrectly. The right side doesn't match the left side. During the middle stage of training object hues vary on the left, but floor color becomes less specific on the right. Shouldn't object color become less specific?\n\n\nPrelimary Evaluation\n---\n\nThis clear and well written paper describes an interesting and novel way of learning a model of hierarchical concepts. It's missing some evaluation that would help establish the sample complexity benefit more precisely (a claimed contribution) and add important details about unsupervised disentangled representations. I would happy to increase my rating if these are addressed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SCAN: Learning Hierarchical Compositional Visual Concepts","abstract":"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","pdf":"/pdf/019d3d150db4cc7483e5b168eab91c35ab4214dd.pdf","TL;DR":"We present a neural variational model for learning language-guided compositional visual concepts.","paperhash":"anonymous|scan_learning_hierarchical_compositional_visual_concepts","_bibtex":"@article{\n  anonymous2018scan:,\n  title={SCAN: Learning Hierarchical Compositional Visual Concepts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkN2Il-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper593/Authors"],"keywords":["grounded visual concepts","compositional representation","concept hierarchy","disentangling","beta-VAE","variational autoencoder","deep learning","generative model"]}},{"tddate":null,"ddate":null,"tmdate":1512275866349,"tcdate":1512275866349,"number":2,"cdate":1512275866349,"id":"rkzoyZW-M","invitation":"ICLR.cc/2018/Conference/-/Paper593/Official_Review","forum":"rkN2Il-RZ","replyto":"rkN2Il-RZ","signatures":["ICLR.cc/2018/Conference/Paper593/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting idea, but limited experimental evaluation","rating":"6: Marginally above acceptance threshold","review":"This paper introduces a VAE-based model for translating between images and text. The main way that their model differs from other multimodal methods is that their latent representation is well-suited to applying symbolic operations, such as AND and IGNORE, to the text. This gives them a more expressive language for sampling images from text.\n\nPros:\n- The paper is well written, and it provides useful visualizations and implementation details in the appendix.\n\n- The idea of learning compositional representations inside of a VAE framework is very appealing.\n\n- They provide a modular way of learning recombination operations.\n\nCons:\n- The experimental evaluation is limited. They test their model only on a simple, artificial dataset. It would also be helpful to see a more extensive evaluation of the model's ability to learn logical recombination operators, since this is their main contribution.\n\n- The approach relies on first learning a pretrained visual VAE model, but it is unclear how robust this is. Should we expect visual VAEs to learn features that map closely to the visual concepts that appear in the text? What happens if the visual model doesn't learn such a representation? This again could be addressed with experiments on more challenging datasets.\n\n- The paper should explain the differences and trade offs between other multimodal VAE models (such as their baselines, JMVAE and TrELBO) more clearly. It should also clarify differences between the SCAN_U baseline and SCAN in the main text.\n\n- The paper suggests that using the forward KL-divergence is important, but this does not seem to be tested with experiments.\n\n- The three operators (AND, IN COMMON, and IGNORE) can easily be implemented as simple transformations of a (binary) bag-of-words representation. What about more complex operations, such as OR, which seemingly cannot be encoded this way?\n\nOverall, I am borderline on this paper, due to the limited experimental evaluation, but lean slightly towards acceptance.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SCAN: Learning Hierarchical Compositional Visual Concepts","abstract":"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","pdf":"/pdf/019d3d150db4cc7483e5b168eab91c35ab4214dd.pdf","TL;DR":"We present a neural variational model for learning language-guided compositional visual concepts.","paperhash":"anonymous|scan_learning_hierarchical_compositional_visual_concepts","_bibtex":"@article{\n  anonymous2018scan:,\n  title={SCAN: Learning Hierarchical Compositional Visual Concepts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkN2Il-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper593/Authors"],"keywords":["grounded visual concepts","compositional representation","concept hierarchy","disentangling","beta-VAE","variational autoencoder","deep learning","generative model"]}},{"tddate":null,"ddate":null,"tmdate":1512222695361,"tcdate":1511938455642,"number":1,"cdate":1511938455642,"id":"BkeoFCjgG","invitation":"ICLR.cc/2018/Conference/-/Paper593/Official_Review","forum":"rkN2Il-RZ","replyto":"rkN2Il-RZ","signatures":["ICLR.cc/2018/Conference/Paper593/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A neural network that learns visual concepts and basic operators over them.","rating":"5: Marginally below acceptance threshold","review":"This paper proposed a novel neural net architecture that learns object concepts by combining a beta-VAE and SCAN. The SCAN is actually another beta-VAE with an additional term that minimizes the KL between the distribution of its latent representation and the first beta-VAE’s latent distribution. The authors also explored how this structure could be further expanded to incorporate another neural net that learns operators (and, in common, ignore), and demonstrated that the proposed system is able to generate accurate and diverse scenes given the visual descriptions.\n\nIn general, I think this paper is interesting. It’s studying an important problem with a newly proposed neural net structure. The experimental results are good and the model is compared with very recent baselines.\n\nI am, however, still lukewarm on this submission for its limited technical innovation and over-simplified experimental setup.\n\nThis paper does have technical innovations: the SCAN architecture and the way they learn “recombination operators” are newly proposed. However, there are in essence very straightforward extensions of VAE and beta-VAE (this is based on the fact that beta-VAE itself is a simple modification of VAE and the effect was discussed in a number of concurrent papers).\n\nThis would still be fine, as many small modifications of neural net architecture turn out to reveal fundamental insights that push the field forward. This is, however, not the case in this paper (at least not in the current manuscript) due to its over-simplified experiments. The authors are using images as input, but the images are all synthetic, and further, they are all synthesized to have highly regular structure. This suggests the network is likely to overfit the data and learn a straightforward mapping from input to the code. It’s unclear how well the system is able to generalize to real-world scenarios. Note that even datasets like MNIST has much higher complexity than the dataset used in this paper (though the dataset in this paper is more colorful).\n\nI agree that the proposed method performs better that its recent competitors. However, many of those methods like TripleELBO are not explicitly designed for these ‘recombination operators’. In contrast, they seem to perform well on real datasets. I would strongly suggest the authors perform additional experiments on standard benchmarks for a fair comparison.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SCAN: Learning Hierarchical Compositional Visual Concepts","abstract":"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","pdf":"/pdf/019d3d150db4cc7483e5b168eab91c35ab4214dd.pdf","TL;DR":"We present a neural variational model for learning language-guided compositional visual concepts.","paperhash":"anonymous|scan_learning_hierarchical_compositional_visual_concepts","_bibtex":"@article{\n  anonymous2018scan:,\n  title={SCAN: Learning Hierarchical Compositional Visual Concepts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkN2Il-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper593/Authors"],"keywords":["grounded visual concepts","compositional representation","concept hierarchy","disentangling","beta-VAE","variational autoencoder","deep learning","generative model"]}},{"tddate":null,"ddate":null,"tmdate":1509739212795,"tcdate":1509127851938,"number":593,"cdate":1509739210138,"id":"rkN2Il-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkN2Il-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SCAN: Learning Hierarchical Compositional Visual Concepts","abstract":"The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts.","pdf":"/pdf/019d3d150db4cc7483e5b168eab91c35ab4214dd.pdf","TL;DR":"We present a neural variational model for learning language-guided compositional visual concepts.","paperhash":"anonymous|scan_learning_hierarchical_compositional_visual_concepts","_bibtex":"@article{\n  anonymous2018scan:,\n  title={SCAN: Learning Hierarchical Compositional Visual Concepts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkN2Il-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper593/Authors"],"keywords":["grounded visual concepts","compositional representation","concept hierarchy","disentangling","beta-VAE","variational autoencoder","deep learning","generative model"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}