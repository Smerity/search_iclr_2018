{"notes":[{"tddate":null,"ddate":null,"tmdate":1516410389550,"tcdate":1516410389550,"number":6,"cdate":1516410389550,"id":"H1RMUzgBM","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Comment","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper162/Authors"],"content":{"title":"More Updates","comment":"We updated figure 1 in the paper to show a diagram for the other added baselines in this work (fine-tuning).\nThe figures in the paper have been updated, showing the results over 5 runs of each method. The findings are very similar.\nTable 1 has also been updated with data for the MultiTasker. This table now better illustrates the effects distillation on \"forgetting\"."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515874042989,"tcdate":1515874042989,"number":5,"cdate":1515874042989,"id":"SJmZPJd4z","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Comment","forum":"B13njo1R-","replyto":"ByMViwPef","signatures":["ICLR.cc/2018/Conference/Paper162/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper162/AnonReviewer3"],"content":{"title":"Revision to review","comment":"I find the additions to the paper satisfactory and will increase my score accordingly."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515186668720,"tcdate":1515186668720,"number":3,"cdate":1515186668720,"id":"BkSxcvaQM","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Comment","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper162/Authors"],"content":{"title":"Summary of revisions, and additional baseline","comment":"We have uploaded a revised version of the paper. Changes are currently highlighted in green. \nHere is a summary of the changes:\n(1) Clarified our goal for PLAID as a continual learning method, while also evaluating its effectiveness as a multi-task solution method, and comparing to multi-task benchmarks.\n(2) Updated Figure 2 making it more readable and understandable.\n(3) Added text pertaining to noted related work by reviewers.\n(4) Clarified how the distillation method is used in PLAiD with DAGGER\n  (a) At most 2 experts are used over a set of tasks\n  (b) Start by selecting actions from the expert policies and anneal the probability down, leading to more actions being selected \n   from the new student policy\n(5) Clearer description of why and how feature injection is used.\n  (a) Included a diagram showing how and where the new network parameters are added into the network.\n  (b) This injection is performed when the policy is learning how to differentiate between the flat and incline tasks.\n  (c) Additional figure visualizing the state features.\n(6) TL_Only (fine-tuning) comparison -- see Section 8.4\n  (a) Ran an additional baseline (average over 5 runs) where TL is done sequentially between tasks WITHOUT distillation. \n  (b) Found that TL_Only can also learn new tasks quickly, however the TL_Only method suffers from increased forgetting of \n   previously learned tasks. The final distillation to merge all the expert policies together proves to be much more challenging \n   than the use of simpler, progressive distillations. \n  (c) This method can be considered a version of PLAiD where tasks are learned in groups and after some number of tasks a \n     collection of policies/skills are distilled together.\n  (d) Still to be done:  adding the TL_Only baseline to Figure 1.\n(7) Multiple runs\n  We have completed 5 runs for TL_Only and PLAID, with no surprises. The remaining two baselines are still in progress. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1514419856987,"tcdate":1514419856987,"number":4,"cdate":1514419856987,"id":"HkY9Ln-7G","invitation":"ICLR.cc/2018/Conference/-/Paper162/Public_Comment","forum":"B13njo1R-","replyto":"ByMViwPef","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for the interest, questions, and suggestions!","comment":"We believe that there is much to be explored for progressive learning and distillation of continuous action tasks, as exemplified by our control problems.\n\nRe:  during distillation, do we start from random policy or expert policy?\nThe networks were initialized from the most recently trained policy, i.e., the one trained on the new task.\n\nRe: data used for distillation;  do all previous expert policies need to be kept in memory?\nWe have added the following paragraph to the paper Appendix to address this.\n   For each of the distillation steps we initialize the policy from the most recently trained policy. This policy has seen all of the tasks thus far but may have overfit the most recent tasks. We us a version of the DAGGER algorithm for the distillation process (Ross et al., 2010). We anneal from select- ing actions from the expert policies to selecting actions from the student policy The probability of selecting an action from the expert is annealed to near zero after 10, 000 training updates. We still add exploration noise to the policies when generating actions to take in the simulation. This is also annealed along with the probability of selecting from the expert policy. The actions used for training always come from the expert policy. Although some actions are applied in the simulation from the student, during a training update those actions will be replaced with ones from the proper expert. The expert used to generate actions for tasks 0 − i is πi and the expert used to generate action for task i + 1 is πi+1. We keep around at most 2 policies at any time.\n\nRe: purpose of input injection\nWe have added further details and explanations to the paper. Specifically, the “flat” walking is not provided with information about the upcoming terrain, while other policies (e.g., incline, steps, slopes, gaps) are provided with extra inputs (a linear height map) of the upcoming terrain.  Input injection allows for the “flat” walking policy to be used as the starting point for policies that have these additional inputs.\n\n\n\nRe: comparisons with prior work\nWe compare the PLAID method to three other baselines, two of which are in the paper (MultiTasker, Parallel-Learn-then-Distill), and an additional baseline that we will soon have completed in response to the feedback (Successive Transfers then Distill).  \nIn what follows below, we comment further on other specific previous work.\nProgressive nets: While this is only tested on discrete actions, the idea itself is orthogonal to this issue. However, the existing baselines using DeepRL are not very applicable for different reasons. For the progressive net, you will get a set of experts in one net but the net itself does not know which expert to choose when it is given a task, i.e., which head of the network to choose.\nAttend Adapt and Transfer:  This method explains how to combine K experts in learning task T_i but it is not obvious how it would extend to learning T_i+1.\nLifelong Learning in Minecraft: This relies on options with clear end definitions in sequential tasks. It is not directly obvious how this could be applied to our domain with continuously varying terrain and continuous actions.\nDistral:  This trains multiple policies in parallel, rather than one policy continually, and so it is largely captured by one of our baselines. Also, we note that the method is specific to discrete actions in its KL regularization term.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1514419733252,"tcdate":1514419733252,"number":3,"cdate":1514419733252,"id":"B1pGI2Zmf","invitation":"ICLR.cc/2018/Conference/-/Paper162/Public_Comment","forum":"B13njo1R-","replyto":"H1BAcZ9eG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks for the review. We appreciate the various comments and questions!","comment":"Our work is (to the best of our knowledge) among the first to study, with a detailed evaluation, multi-task and continual learning on problems with continuous action spaces using deep learning.  \n\n\nRe: for sequences of related tasks, transfer learning, e.g., fine-tuning, is “known to work well”\n⇒ We are computing an additional “Successive Transfers then Distill” benchmark to show that this is not the case here. This learns the tasks, in sequence, followed by a final distillation step, and thus without the progressive intermediate distillation steps used in PLAID. Our current results for this benchmark show a significant benefit for PLAID.  For comments regarding other potential baselines, please also see our replies to the other reviewers.\n\nRe: single runs\nYes, currently these are single runs. We are re-running all simulations 5 times in order to provide more sound comparisons regarding the relative merit of the methods. We will post these in the coming days. There are no surprises in the results to date.\n\n\nRe: zero-shot performance\nAlthough not directly discussed in the paper, the zero-shot performance can be seen by looking at the first iterations of the training graphs. In most cases there is some zero-shot performance, as we hope would be the case, given the transfer we are hoping to achieve between tasks. However, further training greatly improves the performance of each of the tasks.\n\nRe: network architecture, network size \nWe focused on designing the PLAiD method and minimal architecture tuning was performed. The network architecture is based on that used in the following paper: “Learning Locomotion Skills Using DeepRL: Does the choice of action space matter?”.\n\nRe: Figure 1(c) confusion\nPerformance is maintained across all previous tasks (and not just the two most recent). We will clarify this in the text.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1514419676994,"tcdate":1514419676994,"number":2,"cdate":1514419676994,"id":"ryByL2WmG","invitation":"ICLR.cc/2018/Conference/-/Paper162/Public_Comment","forum":"B13njo1R-","replyto":"ByFImz_ZG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for the thorough review, questions, suggestions, and shared interest in the problem at hand!","comment":"  We believe that there is much to be explored for progressive learning and distillation of continuous action tasks, as exemplified by our control problems. \n\nRe: Figure 2\nWe will make these figures larger and enhance the explanations.\nAs suggested, they do represent a sequence, moving from (a) to (b) to (c) to (d).\nThe cost of distillation is accounted for by giving an equal number of simulation time steps to both PLAiD and the MultiTasker. For example if we give the MultiTasker 300k iterations, for PLAID we may use 250k for transfer and 50k for distillation. We show this in Figure 2 by colouring the TL phase in green and the distillation phase in red.\n\nRe: Frequent distillation vs fine-tuning a model through all stages plus a final distillation step\n\nThis is an excellent idea for an additional baseline, and we are currently running this “Successive Transfers then Distill” baseline. We will notify the reviewers of the results, as well as update the paper. The results for this new baseline thus far look very similar to that of the MultiTasker; it has a more difficult time learning the additional tasks. We also note that we seek a method that would work when the agent is given new tasks it does not know are coming. For example, after learning to walk on “incline” terrain, the agent does not know about the next three (steps, slopes and gaps), but we want the agent to best prepared to learn it’s next skill, whatever it may be.\n\nRe: details on input injection\nWe have added further details and explanations to the paper. Specifically, the “flat” walking is not provided with information about the upcoming terrain, while other policies (e.g., incline, steps, slopes, gaps) are provided with extra inputs (a linear height map) of the upcoming terrain.  Input injection allows for the “flat” walking policy to be used as the starting point for policies that have these additional inputs.\n\nRe:  role of PLAID: continual learning  vs  multitask-solution with maximum transfer\nWe view PLAID as a continual learning method, in that we consider the problem of not knowing all tasks beforehand and want to learn any new task as easily/quickly as possible.\nHowever, it is also proves surprisingly effective as a multitask solution, given the three specific benchmarks that we compare against.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642401046,"tcdate":1512739664812,"number":3,"cdate":1512739664812,"id":"ByFImz_ZG","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Review","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Interesting","rating":"5: Marginally below acceptance threshold","review":"Hi, \n\nThis was a nice read. I think overall it is a good idea. But I find the paper lacking a lot of details and to some extend confusing. \nHere are a few comments that I have:\n\nFigure 2 is very confusing for me. Please first of all make the figures much larger. ICLR does not have a strict page limit, and the figures you have are hard to impossible to read. So you train in (a) on the steps task until 350k steps? Is (b), (d),(c) in a sequence or is testing moving from plain to different things? The plot does not explicitly account for the distillation phase. Or at least not in an intuitive way. But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted. \n\nGoing then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines. Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all. So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough). \n\nWhere exactly is input injection used? Is it experiments from figure 3. What input is injecting? What do you do when you go back to the task that doesn't have the input, feed 0? What happens if 0 has semantics ? \n\nPlease say in the main text that details in terms of architecture and so on are given in the appendix. And do try to copy a bit more of them in the main text where reasonable. \n\nWhat is the role of PLAID? Is it to learn a continual learning solution? So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills? Will this be feasible? Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning? \n \nOr is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)?  And even then maximize transfer with respect to what? Frames required from the environment? If that are you reusing the frames you used during training to distill? Can we afford to keep all of those frames around? If not we have to count the distillation frames as well. Also more baselines are needed. A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way.  Or at least have a good argument of why this is suboptimal compared to PLAID. \n\nI think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score. But I want to make sure the authors put a bit more effort into cleaning up the paper, making it more clear and easy to read. Providing at least one more baseline (if not more considering the other things cited by them). \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642401084,"tcdate":1511819981446,"number":2,"cdate":1511819981446,"id":"H1BAcZ9eG","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Review","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice continual learning study","rating":"7: Good paper, accept","review":"This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum.\n\nPros:\n- PLAID masters several distinct tasks in sequence, building up “skills” by learning “related” tasks of increasing difficulty.\n- Although the main focus of this paper is on continual learning of “related” tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain.\n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case.\n- Are the experiments single runs? Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour.\n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks?\n- How were the network architecture and network size chosen, especially for the multitasker? Would policies generalize to later tasks better with larger, or smaller networks?\n- Was any kind of regularization used, how does it influence task performance vs. transfer?\n- I find figure 1 (c) somewhat confusing. Is performance maintained only on the last 2 tasks, or all previously seen tasks? That’s what the figure suggests at first glance, but that’s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1515874073605,"tcdate":1511648042339,"number":1,"cdate":1511648042339,"id":"ByMViwPef","invitation":"ICLR.cc/2018/Conference/-/Paper162/Official_Review","forum":"B13njo1R-","replyto":"B13njo1R-","signatures":["ICLR.cc/2018/Conference/Paper162/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good approach, needs more details.","rating":"7: Good paper, accept","review":"This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially. The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task. The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously. \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?\n- What data do you use for the distillation? Section 4.1 states\"We use a method similar to the DAGGER algorithm\", but what is your method. If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?\n- I do not understand the purpose of \"input injection\" nor where it is used in the paper. \n\nStrengths:\n- The method is simple but novel. The results support the method's utility.\n- The testbed is nice; the tasks seem significantly different from each other. It seems that no reward shaping is used.\n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.\n\nWeaknesses:\n- Figure 2: the plots are too small.\n- Distilling may hurt performance ( Figure 2.d)\n- The method lacks details (see Questions above)\n- No comparisons with prior work are provided. The paper cites many previous approaches to this but does not compare against any of them. \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch. \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work. The method is clear but not precisely described. The results are promising. I think that this is a good approach to the problem that could be used in real-world scenarios. With some filling out, this could be a great paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]}},{"tddate":null,"ddate":null,"tmdate":1516410118889,"tcdate":1509043124096,"number":162,"cdate":1509739448397,"id":"B13njo1R-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B13njo1R-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control","abstract":"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems,\nincluding agents that can move with skill and agility through their environment. \nAn open problem in this setting is that of developing good strategies for integrating or merging policies\nfor multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. \nWe extend policy distillation methods to the continuous action setting and leverage this technique to combine \\expert policies,\nas evaluated in the domain of simulated bipedal locomotion across different classes of terrain.\nWe also introduce an input injection method for augmenting an existing policy network to exploit new input features.\nLastly, our method uses transfer learning to assist in the efficient acquisition of new skills.\nThe combination of these methods allows a policy to be incrementally augmented with new skills.\nWe compare our progressive learning and integration via distillation (PLAID) method\nagainst three alternative baselines.","pdf":"/pdf/14f6bafdc2b991100d172a06271801a3ed434c01.pdf","TL;DR":"A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.","paperhash":"anonymous|progressive_reinforcement_learning_with_distillation_for_multiskilled_motion_control","_bibtex":"@article{\n  anonymous2018progressive,\n  title={Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B13njo1R-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper162/Authors"],"keywords":["Reinforcement Learning","Distillation","Transfer Learning","Continual Learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}