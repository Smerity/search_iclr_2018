{"notes":[{"tddate":null,"ddate":null,"tmdate":1515738442207,"tcdate":1515738385617,"number":4,"cdate":1515738385617,"id":"ry5GSRHNf","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Comment","forum":"BkwHObbRZ","replyto":"rk0Ek5vgM","signatures":["ICLR.cc/2018/Conference/Paper697/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper697/Authors"],"content":{"title":"revision","comment":"Thanks for the review again!\n\nWe apologize that we didn't know that the paper was expected to be updated. We just added the results for sigmoid that answers the question \"does sigmoid suffer from the same problem?\" as we claimed in the response before. Please see page 9, figure 2 in the current version. \n\nWe will revise the paper with more intuitions/explanations as promised in the previous response as soon as possible. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1513892733833,"tcdate":1513892733833,"number":3,"cdate":1513892733833,"id":"HkLFssYfM","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Comment","forum":"BkwHObbRZ","replyto":"rk0Ek5vgM","signatures":["ICLR.cc/2018/Conference/Paper697/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper697/Authors"],"content":{"title":"response","comment":"Thanks for the review and comments.\n\nResponse to your questions:\n\n--- \"empirical fix seems sufficient\": We do consider the proposal of the empirical fix as part of the contribution of the paper. It's driven by the theoretical analysis of the squared loss (more intuition below), and it's novel as far as we know.\n\nThat said, we think it's also valuable to pursue a provably nice loss function without bad local minima. Note that there is no known method to empirically verify whether a function has no bad local minima, and such statements can only be established by proofs. Admittedly our first-cut design of the loss function is complicated and sub-optimal in terms of sample complexity, we do hope that our technique can inspire better loss function and model design in the future.\n\n--- \"does sigmoid suffer from the same problem?\": we did experiment with the sigmoid activation and found that the sigmoid activation function also has bad local minima. We will add this experiment to the next version of the paper. We conjecture that the activation h_2 +h_4 or the activation 1/2 |z|  has no spurious local minima.\n\n--- \"actual insights about the landscape\": Our insights is that the squared loss objective function is trying to perform an infinite number of tensor decomposition problems simultaneously, and the mixing of all of these problems very likely creates bad local minima, as we empirically observed. The intuition behind the empirical fix is that removing some of these tensor decomposition problems would make the landscape simpler and nicer.\n\n--- \"comparison with over-parameterization\":  over-parameterization is indeed a powerful way to remove the bad local minima, and it gives models with good prediction. But it doesn't recover the parameters of the true model because the training parameters space is larger. Our method is guaranteed to recover the true parameters of the model, which in turns guarantees a ``complete\" generalization to any unseen examples, even including e.g., adversarial examples or test examples drawn from another distribution. In this sense, our approaches (both the empirical fix and theoretical one) return solutions with stronger guarantees than over-parameterization can provide.\n\nOf course, it's also a very important open problem to understand better other alternatives to landscape design such as over-parametrization.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1513892403423,"tcdate":1513892403423,"number":2,"cdate":1513892403423,"id":"H1jVcsFMf","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Comment","forum":"BkwHObbRZ","replyto":"SyfsN8tef","signatures":["ICLR.cc/2018/Conference/Paper697/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper697/Authors"],"content":{"title":"response","comment":"Thanks for the comments.\nRegarding the questions:\n\n1) Yes, since we mostly focus on the ReLU activation, we assume that the rows of B^* have unit norms. For ReLU activation, the row norms are inherently unidentifiable.\n\n2) The technical version of the landscape analysis in Theorem B.1 specifies the precise dependencies of the landscape properties on the dimension, etc. To get a convergence rate, one can combine our Theorem B.1 with an analysis of gradient descent or other algorithms on non-convex functions. The best-known analysis for SGD in Ge et al. 2015 does not specify the precise polynomial dependencies. Since developing stochastic algorithms (beyond SGD) with lower iteration complexity is an active area of research, the best-known convergence rate is constantly changing.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1513892225478,"tcdate":1513892225478,"number":1,"cdate":1513892225478,"id":"BkFFKsKfz","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Comment","forum":"BkwHObbRZ","replyto":"SJjI7pKlz","signatures":["ICLR.cc/2018/Conference/Paper697/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper697/Authors"],"content":{"title":"response","comment":"Thanks for the comments. We will add more intuitions in the paper and fix the typos.\n\nThe high-level intuition is that the squared loss objective function is trying to perform an infinite number of tensor decomposition problems simultaneously and the mixing of all of these problems very likely creates bad local minima, as we empirically observed. Thus we design an objective function that selects only two of these tensor decompositions problems, which empirically removes the bad local minima. Finally, we design an objective function that resembles more a 4th order tensor decomposition objective in Ge et al. (2015), which are known to be good."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1515642493580,"tcdate":1511801683398,"number":3,"cdate":1511801683398,"id":"SJjI7pKlz","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Review","forum":"BkwHObbRZ","replyto":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference/Paper697/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An interesting tensor factorization-type method for learning one hidden-layer neural network","rating":"7: Good paper, accept","review":"This paper proposes a tensor factorization-type method for learning one hidden-layer neural network. The most interesting part is the Hermite polynomial expansion of the activation function. Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem. They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015. At last, they also establish the sample complexity for recovery.\n\nThe organization and presentation of the paper need some improvement. For example, the authors defer many technical details. To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages.\n\nThere are also some typos: For example, the dimension of a is inconsistent. In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector. On Page 8, P(B) should be a degree-4 polynomial of B.\n\nThe paper does not contains any experimental results on real data.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1515642493622,"tcdate":1511773338173,"number":2,"cdate":1511773338173,"id":"SyfsN8tef","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Review","forum":"BkwHObbRZ","replyto":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference/Paper697/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting paper and solid contribution: Accept.","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper studies the problem of learning one-hidden layer neural networks and is a theory paper. A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent. This paper establishes an interesting connection between least squares population loss and Hermite polynomials. Following from this connection authors propose a new loss function. Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix. Simulations confirm the findings.\n\nOverall, pretty interesting result and solid contribution. The paper also raises good questions for future works. For instance, is designing alternative loss function useful in practice? In summary, I recommend acceptance. The paper seems rushed to me so authors should polish up the paper and fix typos.\n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows? If so they should clarify this otherwise it confuses the reader a bit.\n2) What can be said about rate of convergence in terms of network parameters? Currently a generic bound is employed which is not very insightful in my opinion.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1515642493660,"tcdate":1511657270853,"number":1,"cdate":1511657270853,"id":"rk0Ek5vgM","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Review","forum":"BkwHObbRZ","replyto":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference/Paper697/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Overall an interesting heuristic approach to solving SGD problems on one particular type of data, with limited practical applications","rating":"6: Marginally above acceptance threshold","review":"[ =========================== REVISION ===============================================================]\nI am satisfied with the answers to my questions. The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating. However I am fine accepting it.\n[ ============================== END OF REVISION =====================================================]\n\nThis paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net). Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization). First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD\n\nOverall the paper is well written. The authors first introduce their suggested loss function and then go into details about what inspired its creation. I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful\n\nMy issues with the paper are as follows:\n- The loss function designed seems overly complicated. On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used. I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework\n- I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima. Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched. \n- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem\n- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better.\n\nMinor: fix margins in formula 2.7. \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1515738175828,"tcdate":1509132350987,"number":697,"cdate":1509739151736,"id":"BkwHObbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/86c29bad02770ad4ec75b97bbcec8c5963fb39b3.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}