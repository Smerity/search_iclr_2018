{"notes":[{"tddate":null,"ddate":null,"tmdate":1515781526915,"tcdate":1515781526915,"number":7,"cdate":1515781526915,"id":"BykoTdLNG","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Comment","forum":"SyjjD1WRb","replyto":"BJaIxMSGz","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper484/AnonReviewer2"],"content":{"title":"RE: benchmarks","comment":"Just to clarify my position, I would suggest that any \"proof of concept\" paper DOES need to position itself clearly with respect to other approaches. You want your reader to walk away with clear understand of when you use your approach and why.\n\nEven a \"conceptual\" positioning would be a step in the right direction, such as clearly explaining an instance where your method would have lower runtime than an alternative E-step, or avoid local optima better than an alternative.\n\nThis is not to say you need some exhaustive benchmark table. To me, it would be totally fine if you had a few results where your method was better, and some where it wasn't, as long as there was clear understanding of when your method improves on baselines and why.\n\nGood luck on future submissions!\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515780924821,"tcdate":1515780924821,"number":6,"cdate":1515780924821,"id":"B1HSi_8Vf","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Comment","forum":"SyjjD1WRb","replyto":"ByuaozrzM","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper484/AnonReviewer2"],"content":{"title":"Response to Author Feedback and Other Reviews","comment":"Thanks to the authors for their reply. For this review cycle, I stand by my original rating of \"4\".  I think the broad idea of using EA as a substep within a monotonically improving free energy algorithm could be interesting, but needs far more experimental justification than presented here as well as more insightful suggestions about how to select the best EA procedure (use crossover? use mutations?) for new problems.\n\nI'm glad to hear that clarifications about hyperparameters and the definition of logP are on the TODO list. These are definitely needed to help others understand and deploy this method effectively."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1513595410430,"tcdate":1513595410430,"number":4,"cdate":1513595410430,"id":"SkqzzmSzG","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Comment","forum":"SyjjD1WRb","replyto":"ryWjGsdef","signatures":["ICLR.cc/2018/Conference/Paper484/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper484/Authors"],"content":{"title":"title should be amended, as suggested. replies to other comments","comment":"We agree that the title as-is (which only consists of the given name of the novel learning algorithm here developed) can be misleading in that this paper has smaller scope than a full replacement of standard variational EM techinques. It will therefore be amended to reflect the applicability of the method uniquely to models with binary latent variables.\n\nThe crossover is performed on one-dimensional bit-strings (the latent states), so we are not sure a 2D crossover would benefit the algorithm.\n\nWe posted another comment in reply to the other issues raised here."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1513593792299,"tcdate":1513593792299,"number":3,"cdate":1513593792299,"id":"ByuaozrzM","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Comment","forum":"SyjjD1WRb","replyto":"SyHYDG5lf","signatures":["ICLR.cc/2018/Conference/Paper484/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper484/Authors"],"content":{"title":"clarification of the issues raised","comment":"Many thanks for the thorough review.\n\nRegarding hyperparameter sensitivity, our experience suggests that EEM is robust w.r.t. changes in all hyperparameters (within reasonable bounds), the most significant hyperparameter being the size of the sets of latent states K. Indeed, the paper would benefit from the addition of a systematic study of hyperparameter sensitivity. Future revisions will  add such a section (most likely to the appendix).\n\nRegarding the choice of fitness function, as discussed in the paper the only requirement is that it satisfies (6) (and, pragmatically, that it is cheap to compute). Any such fitness function would guarantee that adopting states with higher fitness would also increase the free energy (our true objective function).\nNonetheless, we will add a more rigorous definition of \\tilde{log P} in future revisions, as well as amend equation (7) -- the minimum should be taken over all states in the set K^n, not over all possible states. That second factor on the right hand side of (7) is only there to guarantee positivity of the fitness function (necessary for fitness-proportional parent selection).\n\nWe also agree with the reviewer that a generic procedure to select the best EA for a given generative model is desirable. In fact, it would be best if this procedure was automatic and did not require user intervention. Work in this direction is underway.\nImproving our understanding of the difference in performance of the different EAs for different models will also be part of future work on this topic.\n\nWe posted another comment in reply to the issues raised here that are common with the other reviewers."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1513591248919,"tcdate":1513591248919,"number":2,"cdate":1513591248919,"id":"ByFAWfSGz","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Comment","forum":"SyjjD1WRb","replyto":"S15xOyjgf","signatures":["ICLR.cc/2018/Conference/Paper484/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper484/Authors"],"content":{"title":"will amend the title to reflect applicability to latent variables only","comment":"We agree that the title as-is (which only consists of the given name of the novel learning algorithm here developed) can be misleading in that this paper has smaller scope than a full replacement of standard variational EM techinques. It will therefore be amended to reflect the applicability of the method uniquely to models with binary latent variables.\n\nWe posted another comment in reply to the other issues raised here."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1513599359432,"tcdate":1513590868854,"number":1,"cdate":1513590868854,"id":"BJaIxMSGz","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Comment","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper484/Authors"],"content":{"title":"regarding the absence of competitive benchmarks","comment":"We thank the reviewers for their comments. As several of the points raised appear to be similar for all reviewers, we will address them here together. Other more specific points will be clarified in their relevant comment thread.\nAs underlined by abstract and discussion, we see the main contribution of this paper to be the development of a variational EM learning algorithm that directly employs evolutionary optimization techniques to monotonically increase a variational free-energy. Experimental results are provided as a proof of concept to show 1) general viability of the approach and 2) scalability up to hundreds of latent variables (also for models with challenging posterior structure such as noisy-OR).\nThe authors hope the theoretical link established here will pave the way to a wider range of new  techniques which can leverage results on both variational and evolutionary approaches. We stressed that the presented results are \"a proof of concept\" (see abstract), and they were as such not meant to compete with current benchmarks, nor did we focus our research on achieving such results at this stage. We are happy that all reviewers appreciated the general novel direction, i.e., novel combination of different research fields. At the same time, we are, of course, disappointed that the absence of numerical results that were competitive with recent benchmarks was rated so negatively.\nWe will follow the feedback of the reviewers to improve that shortcoming of the paper in future versions. Given the exceptional agreement of the reviewers on the current version, such efforts are presumably better targeted at a submission to another venue - and we thank the reviewers again for their feedback which, we believe, will improve such future submissions."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642455343,"tcdate":1511876593596,"number":3,"cdate":1511876593596,"id":"S15xOyjgf","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Review","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting combination of evolutionary algorithms and EM; however, key technical details are missing and evaluation is contrived","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an evolutionary algorithm for solving the variational E step in expectation-maximization algorithm for probabilistic models with binary latent variables. This is done by (i) considering the bit-vectors of the latent states as genomes of individuals, and by (ii) defining the fitness of the individuals as the log joint distribution of the parameters and the latent space.\n \nPros:\nThe paper is well written and the methodology presented is largely clear.\n\nCons:\nWhile the reviewer is essentially fine with the idea of the method, the reviewer is much less convinced of the empirical study. There is no comparison with other methods such as Monte carlo sampling.\nIt is not clear how computationally Evolutionary EM performs comparing to Variational EM algorithm and there is neither experimental results nor analysis for the computational complexity of the proposed model.\nThe datasets used in the experiments are quite old. The reviewer is concerned that these datasets may not be representative of real problems.\nThe applicability of the method is quite limited. The proposed model is only applicable for the probabilistic models with binary latent variables, hence it cannot be applied to more realistic complex model with real-valued latent variables.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642455387,"tcdate":1511823228902,"number":2,"cdate":1511823228902,"id":"SyHYDG5lf","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Review","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer2"],"readers":["everyone"],"content":{"title":"lacks comparisons to non-EA approaches, hard to gain insight on how to design the EA substep for new applications","rating":"4: Ok but not good enough - rejection","review":"## Review summary\n\nOverall, the paper makes an interesting effort to tightly integrate\nexpectation-maximization (EM) training algorithms with evolutionary algorithms\n(EA). However, I found the technical description lacking key details and the\nexperimental comparisons inadequate. There were no comparisons to non-\nevolutionary EM algorithms, even though they exist for the models in question.\nFurthermore, the suggested approach lacks a principled way to select\nand tune key hyperparameters. I think the broad idea of using EA as a substep\nwithin a monotonically improving free energy algorithm could be interesting,\nbut needs far more experimental justification.\n\n\n## Pros / Stengths\n\n+ effort to study more than one model family\n\n+ maintaining monotonic improvement in free energy\n\n\n## Cons / Limitations\n\n- poor technical description and justification of the fitness function\n\n- lack of comparisons to other, non-EA algorithms\n\n- lack of study of hyperparameter sensitivity\n\n\n## Paper summary\n\nThe paper suggests a variant of the EM algorithm for binary hidden variable\nmodels, where the M-step proceeds as usual but the E-step is different in two\nways. First, following work by J. Lucke et al on Truncated Posteriors, the\ntrue posterior over the much larger space of all possible bit vectors is\napproximated by a more tractable small population of well-chosen bit vectors,\neach with some posterior weight. Second, this set of bit vectors is updated\nusing an evolutionary/genetic algorithm. This EA is the core contribution,\nsince the work on Trucated Posteriors has appeared before in the literature.\nThe overall EM algorithm still maintains monotonic improvement of a free\nenergy objective.\n\nTwo well-known generative models are considered: Noisy-Or models for discrete\ndatasets and Binary Sparse Coding for continuous datasets. Each has a\npreviously known, closed-form M-step (given in supplement). The focus is on\nthe E-step: how to select the H-dimensional bit vector for each data point.\n\nExperiments on artificial bars data and natural image patch datasets compare\nseveral variants of the proposed method, while varying a few EA method\nsubsteps such as selecting parents by fitness or randomly, including crossover\nor not, or using generic or specialized mutation rates.\n\n\n## Significance\n\nCombining evolutionary algorithms (EA) within EM has been done previously, as\nin Martinez and Vitria (Pattern Recog. Letters, 2000) or Pernkopf and\nBouchaffra (IEEE TPAMI, 2005) for mixture models. However, these efforts seem\nto use EA in an \"outer loop\" to refine different runs of EM, while the present\napproach uses EA in a substep of a single run of EM. I guess this is\ntechnically different, but it is already well known that any E-step method\nwhich monotonically improves the free energy is a valid algorithm. Thus, the\npaper's significance hinges on demonstrating that the particular E step chosen\nis better than alternatives. I don't think the paper succeeded very well at\nthis: there were no comparisons to non-EA algorithms, or to approaches that\nuse EA in the \"outer loop\" as above.\n\n\n## Clarity of Technical Approach\n\nWhat is \\tilde{log P} in Eq. 7? This seems a fundamental expression. Its\nplain-text definition is: \"the logarithm of the joint probability where\nsummands that do not depend on the state s have been elided\". To me, this\ndefinition is not precise enough for me to reproduce confidently... is it just\nlog p(s_n, y_n | theta)? I suggest revisions include a clear mathematical\ndefinition. This omission inhibits understanding of this paper's core\ncontributions.\n\nWhy does the fitness expression F defined in Eq. 7 satisfy the necessary\ncondition for fitness functions in Eq. 6? This choice of fitness function does\nnot seem intuitive to me. I think revisions are needed to *prove* this fitness\nfunction obeys the comparison property in Eq. 6.\n\nHow can we compute the minimization substep in Eq. 7 (min_s \\tilde{logP})? Is\nthis just done by exhaustive search over bit vectors? I think this needs\nclarification.\n\n\n## Quality of Experiments\n\nThe experiments are missing a crucial baseline: non-EA algorithms. Currently\nonly several varieties of EA are compared, so it is impossible to tell if the\nsuggested EA strategies even improve over non-EA baselines. As a specific\nexample, previous work already cited in this paper -- Henniges et al (2000) --\nhas developed a non-EA EM algorithm for Binary Sparse Coding, which already\nuses the truncated posterior formulation. Why not compare to this?\n\nThe proposed algorithm has many hyperparameters, including number of\ngenerations, number of parents, size of the latent space H, size of the\ntruncation, etc. The current paper offers little advice about selecting these\nvalues intelligently, but presumably performance is quite sensitive to these\nvalues. I'd like to see some more discussion of this and (ideally) more\nexperiments to help practitioners know which parameters matter most,\nespecially in the EA substep.\n\nRuntime analysis is missing as well: Is runtime dominated by the EA step? How\ndoes it compare to non-EA approaches? How big of datasets can the proposed\nmethod scale to?\n\nThe reader walks away from the current toy bars experiment somewhat confused.\nThe Noisy-Or experiment did not favor crossover and and favored specialized\nmutations, while the BSC experiment reached the opposite conclusions. How does\none design an EA for a new dataset, given this knowledge? Do we need to\nexhaustively try all different EA substeps, or are there smarter lessons to\nlearn?\n\n\n\n## Detailed comments\n\nBottom of page 1: I wouldn't say that \"variational EM\" is an approximation to\nEM. Sometimes moving from EM to variational EM can mean we estimate posteriors\n(not point estimates) for both local (example-specific) and global parameters.\nInstead, the *approximation* comes simply from restricting the solution space\nto gain tractability.\n\nSec. 2: Make clear earlier that hidden var \"s\" is assumed to be discrete, not\ncontinuous.\n\nAfter Mutation section: Remind readers that \"N_g\" is number of generations\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515642455427,"tcdate":1511727768991,"number":1,"cdate":1511727768991,"id":"ryWjGsdef","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Review","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Very specialised evolutionary EM algorithm","rating":"4: Ok but not good enough - rejection","review":"The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation.\n\nThe scope of the paper is quite narrow as the proposed method is only applicable to very specialised models. Furthermore, the authors do not seem to present any realistic modelling problems where the proposed approach would clearly advance the state of the art. There are no empirical comparisons with state of the art, only between different variants of the proposed method.\n\nBecause of these limitations, I do not think the paper can be considered for acceptance.\n\nDetailed comments:\n\n1. When revising the paper for next submission, please make the title more specific. Papers with very broad titles that only solve a very small part of the problem are very annoying.\n\n2. Your use of crossover operators seems quite unimaginative. Genomes have a linear order but in the case of 2D images you use it is not obvious how that should be mapped to 1D. Combining crossovers in different representations or 2D crossovers might fit your problem much better.\n\n3. Please present a real learning problem where your approach advances state of the art.\n\n4. For the results in Fig. 7, please run the algorithm until convergence or justify why that is not necessary.\n\n5. Please clarify the notation: what is the difference between y^n and y^(n)?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1515001149100,"tcdate":1509124003501,"number":484,"cdate":1509739274570,"id":"SyjjD1WRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Evolutionary Expectation Maximization for Generative Models with Binary Latents","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/9a0e69b40eaaf268ec6baee744a45a4245948f45.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization_for_generative_models_with_binary_latents","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}