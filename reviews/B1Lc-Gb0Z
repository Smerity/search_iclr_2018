{"notes":[{"tddate":null,"ddate":null,"tmdate":1512242191610,"tcdate":1512242191610,"number":3,"cdate":1512242191610,"id":"BkOfh_eWM","invitation":"ICLR.cc/2018/Conference/-/Paper779/Official_Review","forum":"B1Lc-Gb0Z","replyto":"B1Lc-Gb0Z","signatures":["ICLR.cc/2018/Conference/Paper779/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice discussion and solution to optimizing neural networks with hard thresholds but with some flaws","rating":"7: Good paper, accept","review":"The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it. The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy. The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems. However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem. These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such. Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently. They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet. They show superiority of their algorithm over SSTE. \n\nI thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds. The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting. The results are moderately convincing in favor of the proposed approach. Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem. Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community. \n\nThere are a few flaws/weaknesses in the paper though, making it somewhat lose. \n- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1. Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier. \n- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees.  \n- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition\n- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach. The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Learning as a Mixed Convex-Combinatorial Optimization Problem","abstract":"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","pdf":"/pdf/bcfb650cc2ee428be01803b4f5ecbda6953488f3.pdf","TL;DR":"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet.","paperhash":"anonymous|deep_learning_as_a_mixed_convexcombinatorial_optimization_problem","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Learning as a Mixed Convex-Combinatorial Optimization Problem},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Lc-Gb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper779/Authors"],"keywords":["hard-threshold units","combinatorial optimization","target propagation","straight-through estimation","quantization"]}},{"tddate":null,"ddate":null,"tmdate":1512222763789,"tcdate":1511808692190,"number":2,"cdate":1511808692190,"id":"Byn3CAYlM","invitation":"ICLR.cc/2018/Conference/-/Paper779/Official_Review","forum":"B1Lc-Gb0Z","replyto":"B1Lc-Gb0Z","signatures":["ICLR.cc/2018/Conference/Paper779/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well-organized analysis of the hard-threshold networks","rating":"7: Good paper, accept","review":"This paper examines the problem of optimizing deep networks of hard-threshold units. This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks. While none of the contributions are especially novel, the analysis is clear and well-organized, and the authors do a nice job in connecting their analysis to other work. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Learning as a Mixed Convex-Combinatorial Optimization Problem","abstract":"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","pdf":"/pdf/bcfb650cc2ee428be01803b4f5ecbda6953488f3.pdf","TL;DR":"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet.","paperhash":"anonymous|deep_learning_as_a_mixed_convexcombinatorial_optimization_problem","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Learning as a Mixed Convex-Combinatorial Optimization Problem},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Lc-Gb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper779/Authors"],"keywords":["hard-threshold units","combinatorial optimization","target propagation","straight-through estimation","quantization"]}},{"tddate":null,"ddate":null,"tmdate":1512222763832,"tcdate":1511735162705,"number":1,"cdate":1511735162705,"id":"SJ7YJpueM","invitation":"ICLR.cc/2018/Conference/-/Paper779/Official_Review","forum":"B1Lc-Gb0Z","replyto":"B1Lc-Gb0Z","signatures":["ICLR.cc/2018/Conference/Paper779/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting way of explaining and generalizing approaches for learning neural nets with hard activation","rating":"7: Good paper, accept","review":"The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x). Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches. Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation. More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z).\n\nThe paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen. Deciding on these targets is formulated as a combinatorial optimization problem. Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization. The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation. At each layer, the targets can be chosen using a variety of search algorithms such as beam search.\n\nExperiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks.\n\nThis paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions. The experimental results are encouraging, though I have a few questions below that prevent me for now from rating the paper higher.\n\nComments and questions:\n\n1) How computationally expensive is FTP? The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details.\n\n2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks? I do not see a comparison. If that is so, should this paper also compare with STE? How do we know if generalizing saturated STE is more worthwhile than generalizing STE?\n\n3) It took me a while to understand the authors' subtle comparison with target propagation, where they say \"Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets, whereas previous approaches employed continuous optimization.\" It seems that the difference is greater than explicitly stated, that prior target propagation used continuous optimization to set *continuous targets*. (One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem.) Focusing on discrete targets gains the benefits of quantized networks. If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear.\n\n4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation?\n\n5) In Table 1, the significance of the last two columns is unclear. It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good). I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Learning as a Mixed Convex-Combinatorial Optimization Problem","abstract":"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","pdf":"/pdf/bcfb650cc2ee428be01803b4f5ecbda6953488f3.pdf","TL;DR":"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet.","paperhash":"anonymous|deep_learning_as_a_mixed_convexcombinatorial_optimization_problem","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Learning as a Mixed Convex-Combinatorial Optimization Problem},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Lc-Gb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper779/Authors"],"keywords":["hard-threshold units","combinatorial optimization","target propagation","straight-through estimation","quantization"]}},{"tddate":null,"ddate":null,"tmdate":1509739107652,"tcdate":1509134734120,"number":779,"cdate":1509739104989,"id":"B1Lc-Gb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1Lc-Gb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Learning as a Mixed Convex-Combinatorial Optimization Problem","abstract":"As neural networks grow deeper and wider, learning networks with hard-threshold activations is becoming increasingly important, both for network quantization, which can drastically reduce time and energy requirements, and for creating large integrated systems of deep networks, which may have non-differentiable components and must avoid vanishing and exploding gradients for effective learning. However, since gradient descent is not applicable to hard-threshold functions, it is not clear how to learn them in a principled way. We address this problem by observing that setting targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem, and can be solved as such. The discrete optimization goal is to find a set of targets such that each unit, including the output, has a linearly separable problem to solve. Given these targets, the network decomposes into individual perceptrons, which can then be learned with standard convex approaches. Based on this, we develop a recursive mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. Empirically, we show that our algorithm improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet, when compared to the straight-through estimator.","pdf":"/pdf/bcfb650cc2ee428be01803b4f5ecbda6953488f3.pdf","TL;DR":"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet.","paperhash":"anonymous|deep_learning_as_a_mixed_convexcombinatorial_optimization_problem","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Learning as a Mixed Convex-Combinatorial Optimization Problem},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1Lc-Gb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper779/Authors"],"keywords":["hard-threshold units","combinatorial optimization","target propagation","straight-through estimation","quantization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}