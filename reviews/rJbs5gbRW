{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642479668,"tcdate":1511808329167,"number":3,"cdate":1511808329167,"id":"r1bLTRKxG","invitation":"ICLR.cc/2018/Conference/-/Paper617/Official_Review","forum":"rJbs5gbRW","replyto":"rJbs5gbRW","signatures":["ICLR.cc/2018/Conference/Paper617/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An analysis paper with only 4 citations.","rating":"3: Clear rejection","review":"The paper studies the effect of different network structures (plain CNN, ResNet and DenseNet). This is an interesting line of research to pursue, however, it gives an impression that a large amount of recent work in this direction has not been considered by the authors. The paper contains ONLY 4 references. \n\nSome references that might be useful to consider in the paper:\n- K. Greff et. al. Highway and Residual Networks learn Unrolled Iterative Estimation.\n- C. Zang et. al. UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION\n- Q. Liao el. al. Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex\n- A. Veit et. al. Residual Networks Behave Like Ensembles of Relatively Shallow Networks\n- K. He at. Al Identity Mappings in Deep Residual Networks\n\nThe writing and the structure of the paper could be significantly improved. From the paper, it is difficult to understand the contributions. From the ones listed in Section 1, it seems that most of the contributions were shown in the original ResNet and DenseNet papers. Given, questionable contribution and a lack of relevant citations, it is difficult to recommend for acceptance of the paper. \n\nOther issues:\nSection 2: “Skip connection …. overcome the overfitting”, could the authors comment on this a bit more or point to relevant citation?  \nSection 2: “We increase the number of skip connections from 0 to 28”, it is not clear to me how this is done.\nSection 3.1.1 “deep Linear model”, what the authors mean with this? Multiple layers without a nonlinearity? Is it the same as Cascade Net?\nSection 3.2 From the data description, it is not clear how the training data was obtained. Could the authors provide more details on this?\nSection 3.2 “…, only 3 of them are chosen to be displayed…”, how the selection process was done?\nSection 3.2 “Instead of showing every layer’s output we exhibit the 3th, 5th, 7th, 9th, 11th, 13th and the final layer’s output”, according to the description in Fig. 7 we should be able to see 7 columns, this description does not correspond to Fig. 7.\nSection 4 “This paper investigates how skip connections works in vision tasks…” I do not find experiments with vision datasets in the paper. In order to claim this, I would encourage the authors to run tests on a CV benchmark dataset (e. g. ImageNet)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Generalization Effects of DenseNet Model Structures ","abstract":"Modern neural network architectures take advantage of increasingly deeper layers, and various advances in their structure to achieve better performance. While traditional explicit regularization techniques like dropout, weight decay, and data augmentation are still being used in these new models, little about the regularization and generalization effects of these new structures have been studied. \nBesides being deeper than their predecessors, could newer architectures like ResNet and DenseNet also benefit from their structures' implicit regularization properties? \nIn this work, we investigate the skip connection's effect on network's generalization features. Through experiments, we show that certain neural network architectures contribute to their generalization abilities. Specifically, we study the effect that low-level features have on generalization performance when they are introduced to deeper layers in DenseNet, ResNet as well as networks with 'skip connections'. We show that these low-level representations do help with generalization in multiple settings when both the quality and quantity of training data is decreased. ","pdf":"/pdf/6901e6d262707a45229dd350087784d6de993ffb.pdf","TL;DR":"Our paper analyses the tremendous representational power of networks especially with 'skip connections', which may be used as a method  for better generalization.","paperhash":"anonymous|on_the_generalization_effects_of_densenet_model_structures","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Generalization Effects of DenseNet Model Structures },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJbs5gbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper617/Authors"],"keywords":["Skip connection","generalization","gegularization","deep network","representation."]}},{"tddate":null,"ddate":null,"tmdate":1515642479704,"tcdate":1511807008225,"number":2,"cdate":1511807008225,"id":"r1OmdAYxz","invitation":"ICLR.cc/2018/Conference/-/Paper617/Official_Review","forum":"rJbs5gbRW","replyto":"rJbs5gbRW","signatures":["ICLR.cc/2018/Conference/Paper617/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Analysis paper on the generalization effect of skip connections, motivation and contributions are not clear, references are very limited","rating":"3: Clear rejection","review":"This paper analyzes the role of skip connections with respect to generalization in recent architectures such as ResNets or DenseNets. The authors perform an analysis of the performance of ResNets and DenseNets under data scarcity constraints and noisy training samples. They also run some experiments assessing the importance of the number of skip connections in such networks.\n\nThe presentation of the paper could be significantly improved. The motivation is difficult to grasp and the contributions do not seem compelling.\n\nMy main concern is about the contribution of the paper. The hypothesis that skip connections ease the training and improve the generalization has already been highlighted in the ResNet and DenseNet paper, see e.g. [a].\n\n[a] https://arxiv.org/pdf/1603.05027.pdf\n\nMoreover, the literature review is very limited. Although there is a vast existing literature on ResNets, DenseNets and, more generally, skip connections, the paper only references 4 papers. Many relevant papers could be referenced in the introduction as examples of successes in computer vision tasks,  identity mapping initialization, recent interpretations of ResNets/DensetNets, etc.\n\nThe title suggests that the analysis is performed on DenseNet architectures, but experiments focus on comparing both ResNets and DenseNets to sequential convolutional networks and assessing the importance of skip connections.\n\nIn section 3.1. (1st paragraph) proposes adding noise to groundtruth labels; however, in section 3.1.2,. it would seem that noise is added by changing the input images (by setting some pixel channels to 0). Could the authors clarify that? Wouldn’t the noise added to the groundtruth act as a regularizer?\n\nIn section 4, the paper claims to investigate the role of skip connections in vision tasks. However, experiments are performed on MNIST, CIFAR100, a curve fitting problem and a presumably synthetic 2D classification problem. Performing the analysis on computer vision datasets such as ImageNet would be more compelling to back the statement in section 4.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Generalization Effects of DenseNet Model Structures ","abstract":"Modern neural network architectures take advantage of increasingly deeper layers, and various advances in their structure to achieve better performance. While traditional explicit regularization techniques like dropout, weight decay, and data augmentation are still being used in these new models, little about the regularization and generalization effects of these new structures have been studied. \nBesides being deeper than their predecessors, could newer architectures like ResNet and DenseNet also benefit from their structures' implicit regularization properties? \nIn this work, we investigate the skip connection's effect on network's generalization features. Through experiments, we show that certain neural network architectures contribute to their generalization abilities. Specifically, we study the effect that low-level features have on generalization performance when they are introduced to deeper layers in DenseNet, ResNet as well as networks with 'skip connections'. We show that these low-level representations do help with generalization in multiple settings when both the quality and quantity of training data is decreased. ","pdf":"/pdf/6901e6d262707a45229dd350087784d6de993ffb.pdf","TL;DR":"Our paper analyses the tremendous representational power of networks especially with 'skip connections', which may be used as a method  for better generalization.","paperhash":"anonymous|on_the_generalization_effects_of_densenet_model_structures","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Generalization Effects of DenseNet Model Structures },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJbs5gbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper617/Authors"],"keywords":["Skip connection","generalization","gegularization","deep network","representation."]}},{"tddate":null,"ddate":null,"tmdate":1515642479741,"tcdate":1511800753274,"number":1,"cdate":1511800753274,"id":"H1YnyaKgG","invitation":"ICLR.cc/2018/Conference/-/Paper617/Official_Review","forum":"rJbs5gbRW","replyto":"rJbs5gbRW","signatures":["ICLR.cc/2018/Conference/Paper617/AnonReviewer2"],"readers":["everyone"],"content":{"title":"analysing skip connections","rating":"2: Strong rejection","review":"The ms analyses a number of simulations how skip connections effect the generalization of different network architectures. The experiments are somewhat interesting but they appear rather preliminary. To indeed show the claims made, error bars in the graphs would be necessary as well will more careful and more generic analysis. In addition clear hypotheses should be stated. \nThe fact that some behaviour is seen in MNIST or CIFAR in the simulations does not permit conclusion for other data sets. Typically extensive teacher student simulations are required to validly make points. Also formally the paper is not in good shape. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the Generalization Effects of DenseNet Model Structures ","abstract":"Modern neural network architectures take advantage of increasingly deeper layers, and various advances in their structure to achieve better performance. While traditional explicit regularization techniques like dropout, weight decay, and data augmentation are still being used in these new models, little about the regularization and generalization effects of these new structures have been studied. \nBesides being deeper than their predecessors, could newer architectures like ResNet and DenseNet also benefit from their structures' implicit regularization properties? \nIn this work, we investigate the skip connection's effect on network's generalization features. Through experiments, we show that certain neural network architectures contribute to their generalization abilities. Specifically, we study the effect that low-level features have on generalization performance when they are introduced to deeper layers in DenseNet, ResNet as well as networks with 'skip connections'. We show that these low-level representations do help with generalization in multiple settings when both the quality and quantity of training data is decreased. ","pdf":"/pdf/6901e6d262707a45229dd350087784d6de993ffb.pdf","TL;DR":"Our paper analyses the tremendous representational power of networks especially with 'skip connections', which may be used as a method  for better generalization.","paperhash":"anonymous|on_the_generalization_effects_of_densenet_model_structures","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Generalization Effects of DenseNet Model Structures },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJbs5gbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper617/Authors"],"keywords":["Skip connection","generalization","gegularization","deep network","representation."]}},{"tddate":null,"ddate":null,"tmdate":1509739199114,"tcdate":1509128857420,"number":617,"cdate":1509739196460,"id":"rJbs5gbRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJbs5gbRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the Generalization Effects of DenseNet Model Structures ","abstract":"Modern neural network architectures take advantage of increasingly deeper layers, and various advances in their structure to achieve better performance. While traditional explicit regularization techniques like dropout, weight decay, and data augmentation are still being used in these new models, little about the regularization and generalization effects of these new structures have been studied. \nBesides being deeper than their predecessors, could newer architectures like ResNet and DenseNet also benefit from their structures' implicit regularization properties? \nIn this work, we investigate the skip connection's effect on network's generalization features. Through experiments, we show that certain neural network architectures contribute to their generalization abilities. Specifically, we study the effect that low-level features have on generalization performance when they are introduced to deeper layers in DenseNet, ResNet as well as networks with 'skip connections'. We show that these low-level representations do help with generalization in multiple settings when both the quality and quantity of training data is decreased. ","pdf":"/pdf/6901e6d262707a45229dd350087784d6de993ffb.pdf","TL;DR":"Our paper analyses the tremendous representational power of networks especially with 'skip connections', which may be used as a method  for better generalization.","paperhash":"anonymous|on_the_generalization_effects_of_densenet_model_structures","_bibtex":"@article{\n  anonymous2018on,\n  title={On the Generalization Effects of DenseNet Model Structures },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJbs5gbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper617/Authors"],"keywords":["Skip connection","generalization","gegularization","deep network","representation."]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}