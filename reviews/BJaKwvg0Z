{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222615734,"tcdate":1511840622709,"number":3,"cdate":1511840622709,"id":"rJwui85gz","invitation":"ICLR.cc/2018/Conference/-/Paper293/Official_Review","forum":"BJaKwvg0Z","replyto":"BJaKwvg0Z","signatures":["ICLR.cc/2018/Conference/Paper293/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"In this paper, the authors proposed a new method to improve the robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. More specifically, a HyperNetwork is used to take into account the statistical properties of input data and features for computation of statistical adaptive maps. Then, convolution weights of CNNs are filtered with the learned statistical maps . The weights and kernels are collectively optimized for learning of image classification models. The proposed method does not employ additional target detection and rejection algorithms.\n\nThe robustness of the new method is verified using 3 different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods and a black-box attack, with state-of-the-art CNN models trained on the ILSVRC-2012 dataset. \n\nSummary:\n——\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method. The current numerical results demonstrated that the HyperNetwork has better classification results than the normal method, but the improvement does not change with the noise level. Therefore, it is not very convincing that the new method is more robust to adversarial examples. Please see comments below for details and other points.\n\nComments:\n——\n1.\tIn the introduction, the authors probably mixed classification with detection. Classification wouldn’t be used in autonomous driving but rather detection. See, e.g., work in https://arxiv.org/abs/1707.03501 on detection. \n\n2.\tTable 1 showed that the performance of the new network is better than the previous classification methods. However, the differences between normal and SHC approach remain roughly identical for various noise levels; why is the network more robust then? Could the authors show some experimental results that with increasing noise level the performance gap becomes larger between the normal and SHC?  Below the table, in “Gaussian noise”, the improvement in the performance is due to the effect of different network architecture.  However, it is not necessarily more robust to noise. \n\n3.\tIn the following paragraph for “Gradient based attack” test, the authors select 5,000 validation images that are correctly classified by both reference networks for each comparison pairs. Why `for each comparison pair’? This seems very selective and very biased. A fair assessment does not seem possible in this case. Also, why only assessing the images that are classified with high confidence? What happens to the other ones? How would the results change?\n\n4.\tIn the caption of Table 3, why do the authors select 500 images that are initially correctly classified with high confidence (> 0.9) instead of showing the results for the whole dataset? This seems to be biased.\n\n\nMinor Comments:\n——\n1.\tThere are some grammatical errors, for example, the first sentence in Section 4.2, “The results in Table 2 appears that.” \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HyperNetworks with statistical filtering for defending adversarial examples","abstract":"Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification. This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks. However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms. In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels. Thereby, weights and kernels are collectively optimized for learning of image classification models robust to\nadversarial attacks without employment of additional target detection and rejection algorithms.\nWe empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).","pdf":"/pdf/330bf465e6c02e4667d027c4942074c420b34bdf.pdf","TL;DR":"We modified the CNN using HyperNetworks and observed better robustness against adversarial examples.","paperhash":"anonymous|hypernetworks_with_statistical_filtering_for_defending_adversarial_examples","_bibtex":"@article{\n  anonymous2018hypernetworks,\n  title={HyperNetworks with statistical filtering for defending adversarial examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJaKwvg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper293/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222615773,"tcdate":1511809614883,"number":2,"cdate":1511809614883,"id":"rJPUMJ9gz","invitation":"ICLR.cc/2018/Conference/-/Paper293/Official_Review","forum":"BJaKwvg0Z","replyto":"BJaKwvg0Z","signatures":["ICLR.cc/2018/Conference/Paper293/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Unjustified application of existing technique (hypernetwork) in defending CNN against adversarial attacks.","rating":"4: Ok but not good enough - rejection","review":"This paper proposed to improve the robustness and reliability of deep convolution neural networks (with regard to adversarial  attacks such as noise or perturbations) by using data-dependent  convolution kernels. Basically the authors designed a new type of hypernetwork that takes as input the statistical properties of the input images (such as the mean and the covariance) and generates a dynamic template that is used to rectify some shared kernel parameters. Empirical results are reported on some benchmark images with adversarial attacks (Gaussian noise, gradient based attackes, etc.). \n\nThe proposed method basically employs the idea of hypernetwork, in which case a smaller network (which contains the structural information) is used to generate the parameters of a larger, main network. The proposed method bears a strong resemblance with prior work: the Dynamic filter approach by De Brabandere et. al, and the hypernetwork by Ha et. al. It appears to me that the most important discussions on why the application of hyper-network can help combacting adversarial attacks, are missing. Note that the input to the hypernetwork is the mean and the variance of input images, which are very rough descriptions and it is difficult to expect such statistics could fine-tune the learning of the convolution kernels. For example, in case of Gaussian noise with zero means, the sample mean might not change and so the attacks are actually unidentifiable by feeding the statstics to the hypernetwork. More elaborate discussions on why statistics-based hypernetworks can help in defending convolution networks are needed to judge the validity of the proposed method. \n\nOverall the paper is more like a simple application of existing approach (hypernetwok) in the problems of defending neural networks against attacks. The basic motivation and design principle are not stated clearly; the structure of the hypernetwork seems to be inadequate in capturing important structures in the data or the attacks (which can vary a lot), and so from conceptual level it is unclear why the proposed method should work.   ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HyperNetworks with statistical filtering for defending adversarial examples","abstract":"Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification. This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks. However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms. In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels. Thereby, weights and kernels are collectively optimized for learning of image classification models robust to\nadversarial attacks without employment of additional target detection and rejection algorithms.\nWe empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).","pdf":"/pdf/330bf465e6c02e4667d027c4942074c420b34bdf.pdf","TL;DR":"We modified the CNN using HyperNetworks and observed better robustness against adversarial examples.","paperhash":"anonymous|hypernetworks_with_statistical_filtering_for_defending_adversarial_examples","_bibtex":"@article{\n  anonymous2018hypernetworks,\n  title={HyperNetworks with statistical filtering for defending adversarial examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJaKwvg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper293/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222615814,"tcdate":1511522182483,"number":1,"cdate":1511522182483,"id":"SJAYJKHgG","invitation":"ICLR.cc/2018/Conference/-/Paper293/Official_Review","forum":"BJaKwvg0Z","replyto":"BJaKwvg0Z","signatures":["ICLR.cc/2018/Conference/Paper293/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper uses networks whereby the statistics of the inputs to each convolutional layer go through a hypernetwork that modifies the kernel then applied to the inputs. These networks are more robust to adversarial attacks.\n\nThe paper isn't written too badly, although there are plenty of grammatical and spelling errors, some of which are listed below:\n\n\"a highly confident misclassification results\"\n\"a sufficient part or the whole training dataestes\"\n\"have less or none robustness toward\"\n\"can be expressed as a series transformations\"\n\"it is worthy mentioning\"\n\"we evaluate the robustness of network to\"\n\"method and it's variants\"\n\"this shed light on\"\n\nIt may seem like I'm being pedantic, but it really does make for a better read to correct things like these.\n\nSome comments in roughly chronological order:\n\nThe related work section is good, although it may be worth splitting the two parts with paragraph headers.\n\nThe caption for Figure 1 is rather terse. A description of the \"H\", \"M\", \"W\" etc. would be helpful.\n\nWhy did you choose 2 layers, and D/2 neurons in the hidden layer for your hypernetworks? Did you try anything else?\n\nAt the end of 2.2 you discuss the different gradient paths, and in section 4.2 you mention attacking each path separately. From this, I assume for the bulk of your experiments in section 3 you are using the full gradient. This isn't entirely obvious so should be explicitly stated so as to not cause confusion.\n\nI don't agree that Gaussian Noise is the most commonly used attack type. It is frequently used as a form of data augmentation and most neural networks are robust to it. When you do add it it is in massive quantities and it obvious the image has been changed (the images on the right of figure 2) and therefore could be rejected outright. What exactly is the \\epsilon you use to limit a change in pixel value? You do at least acknowledge that humans may/can tell the difference in 3.2.\n\nI'm not entirely sure about the value of RCD but that may be because its changes appear to be much more subtle than those in accuracy.\n\nIn the first line of 3.2 \"ResNest\" should be changes to \"ResNet\".  The results are pretty good. However, gradient based attacks being better than adding noise isn't a surprising observation.\n\nThe discussion section unfortunately lacks discussion. It appears to consist of further experiments.\n\nI believe this paper to be borderline. The results are promising, and the use of hypernetworks is intriguing. However, I am unconvinced by the arguments made beyond it “just working”.\n\nPros:\n------\n- Novel use of hypernetworks\n- Promising results\n- Good literature review\n\nCons:\n--------\n- Far too much focus on Gaussian noise\n- Unconvincing argument/ lacking discussion\n- Lack of clarity\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HyperNetworks with statistical filtering for defending adversarial examples","abstract":"Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification. This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks. However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms. In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels. Thereby, weights and kernels are collectively optimized for learning of image classification models robust to\nadversarial attacks without employment of additional target detection and rejection algorithms.\nWe empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).","pdf":"/pdf/330bf465e6c02e4667d027c4942074c420b34bdf.pdf","TL;DR":"We modified the CNN using HyperNetworks and observed better robustness against adversarial examples.","paperhash":"anonymous|hypernetworks_with_statistical_filtering_for_defending_adversarial_examples","_bibtex":"@article{\n  anonymous2018hypernetworks,\n  title={HyperNetworks with statistical filtering for defending adversarial examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJaKwvg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper293/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739381294,"tcdate":1509091204932,"number":293,"cdate":1509739378641,"id":"BJaKwvg0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJaKwvg0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"HyperNetworks with statistical filtering for defending adversarial examples","abstract":"Deep learning algorithms have been known to be vulnerable to adversarial perturbations in various tasks such as image classification. This problem was addressed by employing several defense methods for detection and rejection of particular types of attacks. However, training and manipulating networks according to particular defense schemes increases computational complexity of the learning algorithms. In this work, we propose a simple yet effective method to improve robustness of convolutional neural networks (CNNs) to adversarial attacks by using data dependent adaptive convolution kernels. To this end, we propose a new type of HyperNetwork in order to employ statistical properties of input data and features for computation of statistical adaptive maps. Then, we filter convolution weights of CNNs with the learned statistical maps to compute dynamic kernels. Thereby, weights and kernels are collectively optimized for learning of image classification models robust to\nadversarial attacks without employment of additional target detection and rejection algorithms.\nWe empirically demonstrate that the proposed method enables CNNs to spontaneously defend against different types of attacks, e.g. attacks generated by Gaussian noise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box attack (Narodytska & Kasiviswanathan, 2016).","pdf":"/pdf/330bf465e6c02e4667d027c4942074c420b34bdf.pdf","TL;DR":"We modified the CNN using HyperNetworks and observed better robustness against adversarial examples.","paperhash":"anonymous|hypernetworks_with_statistical_filtering_for_defending_adversarial_examples","_bibtex":"@article{\n  anonymous2018hypernetworks,\n  title={HyperNetworks with statistical filtering for defending adversarial examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJaKwvg0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper293/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}