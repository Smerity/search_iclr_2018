{"notes":[{"tddate":null,"ddate":null,"tmdate":1512394966994,"tcdate":1512394966994,"number":3,"cdate":1512394966994,"id":"HJykWAMWG","invitation":"ICLR.cc/2018/Conference/-/Paper1074/Official_Review","forum":"Sy3fJXbA-","replyto":"Sy3fJXbA-","signatures":["ICLR.cc/2018/Conference/Paper1074/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Another image classification architecture","rating":"5: Marginally below acceptance threshold","review":"The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel.  The input to each model is a sparse sum of the outputs of modules in the previous set.  The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on.  \n\nPros:\n- The connectivity is constrained to be sparse between modules, and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weights.  Furthermore, this learning extends to large-scale image datasets.\n- There is indeed a boost in classification performance, and the approach shows promise for automatically reducing the number of parameters in the network.\n\nCons:\n- Overall, the approach seems to be an incremental improvement over the previous work ResNeXt.\n- The datasets used are not very interesting: Cifar is too small, and ImageNet is essentially solved.  From the standpoint of the computer vision community, increasing performance on these datasets is no longer a meaningful objective.\n- The modifications add complexity.\n\nThe paper is well written and conceptually simple.  However, I feel the paper demonstrates neither enough novelty nor enough of a performance gain for me to advocate acceptance.   ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Connectivity Learning in Multi-Branch Networks","abstract":"While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that  branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points. \n\nIn this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using four different datasets where it yields consistently higher accuracy compared to the state-of-the-art ``ResNeXt'' multi-branch network given the same learning capacity.","pdf":"/pdf/8d9ca4f54cbd879c63926ba3c6a466f8593c6168.pdf","TL;DR":"In this paper we introduced an algorithm to learn the connectivity of deep multi-branch networks. The approach is evaluated on image categorization where it consistently yields accuracy gains over state-of-the-art models that use fixed connectivity.","paperhash":"anonymous|connectivity_learning_in_multibranch_networks","_bibtex":"@article{\n  anonymous2018connectivity,\n  title={Connectivity Learning in Multi-Branch Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3fJXbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1074/Authors"],"keywords":["connectivity learning","multi-branch networks","image categorization"]}},{"tddate":null,"ddate":null,"tmdate":1512222548414,"tcdate":1512202849763,"number":2,"cdate":1512202849763,"id":"BJ9DfkxWM","invitation":"ICLR.cc/2018/Conference/-/Paper1074/Official_Review","forum":"Sy3fJXbA-","replyto":"Sy3fJXbA-","signatures":["ICLR.cc/2018/Conference/Paper1074/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting paper, not really gating, not justified.","rating":"5: Marginally below acceptance threshold","review":"The paper is clear and well written.\nIt is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.\n\nThis paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the \"gates\" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2)).\n\nThe main contribution appears to be the optimisation procedure for the binary masking tensor g. But this procedure is not justified: does each step minimise the loss? This seems unlikely due to the sampling. Can the authors show that the procedure will always converge? It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Connectivity Learning in Multi-Branch Networks","abstract":"While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that  branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points. \n\nIn this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using four different datasets where it yields consistently higher accuracy compared to the state-of-the-art ``ResNeXt'' multi-branch network given the same learning capacity.","pdf":"/pdf/8d9ca4f54cbd879c63926ba3c6a466f8593c6168.pdf","TL;DR":"In this paper we introduced an algorithm to learn the connectivity of deep multi-branch networks. The approach is evaluated on image categorization where it consistently yields accuracy gains over state-of-the-art models that use fixed connectivity.","paperhash":"anonymous|connectivity_learning_in_multibranch_networks","_bibtex":"@article{\n  anonymous2018connectivity,\n  title={Connectivity Learning in Multi-Branch Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3fJXbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1074/Authors"],"keywords":["connectivity learning","multi-branch networks","image categorization"]}},{"tddate":null,"ddate":null,"tmdate":1512222548457,"tcdate":1511393494923,"number":1,"cdate":1511393494923,"id":"ByyJKKXgz","invitation":"ICLR.cc/2018/Conference/-/Paper1074/Official_Review","forum":"Sy3fJXbA-","replyto":"Sy3fJXbA-","signatures":["ICLR.cc/2018/Conference/Paper1074/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Extension of ResNeXt but at a price of more complicated training protocols","rating":"5: Marginally below acceptance threshold","review":"The authors extend the ResNeXt architecture. They substitute the simple add operation with a selection operation for each input in the residual module. The selection of the inputs happens through gate weights, which are sampled at train time. At test time, the gates with the highest values are kept on, while the other ones are shut. The authors fix the number of the allowed gates to K out of C possible inputs (C is the multi-branch factor in the ResNeXt modules). They show results on CIFAR-100 and ImageNet (as well as mini ImageNet). They ablate the choice of K, the binary nature of the gate weights.\n\nPros:\n(+) The paper is well written and the method is well explained\n(+) The authors ablate and experiment on large scale datasets\n\nCons:\n(-) The proposed method is a simple extension of ResNeXt \n(-) The gains are reasonable, yet not SOTA, and come at a price of more complex training protocols (see below)\n(-) Generalization to other tasks not shown\n\nThe authors do a great job walking us through the formulation and intutition of their proposed approach. They describe their training procedure and their sampling approach for the gate weights. However, the training protocol gets complicated with the introduction of gate weights. In order to train the gate weights along with the network parameters, the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen. This makes training of such networks cumbersome.\n\nIn addition, the authors report a loss in performance when the gates are not discretized to {0,1}. This means that a liner combination with the real-valued learned gate parameters is suboptimal. Could this be a result of suboptimal, possibly compromised training? \n\nWhile the CIFAR-100 results look promising, the ImageNet-1k results are less impressive. The gains from introducing gate weights in the input of the residual modules vanish when increasing the network size. \n\nLast, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks. Have the authors experimented with other tasks, e.g. object detection, to verify that their approach leads to better performance in a more diverse set of problems?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Connectivity Learning in Multi-Branch Networks","abstract":"While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that  branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points. \n\nIn this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using four different datasets where it yields consistently higher accuracy compared to the state-of-the-art ``ResNeXt'' multi-branch network given the same learning capacity.","pdf":"/pdf/8d9ca4f54cbd879c63926ba3c6a466f8593c6168.pdf","TL;DR":"In this paper we introduced an algorithm to learn the connectivity of deep multi-branch networks. The approach is evaluated on image categorization where it consistently yields accuracy gains over state-of-the-art models that use fixed connectivity.","paperhash":"anonymous|connectivity_learning_in_multibranch_networks","_bibtex":"@article{\n  anonymous2018connectivity,\n  title={Connectivity Learning in Multi-Branch Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3fJXbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1074/Authors"],"keywords":["connectivity learning","multi-branch networks","image categorization"]}},{"tddate":null,"ddate":null,"tmdate":1510092381045,"tcdate":1509138200319,"number":1074,"cdate":1510092360132,"id":"Sy3fJXbA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy3fJXbA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Connectivity Learning in Multi-Branch Networks","abstract":"While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that  branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points. \n\nIn this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using four different datasets where it yields consistently higher accuracy compared to the state-of-the-art ``ResNeXt'' multi-branch network given the same learning capacity.","pdf":"/pdf/8d9ca4f54cbd879c63926ba3c6a466f8593c6168.pdf","TL;DR":"In this paper we introduced an algorithm to learn the connectivity of deep multi-branch networks. The approach is evaluated on image categorization where it consistently yields accuracy gains over state-of-the-art models that use fixed connectivity.","paperhash":"anonymous|connectivity_learning_in_multibranch_networks","_bibtex":"@article{\n  anonymous2018connectivity,\n  title={Connectivity Learning in Multi-Branch Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3fJXbA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1074/Authors"],"keywords":["connectivity learning","multi-branch networks","image categorization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}