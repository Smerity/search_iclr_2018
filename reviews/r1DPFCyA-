{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222587431,"tcdate":1511806151429,"number":2,"cdate":1511806151429,"id":"SyJRNAKeG","invitation":"ICLR.cc/2018/Conference/-/Paper193/Official_Review","forum":"r1DPFCyA-","replyto":"r1DPFCyA-","signatures":["ICLR.cc/2018/Conference/Paper193/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting interpretation of regularized Logistic regression but missing details and a very strong assumption","rating":"5: Marginally below acceptance threshold","review":"Authors present a k-shot learning method that is based on generating representations with a pre-trained network and learning a regularized logistic regression using the available data.  The regularised regression is formulated as a MAP estimation problem with the prior estimated from the weights of the original network connected final hidden layer to the logits — before the soft-max layer.  \n\nThe motivation of the article regarding “concepts” is interesting.  It seems especially justified when the training set that is used to train the original network has similar objects as the smaller set that is used for k-shot learning.  Maps shown in Figures 6 and 7 provide good motivation for this approach. \n\nDespite the strong motivation, the article raises some concerns regarding the method. \n1. The assumption about independence of w vectors across classes is a very strong one and as far as I can see, it does not have a sound justification.  The original networks are trained to distinguish between classes.  The weight vectors are estimated with this goal. Therefore, it is very likely that vectors of different classes are highly correlated. Going beyond this assumption also seems difficult.  The proposed model estimates $\\theta^{MAP}$ using only one W matrix, the one that is estimated by training the original network in the most usual way.  In this case, the prior over $\\theta$ would have a large influence on the MAP estimate and setting it properly becomes important.  As far as I can see, there is no good recipe presented in the article for setting this prior. \n2. How is the prior model defined?  It is the most important component of the method while precise details are not provided.  How are the hyperparameters set?  Furthermore, this detail needs to be in the main text. \n3. With the isotropic assumption on the covariance matrix, the main difference between logistic regression, which is regularized by L2 norm and coefficient set proportional to the empirical variance, and the proposed method seems to be the mean vector $\\mu^{MAP}$.  From the details provided in the appendix — which should be in the main text in my opinion — I believe this vector is a combination of the prior and mean of w_c across classes.  If the prior is set to 0, how different is this vector from 0?  Authors should focus on this in my opinion to explain why methods work differently in 1-shot learning.  In the other problems, the results suggest they are pretty much the same. \n4. Authors’ motivation about concepts is interesting however, if the model bases its prediction on mean of w_c vectors over classes, then I am not sure if authors really achieve what they motivate for.  \n5. Results are not very convincing.  If the method was substantially different than baseline, I believe this would have been no problem.  Given the proximity of the proposed method to the baseline with regularised logistic regression, lack of empirical advantage is an issue.  If the proposed model works better in the 1-shot scenario, then authors should delve into it to explain the advantage. \n\nMinor comments: \nEvaluation in an online setting section is unclear.  It needs to be rewritten in my opinion. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discriminative k-shot learning using probabilistic models","abstract":"This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new  classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.","pdf":"/pdf/c98e564bd99e7a5e1ca2dec57f97dfd703ee986f.pdf","TL;DR":"This paper introduces a probabilistic framework for k-shot image classification that achieves state-of-the-art results","paperhash":"anonymous|discriminative_kshot_learning_using_probabilistic_models","_bibtex":"@article{\n  anonymous2018discriminative,\n  title={Discriminative k-shot learning using probabilistic models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1DPFCyA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper193/Authors"],"keywords":["discriminative k-shot learning","probabilistic inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222587471,"tcdate":1511649493128,"number":1,"cdate":1511649493128,"id":"rJ60euDeG","invitation":"ICLR.cc/2018/Conference/-/Paper193/Official_Review","forum":"r1DPFCyA-","replyto":"r1DPFCyA-","signatures":["ICLR.cc/2018/Conference/Paper193/AnonReviewer1"],"readers":["everyone"],"content":{"title":"interesting but perhaps incremental ","rating":"5: Marginally below acceptance threshold","review":"This paper presents a procedure to efficiently do K-shot learning in a classification setting by creating informative priors from information learned from a large, fully labeled dataset.  Image features are learned using a standard convolutional neural network---the last layer form image features, while the last set of weights are taken to be image \"concepts\".  The method treats these weights as data, and uses these data to construct an informative prior over weights for new features.\n\n- Sentence two: would be nice to include a citation from developmental psychology.\n\n- Probabilistic modeling section: treating the trained weights like \"data\" is a good way to convey intuition about your method.  It might be good to clarify some specifics earlier on in the \"Probabilistic Modeling\" paragraph, e.g. how many \"observations\" are associated with this matrix. \n \n- In the second phase, concept transfer, is the only information from the supervised weights the mean and estimated covariance?  For instance, if there are 80 classes and 256 features from the supervised phase, the weight \"data\" model is 80 conditionally IID vectors of length 256 ~ Normal(\\mu, \\Sigma).  The posterior MAP for \\mu and \\Sigma are then used as a prior for weights in the K-shot task.  How many parameters are estimated for \n\n  * gauss iso: mu = 256-length vector, \\sigma = scalar variance value of weights\n  * log reg: mu = 256-length zero vector, \\sigma = scalar variance value of weights\n  * log reg cross val: mu = 256-length zero vector, \\sigma = cross validated value\n\nIf the above is correct, the information boosts K-shot accuracy is completely contained in the 256-length posterior mean vector and the scalar weight variance value?\n\n- Is any uncertainty about \\mu_MAP or \\Sigma_MAP propagated through to uncertainty in the K-shot weights?  If not, would this influence the choice of covariance structure for \\Sigma_MAP? How sensitive are inferences to the choice of Normal inverse-Wishart hyper parameters? \n\n- What do you believe is the source of the mis-calibration in the \"predictied probability vs. proportion of times correct\" plot in Figure 2?  \n\nTechnical: The method appears to be technically correct.\n\nClarity: The paper is pretty clearly written, however some specific details of the method are difficult to understand.\n\nNovel: I am not familiar with K-shot learning tasks to assess the novelty of this approach. \n\nImpact: While the reported results seem impressive and encouraging, I believe this a relatively incremental approach. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discriminative k-shot learning using probabilistic models","abstract":"This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new  classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.","pdf":"/pdf/c98e564bd99e7a5e1ca2dec57f97dfd703ee986f.pdf","TL;DR":"This paper introduces a probabilistic framework for k-shot image classification that achieves state-of-the-art results","paperhash":"anonymous|discriminative_kshot_learning_using_probabilistic_models","_bibtex":"@article{\n  anonymous2018discriminative,\n  title={Discriminative k-shot learning using probabilistic models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1DPFCyA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper193/Authors"],"keywords":["discriminative k-shot learning","probabilistic inference"]}},{"tddate":null,"ddate":null,"tmdate":1509739435356,"tcdate":1509054815036,"number":193,"cdate":1509739432697,"id":"r1DPFCyA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1DPFCyA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discriminative k-shot learning using probabilistic models","abstract":"This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new  classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.","pdf":"/pdf/c98e564bd99e7a5e1ca2dec57f97dfd703ee986f.pdf","TL;DR":"This paper introduces a probabilistic framework for k-shot image classification that achieves state-of-the-art results","paperhash":"anonymous|discriminative_kshot_learning_using_probabilistic_models","_bibtex":"@article{\n  anonymous2018discriminative,\n  title={Discriminative k-shot learning using probabilistic models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1DPFCyA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper193/Authors"],"keywords":["discriminative k-shot learning","probabilistic inference"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}