{"notes":[{"tddate":null,"ddate":null,"tmdate":1512275174687,"tcdate":1512275174687,"number":1,"cdate":1512275174687,"id":"H1JlpxZZf","invitation":"ICLR.cc/2018/Conference/-/Paper821/Public_Comment","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Skip connections formed by orthogonal and idempotent transformations","comment":"This paper presents a good analysis on analyzing skip connections for training deep NNs. I noticed that the paper, \"Orthogonal and Idempotent Transformations for Learning Deep Neural Networks\" by Jingdong Wang, Yajie Xing, Kexin Zhang, Cha Zhang, provided two methods for designing skip connections, which might be related to \"orthogonality\" mentioned in this paper. Is there any possible discussion?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Two such singularities have been identified in previous work: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer and (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes. These singularities cause degenerate manifolds in the loss landscape previously shown to slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes and by reducing the possibility of node elimination. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on CIFAR-10 and CIFAR-100.","pdf":"/pdf/1ac1aea9d028516b39570b5c19dd81f5e101f240.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1512222776109,"tcdate":1511924947312,"number":3,"cdate":1511924947312,"id":"HJiCEsseG","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Review","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function. The discussion is largely based on a sequence of experiments, some of which are interesting and insightful. The discussion here can be useful for other researchers. \n\nMy main concern is that the result here is purely empirical, with no concrete theoretical justification. What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation. Therefore, i found many of the discussion to be questionable. I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper.\n \nDo you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues? Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper. \n\nAll the conclusions are based on one or two datasets. Could you consider testing the result on more different datasets to verify if the results are generalizable? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Two such singularities have been identified in previous work: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer and (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes. These singularities cause degenerate manifolds in the loss landscape previously shown to slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes and by reducing the possibility of node elimination. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on CIFAR-10 and CIFAR-100.","pdf":"/pdf/1ac1aea9d028516b39570b5c19dd81f5e101f240.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1512222776152,"tcdate":1511816889042,"number":2,"cdate":1511816889042,"id":"SJWa0g9xM","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Review","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well-written paper examining how skip connections influence training dynamics in deep nets.","rating":"8: Top 50% of accepted papers, clear accept","review":"Paper examines the use of skip connections (including residual layers) in deep networks as a way of alleviating two perceived difficulties in training: 1) when a neuron does not contain any information, and 2) when two neurons in a layer compute the same function. Both of these cases lead to singularities in the Hessian matrix, and this work includes a number of experiments showing the effect of skip connections on the Hessian during training. \n\nThis is a significant and timely topic. While I may not be the best one to judge the originality of this work, I appreciated how the authors presented clear and concise arguments with experiments to back up their claims.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Two such singularities have been identified in previous work: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer and (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes. These singularities cause degenerate manifolds in the loss landscape previously shown to slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes and by reducing the possibility of node elimination. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on CIFAR-10 and CIFAR-100.","pdf":"/pdf/1ac1aea9d028516b39570b5c19dd81f5e101f240.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1512222776192,"tcdate":1511391815378,"number":1,"cdate":1511391815378,"id":"rJkUMYQgz","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Review","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Thorough study, useful result","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights. They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning.\n\nThe analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims.\n\nI have no major criticisms.\n\nOne suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy. The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Two such singularities have been identified in previous work: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer and (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes. These singularities cause degenerate manifolds in the loss landscape previously shown to slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes and by reducing the possibility of node elimination. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on CIFAR-10 and CIFAR-100.","pdf":"/pdf/1ac1aea9d028516b39570b5c19dd81f5e101f240.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1509739082692,"tcdate":1509135422936,"number":821,"cdate":1509739080031,"id":"HkwBEMWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Two such singularities have been identified in previous work: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer and (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes. These singularities cause degenerate manifolds in the loss landscape previously shown to slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes and by reducing the possibility of node elimination. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on CIFAR-10 and CIFAR-100.","pdf":"/pdf/1ac1aea9d028516b39570b5c19dd81f5e101f240.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}