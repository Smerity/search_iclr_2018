{"notes":[{"tddate":null,"ddate":null,"tmdate":1515022592799,"tcdate":1515022592799,"number":3,"cdate":1515022592799,"id":"rytZt1jXM","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Comment","forum":"HkwBEMWCZ","replyto":"HJiCEsseG","signatures":["ICLR.cc/2018/Conference/Paper821/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper821/Authors"],"content":{"title":"Response to reviewer 3","comment":"We agree with the reviewer that a theoretically more rigorous analysis of the results presented in our paper would be desirable. However, both because the paper is already quite long and because the analysis requested by the reviewer is not straightforward even in simplified models, we would like to leave this for future work. We would also like to point out that the idea that degeneracies cause training difficulties in neural networks is not wholly without theoretical precedent. Although the models they deal with are highly simplified models, prior work by Amari and colleagues, which we discuss in our paper, already established this connection. Similarly, as also discussed in our paper, Saxe et al. (2013) showed that randomly initialized linear networks become increasingly degenerate with depth and they identified this as the source of the training difficulties in such deep networks.\n\nSupplementary Note 4 validates our methods for quantifying degeneracies in smaller, numerically tractable networks. The results from these smaller networks agree with the results from the larger networks presented in the main text. For these smaller networks, the mixture model slightly underestimated the fraction of degenerate eigenvalues and overestimated the fraction of degenerate eigenvalues. However, in both cases, there was a highly significant linear relationship between the estimated and actual fractions. This suggests that inferences drawn from the mixture model about the relative degeneracy of models should be reliable, while inferences about the exact values of the eigenvalue degeneracy or negativity of the models should be made more cautiously.\n\nAs requested, we tested the results shown in Figure 4 in two more image recognition datasets (SVHN and STL-10). The results from these new datasets are presented in the supplementary Figure S7. These results are in agreement with the ones from CIFAR-100 shown in Figure 4."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1515021897043,"tcdate":1515021897043,"number":2,"cdate":1515021897043,"id":"B1W8I1jmf","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Comment","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper821/Authors"],"content":{"title":"Updates","comment":" We thank the reviewers for their comments and positive feedback.\n\nSince the original submission, we have noticed that another degeneracy that could potentially be significant in training deep networks is linear dependence between hidden units. The paper is updated with a discussion and some new results regarding this degeneracy. Results in Figure 4 are replicated for two more datasets (SVHN and STL-10, supplementary Figure S7). Several more minor changes are also made in the revision to improve the presentation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1515021735369,"tcdate":1515021735369,"number":1,"cdate":1515021735369,"id":"B11hBJsXM","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Comment","forum":"HkwBEMWCZ","replyto":"H1JlpxZZf","signatures":["ICLR.cc/2018/Conference/Paper821/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper821/Authors"],"content":{"title":"Orthogonal skip connections","comment":"Yes, the orthogonal skip connectivity proposed in the preprint mentioned is the same as the one we propose in our submission. We note that this preprint appeared several months after our preprint."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1512275174687,"tcdate":1512275174687,"number":1,"cdate":1512275174687,"id":"H1JlpxZZf","invitation":"ICLR.cc/2018/Conference/-/Paper821/Public_Comment","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Skip connections formed by orthogonal and idempotent transformations","comment":"This paper presents a good analysis on analyzing skip connections for training deep NNs. I noticed that the paper, \"Orthogonal and Idempotent Transformations for Learning Deep Neural Networks\" by Jingdong Wang, Yajie Xing, Kexin Zhang, Cha Zhang, provided two methods for designing skip connections, which might be related to \"orthogonality\" mentioned in this paper. Is there any possible discussion?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1515642516794,"tcdate":1511924947312,"number":3,"cdate":1511924947312,"id":"HJiCEsseG","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Review","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function. The discussion is largely based on a sequence of experiments, some of which are interesting and insightful. The discussion here can be useful for other researchers. \n\nMy main concern is that the result here is purely empirical, with no concrete theoretical justification. What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation. Therefore, i found many of the discussion to be questionable. I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper.\n \nDo you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues? Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper. \n\nAll the conclusions are based on one or two datasets. Could you consider testing the result on more different datasets to verify if the results are generalizable? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1515642516832,"tcdate":1511816889042,"number":2,"cdate":1511816889042,"id":"SJWa0g9xM","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Review","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well-written paper examining how skip connections influence training dynamics in deep nets.","rating":"8: Top 50% of accepted papers, clear accept","review":"Paper examines the use of skip connections (including residual layers) in deep networks as a way of alleviating two perceived difficulties in training: 1) when a neuron does not contain any information, and 2) when two neurons in a layer compute the same function. Both of these cases lead to singularities in the Hessian matrix, and this work includes a number of experiments showing the effect of skip connections on the Hessian during training. \n\nThis is a significant and timely topic. While I may not be the best one to judge the originality of this work, I appreciated how the authors presented clear and concise arguments with experiments to back up their claims.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1515642516869,"tcdate":1511391815378,"number":1,"cdate":1511391815378,"id":"rJkUMYQgz","invitation":"ICLR.cc/2018/Conference/-/Paper821/Official_Review","forum":"HkwBEMWCZ","replyto":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference/Paper821/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Thorough study, useful result","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights. They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning.\n\nThe analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims.\n\nI have no major criticisms.\n\nOne suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy. The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]}},{"tddate":null,"ddate":null,"tmdate":1515021553051,"tcdate":1509135422936,"number":821,"cdate":1509739080031,"id":"HkwBEMWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkwBEMWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Skip Connections Eliminate Singularities","abstract":"Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the \"ghosts\" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.","pdf":"/pdf/b8bdaaed27acbab6876e316d1563872b0767ecda.pdf","TL;DR":"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.","paperhash":"anonymous|skip_connections_eliminate_singularities","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip Connections Eliminate Singularities},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwBEMWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper821/Authors"],"keywords":["deep learning","optimization","skip connections"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}