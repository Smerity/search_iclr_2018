{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222677696,"tcdate":1512005206618,"number":3,"cdate":1512005206618,"id":"HyywCCnef","invitation":"ICLR.cc/2018/Conference/-/Paper510/Official_Review","forum":"By3v9k-RZ","replyto":"By3v9k-RZ","signatures":["ICLR.cc/2018/Conference/Paper510/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting, but weird framework for bAbI QA","rating":"4: Ok but not good enough - rejection","review":"The paper presents an interesting framework for bAbI QA.  Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences).  The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index.  While the argument makes sense, it is not clear to me why one cannot simply index the original text. The additional encode/decode mechanism seems to introduce unnecessary noise.  The framework does include several components and techniques from latest recent work, which look pretty sophisticated. However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.\n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams\n\nCons:\n  1. The overall justification is somewhat unclear\n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES","abstract":"Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (“life-long bAbI”) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM’s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.\n","pdf":"/pdf/7f9f452c0ff0c4689f378e2a767e89769fa02a0f.pdf","TL;DR":"We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.","paperhash":"anonymous|learning_to_organize_knowledge_with_ngram_machines","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By3v9k-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper510/Authors"],"keywords":["neuro-symbolic reasoning","information extraction","learn to search"]}},{"tddate":null,"ddate":null,"tmdate":1512222677738,"tcdate":1511920307062,"number":2,"cdate":1511920307062,"id":"BkonMcoef","invitation":"ICLR.cc/2018/Conference/-/Paper510/Official_Review","forum":"By3v9k-RZ","replyto":"By3v9k-RZ","signatures":["ICLR.cc/2018/Conference/Paper510/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (\"n-grams\") which can be queried efficiently. The authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate NGMs on five of the 20 bAbI tasks. I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA. However, I have some concerns about the specific implementation and model discussed here. How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose QA model for natural language? Addressing some of these issues would likely prevent scaling to millions of (real) sentences, as the scalability is reliant on programs being efficiently executed (by simple string matching) against a knowledge storage. The paper is missing a clear analysis of NGM's limitations... the examples of knowledge storage from bAbI in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simple. In its current form, I am borderline but leaning towards rejecting this paper.\n\nOther questions:\n- is \"n-gram\" really the most appropriate term to use for the symbolic representation? N-grams are by definition contiguous sequences... The authors may want to consider alternatives.\n- why focus only on extractive QA? The evaluations are only conducted on 5 of the 20 bAbI tasks, so  it is hard to draw any conclusions from the results as to the validity of this approach. Can the authors comment on how difficult it will be to add functions to the list in Table 2 to handle the other 15 tasks? Or is NGM strictly for extractive QA?\n- beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in Figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny RNN sizes used in the paper. How long does the encoding time take with 10 million sentences?\n- Need more detail on the programmer architecture, is it identical to the one used in Liang et al., 2017?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES","abstract":"Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (“life-long bAbI”) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM’s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.\n","pdf":"/pdf/7f9f452c0ff0c4689f378e2a767e89769fa02a0f.pdf","TL;DR":"We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.","paperhash":"anonymous|learning_to_organize_knowledge_with_ngram_machines","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By3v9k-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper510/Authors"],"keywords":["neuro-symbolic reasoning","information extraction","learn to search"]}},{"tddate":null,"ddate":null,"tmdate":1512222677778,"tcdate":1511823527681,"number":1,"cdate":1511823527681,"id":"rJg3uzqlG","invitation":"ICLR.cc/2018/Conference/-/Paper510/Official_Review","forum":"By3v9k-RZ","replyto":"By3v9k-RZ","signatures":["ICLR.cc/2018/Conference/Paper510/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Lacks proper evaluation and sufficient novelty","rating":"4: Ok but not good enough - rejection","review":"The authors propose the N-Gram machine to answer questions over long documents. The model first encodes the document via tuple extraction. An autoencoder objective is used to produce meaningful tuples. Then, the model generates a program, based on the extracted tuple collection and the question, to find an answer.\n\nI am very disappointed in the authors' choice of evaluation, namely bAbI - a toy, synthetic task long abandoned by the NLP community because of its lack of practicality. If the authors would like to demonstrate question answering on long documents, they have the luxury of choosing amongst several large scale, realistic question answering datasets such as the Stanford Question answering dataset or TriviaQA.\nBeyond the problem of evaluation, the model the authors propose does not provide new ideas, and rather merges existing ones. This, in itself, is not a problem. However, the authors decline to cite many, many important prior work. For example, the tuple extraction described by the authors has significant prior work in the information retrieval community (e.g. knowledge base population, relation extraction). The idea of generating programs to query over populated knowledge bases, again, has significant related work in semantic parsing and program synthesis. Question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact I believe Matt Gardner wrote his entire thesis on this topic). Finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years. Non of these areas, with the exception of semantic parsing, are addressed by the author. With sufficient knowledge of related works from these areas, I find that the authors' proposed method lacks proper evaluation and sufficient novelty.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES","abstract":"Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (“life-long bAbI”) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM’s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.\n","pdf":"/pdf/7f9f452c0ff0c4689f378e2a767e89769fa02a0f.pdf","TL;DR":"We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.","paperhash":"anonymous|learning_to_organize_knowledge_with_ngram_machines","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By3v9k-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper510/Authors"],"keywords":["neuro-symbolic reasoning","information extraction","learn to search"]}},{"tddate":null,"ddate":null,"tmdate":1509739263369,"tcdate":1509124708264,"number":510,"cdate":1509739260707,"id":"By3v9k-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By3v9k-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES","abstract":"Deep neural networks (DNNs) had great success on NLP tasks such as language modeling, machine translation and certain question answering (QA) tasks. However, the success is limited at more knowledge intensive tasks such as QA from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; Weston et al., 2014) need to read the entire text after observing the question, and therefore their complexity in responding a question is linear in the text size. This is prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web. We propose to solve this scalability issue by using symbolic meaning representations, which can be indexed and retrieved efficiently with complexity that is independent of the text size. More specifically, we use sequence-to-sequence models to encode knowledge symbolically and generate programs to answer questions from the encoded knowledge. We apply our approach, called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a special version of them (“life-long bAbI”) which has stories of up to 10 million sentences. Our experiments show that NGM can successfully solve both of these tasks accurately and efficiently. Unlike fully differentiable memory models, NGM’s time complexity and answering quality are not affected by the story length. The whole system of NGM is trained end-to-end with REINFORCE (Williams, 1992). To avoid high variance in gradient estimation, which is typical in discrete latent variable models, we use beam search instead of sampling. To tackle the exponentially large search space, we use a stabilized auto-encoding objective and a structure tweak procedure to iteratively reduce and refine the search space.\n","pdf":"/pdf/7f9f452c0ff0c4689f378e2a767e89769fa02a0f.pdf","TL;DR":"We propose a framework that learns to encode knowledge symbolically and generate programs to reason about the encoded knowledge.","paperhash":"anonymous|learning_to_organize_knowledge_with_ngram_machines","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING TO ORGANIZE KNOWLEDGE WITH N-GRAM MACHINES},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By3v9k-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper510/Authors"],"keywords":["neuro-symbolic reasoning","information extraction","learn to search"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}