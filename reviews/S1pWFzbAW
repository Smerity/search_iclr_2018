{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222802189,"tcdate":1511823215875,"number":3,"cdate":1511823215875,"id":"Bk_dwGqeG","invitation":"ICLR.cc/2018/Conference/-/Paper880/Official_Review","forum":"S1pWFzbAW","replyto":"S1pWFzbAW","signatures":["ICLR.cc/2018/Conference/Paper880/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea, limited applicability (storing and transmission of models), very limited results and missing details. I would like to see clearer and more comprehensive results in terms of modern models and in the complete model, not only in the FC layer, including accuracy impact.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an interesting approach to compress the weights of a network for storage or transmission purposes. My understanding is, at inference, the network is 'recovered' therefore there is no difference in processing time (slight differences in accuracy due to the approximation in recovering the weights).\n\n- The idea is nice although it's applicability is limited as it is only for distribution of the model and storing (is storage really a problem?). \n\nMethod:\n- the idea of using the Bloomier filter is new to me. However, the paper is miss-leading as the filtering is a minor part of the complete process. The paper introduces a complete pipeline including quantization, and pruning to maximize the benefits of the filter and an additional (optional) step to achieve further compression. \n\n- The method / idea seems simply and easy to reproduce (except the subsequent steps that are not clearly detailed).\n\nClarity\n\n- The paper could improve its clarity. At the moment, the Bloomier is the core but needs many other components to make it effective. Those components are not detailed to the level of being reproducible.\n- One interesting point is the self-implementation of the Deep compression algorithm. The paper claims this is a competitive representation as it achieves better compression than the original one. However, those numbers are not clear in tables (only in table 3 numbers seem to be equivalent to the ones in the text). This needs clarification, CSR achieves 81.8% according to Table 2 and 119 according to the text.\n\nResults:\n- Current results are interesting. However I have several concerns:\n1) it is not clear to me why assuming similar performance. While Bloomier is weightless the complete process involves many retraining steps involving performance loss. Analysis on this would be nice to see (I doubt it ends exactly at the same number). Section 3 explicitly suggest there is the need of retraining to mitigate the effect of false positives which is then increased with pruning and quantization. Therefore, would be nice to see the impact in accuracy (even it is not the main focus of the work). \n\n2) Resutls are focused on fully connected layers which carry (for the given models) the larger number of weights (and therefore it is easy to get large compression numbers). What would happen in newer models where the fully connected layer is minimal compared to conv. layers? What about the accuracy impact there? Let's say in a Resnet-34.\n3) I would like to see further analysis on why Bloomier filter encoding improves accuracy (or is a typo and meant to be error?) by 2%. This is a large improvement without training from scractch.\n4) It is interesting to me how the retraining process is 'hidden' all over the paper. At the beginning it is claimed that it takes about one hour for VGG-16 to compute the Bloomier filters. Howerver, that is only a minimal portion of the entire pipeline. Later in the experimental section it is mentioned that 'tens of epochs' are needed for retraining (assuming to compensate for errors) after retraining for compensating l1 pruning?.... tens of epochs is a significant portion of the entire training process assuming VGG is trained for 90epochs max.\n\n5) Interestingly, as mentioned in the paper, this is 'static compression'. That is, the model needs to be completely 'restored' before inference. This is miss-leading as an embedded device will need the same requirements as any other at inferece time(or maybe I am missing something). That is, the benefit is mainly for storing and transmission.\n\n6) I would like to see the sensibility analysis with respect to t and the number of clusters. \n\n7) As mentioned before, LeNet is great but would be nice to see more complicated models (even resnet on CIFAR). These models are not only large in terms of parameters but also quite sensitive to modifications in the weight structure.\n\n8) Results are focused on a single layer. What happens if all the layers are considered at the same time? Here I am also concerned about the retraining process (fixing one layer and retraining the deeper ones). How is this done using only fully connected layers? What is the impact of doing it all over the network (let's say VGG-16 from the first convolutional layer to the very last).\n\nSummary:\n\nAll in all, the idea has potential but there are many missing details. I would like to see clearer and more comprehensive results in terms of modern models and in the complete model, not only in the FC layer, including accuracy impact.  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Weightless: Lossy Weight Encoding For Deep Neural Network Compression","abstract":"The large memory requirements of deep neural networks strain the capabilities of many devices, limiting their deployment and adoption. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding which complements conventional compression techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, Weightless, can compress DNN weights by up to 496x; with the same model accuracy, this results in up to a 1.51x improvement over the state-of-the-art.","pdf":"/pdf/853cfd55f56c721fc8f6c689cb549bdf6c9c9809.pdf","TL;DR":"We propose a new way to compress neural networks using probabilistic data structures.","paperhash":"anonymous|weightless_lossy_weight_encoding_for_deep_neural_network_compression","_bibtex":"@article{\n  anonymous2018weightless:,\n  title={Weightless: Lossy Weight Encoding For Deep Neural Network Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1pWFzbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper880/Authors"],"keywords":["Deep Neural Network","Compression","Sparsity"]}},{"tddate":null,"ddate":null,"tmdate":1512222802231,"tcdate":1511729494572,"number":2,"cdate":1511729494572,"id":"SyJvYjugz","invitation":"ICLR.cc/2018/Conference/-/Paper880/Official_Review","forum":"S1pWFzbAW","replyto":"S1pWFzbAW","signatures":["ICLR.cc/2018/Conference/Paper880/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lacking evaluations with few technical concerns","rating":"6: Marginally above acceptance threshold","review":"The problem of lossy compression of neural networks is essentially important and relevant. The paper proposes an interesting usage of Bloomier filters in lossy compression of neural net weights. The Bloomier filter is proposed by others. It is a data structure that maps from sparse indices to their corresponding values with chances that returns incorrect values for non-existing indices. The paper compares its method with two baseline methods (Magnitude and Dynamic network surgery DNS) to demonstrates its performance.\n\nI find the paper fairly interesting but still have some concerns in the technical part and experiments.\n\nPros:\n1. The paper seems the first to introduce Bloomier filter into the network compression problem. I think its contribution is novel and original. The paper may interest those who work in the network compression domain.\n2. The method works well in the demonstrated experimental cases.\n\nCons:\n1. The technical part is partially clear. It might be worthwhile to briefly describe the encoding/construction algorithm used in the paper. It is recommended to describe a bit more details about how such encoding/decoding methods are applied in reducing neural net weights.\n2. One drawback of the proposed method is that it has to work with sparse weights. That requires the method to be used together with network pruning methods, which seems limiting its applicability. I believe the paper can be further improved by including a study of the compression results without a pruning method (e.g., comparing with Huffman in table 3). \n3. What is the reason there is no DNS results reported for VGG-16? Is it because the network is deeper?\n4. The experimental part can be improved by reporting the compression results for the whole network instead of a single layer.\n5. It seems the construction of Bloomier filter is costly and the proposed method has to construct Bloomier filters for all layers. What is the total time cost in terms of encoding and decoding those networks (LeNet and VGG)? It would be nice to have a separate comparison on the time consumption of  different methods.\n6. Figure 4 seems a bit misleading. The comparison should be conducted on the same accuracy level instead of the ratio of nonzero weights. I recommend producing another new figure of doing such comparison.\n7. The proposed idea seems somewhat related to using low rank factorization of weight matrices for compression. It might be worthwhile to compare the two approaches in experiments.\n8. I am specifically interested in discussions about the possibility of encoding the whole network instead of layer-by-layer retraining.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Weightless: Lossy Weight Encoding For Deep Neural Network Compression","abstract":"The large memory requirements of deep neural networks strain the capabilities of many devices, limiting their deployment and adoption. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding which complements conventional compression techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, Weightless, can compress DNN weights by up to 496x; with the same model accuracy, this results in up to a 1.51x improvement over the state-of-the-art.","pdf":"/pdf/853cfd55f56c721fc8f6c689cb549bdf6c9c9809.pdf","TL;DR":"We propose a new way to compress neural networks using probabilistic data structures.","paperhash":"anonymous|weightless_lossy_weight_encoding_for_deep_neural_network_compression","_bibtex":"@article{\n  anonymous2018weightless:,\n  title={Weightless: Lossy Weight Encoding For Deep Neural Network Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1pWFzbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper880/Authors"],"keywords":["Deep Neural Network","Compression","Sparsity"]}},{"tddate":null,"ddate":null,"tmdate":1512222802276,"tcdate":1511726467865,"number":1,"cdate":1511726467865,"id":"HynYT5_xz","invitation":"ICLR.cc/2018/Conference/-/Paper880/Official_Review","forum":"S1pWFzbAW","replyto":"S1pWFzbAW","signatures":["ICLR.cc/2018/Conference/Paper880/AnonReviewer3"],"readers":["everyone"],"content":{"title":"ICLR 2018 review. Weightless","rating":"6: Marginally above acceptance threshold","review":"Summary: The paper addresses the actual problem of compression of deep neural networks. Authors propose to use another technique for sparse matrix storage. Namely, authors propose to use Bloomier filter for more efficient storage of sparse matrices obtained from Dynamic Network Surgery (DNS) method. Moreover, authors propose elegant and efficient trick for mitigating errors of Bloomier filter. Overall, the paper present clear and completed research of the proposed technique.\nClarity and Quality: The paper is well structured and easy to follow. Authors provide a reader with a large amount of technical details, which helps to reproduce their method. The paper contains detailed investigation of every step and aspect in the proposed pipeline, which made this research well-organized and complete.\nThough the presentation of some results can be improved. Namely, core values for compression and improvement are presented only for two biggest layers in networks, but more important values are compression and improvement for whole networks.\nOriginality and Significance: The main contribution of the paper is the adaptation of Bloomier filter for sparse network obtained from almost any procedure of networks sparsification. However, this adaptation is almost straightforward, except the proposed trick of network fine-tuning for compensating false positive values of Bloomier filter. Significance of the results is hard to estimate because of several reasons:\nValues of compression and improvement are presented only for two layers, not for the whole network.\nAccording to Fig. 4, encoding of sparse matrices via Bloomier filter is efficient (compared to CSR) only for matrices with nonzero ratio greater than 0.04. So this method can’t be applied to all layers in network, that can significantly influence overall compression.\n\nOther Comments:\nThe procedure of network evaluation is totally omitted in the paper. So a model supposed to be “unpacked” (to the dense or CSR format) before evaluation. Considering this, comparison with CSR could be made only for sending the model over a network. Since, CSR format can be efficiently used during evaluation.\nMinor Comments:\n(Page 5) “Because we encode $k$ clusters, $t$ must be greater than $\\lceil \\log_2 k\\rceil$”. Perhaps, “...$r$ must be greater than $\\lceil \\log_2 k\\rceil$” would be better for understanding.\n(Page 7) “Bloomier filter encoding increased the top-1 accuracy by 2.0 percentage points”. Perhaps, authors have meant top-1 error.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Weightless: Lossy Weight Encoding For Deep Neural Network Compression","abstract":"The large memory requirements of deep neural networks strain the capabilities of many devices, limiting their deployment and adoption. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding which complements conventional compression techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, Weightless, can compress DNN weights by up to 496x; with the same model accuracy, this results in up to a 1.51x improvement over the state-of-the-art.","pdf":"/pdf/853cfd55f56c721fc8f6c689cb549bdf6c9c9809.pdf","TL;DR":"We propose a new way to compress neural networks using probabilistic data structures.","paperhash":"anonymous|weightless_lossy_weight_encoding_for_deep_neural_network_compression","_bibtex":"@article{\n  anonymous2018weightless:,\n  title={Weightless: Lossy Weight Encoding For Deep Neural Network Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1pWFzbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper880/Authors"],"keywords":["Deep Neural Network","Compression","Sparsity"]}},{"tddate":null,"ddate":null,"tmdate":1510092386398,"tcdate":1509136645211,"number":880,"cdate":1510092362855,"id":"S1pWFzbAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1pWFzbAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Weightless: Lossy Weight Encoding For Deep Neural Network Compression","abstract":"The large memory requirements of deep neural networks strain the capabilities of many devices, limiting their deployment and adoption. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding which complements conventional compression techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, Weightless, can compress DNN weights by up to 496x; with the same model accuracy, this results in up to a 1.51x improvement over the state-of-the-art.","pdf":"/pdf/853cfd55f56c721fc8f6c689cb549bdf6c9c9809.pdf","TL;DR":"We propose a new way to compress neural networks using probabilistic data structures.","paperhash":"anonymous|weightless_lossy_weight_encoding_for_deep_neural_network_compression","_bibtex":"@article{\n  anonymous2018weightless:,\n  title={Weightless: Lossy Weight Encoding For Deep Neural Network Compression},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1pWFzbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper880/Authors"],"keywords":["Deep Neural Network","Compression","Sparsity"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}