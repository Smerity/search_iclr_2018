{"notes":[{"tddate":null,"ddate":null,"tmdate":1514852918946,"tcdate":1514852839167,"number":7,"cdate":1514852839167,"id":"rJJlG8OmM","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"S155ZUOXf","signatures":["ICLR.cc/2018/Conference/Paper472/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/Authors"],"content":{"title":"2nd part of the answer","comment":"- The task seems rather artificial \n=> The experimental setup developed in this article is built to reflect skills a young infant could develop (reaching hand, moving objects, controlling a virtual character using controllers). We are inspired on studies of developmental psychology on how infants learn and how they explore spaces of possible outcomes. We chose these examples because they offer a generic illustration of interrelated fields of tasks defining continuous spaces. Indeed, we target continuous task spaces, to test how the robot can adapt and generalise to achieve unknown tasks, exploiting its trials. We are considering a more realistic set of tasks for an ongoing experiment. We added this precision in the future work section.\n\n-  How limiting is always starting at the same position? \n=> It is limiting for combining known policies into more complex ones. When executing the second policy for the second subtask of a procedure, although using DMPs helps the robot reach its final state despite different starting states, the robot can't really know if it will reach the subtask, as it will start from a different context (the final context of the first policy). We are currently thinking about a way to enable our procedure to select intermediary policies (parts of complex policies not starting from the rest pose) and add initial context in the learning algorithm. \nHowever, from the developmental point of view, starting the learning by always starting at the same position has been supported by observations of infants who sometimes prepare their reaching movements by starting from a same rest position (N. E. Berthier, R. Clifton, D. McCall, D. Robin, Proximodistal structure of early reaching in human infants, Exp Brain Res (1999))\n\n- Sect. 3.2.2. \"the robot learns\" \n=> Yes, the outcome subspaces are predefined to the learner and it learns how to reach them, i.e.: it learns the mapping between outcomes and the sequence of policies. I fixed the sentence. However, to be more thorough, the outcome subspaces are pre-defined roughly by the variables from the sensors needed and a rough range of their values. We do not need a precise estimation as the algorithm can learn which areas are reachable, which are unreachable (see S. M. Nguyen and P.-Y. Oudeyer. Socially guided intrinsic motivation for robot learning of motor skills. Autonomous Robots, 36(3):273–294, 2014 and S. Ivaldi, S. M. Nguyen, N. Lyubova, A. Droniou, V. Padois, D. Filliat, P.-Y. Oudeyer, and O. Sigaud. Object learning through active exploration. Transactions on Autonomous Mental Development, PP(99):1–1, 2013.)\n\n- How general is this approach?\n=> In this experiment, we did not include bad teachers, but the algorithm we extended, SGIM-ACTS, has been shown to be robust to bad teachers:  it could select the most adapted teachers for the tasks it wants to learn, or prefer autonomous exploration when the teacher is not helpful. In our case, the task spaces are the skill we want the agent to learn. (see S. M. Nguyen and P.-Y. Oudeyer. Socially guided intrinsic motivation for robot learning of motor skills. Autonomous Robots, 36(3):273–294, 2014)\nRegarding the prior knowledge, our framework requires : \n1) to define the primitive policies of the robot (we use in this paper DMP) \n2) to define the different outcomes the user is interested in by defining the variables from the sensors and a rough range of their values (we do not need a precise estimation as the algorithm is robust to overestimations of these ranges, see (Nguyen, 2013)). \n3) a measure for the robot to assess its own performance such as a distance, as in all intrinsic motivation based algorithms. \n4) the environment and robot can reset to an initial state, as in most reinforcement learning algorithms.\nWe updated section 2 (p. 6) to make these requirements clearer.\n\n=> We updated the quote marks, the formatting of the references and removed the acknowlegements. We have added the final standard deviation of each algorithm in figure 3. We replaced (Theodorou et al, 2010) by another using reinforcement learning through Actor Critic\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1514853683021,"tcdate":1514852754244,"number":6,"cdate":1514852754244,"id":"S155ZUOXf","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"By50raBlf","signatures":["ICLR.cc/2018/Conference/Paper472/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/Authors"],"content":{"title":"the algorithm learns to map outcomes to sequences of subgoals or policies based on an internal reward function","comment":"-  Euclidean distance\n=> We need a measure for the robot to assess its own performance such as a distance, as in all intrinsic motivation based algorithms. This measure is used as an “internal reward” function. Contrarily to classical reinforcement learning problems, this reward function is not fine tuned to the specific goal at hand, but is a generic function for all the goals in the outcome space. Thus, this reward function aims at a more generic method. We have updated section 2 to make this explicit (p.6). In this study, we consider one of the most basic metrics which can, at least locally, apply to most kinds of outcomes. Besides, it does not bring to many prior knowledge as a manually designed reward function might. We choose euclidean distance for the sake of simplicity and genericity. However, other metrics could also be used.\n\n-  why we are limited to combining 2 procedures \n=> Thank you for your comment. We followed your suggestion and updated in section 2.2 (p.3) the procedures definition so as to define procedures of any size, although we implemented only procedures of 2 tasks in the experiment. Indeed, in this first experimental illustration of this work where we introduce the framework of procedures, we limit the use of procedures to the combination of only 2 tasks, and do not allow the procedures of more tasks. This is indeed a more interesting definition, but we fear that in practice for the experiment, the curse of dimensionality would make it impossible for the learner to explore this infinite space. We wished first to test the idea that owing to our hierarchical search in the outcome space, the procedure space and the policy space, the learning agent could achieve complex tasks with policies chosen to reach subtasks, built by a lower level procedure. For instance for the drawing task, it learns to use subtasks: (1) move the pen , and (2) move the hand. Then it can use the best policies for these subtasks for the global goal: the drawing task. \n\n\n- How about recovery movements if something does not go as planned? \n=> Recovery and adaptation in the case of unforeseen changes is an important issue for robotics. Unfortunately, this adaptation in case of environment change is not explicitly handled in our algorithm. This issue can be dealt with in an implicit way. On one side, we can have this system of recovery of movements in the low-level of policies. The way we encode policies can be able to recover from obstacles or other disruptions. This is one of the reasons we have used Dynamic Movement Primitives to encode our policies. They have been shown to be robust to noise and to allow good recovery from obstacles. In the policy level, our algorithm learns if a policy does not induce the goal outcome. When testing a policy for a given goal, if policy does not work, the agent will notice it and won't use the same policy again. Even when the agent has learned a fixed procedure, this does not mean a fixed sequence of movements, the agent will recruit the best policies for the subgoals of the procedure, thus being able to adapt the policies. We have updated section 2 (p.6) to clarify this point.\n\n- Algo 1: is totally unclear \n=> It consists in a simple algorithm to find the policies for each subtask: we use the nearest neighbour for each subtask to recruit the best policy. We added comments in the pseudo-code and rewrote the algorithm description to add more details in section 2.\n\n-  what's different compared to (Nguyen & Oudeyer, 2012) \n=> I added explanations about the similitudes between both algorithms in sections 1.4 (p.3) and section 2, and outlined the main contributions of our study in the conclusion (p.12).  SGIM-PB is actually an extension of SGIM-ACTS to complex policies which uses the procedures framework and its two associated strategies.\nTo enable a robot learner to learn sequences of actions of undetermined length to achieve a field of tasks, we extended SGIM-ACTS with the “procedures” framework.  In the results section (section 3.4 and 4, p 9-11), we added SGIM-ACTS, extended so as to learn complex policies, in the compared algorithms. We showed our algorithm had error levels lower than the other algorithms, including SGIM-ACTS. This difference is higher for complex movements that require successions of policies. We also clarified the main contribution of the article compared to SGIM-ACTS in the conclusion (p11,12)."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515026439087,"tcdate":1514219312213,"number":5,"cdate":1514219312213,"id":"ByOVDjAGz","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"BkW3TW2fG","signatures":["ICLR.cc/2018/Conference/Paper472/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/Authors"],"content":{"title":"References for the previous comment","comment":"References : \n[1] N. Duminy, S. M. Nguyen, and D. Duhaut. Strategic and interactive learning of a hierarchical set of tasks by the Poppy humanoid robot. In ICDL-EPIROB 2016 : 6th Joint IEEE International Conference Developmental Learning and Epigenetic Robotics, 2016.\n[2] S. Ivaldi, S. M. Nguyen, N. Lyubova, A. Droniou, V. Padois, D. Filliat, P.-Y. Oudeyer, and O. Sigaud. Ob- ject learning through active exploration. Transactions on Autonomous Mental Development, PP(99):1–1, 2013.\n[3] C. Moulin-Frier, S. M. Nguyen, and P.-Y. Oudeyer. Self-organization of early vocal development in infants and machines: The role of intrinsic motivation. Frontiers in Psychology, 4(1006), 2014.\n[4] S. M. Nguyen and P.-Y. Oudeyer. Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. Paladyn Journal of Behavioural Robotics, 3(3):136–146, 2012.\n[5] S. M. Nguyen and P.-Y. Oudeyer. Socially guided intrinsic motivation for robot learning of motor skills. Autonomous Robots, 36(3):273–294, 2014.\n[6] S. Forestier and P.-Y. Oudeyer. Curiosity-Driven Development of Tool Use Precursors: a Computational Model. In A. Papafragou, D. Grodner, D. Mirman, and J. Trueswell, editors, 38th Annual Conference of the Cognitive Science Society (CogSci 2016), Proceedings of the 38th Annual Conference of the Cognitive Science Society, pages 1859–1864, Philadelphie, PA, United States, Aug. 2016.\n[7] M. Rolf and M. Asada. Latent goal analysis: Learning goals and body schema from generic rewards. In IEEE, editor, IEEE International Conference on Development and Learning and on Epigenetic Robotics Workshop on Development of body representations in humans and robots, 2014.\n[8] Baranes, A., & Oudeyer, P. Y. (2013) Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515026554087,"tcdate":1514218898082,"number":4,"cdate":1514218898082,"id":"S195BiAGM","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"SJvJ_TrxG","signatures":["ICLR.cc/2018/Conference/Paper472/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/Authors"],"content":{"title":"Clarification over the assumptions and applicability","comment":"We would like to thank the reviewer for their shrewd comments, and have updated the article accordingly. Please find below the details of our answers and updates (after the sign =>)\n\n\n- The paper reads well. Quite a lot of details are glossed over and some properties only become clear very late on. \n=> We rewrote the paper to make the assumptions clearer. In particular, we updated sections 1.1 and 1.4 (p1,2,3) to make the multi-task learning problem clearer and to make the requirements and assumptions of our approach clearer in section 2, p.6. The requirements for a new experimental setup are: \n1) to define the primitive policies of the robot (we use in this paper DMP) \n2) to define the different outcomes the user is interested in by defining the variables from the sensors needed and a rough range of their values (we do not need a precise estimation as the algorithm is robust to overestimations of these ranges, see (Nguyen, 2013)). \n3) a measure for the robot to assess its own performance such as a distance, as in all intrinsic motivation based algorithms. \n4) the environment and robot can reset to an initial state, as in most reinforcement learning algorithms.\nWe also added in section 2  more details about the algorithm and in p.6 the properties of the proposed algorithm: it leverages goal-babbling for autonomous exploration, sequences to learn complex policies, and social guidance to bootstrap the learning.\n\n\n- A wider applicability of the approach remains to be demonstrated. \n=> We are currently designing a real robotic experiment with an industrial robotic arm to test our approach further. Besides, this framework based on instrinsically motivated goal-directed exploration has been applied for various experiments already, with little engineering work: a humanoid robot learning to recognise 3D objects of different orientations and positions (Ivaldi et al,2013), a robot learning to use a fishing rod (Nguyen et al, 2013), a robot learning to control its vocal tract to produce sounds (Moulin-Frier et al, 2013), an arm to throw and place a ball (Nguyen et al, 2012), a humanoid robot learning to draw on a tablet (Duminy et al, 2016), robot arms learning to use tools (Forestier et al, 2016)  or robots learning their body schema (Rolf et al, 2014) …  These experimental setups could be enriched with hierarchical fields of tasks to learn.\n\n\nReferences : \n[1] N. Duminy, S. M. Nguyen, and D. Duhaut. Strategic and interactive learning of a hierarchical set of tasks by the Poppy humanoid robot. In ICDL-EPIROB 2016 : 6th Joint IEEE International Conference Developmental Learning and Epigenetic Robotics, 2016.\n[2] S. Ivaldi, S. M. Nguyen, N. Lyubova, A. Droniou, V. Padois, D. Filliat, P.-Y. Oudeyer, and O. Sigaud. Ob- ject learning through active exploration. Transactions on Autonomous Mental Development, PP(99):1–1, 2013.\n[3] C. Moulin-Frier, S. M. Nguyen, and P.-Y. Oudeyer. Self-organization of early vocal development in infants and machines: The role of intrinsic motivation. Frontiers in Psychology, 4(1006), 2014.\n[4] S. M. Nguyen and P.-Y. Oudeyer. Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. Paladyn Journal of Behavioural Robotics, 3(3):136–146, 2012.\n[5] S. M. Nguyen and P.-Y. Oudeyer. Socially guided intrinsic motivation for robot learning of motor skills. Autonomous Robots, 36(3):273–294, 2014.\n[6] S. Forestier and P.-Y. Oudeyer. Curiosity-Driven Development of Tool Use Precursors: a Computational Model. In A. Papafragou, D. Grodner, D. Mirman, and J. Trueswell, editors, 38th Annual Conference of the Cognitive Science Society (CogSci 2016), Proceedings of the 38th Annual Conference of the Cognitive Science Society, pages 1859–1864, Philadelphie, PA, United States, Aug. 2016.\n[7] M. Rolf and M. Asada. Latent goal analysis: Learning goals and body schema from generic rewards. In IEEE, editor, IEEE International Conference on Development and Learning and on Epigenetic Robotics Workshop on Development of body representations in humans and robots, 2014."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1514219080306,"tcdate":1514198405433,"number":3,"cdate":1514198405433,"id":"H1TtSIAzf","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"BJuQ8N8lf","signatures":["ICLR.cc/2018/Conference/Paper472/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/Authors"],"content":{"title":"We added a comparison with SGIM-ACTS and clarified definitions in the paper","comment":"We would like to thank the reviewer for the good comments, and have updated the article accordingly. Please find below the details of our answers and updates (after the sign =>)\n\n- The method description is not self-contained. Key parts of the algorithm are borrowed from Baranes & Oudeyer (2010) and Nguyen & Oudeyer (2012) without sufficient details. \n=> We rewrote the algorithm description to add more details in section 2. In particular, we detailed the algorithm in subsection 2.3 (p 4-6) with the measures used for the interest model of intrinsic motivation. \n\n- The competence progress metric (p(\\omega_{g})) is never specified in the paper. \n=> We added its definition in the description of SGIM-PB algorithm in subsection 2.3 (p.5)\n\n- It is not clear what the agent will do if a goal in the plan is not achieved. \n=> I am afraid I do not quite understand this comment about what the agent will do if a goal in the plan is not achieved.\nWith respect to the learning phase, the agent is not expected to reliably reach its goal.\nWith respect to the evaluation phase, we measure the distance between the goal of the benchmark and the outcome reached by the agent.\nWith respect to the adaptation of the agent to changes, we are considering in our approach that the environment is static between two attempts, only the agent changes it when it is performing policies.\nWith respect to the procedures, when the agent selects a procedure to be executed, this latter is only a way to build the complex policy which will actually be executed. So the agent does not check if the subtasks are actually reached when executing a procedure. We added this precision in subsection 2.2 (P.3-4).\n\n\n- The proposed SGIM-HL is similar to SGIM-ACTS (Nguyen & Oudeyer 2012). The only difference is that SGIM-HL leverages a new procedure mechanism. But the motivation of utilizing procedures is not clear and there is no empirical comparison against SGIM-ACTS. \n=> We clarify the purpose of this study in section 1.2 (P1,2): to enable a robot learner to learn sequences of actions of undetermined length to achieve a field of tasks. To tackle this high-dimensionality learning problem, we extended SGIM-ACTS with the “procedures” framework. We also clarified the main message of the article in the conclusion (p11,12):\nWe aimed to enable a robot learner to learn sequences of actions of undetermined length to achieve a field of tasks. To tackle this high-dimensionality learning between a continuous high-dimensional space of outcomes and a continuous infinite dimensionality space of sequences of actions , we used techniques that have proven efficient in SGIM-ACTS: goal-babbling, social guidance and strategic learning based on intrinsic motivation.\nIn the results section (section 3.4 and 4, p 9-11), we added SGIM-ACTS, extended so as to learn complex policies, in the compared algorithms. We showed our algorithm had error levels lower than the other algorithms, including SGIM-ACTS. This difference is higher for complex movements that require successions of policies.\n\n\n- The method requires pre-defined policy space split and task space split. Such requirement may limit the practical value of the method. \n=> The method requires a definition of the policy space and task space, especially the sensorimotor channels of interest. Nevertheless, the method does not require the definition of the hierarchy between the tasks. Moreover, based on the basic definition of the possible tasks to learn, the learning agent discovers by itself how to organise its learning process and partition its task space into unreachable regions, easy regions and difficult regions, based on empirical measures of competence and interest. We have added details about it in sub subsection 2.3.2 (p5).\n\nMinor comments\n=> we removed the acknowledgements. We added standard deviation values for the results in fig. 3."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515026378620,"tcdate":1514048937201,"number":2,"cdate":1514048937201,"id":"BkW3TW2fG","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"Hy-CEZ5lM","signatures":["ICLR.cc/2018/Conference/Paper472/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/Authors"],"content":{"title":"Clarifications expecially on the purpose and main message of the paper","comment":"Thank you for your very detailed comments and all your suggestions. We have updated the article. Please find below our responses \n\n=> we did not limit the procedures to chaining two primitives. We defined procedures to choose between two tasks t_i and t_j to combine, this procedures are primarily combinations of tasks. The execution of this procedure will execute the succession of policies pi_i and pi_j to reach these two tasks t_i and t_j. As pi_i and pi_j can be complex policies, the resulting complex policy can be of length greater than 2. Indeed, figure 6 shows that policies chosen by the robot learner chain 1 to 6 primitives, with a mean value around 3. We also recognize that in the experiment of this first work where we introduce the framework of procedures, we limit the use of procedures to the combination of only 2 tasks. This indeed is a more interesting definition, but we fear that in practice for the experiment, the curse of dimensionality would make it impossible for the learner to explore this infinite space. Although we implemented only procedures of 2 tasks in the experiment, we followed your suggestion and updated in section 2.2 (p.3) the procedures definition so as to define procedures of any size.\n\n=> We are inspired by studies of developmental psychology on how infants learn and how they explore spaces of possible outcomes. We are considering a set of tasks which reflects what an infant could learn to master (moving its hand, grabbing objects, controlling a character using joysticks). We chose these examples because they offer a generic illustration of interrelated fields of tasks defining continuous spaces. Indeed, we target continuous task spaces, to test how the robot can adapt and generalise to achieve unknown tasks, exploiting its trials. A task, like operating a microwave could be interesting as well, and would be a good application case, and our algorithmic architecture could learn how to undercook/overcook/cook well different kinds of food depending on the duration and heat of the microwave program, or position of the plate/quantity of food … This framework based on instrinsically motivated goal-directed exploration has been applied for various experiments already, with little engineering work: a humanoid robot learning to recognise 3D objects of different orientations and positions (Ivaldi et al,2013), a robot learning to use a fishing rod (Nguyen et al, 2013), a robot learning to control its vocal tract to produce sounds (Moulin-Frier et al, 2013), an arm to throw and place a ball (Nguyen et al, 2012), a humanoid robot learning to draw on a tablet (Duminy et al, 2016), robot arms learning to use tools (Forestier et al, 2016)  or robots learning their body schema (Rolf et al, 2014) … \nWe detailed the requirements for an experimental setup in p.6. \n\n=> We clarify the purpose of this study in section 1.2 (P1,2): to enable a robot learner to learn sequences of actions of undetermined length to achieve a field of tasks. To tackle this high-dimensionality learning problem, we extended SGIM-ACTS with the “procedures” framework. We also clarified the main message of the article in the conclusion (p11,12)\n\n=> we built our learner so as to combine the ability of the different approaches we inspired from, and we were happy to see it worked as expected. A precise analysis of the impact of each of the different strategies used by our learning algorithm would be interesting, but would need more space than a conference article. However, we would like to refer to the study made in (Baranes and Oudeyer, 2013), which already compared a goal-directed intrinsically motivated learner to a learner with same strategies but other strategical choice methods such as a random choice of outcome & strategy. Moreover, in our results sections, the analysis of the Choices of teachers and target outcomes of the SGIM-PB learner (Fig. 5) shows that the active learner made choices that optimise its learning process. These choices are clearly different from random.\n\n=> Our evaluation focuses both on the ability to cover the largest region of the outcome space and the precision to each each outcome. The concern for covering the largest region comes from our concern that our robot should be able to generalise best from the set of data it has collected. The evaluation does not focus on the ability of the robot to collect data covering the largest region but that it can generalise over a largest region. \nBesides, we are interested in emulating the capability of young infants to generalize over their experience, which is why we are considering spaces of tasks and evaluating coverage. However, we still indirectly see that some specific tasks are more useful than others, specifically the low-level ones as they are needed to learn efficiently the higher level ones (for example knowing how to take the pen, which can be seen as a standalone task, is necessary for the robot before moving it, let alone drawing). "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515642453796,"tcdate":1511818441439,"number":3,"cdate":1511818441439,"id":"Hy-CEZ5lM","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Review","forum":"S1szZkb0b","replyto":"S1szZkb0b","signatures":["ICLR.cc/2018/Conference/Paper472/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A learning framework that does not seem to be greater than its parts","rating":"5: Marginally below acceptance threshold","review":"The authors present a framework that combines multiple learning approaches, including active, interactive, and strategic learning. The robot can improve motor primitives or create procedures (sequences of two motor primitives) either through experience or from a teacher. The framework was evaluated on a simulated manipulation task with two joysticks and a pen for drawing.\n\n\nThe paper could be written more clearly and concretely. The introduction is quite vague and the terminology is at times confusing (A policy is not an action, and an outcome is not a task). \n\nWhy limit the procedures to only chaining two primitives? Why not allow procedures of procedures?  \n\nThe evaluation environment is somewhat abstract. How much engineering would be required for defining the prior knowledge and hierarchical task structure for more complex real world scenarios? The authors should consider using a more realistic interrelated task environment, such as operating a microwave. \n\nMy main concern is that the bigger message of the paper is not clear. Combining different types of learning is an important research topic. It is not clear though if there is any synergy between these learning approaches, and whether or not the overall approach for selecting between methods is suitable.  The fact that the frameworks that use procedures explored further then those that do not is not surprising. The fact that the proposed method learned faster because it had access to expert teachers is also to be expected. The authors should include a comparison to a method that has access to the same resources as the proposed method, but selects them randomly or in a fixed order. A table highlighting the differences between the evaluated methods would also help the reader interpret the results of the experiments. \n\nThe evaluations seem to focus on the ability to cover the largest region of the outcome space. Is this really the most important/relevant measure of success?  One of the challenges of real world manipulation is that there are vast numbers of things an agent could do, but only a very small subset of these are actually useful. The authors should include an additional set of manually designed outcomes, e.g., drawing a circle or moving the game agent into a corner, and evaluating on these as well as the linear outcomes. \n\n\nMinor comments:\n- Add error bars to figure 3\n- I don’t understand Alg. 1, please add text explaining the steps.\n- Rather than introducing “procedures”, could you not use an existing term like macro actions, skill chains, or sequences?\n- The Acknowledgements should ideally be omitted for double-blind reviews.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515642453831,"tcdate":1511568927997,"number":2,"cdate":1511568927997,"id":"BJuQ8N8lf","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Review","forum":"S1szZkb0b","replyto":"S1szZkb0b","signatures":["ICLR.cc/2018/Conference/Paper472/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not self-contained.  ","rating":"5: Marginally below acceptance threshold","review":"Clarity \n- The method description is not self-contained. Key parts of the algorithm are borrowed from Baranes & Oudeyer (2010) and Nguyen & Oudeyer (2012) without sufficient details.  \n- The competence progress metric (p(\\omega_{g})) is never specified in the paper. \n- It is not clear what the agent will do if a goal in the plan is not achieved.\n\nOriginality\nThe proposed SGIM-HL is similar to SGIM-ACTS (Nguyen & Oudeyer 2012). The only difference is that SGIM-HL leverages a new procedure mechanism. But the motivation of utilizing procedures is not clear and there is no empirical comparison against SGIM-ACTS.  \n\nSignificance\n- The method requires pre-defined policy space split and task space split. Such requirement may limit the practical value of the method. \n- The test domain is not very interesting in that it assumes perfect low-dimensional perception and the transitions are deterministic by design. \n- The method is not compared against some general RL method. \n- The method is closely related to SGIM-ACTS, but no comparison is provided. \n- Figure 3 and 4 require confidence intervals or standard errors. \n\nThe acknowledgements might violate the double-blind reviews.\n\nPros:\n- Learning to choose when to request human information or learn in autonomy sounds like a good topic.\n\nCons:\n- Method description is not self-contained.  \n- The test domain is artificial and not interesting. \n- There is no evidence that the method could work on general domains. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515750359555,"tcdate":1511540702743,"number":1,"cdate":1511540702743,"id":"SJvJ_TrxG","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Review","forum":"S1szZkb0b","replyto":"S1szZkb0b","signatures":["ICLR.cc/2018/Conference/Paper472/AnonReviewer1"],"readers":["everyone"],"content":{"title":"How general is this?","rating":"5: Marginally below acceptance threshold","review":"Thanks a lot for the revisions and the very extensive replies (also to my more detailed comments in \"many open questions... \")!\nWith access to all your detailed replies the picture becomes much clearer, but I still have the feeling that they are only partially reflected in the paper itself and that it could be significantly improved.\n\n\nQuality\n======\nThe paper proposes an extension of a (somewhat niche) method and evaluates it on a task that seems fairly constructed. The new approach consistently outperforms prior approaches.\n\nClarity\n=====\nThe paper reads well. Quite a lot of details are glossed over and some properties only become clear very late on.\n\nOriginality\n=========\nThe approach seems original.\n\nSignificance\n==========\nA wider applicability of the approach remains to be demonstrated.\n\nPros and Cons\n============\n+ challenging task\n+ interesting framework\n- constructed evaluation\n- many assumptions and a lot of prior knowledge needed\n- embedding in the state-of-the-art missing","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1511540246754,"tcdate":1511540177725,"number":1,"cdate":1511540177725,"id":"By50raBlf","invitation":"ICLR.cc/2018/Conference/-/Paper472/Official_Comment","forum":"S1szZkb0b","replyto":"S1szZkb0b","signatures":["ICLR.cc/2018/Conference/Paper472/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper472/AnonReviewer1"],"content":{"title":"many open questions...","comment":"What a long title...\n\n- I did not get why using the Euclidean distance is a good idea for the outcomes (e.g., when considering positions and orientations, forces, etc.)\n- Section 2.2: I was already wondering here why we are limited to combining 2 procedures\n- Section 2.2.: So we have a fixed sequence. How about recovery movements if something does not go as planned?\n- Algo 1: is totally unclear\n- I would have liked a more explicit discussion on what's different compared to (Nguyen & Oudeyer, 2012)\n- The task seems rather artificial\n- Sect. 3.1.: How limiting is always starting at the same position?\n- Sect. 3.2.2. \"the robot learns\", I don't get that, it learns which ones to use, but not the task spaces themselves, I think\n- Sect. 3: You include lots and lots of prior knowledge. How general is this approach? How quickly does it break down if the task spaces and/or teachers don't match the skills you want to learn?\n\nMinor comments:\n===============\n- Correct (opening) quote marks in LaTeX https://en.wikibooks.org/wiki/LaTeX/Text_Formatting#Quote-marks\n- The formatting of the references is strange \"(Lungarella et al. (2003))\" => \"(Lungarella et al., 2003)\"\n- (Theodorou et al., 2010) is a great paper but probably not the best reference for general RL\n- The related work all seem to be a bit on the old side (mainly up to 2010)\n- \"don't\" => \"do not\" etc.\n- would be nice to have variances for the figures\n- acknowledgements should be removed for double blind review\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]}},{"tddate":null,"ddate":null,"tmdate":1515158790021,"tcdate":1509122323103,"number":472,"cdate":1509739280695,"id":"S1szZkb0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1szZkb0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner","abstract":"We propose an active learning algorithmic architecture, capable of organizing its learning process in order to achieve a field of complex tasks by learning sequences of primitive motor policies : Socially Guided Intrinsic Motivation with Procedure Babbling (SGIM-PB). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized.\n\nWe introduce a new framework called \"procedures\", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our \"procedures\" increases the agent's capability to learn complex tasks.","pdf":"/pdf/f42d66613b13f037bd3d6af1172fee3e330bff10.pdf","TL;DR":"The paper describes a strategic intrinsically motivated learning algorithm which tackles the learning of complex motor policies.","paperhash":"anonymous|learning_a_set_of_interrelated_tasks_by_using_a_succession_of_motor_policies_for_a_socially_guided_intrinsically_motivated_learner","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1szZkb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper472/Authors"],"keywords":["developmental robotics","intrinsic motivation","strategic learning","complex motor policies"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}