{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222706814,"tcdate":1511839507829,"number":2,"cdate":1511839507829,"id":"HJ3Gw8clz","invitation":"ICLR.cc/2018/Conference/-/Paper635/Official_Review","forum":"S1EzRgb0W","replyto":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference/Paper635/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"In this paper, the authors aim to better understand the classification of neural networks. The authors explore the latent space of a variational auto encoder and consider the perturbations of the latent space in order to obtain the correct classification. They evaluate their method on CelebA and MNIST datasets.\n\nPros:\n\n 1) The paper explores an alternate methodology that uses perturbation in latent spaces to better understand neural networks \n2) It takes inspiration from adversarial examples and uses the explicit classifier loss to better perturb the $z$ in the latent space\n3) The method is quite simple and captures the essence of the problem well\n\nCons:\nThe main drawback of the paper is it claims to understand working of neural networks, however, actually what the authors end up doing are perturbations of the encoded latent space. This would evidently not explain why a deep network generates misclassifications for instance understanding the failure modes of ResNet or DenseNet cannot be obtained through this method. Other drawbacks include:\n\n1)     They do not show how their method would perform against standard adversarial attack techniques, since by explaining a neural network they should be able to guard against attacks, or at-least explain why they work well. \n2) The paper reports results on 2 datasets, out of which on 1 of them it does not perform well and gets stuck in a local minima therefore implying that it is not able to capture the diversity in the data well. \n\n3) The authors provide limited evaluation on a few attributes of CelebA. Extensive evaluation that would show on a larger scale with more attributes is not performed.\n\n4) The authors also have claimed that the added parts should be interpretable and visible. However, the perturbations of the latent space would yield small $\\epsilon$ variation in the image and it need not actually explain why the modification is yielding a correct classification, the same way an imperceptible adversarial attack yields a misclassification. Therefore there is no guarantee that the added parts would be interpretable. What would be more reasonable to claim would be that the latent transformations that yield correct classifications are projected into the original image space. Some of these yield interpretations that are semantically meaningful and some of these do not yield semantically meaningful interpretations.\n\n5)  Solving mis-classification does not seem to equate with explaining the neural network, but rather only suggest where it makes mistakes. That is not equal to an explanation about how it is making a classification decision. That would rather be done by using the same input and perturbing the weights of the classifier network. \n\nIn conclusion, the paper in its current form provides a direction in terms of using latent space exploration to understand classification errors and corrections to them in terms of perturbations of the latent space. However, these are not conclusive yet and actually verifying this would need a more thorough evaluation.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]}},{"tddate":null,"ddate":null,"tmdate":1512222706851,"tcdate":1511813267603,"number":1,"cdate":1511813267603,"id":"BJncxx9gf","invitation":"ICLR.cc/2018/Conference/-/Paper635/Official_Review","forum":"S1EzRgb0W","replyto":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference/Paper635/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Limited technical novelty. Unclear applicational benefit. ","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a method for explaining the classification mistakes of neural networks. For a misclassified image, gradient descent is used to find the minimal change to the input image so that it will be correctly classified. \n\nMy understanding is that the proposed method does not explain why a classifier makes mistakes. Instead, it is about: what can be added/removed from the input image so that it can be correctly classified. Strictly speaking, \"Explaining the Decisions of Neural Networks\" is not the most relevant title for the proposed method. \n\nBased on my understanding about what the paper proposes, I am not sure how useful this method is, from the application point of view. It is unclear to me how this method can shed light to the mistakes of a classifier. \n\nThe technical aspects of the paper are straight forward optimization problem, with a sensible formulation and gradient descent optimization problem. There is nothing extraordinary about the proposed technique. \n\nThe method assumes the availability of a generative model, VAE. The implicit assumption is that this VAE performs well, and it raises a concern about the application domains where VAE does not work well. In this case, would the visualization reflect the shortcoming of VAE or the mistake of the classifier? \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]}},{"tddate":null,"ddate":null,"tmdate":1509739189171,"tcdate":1509129739993,"number":635,"cdate":1509739186508,"id":"S1EzRgb0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}