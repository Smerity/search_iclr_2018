{"notes":[{"tddate":null,"ddate":null,"tmdate":1512328512979,"tcdate":1512328512979,"number":3,"cdate":1512328512979,"id":"B1IHpaWWM","invitation":"ICLR.cc/2018/Conference/-/Paper635/Official_Review","forum":"S1EzRgb0W","replyto":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference/Paper635/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting explanation for images, but lacks discussion about perturbations in the latent space which is most interesting part.","rating":"6: Marginally above acceptance threshold","review":"Summary: The authors propose a method for explaining why neural networks make mistakes by learning how to modify an image on a mistaken classification to make it a correct classification. They do this by perturbing the image in an encoded latent space and then reconstructing the perturbed image.  The explanation is the difference between the reconstructed perturbed encoded image and the reconstructed original encoded image.\n\nThe title is too general as this paper only offers an explanation in the area of image classification, which by itself, is still interesting.  \n\nA method for explaining the results of neural networks is still open ended and visually to the human eye, this paper does offer an explanation of why the 8 is misclassified. However, if this works very well for MNIST, more examples should be given. This single example is interesting but not sufficient to illustrate the success of this method.  \n\nThe examples from CelebA are interesting but inconclusive. For example, why should adding blue to the glasses fix the misclassification. If the explanation is simply visual for a human, then this explanation does not suffice. And the two examples with one receding and the other not receding hairlines look like their correct classifications could be flipped.\n\nRegarding epsilon, it is unclear what a small euclidean distance for epsilon is without more examples. It would also help to see how the euclidean distance changes along the path.  But also it is not clear why we care about the size of epsilon, but rather the size of the perturbation that must be made to the original image, which is what is defined in the paper as the explanation. \n\nSince it is the encoded image that is perturbed, and this is what causes the perturbations to be selective to particular features of the image, an analysis of what features in the encoded space that are modified would greatly help in the interpretability of this explanation. The fact that perturbations are made in the latent space, and that this perturbation gets reflected in particular areas in the reconstructed image, is the most interesting part of this work.  More discussion around this would greatly enhance the paper, especially since the technical tools of this method are not very strong.\n\nPros: Interesting explanation, visually selects certain parts of the image relevant to classification rather than obscure pixels\n\nCons: No discussion or analysis about the latent space where perturbations occur. Only one easy example from MNIST shown and examples on CelebA are not great. No way (suggested) to use this tool outside of image recognition. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]}},{"tddate":null,"ddate":null,"tmdate":1512222706814,"tcdate":1511839507829,"number":2,"cdate":1511839507829,"id":"HJ3Gw8clz","invitation":"ICLR.cc/2018/Conference/-/Paper635/Official_Review","forum":"S1EzRgb0W","replyto":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference/Paper635/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"In this paper, the authors aim to better understand the classification of neural networks. The authors explore the latent space of a variational auto encoder and consider the perturbations of the latent space in order to obtain the correct classification. They evaluate their method on CelebA and MNIST datasets.\n\nPros:\n\n 1) The paper explores an alternate methodology that uses perturbation in latent spaces to better understand neural networks \n2) It takes inspiration from adversarial examples and uses the explicit classifier loss to better perturb the $z$ in the latent space\n3) The method is quite simple and captures the essence of the problem well\n\nCons:\nThe main drawback of the paper is it claims to understand working of neural networks, however, actually what the authors end up doing are perturbations of the encoded latent space. This would evidently not explain why a deep network generates misclassifications for instance understanding the failure modes of ResNet or DenseNet cannot be obtained through this method. Other drawbacks include:\n\n1)     They do not show how their method would perform against standard adversarial attack techniques, since by explaining a neural network they should be able to guard against attacks, or at-least explain why they work well. \n2) The paper reports results on 2 datasets, out of which on 1 of them it does not perform well and gets stuck in a local minima therefore implying that it is not able to capture the diversity in the data well. \n\n3) The authors provide limited evaluation on a few attributes of CelebA. Extensive evaluation that would show on a larger scale with more attributes is not performed.\n\n4) The authors also have claimed that the added parts should be interpretable and visible. However, the perturbations of the latent space would yield small $\\epsilon$ variation in the image and it need not actually explain why the modification is yielding a correct classification, the same way an imperceptible adversarial attack yields a misclassification. Therefore there is no guarantee that the added parts would be interpretable. What would be more reasonable to claim would be that the latent transformations that yield correct classifications are projected into the original image space. Some of these yield interpretations that are semantically meaningful and some of these do not yield semantically meaningful interpretations.\n\n5)  Solving mis-classification does not seem to equate with explaining the neural network, but rather only suggest where it makes mistakes. That is not equal to an explanation about how it is making a classification decision. That would rather be done by using the same input and perturbing the weights of the classifier network. \n\nIn conclusion, the paper in its current form provides a direction in terms of using latent space exploration to understand classification errors and corrections to them in terms of perturbations of the latent space. However, these are not conclusive yet and actually verifying this would need a more thorough evaluation.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]}},{"tddate":null,"ddate":null,"tmdate":1512222706851,"tcdate":1511813267603,"number":1,"cdate":1511813267603,"id":"BJncxx9gf","invitation":"ICLR.cc/2018/Conference/-/Paper635/Official_Review","forum":"S1EzRgb0W","replyto":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference/Paper635/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Limited technical novelty. Unclear applicational benefit. ","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a method for explaining the classification mistakes of neural networks. For a misclassified image, gradient descent is used to find the minimal change to the input image so that it will be correctly classified. \n\nMy understanding is that the proposed method does not explain why a classifier makes mistakes. Instead, it is about: what can be added/removed from the input image so that it can be correctly classified. Strictly speaking, \"Explaining the Decisions of Neural Networks\" is not the most relevant title for the proposed method. \n\nBased on my understanding about what the paper proposes, I am not sure how useful this method is, from the application point of view. It is unclear to me how this method can shed light to the mistakes of a classifier. \n\nThe technical aspects of the paper are straight forward optimization problem, with a sensible formulation and gradient descent optimization problem. There is nothing extraordinary about the proposed technique. \n\nThe method assumes the availability of a generative model, VAE. The implicit assumption is that this VAE performs well, and it raises a concern about the application domains where VAE does not work well. In this case, would the visualization reflect the shortcoming of VAE or the mistake of the classifier? \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]}},{"tddate":null,"ddate":null,"tmdate":1509739189171,"tcdate":1509129739993,"number":635,"cdate":1509739186508,"id":"S1EzRgb0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1EzRgb0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Explaining the Decisions of Neural Networks with Latent Sympathetic Examples","abstract":"Neural networks make mistakes. The reason why a mistake is made often remains a mystery. As such neural networks often are considered a black box. It would be useful to have a method that can give an explanation that is intuitive to a user as to why an image is misclassified. In this paper we develop a method for explaining the mistakes of a classifier model by visually showing what must be added to an image such that it is correctly classified. Our work combines the fields of adversarial examples, generative modeling and a correction technique based on difference target propagation to create an technique that creates explanations of why an image is misclassified. In this paper we explain our method and demonstrate it on MNIST and CelebA. This approach could aid in demystifying neural networks for a user.\n","pdf":"/pdf/f579a9aa7661dd81873b7ffafa2f3a97d6aad8a5.pdf","TL;DR":"New way of explaining why a neural network has misclassified an image","paperhash":"anonymous|explaining_the_decisions_of_neural_networks_with_latent_sympathetic_examples","_bibtex":"@article{\n  anonymous2018explaining,\n  title={Explaining the Decisions of Neural Networks with Latent Sympathetic Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1EzRgb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper635/Authors"],"keywords":["Deep learning","Adversarial Examples","Difference Target Propagation","Generative Modelling","Classifiers","Explaining","Sympathetic Examples"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}