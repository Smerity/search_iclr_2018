{"notes":[{"tddate":null,"ddate":null,"tmdate":1512278290451,"tcdate":1512278290451,"number":1,"cdate":1512278290451,"id":"By5zFb-bG","invitation":"ICLR.cc/2018/Conference/-/Paper523/Official_Comment","forum":"B1zlp1bRW","replyto":"rJ81OAtgM","signatures":["ICLR.cc/2018/Conference/Paper523/AnonReviewer4"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper523/AnonReviewer4"],"content":{"title":"Convergence rate","comment":"\"In particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential.\"\n\nThe direct empirical Wasserstein estimator actually has a rate exponential in the input dimension (Sriperumbudur et al. 2012, On the empirical estimation of integral probability metrics, Corollary 3.5 â€“ this is an upper bound and I don't know if there's a known matching lower bound, but I think it's relatively accepted that this is the case), so it's probably likely that this estimator's rate would be as well in the general nonparametric case. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]}},{"tddate":null,"ddate":null,"tmdate":1512277744338,"tcdate":1512277744338,"number":3,"cdate":1512277744338,"id":"H1dxvZWWM","invitation":"ICLR.cc/2018/Conference/-/Paper523/Official_Review","forum":"B1zlp1bRW","replyto":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference/Paper523/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Interesting idea for expanding optimal transport estimation, but some aspects unclear","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a new method for estimating optimal transport plans and maps among continuous distributions, or discrete distributions with large support size. First, the paper proposes a dual algorithm to estimate Kantorovich plans, i.e. a coupling between two input distributions minimizing a given cost function, using dual functions parameterized as neural networks. Then an algorithm is given to convert a generic plan into a Monge map, a deterministic function from one domain to the other, following the barycenter of the plan. The algorithms are shown to be consistent, and demonstrated to be more efficient than an existing semi-dual algorithm. Initial applications to domain adaptation and generative modeling are also shown.\n\nThese algorithms seem to be an improvement over the current state of the art for this problem setting, although more of a discussion of the relationship to the technique of Genevay et al. would be useful: how does your approach compare to the full-dual, continuous case of that paper if you simply replace their ball of RKHS functions with your class of deep networks?\n\nThe consistency properties are nice, though they don't provide much insight into the rate at which epsilon should be decreased with n or similar properties. The proofs are clear, and seem correct on a superficial readthrough; I have not carefully verified them.\n\nThe proofs are mainly limited in that they don't refer in any way to the class of approximating networks or the optimization algorithm, but rather only to the optimal solution. Although of course proving things about the actual outcomes of optimizing a deep network is extremely difficult, it would be helpful to have some kind of understanding of how the class of networks in use affects the solutions. In this way, your guarantees don't say much more than those of Arjovsky et al., who must assume that their \"critic function\" reaches the global optimum: essentially you add a regularization term, and show that as the regularization decreases it still works, but under seemingly the same kind of assumptions as Arjovsky et al.'s approach which does not add an explicit regularization term at all. Though it makes sense that your regularization might lead to a better estimator, you don't seem to have shown so either in theory or empirically.\n\nThe performance comparison to the algorithm of Genevay et al. is somewhat limited: it is only on one particular problem, with three different hyperparameter settings. Also, since Genevay et al. propose using SAG for their algorithm, it seems strange to use plain SGD; how would the results compare if you used SAG (or SAGA/etc) for both algorithms?\n\nIn discussing the domain adaptation results, you mention that the L2 regularization \"works very well in practice,\" but don't highlight that although it slightly outperforms entropy regularization in two of the problems, it does substantially worse in the other. Do you have any guesses as to why this might be?\n\nFor generative modeling: you do have guarantees that, *if* your optimization and function parameterization can reach the global optimum, you will obtain the best map relative to the cost function. But it seems that the extent of these guarantees are comparable to those of several other generative models, including WGANs, the Sinkhorn-based models of Genevay et al. (2017, https://arxiv.org/abs/1706.00292/), or e.g. with a different loss function the MMD-based models of Li, Swersky, and Zemel (ICML 2015) / Dziugaite, Roy, and Ghahramani (UAI 2015). The different setting than the fundamental GAN-like setup of those models is intriguing, but specifying a cost function between the source and the target domains feels exceedingly unnatural compared to specifying a cost function just within one domain as in these other models.\n\nMinor:\n\nIn (5), what is the purpose of the -1 term in R_e? It seems to just subtract a constant 1 from the regularization term.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]}},{"tddate":null,"ddate":null,"tmdate":1512222679624,"tcdate":1511997906456,"number":2,"cdate":1511997906456,"id":"B1cR-6neM","invitation":"ICLR.cc/2018/Conference/-/Paper523/Official_Review","forum":"B1zlp1bRW","replyto":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference/Paper523/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting paper on large scale optimal transport, though \"overstating\" some properties ","rating":"6: Marginally above acceptance threshold","review":"The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems.\n\nI like the weak convergence results, but this is just weak convergence. It appears to be an overstatement to claim that the approach \"nearly-optimally\" transports one distribution to the other (Cf e.g. Conclusion). There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2. Also, near-optimality would refer to some parameters being chosen in the best possible way. I do not see that from the paper. However, the weak convergence results are good.\n\nA better result, hinting on how \"optimal\" this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist). This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum. \n\nI also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ? After all, the L2 one is just an approximation of the entropic one.\n\nTypoes:\n\n1- Kanthorovich -> Kantorovich (Intro)\n2- Cal C <-> C (eq. 4)","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]}},{"tddate":null,"ddate":null,"tmdate":1512222679667,"tcdate":1511806941875,"number":1,"cdate":1511806941875,"id":"rJ81OAtgM","invitation":"ICLR.cc/2018/Conference/-/Paper523/Official_Review","forum":"B1zlp1bRW","replyto":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference/Paper523/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper presents interestings results about consistency of learning OT/Monge maps although weak and stochastic learning algorithms able to scale, however some parts should deserve more discussion and experimental evaluation is limited.","rating":"6: Marginally above acceptance threshold","review":"Quality\nThe theoretical results presented in the paper appear to be correct. However, the experimental evaluation is globally limited,  hyperparameter tuning on test which is not fair.\n\nClarity\nThe paper is mostly clear, even though some parts deserve more discussion/clarification (algorithm, experimental evaluation).\n\nOriginality\nThe theoretical results are original, and the SGD approach is a priori original as well.\n\nSignificance\nThe relaxed dual formulation and OT/Monge maps convergence results are interesting and can of of interest for researchers in the area, the other aspects of the paper are limited.\n\nPros:\n-Theoretical results on the convergence of OT/Monge maps\n-Regularized formulation compatible with SGD\nCons\n-Experimental evaluation limited\n-The large scale aspect lacks of thorough analysis\n-The paper presents 2 contributions but at then end of the day, the development of each of them appears limited\n\nComments:\n\n-The weak convergence results are interesting. However, the fact that no convergence rate is given makes the result weak. \nIn particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential.\nThis can be coherent with the problem of Domain Adaptation that can be NP-hard even under the co-variate shift assumption (Ben-David&Urner, ALT2012).\nThen, I think that the claim of page 6 saying that Domain Adaptation can be performed \"nearly optimally\" has then to be rephrased.\nI think that results show that the approach is theoretically justified but optimality is not here yet.\n\nTheorem 1 is only valid for entropy-based regularizations, what is the difficulty for having a similar result with L2 regularization?\n\n-The experimental evaluation on the running time is limited to one particular problem. If this subject is important, it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations.\nIt is also surprising that the efficiency the L2-regularized version is not evaluated.\nFor a paper interesting in large scale aspects, the experimental evaluation is rather weak.\n \nThe 2 methods compared in Fig 2 reach the same objective values at convergence, but is there any particular difference in the solutions found?\n\n-Algorithm 1 is presented without any discussion about complexity, rate of convergence. Could the authors discuss this aspect?\nThe presentation of this algo is a bit short and could deserve more space (in the supplementary)\n\n-For the DA application, the considered datasets are classic but not really \"large scale\", anyway this is a minor remark.\nThe setup is not completely clear, since the approach is interesting for out of sample data, so I would expect the map to be computed on a small sample of source data, and then all source instances to be projected on target with the learned map. This point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point while this is an interesting aspect of the paper: this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification.\nHyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup: it seems that the parameters are tuned on test (for all methods), which is not fair since target label information will not be available from a practical standpoint.\n\nThe authors claim that they did not want to compete with state of the art DA, but the approach of Perrot et al., 2016 seems to a have a similar objective and could be used as a baseline.\n\nExperiments on generative optimal transport are interesting and probably generate more discussion/perspectives.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]}},{"tddate":null,"ddate":null,"tmdate":1509739256239,"tcdate":1509125354302,"number":523,"cdate":1509739253576,"id":"B1zlp1bRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1zlp1bRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Large Scale Optimal Transport and Mapping Estimation","abstract":"This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a Monge map as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.","pdf":"/pdf/6edc7db3dc8c40587130ca9b6ca64d9a08199efe.pdf","TL;DR":"Learning optimal mapping with deepNN between distributions along with theoretical guarantees.","paperhash":"anonymous|large_scale_optimal_transport_and_mapping_estimation","_bibtex":"@article{\n  anonymous2018large,\n  title={Large Scale Optimal Transport and Mapping Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1zlp1bRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper523/Authors"],"keywords":["optimal transport","Wasserstein","domain adaptation","generative models","Monge map","optimal mapping"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}