{"notes":[{"tddate":null,"ddate":null,"tmdate":1515041650138,"tcdate":1515041650138,"number":5,"cdate":1515041650138,"id":"BJq_m4imf","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"HkdCWV0Ab","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"STS14 evaluation","comment":"Thank you for your comment.\n\nWe have included an evaluation of our models on the STS14 task in Appendix C of the supplementary material. \n\nWe evaluate RNN-based and Bag-of-words encoders trained using our objective on this task. Our RNN-based encoder performs strongly compared to previous sequence encoders. Bag-of-words models are known to perform strongly in this task as they are better able to encode word identity information. Our BoW variation performs comparably (or slightly better) than prior BoW based models such as FastSent and Siamese CBOW."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1515188081119,"tcdate":1513752632583,"number":4,"cdate":1513752632583,"id":"SkWHdKwzz","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Rebuttal","comment":"We thank the reviewers for the helpful comments.\n\nR1, R3: Skip-thoughts training time\nWe agree that training the model could be faster with current hardware and software libraries. A more recent implementation of the skip-thoughts model was released by Google early this year [1]. This implementation mentions that the model takes 9 days to train on a GTX 1080 GPU. Training our proposed models on a GTX 1080 takes 11 hours. Both implementations are based on Tensorflow. Our experiment used cuda 8.0 and cuDNN 6.0 libraries. This also agrees with the numbers in the paper which were based on experiments using GTX TITAN X.\n\nR1, R3: Training speed comparison\nWe performed a comparison on the training efficiency of lower-dimensional versions of our model and the skip-thoughts model. The same encoder architecture was trained in identical conditions using our objective and the skip-thoughts objectives and models were evaluated on downstream tasks after a given number of hours. Experimental results are reported in section C of the appendix. The training efficiency of our model compared to the skip-thoughts model is clear from these experiments.\n\nR1: BoW encoders, sentence similarity evaluations\nWe train BoW encoders using our training objective and evaluate them on textual similarity tasks. Experiments and results are discussed in section B of the appendix. Our RNN-based encoder performs strongly against prior sequence models. Our BoW encoder performs comparably to (or slightly better than) popular BoW representations as well.\n\nR2: Balance between performance and computational complexity\n1) Increasing the candidate pool size - We found that RNN encoders are less sensitive to increasing the candidate pool size. Sentences appearing in the context of a given query sentence are natural candidates for the contrastive sentences since they are more likely to be related to the query sentence, and hence make the prediction problem challenging. We observed marginal performance improvements as we added more random choices to the candidate pool.\n2) Increasing corpus size - We have experiments in the paper with increased corpus size. We considered the UMBC corpus (which is about 3 times the size of BookCorpus) and show that augmenting the BookCorpus dataset enables us to obtain monotonic improvements on the downstream tasks.\n3) Increasing embedding size - We have included experiments on varying the embedding size in section D of the supplementary material. We are able to train bigger and better models at the expense of more training time. The smaller models can be trained more efficiently while still being competitive or better than state-of-the-art higher-dimensional models. \nWe also plan to release pre-trained models for different representation sizes so that other researchers/practitioners can use the appropriate size depending on the downstream task and the amount of labelled data available.\n\nR1: Point of learning sentence representations\nIn the vision community it has become common practice to use CNN features (e.g., AlexNet, VGGNet, ResNet, etc.) pre-trained from the large-scale imagenet database for a variety of downstream tasks (e.g., the image-caption experiment in our paper uses pre-trained CNN features as the image embedding). Our overarching goal is to learn analogous high-quality sentence representations in the text domain. The representations can be used as feature vectors for downstream tasks, as we do in the paper. The encoders can also be used for parameter initialization and fine-tuned on data relevant to a particular application. In this respect, we believe that exploring scalable unsupervised learning algorithms for learning ‘universal’ text representations is an important research problem.\n\nR1: Image-caption retrieval experiments\nWe have updated the description of the image-caption retrieval experiments. We hope the description is more clear now and provides better motivation for the task.\n\nR1: Nearest neighbors\nAs we discuss in the paper, the query sentences used for the nearest neighbor experiment were chosen randomly and not cherry picked. We hope the cosine similarity experiments quantify the nearest neighbor analysis.\n\nR2: We have added more details about the architecture and training to the paper (sec 4.3).\n\nWe will release the source code upon publication.\n\nR3: Evaluation\nThe evaluation on downstream tasks was performed using the evaluation scripts from Kiros et al. since most of the unsupervised methods we compare against were published either before (Kiros et al., Hill et al.) or about the same time (Gan et al.) the SentEval tool was released.\n\nR1, R2, R3:\nWe have updated the paper to reflect your comments and concerns. Modifications are highlighted in blue (Omissions not shown). We have added relevant citations pointed out by reviewers and public comments.\n\nReferences\n[1] https://github.com/tensorflow/models/tree/master/research/skip_thoughts"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1515642487316,"tcdate":1511969867750,"number":3,"cdate":1511969867750,"id":"ByVL483xf","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Review","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Intuitive model for sentence representations with good performance","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences. Unique aspects of this skip-gram style model include separate target- and context-sentence encoders, as well as a dot-product similarity measure between representations. A battery of experiments indicate that the learned representations have comparable or better performance compared to other, more computationally-intensive models.\n\nWhile the main constituent ideas of this paper are not entirely novel, I think the specific combination of tools has not been explored previously. As such, the novelty of this paper rests in the specific modeling choices and the significance hinges on the good empirical results. For this reason, I believe it is important that additional details regarding the specific architecture and training details be included in the paper. For example, how many layers is the GRU? What type of parameter initialization is used? Releasing source code would help answer these and other questions, but including more details in the paper itself would also be welcome.\n\nRegarding the empirical results, the method does appear to achieve good performance, especially given the compute time. However, the balance between performance and computational complexity is not investigated, and I think such an analysis would add significant value to the paper. For example, I see at least three ways in which performance could be improved at the expense of additional computation: 1) increasing the candidate pool size 2) increasing the corpus size and 3) increasing the embedding size / increasing the encoder capacity. Does the good performance/efficiency reported in the paper depend on achieving a sweet spot among those three hyperparameters?\n\nOverall, the novelty of this paper is fairly low and there is still substantial room for improvement in some of the analysis. On the other hand, I think this paper proposes an intuitive model and demonstrates good performance. I am on the fence, but ultimately I vote to accept this paper for publication.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1515710092520,"tcdate":1511885721558,"number":2,"cdate":1511885721558,"id":"rJMoj-jxf","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Review","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An elegant and simple alternative to existing methods, but empirical advantages are unclear ","rating":"6: Marginally above acceptance threshold","review":"[REVISION]\n\nThank you for your clarification. I appreciate the effort and think it has improved the paper. I have updated my score accordingly\n\n====== \n\nThis paper proposes a new objective for learning SkipThought-style sentence representations from corpora of ordered sentences. The algorithm is much faster than SkipThoughts as it swaps the word-level decoder for a contrastive classification loss. \n\nComments:\n\nSince one of the key advantages of this method is the speed, I was surprised there was not a more formal comparison of the speed of training different models. For instance, it would be more convincing if two otherwise identical encoders were trained on the same machine on the books corpus with the proposed objective and the skipthoughts decoding objective, and the representations compared after X hours of training. The reported 2 weeks required to train Skipthoughts comes from the paper, but things might be faster now with more up-to-date deep learning libraries etc. If this was what was in fact done, then it's probably just a case of presenting the comparison in a more formal way. I would also lose the sentence \"we are able to train many models in the time it takes to train most unsupervised\" (see next point for reasons why this is questionable).\n\nIt would have been interesting to apply this method with BOW encoders, which should be even faster than RNN-based encoders reported in this paper. The faster BOW models tend to give better performance on cosine-similarity evaluations ( quantifying the nearest-neighbour analysis that the authors use in this paper). Indeed, it would be interesting (although of course not definitive) to see comparison of the proposed algorithm (with BOW and RNN encoders) on cosine sentence similarity evaluations. \n\nThe proposed novelty is simple and intuitive, which I think is a strength of the method. However, a simple idea makes overlap with other proposed approaches more likely, and I'd like the author to check through the public comments to ensure that all previous related ideas are noted in this paper. \n\nI think the authors could do more to emphasise what the point is of trying to learn sentence embeddings. An idea of the eventual applications of these embeddings would make it easier to determine, for instance, whether the supervised ensembling method applied here would be applicable in practice. Moreover, many papers have emphasised the limitations of the evaluations used in this paper (although they are still commonly used) so it would be good to acknowledge that it's hard to draw too many conclusions from such numbers. That said, the numbers are comparable Skipthoughts, so it's clear that this method learns representations of comparable quality. \n\nThe justification for the proposed algorithm is clear in terms of efficiency, but I don't think it's immediately clear from a semantic / linguistic point of view. The statement \"The meaning of a sentence is the property that creates bonds....\" seems to have been cooked up to justify the algorithm, not vice versa. I would cut all of that speculation out and focus on empirically verifiable advantages. \n\nThe section of image embeddings comes completely out of the blue and is very hard to interpret. I'm still not sure I understand this evaluation (short of looking up the Kiros et al. paper), or how the proposed model is applied to a multi-modal task.\n\nThere is much scope to add more structured analysis of the type hinted by the nearest neighbours section. Cherry picked lists don't tell the reader much, but statistics or more general linguistic trends can be found in these neighbours and aggregated, that could be very interesting. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1515642487398,"tcdate":1511129947613,"number":1,"cdate":1511129947613,"id":"SJNPXFyeM","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Review","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference/Paper661/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Great results, with minor concerns","rating":"8: Top 50% of accepted papers, clear accept","review":"==Update==\n\nI appreciate the response, and continue to recommend acceptance. The evaluation metric used in this paper (SentEval) represents an important open problem in NLP—learning reusable sentence representations—and one of the problems in NLP best suited to presentation at IC*LR*. Because of this, I'm willing to excuse the fact that the paper is only moderately novel, in light of the impressive reported results.\n\nWhile I would appreciate a direct (same codebase, same data) comparison with some outside baselines, this paper meets or exceeds the standards for rigor that were established by previous published work in the area, and the existing results are sufficient to support some substantial conclusions.\n\n==========\n\nThis paper proposes an alternative formulation of Kiros's SkipThought objective for training general-purpose sentence encoder RNNs on unlabeled data. This formulation replaces the decoder in that model with a second encoder, and yields substantial improvements to both speed and model performance (as measured on downstream transfer tasks). The resulting model is, for the first time, reasonably competitive even with models that are trained end-to-end on labeled data for the downstream tasks (despite the requirement, imposed by the evaluation procedure, that only the top layer classifier be trained for the downstream tasks here), and is also competitive with models trained on large labeled datasets like SNLI. The idea is reasonable, the topic is important, and the results are quite strong. I recommend acceptance, with some caveats that I hope can be addressed.\n\nConcerns:\n\nA nearly identical idea to the core idea of this paper was proposed in an arXiv paper this spring, as a commenter below pointed out. That work has been out for long enough that I'd urge you to cite it, but it was not published and it reports results that are far less impressive than yours, so that omission isn't a major problem.\n\nI'd like to see more discussion of how you performed your evaluation on the downstream tasks. Did you use the SentEval tool from Conneau et al., as several related recent papers have? If not, does your evaluation procedure differ from theirs or Kiros's in any meaningful way?\n\nI'm also a bit uncomfortable that the paper doesn't directly compare with any baselines that use the exact same codebase, word representations, hyperparameter tuning procedure, etc.. I would be more comfortable with the results if, for example, the authors compared a low-dimensional version of their model with a low-dimensional version of SkipThought, trained in the *exact* same way, or if they implemented the core of their model within the SkipThought codebase and showed strong results there.\n\nMinor points:\n\nThe headers in Table 1 don't make it all that clear which additions (vectors, UMBC) are cumulative with what other additions. This should be an easy fix. \n\nThe use of the check-mark as an output in Figure 1 doesn't make much sense, since the task is not binary classification.\n\n\"Instead of training a model to reconstruct the surface form of the input sentence or its neighbors, our formulation attempts to focus on the semantic aspects of sentences. The meaning of a sentence is the property that creates bonds between a sequence of sentences and makes it logically flow.\" – It's hard to pin down exactly what this means, but it sounds like you're making an empirical claim here: semantic information is more important than non-semantic sources of variation (syntactic/lexical/morphological factors) in predicting the flow of a text. Provide some evidence for this, or cut it.\n\nYou make a similar claim later in the same section: \"In figure 1(a) however, the reconstruction loss forces the model to predict local structural information about target sentences that may be irrelevant to its meaning (e.g., is governed by grammar rules).\" This is a testable prediction: Are purely grammatical (non-semantic) variations in sentence form helpful for your task? I'd suspect that they are, at least in some cases, as they might give you clues as to style, dialect, or framing choices that the author made when writing that specific passage.\n\n\"Our best BookCorpus model (MC-QT) trains in just under 11hrs, compared to skip-thought model’s training time of 2 weeks.\" –  If you say this, you need to offer evidence that your model is faster. If you don't use the same hardware and low-level software (i.e., CuDNN), this comparison tells us nearly nothing. The small-scale replication of SkipThought described above should address this issue, if performed.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509994960021,"tcdate":1509994960021,"number":5,"cdate":1509994960021,"id":"HkdCWV0Ab","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"Sk1dtLqAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Evaluation","comment":"Thanks for your reply.\n\nThere is an interesting difference between the evaluation tasks and the evaluation task used in Siamese CBOW.\n\nIn Siamese CBOW, they mainly focused on unsupervised evaluation tasks, including STS12, 13, and 14. The similarity of 2 sentences is determined by Cosine-similarity, which matches their training objective. Compared with FastSent which applies the dot-product as training objective in FastSent, Siamese CBOW seems to get better results.\n\nCould you also evaluate your proposed model on unsupervised evaluation tasks, like STS14? It would be good to have a comprehensive evaluation of your model. Thanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1510092427164,"tcdate":1509742951006,"number":3,"cdate":1509742951006,"id":"Sk1dtLqAW","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"SyctLIIC-","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Siamese CBOW","comment":"Sure, Thanks. \n\nIn this paper a conceptually similar task of identifying context sentences from candidate sentences based on their bag-of-words representations is considered. Our approach is more general than this work in the following ways\n* Our formulation considers more general scoring functions/classifiers. We found inner products to work best. Using cosine distance as is done in this work led to inferior representations. Cosine distance implicitly requires sentence representations to both lie on the unit ball and be similar (in terms of inner product) to context sentences, which can be a strong constraint. The inner products scoring function only requires the latter. \n* This work uses the same set of parameters to encode both input and context sentences, while we consider using different sets of parameters. This helped learn better representations. We briefly discuss this choice in section 3.\n* Our formulation also allows the use of more general encoder architectures.\n\nAlso, we discuss more recent bag-of-words methods in the paper. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509571954878,"tcdate":1509571954878,"number":4,"cdate":1509571954878,"id":"BJidT3DCb","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Data volume question","comment":"Just out of curiosity, do you have any results on how the quantity of unlabeled training data you use impacts model performance?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509480123323,"tcdate":1509480066526,"number":3,"cdate":1509480066526,"id":"SyctLIIC-","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"SJpU59rCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Siamese CBOW","comment":"Thank you for your reply! Could you also compare your idea with Siamese CBOW? (ACL2016)\n\nhttp://www.aclweb.org/anthology/P16-1089\n\nThanks again!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1510092427207,"tcdate":1509431893238,"number":2,"cdate":1509431893238,"id":"SJpU59rCW","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"BkzJW9QC-","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Related paper","comment":"Thank you for your comment. We will include the paper in a revised version. Please see our response to the previous comment regarding the same paper. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1510092427248,"tcdate":1509430766886,"number":1,"cdate":1509430766886,"id":"SJPgI5BR-","invitation":"ICLR.cc/2018/Conference/-/Paper661/Official_Comment","forum":"rJvJXZb0W","replyto":"SJbLL_zCb","signatures":["ICLR.cc/2018/Conference/Paper661/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper661/Authors"],"content":{"title":"Objective function","comment":"Thank you for your comments. We will include these literature in a revised version of the paper. \n\nDespite similarities in the objective functions, we would like to point out the following key distinctions.\n\nJernite et al. propose to use paragraph level coherence as a learning signal. The following related task is considered in their paper. Given the first three sentences of a paragraph, they choose the next sentence from five candidate sentences later in the paragraph (Paragraphs of length at least 8 are considered). \nOur objective differs from theirs in the following aspects.\n* This work exploits paragraph level coherence signals for learning, while our work derives motivation from the distributional hypothesis. We don’t restrict ourselves to paragraphs in the data as is done in this work. \n* We consider a large number of candidate sentence choices when predicting a context sentence. This is a discriminative approximation to the generation objective (viewing generation as choosing a sentence from all possible sentences)\n* We use a single input sentence and predict the context sentences surrounding it. Using larger input contexts did not yield any significant empirical benefits.\nOur objective further learns richer representations compared to this work, as evidenced by empirical results. \n\nThe local coherence model of Li & Hovy is a feed-forward network which examines a window of sentence embeddings and classifies them as coherent/incoherent (binary classification). We have some discussion about this objective in the paper (section 3). We point out the following key differences between our objective and theirs. \n* Instead of discriminating context windows as plausible/implausible, we encourage observed contexts (in the data) to be more plausible than contrastive (implausible) ones and formulate it as a multi-class classification problem. We experimentally found that this relaxed constraint helps learn better representations.\n* We use a simple scoring function (inner products) in our objective. When using a parameterized classifier, the model has a tendency to learn poor sentence representations and compensate for it using a strong classifier. This is undesirable since the classifier is discarded and only the sentence encoders are used for feature extraction.\n\nHence, Li & Hovy’s objective is better suited for local coherence modeling than it is for learning sentence representations.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509298393854,"tcdate":1509298393854,"number":2,"cdate":1509298393854,"id":"BkzJW9QC-","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"a related paper","comment":"https://arxiv.org/pdf/1705.00557.pdf\n\nThe proposed method in the listed paper is quite close to the one proposed in this submission. I think it'll be good to cite this listed paper and discuss it. (Although I know it is not required to cite arxiv papers.)\n\n(Also, I am not related to the listed arxiv paper, but I'd love to some comprehensive comparisons among existing methods.)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1509226057339,"tcdate":1509226057339,"number":1,"cdate":1509226057339,"id":"SJbLL_zCb","invitation":"ICLR.cc/2018/Conference/-/Paper661/Public_Comment","forum":"rJvJXZb0W","replyto":"rJvJXZb0W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Some literature review suggestions","comment":"This article proposed a framework to learn sentence representation and demonstrated some good results.\n\nIn terms of the main objective of this task - predicting the next sentence out of a group of sampled sentences has already been proposed multiple times in the NLP community. For example, it appeared earlier this year: https://arxiv.org/abs/1705.00557, and a much earlier work (in 2014) has also used sentence ordering to learn sentence representation: http://web.stanford.edu/~jiweil/paper/emnlp_coherence-v2eh.pdf\n\nI am certain this paper brings unique value and insight into this training objective, and is a much-needed addition to the existing pool of literature. I just hope maybe in a revised version of this paper, the author(s) would reference these previous NLP works.\n\n(To clarify on my intent: I am not related to any of these papers, but would love to see NLP researches get recognized.)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]}},{"tddate":null,"ddate":null,"tmdate":1515190334388,"tcdate":1509130975187,"number":661,"cdate":1509739172130,"id":"rJvJXZb0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJvJXZb0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"An efficient framework for learning sentence representations","abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","pdf":"/pdf/ada80596da56dd4dbe2c13d7317fd37e6bcef77c.pdf","TL;DR":"A framework for learning high-quality sentence representations efficiently.","paperhash":"anonymous|an_efficient_framework_for_learning_sentence_representations","_bibtex":"@article{\n  anonymous2018an,\n  title={An efficient framework for learning sentence representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJvJXZb0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper661/Authors"],"keywords":["sentence","embeddings","unsupervised","representations","learning","efficient"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}