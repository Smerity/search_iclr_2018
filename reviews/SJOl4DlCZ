{"notes":[{"tddate":null,"ddate":null,"tmdate":1512332635117,"tcdate":1512332635117,"number":3,"cdate":1512332635117,"id":"BkQD60b-f","invitation":"ICLR.cc/2018/Conference/-/Paper290/Official_Review","forum":"SJOl4DlCZ","replyto":"SJOl4DlCZ","signatures":["ICLR.cc/2018/Conference/Paper290/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice idea, but still need some work","rating":"4: Ok but not good enough - rejection","review":"The paper proposes the use of a GAN to learn the distribution of image classes from an existing classifier, that is a nice and straightforward idea. From the point of view of forensic analysis of a classifier, it supposes a more principled strategy than a brute force attack based on the classification of a database and some conditional density estimation of some intermediate image features. Unfortunately, the experiments are inconclusive.  \n\nQuality: The key question of the proposed scheme is the role of the auxiliary dataset. In the EMNIST experiment, the results for the “exact same” and “partly same” situations are good, but it seems that for the “mutually exclusive” situation the generated samples look like letters, not numbers, and raises the question on the interpolation ability of the generator. In the FaceScrub experiment is even more difficult to interpret the results, basically because we do not even know the full list of person identities. It seems that generated images contain only parts of the auxiliary images related to the most discriminative features of the given classifier. Does this imply that the GAN models a biased probability distribution of the image class? What is the result when the auxiliary dataset comes from a different kind of images? Due to the difficulty of evaluating GAN results, more experiments are needed to determine the quality and significance of this work.\n\nClarity: The paper is well structured and written, but Sections 1-4 could be significantly shorter to leave more space to additional and more conclusive experiments. Some typos on Appendix A should be corrected.\n\nOriginality: the paper is based on a very smart and interesting idea and a straightforward use of GANs. \n\nSignificance: If additional simulations confirm the author’s claims, this work can represent a significant contribution to the forensic analysis of discriminative classifiers.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier","abstract":"Suppose a deep classification model is trained with samples that need to be kept private for privacy or confidentiality reasons. In this setting, can an adversary obtain the private samples if the classification model is given to the adversary? We call this reverse engineering against the classification model the Classifier-to-Generator (C2G) Attack. This situation arises when the classification model is embedded into mobile devices for offline prediction (e.g., object recognition for the automatic driving car and face recognition for mobile phone authentication).\nFor C2G attack, we introduce a novel GAN, PreImageGAN. In PreImageGAN, the generator is designed to estimate the the sample distribution conditioned by the preimage of classification model $f$, $P(X|f(X)=y)$, where $X$ is the random variable on the sample space and $y$ is the probability vector representing the target label arbitrary specified by the adversary. In experiments, we demonstrate PreImageGAN works successfully with hand-written character recognition and face recognition. In character recognition, we show that, given a recognition model of hand-written digits, PreImageGAN allows the adversary to extract numeric images without knowing that the model is built for numeric images. In face recognition, we show that, when an adversary obtains a face recognition model for a set of individuals, PreImageGAN allows the adversary to extract face images of specific individuals contained in the set, even when the adversary has no knowledge of the face of the individuals.","pdf":"/pdf/42be4299dc313ba7608ca09f3072a12616e12365.pdf","TL;DR":"Estimation of training data distribution from trained classifier using GAN.","paperhash":"anonymous|classifiertogenerator_attack_estimation_of_training_data_distribution_from_classifier","_bibtex":"@article{\n  anonymous2018classifier-to-generator,\n  title={Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJOl4DlCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper290/Authors"],"keywords":["Security","Privacy","Model Publication","Generative Adversarial Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222615257,"tcdate":1511584654307,"number":2,"cdate":1511584654307,"id":"r1IcQO8lz","invitation":"ICLR.cc/2018/Conference/-/Paper290/Official_Review","forum":"SJOl4DlCZ","replyto":"SJOl4DlCZ","signatures":["ICLR.cc/2018/Conference/Paper290/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Need more ablation study to clarify the contribution","rating":"4: Ok but not good enough - rejection","review":"This paper proposed to learn a generative GAN model that generates the training data from the labels, given that only the black-box mapping $f$ from data to label is available, as well as an aux dataset that might and might not overlap with the training set. This approach can be regarded as a transfer learning version of ACGAN that generates data conditioned on its label.\n\nOverall I feel it unclear to judge whether this paper has made substantial contributions. The performance critically relies on the structure of aux dataset and how the supervised model $f$ interacts with it. It would be great if the author could show how the aux dataset is partitioned according to the function $f, and what is the representative sample from aux dataset that maximizes a given class label. In Fig. 4, the face of Leonardo DiCaprio was reconstructed successfully, but is that because in the aux dataset there are other identities who look very similar to him and is classified as Leonardo, or it is because GAN has the magic to stitch characteristics of different face identities together?  Given the current version of the paper, it is not clear at all. From the results on EMNIST when the aux set and the training set are disjoint, the proposed model simply picks the most similar shapes as GAN generation, and is not that interesting. In summary, a lot of ablation experiments are needed for readers to understand the proposed method better.\n\nThe writing is ok but a bit redundant. For example, Eqn. 1 (and Eqn. 2) which shows the overall distribution of the training samples (and aux samples) as a linear combinations of the samples at each class, are not involved in the method. Do we really need Eqn. 1 and 2?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier","abstract":"Suppose a deep classification model is trained with samples that need to be kept private for privacy or confidentiality reasons. In this setting, can an adversary obtain the private samples if the classification model is given to the adversary? We call this reverse engineering against the classification model the Classifier-to-Generator (C2G) Attack. This situation arises when the classification model is embedded into mobile devices for offline prediction (e.g., object recognition for the automatic driving car and face recognition for mobile phone authentication).\nFor C2G attack, we introduce a novel GAN, PreImageGAN. In PreImageGAN, the generator is designed to estimate the the sample distribution conditioned by the preimage of classification model $f$, $P(X|f(X)=y)$, where $X$ is the random variable on the sample space and $y$ is the probability vector representing the target label arbitrary specified by the adversary. In experiments, we demonstrate PreImageGAN works successfully with hand-written character recognition and face recognition. In character recognition, we show that, given a recognition model of hand-written digits, PreImageGAN allows the adversary to extract numeric images without knowing that the model is built for numeric images. In face recognition, we show that, when an adversary obtains a face recognition model for a set of individuals, PreImageGAN allows the adversary to extract face images of specific individuals contained in the set, even when the adversary has no knowledge of the face of the individuals.","pdf":"/pdf/42be4299dc313ba7608ca09f3072a12616e12365.pdf","TL;DR":"Estimation of training data distribution from trained classifier using GAN.","paperhash":"anonymous|classifiertogenerator_attack_estimation_of_training_data_distribution_from_classifier","_bibtex":"@article{\n  anonymous2018classifier-to-generator,\n  title={Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJOl4DlCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper290/Authors"],"keywords":["Security","Privacy","Model Publication","Generative Adversarial Networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222615304,"tcdate":1510244872861,"number":1,"cdate":1510244872861,"id":"r1-fz-MJG","invitation":"ICLR.cc/2018/Conference/-/Paper290/Official_Review","forum":"SJOl4DlCZ","replyto":"SJOl4DlCZ","signatures":["ICLR.cc/2018/Conference/Paper290/AnonReviewer2"],"readers":["everyone"],"content":{"title":"a nice paper, some details need to be clarified","rating":"7: Good paper, accept","review":"This paper considers a new problem : given a classifier f trained from D_tr and a set of auxillary samples from D_aux, find D_tr conditioned on label t*. Its solution is based on a new GAN: preImageGAN. Three settings of the similarity between auxillary distribution and training distribution is considered: exact same, partly same, mutually exclusive. Experiments show promising results in generating examples from the original training distribution, even in the \"mutually exclusive\" setting.\n\nQuality: \n1. It is unclear to me if the generated distribution in the experiments is similar to the original distribution D_tr given y = t^*, either from inception accuracy or from pictorial illustration. Since we have hold out the training data, perhaps we can measure the distance between the generated distribution and D_tr given y = t^* directly.\n\n2. It would be great if we can provide experiments quantifying the utility of the auxillary examples. For example, when they are completely noise, can we still get sensible generation of images? \n\n3. How does the experimental result of this approach compare with model attack? For example, we can imagine generating labels by e_t^* + epsilon, where epsilon is random noise. If we invert these random labels, do we get a distribution of examples from class t^*?\n\nClarity:\n1. I think the key here is to first generate auxillary labels (as in Figure 2), then solve optimization problem (3) - this causes my confusion at first sight. (My first impression is that all labels, training or auxillary, are one-hot encoding - but this makes no sense since the dimension of f and y_aux does not match.)\n\nOriginality: I am not familiar with relevant literature - and I think the GAN formulation here is original.\n\nSignificance: I see this as a nice step towards inferring training data from trained classifiers. \n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier","abstract":"Suppose a deep classification model is trained with samples that need to be kept private for privacy or confidentiality reasons. In this setting, can an adversary obtain the private samples if the classification model is given to the adversary? We call this reverse engineering against the classification model the Classifier-to-Generator (C2G) Attack. This situation arises when the classification model is embedded into mobile devices for offline prediction (e.g., object recognition for the automatic driving car and face recognition for mobile phone authentication).\nFor C2G attack, we introduce a novel GAN, PreImageGAN. In PreImageGAN, the generator is designed to estimate the the sample distribution conditioned by the preimage of classification model $f$, $P(X|f(X)=y)$, where $X$ is the random variable on the sample space and $y$ is the probability vector representing the target label arbitrary specified by the adversary. In experiments, we demonstrate PreImageGAN works successfully with hand-written character recognition and face recognition. In character recognition, we show that, given a recognition model of hand-written digits, PreImageGAN allows the adversary to extract numeric images without knowing that the model is built for numeric images. In face recognition, we show that, when an adversary obtains a face recognition model for a set of individuals, PreImageGAN allows the adversary to extract face images of specific individuals contained in the set, even when the adversary has no knowledge of the face of the individuals.","pdf":"/pdf/42be4299dc313ba7608ca09f3072a12616e12365.pdf","TL;DR":"Estimation of training data distribution from trained classifier using GAN.","paperhash":"anonymous|classifiertogenerator_attack_estimation_of_training_data_distribution_from_classifier","_bibtex":"@article{\n  anonymous2018classifier-to-generator,\n  title={Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJOl4DlCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper290/Authors"],"keywords":["Security","Privacy","Model Publication","Generative Adversarial Networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739382938,"tcdate":1509090288447,"number":290,"cdate":1509739380280,"id":"SJOl4DlCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJOl4DlCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier","abstract":"Suppose a deep classification model is trained with samples that need to be kept private for privacy or confidentiality reasons. In this setting, can an adversary obtain the private samples if the classification model is given to the adversary? We call this reverse engineering against the classification model the Classifier-to-Generator (C2G) Attack. This situation arises when the classification model is embedded into mobile devices for offline prediction (e.g., object recognition for the automatic driving car and face recognition for mobile phone authentication).\nFor C2G attack, we introduce a novel GAN, PreImageGAN. In PreImageGAN, the generator is designed to estimate the the sample distribution conditioned by the preimage of classification model $f$, $P(X|f(X)=y)$, where $X$ is the random variable on the sample space and $y$ is the probability vector representing the target label arbitrary specified by the adversary. In experiments, we demonstrate PreImageGAN works successfully with hand-written character recognition and face recognition. In character recognition, we show that, given a recognition model of hand-written digits, PreImageGAN allows the adversary to extract numeric images without knowing that the model is built for numeric images. In face recognition, we show that, when an adversary obtains a face recognition model for a set of individuals, PreImageGAN allows the adversary to extract face images of specific individuals contained in the set, even when the adversary has no knowledge of the face of the individuals.","pdf":"/pdf/42be4299dc313ba7608ca09f3072a12616e12365.pdf","TL;DR":"Estimation of training data distribution from trained classifier using GAN.","paperhash":"anonymous|classifiertogenerator_attack_estimation_of_training_data_distribution_from_classifier","_bibtex":"@article{\n  anonymous2018classifier-to-generator,\n  title={Classifier-to-Generator Attack: Estimation of Training Data Distribution from Classifier},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJOl4DlCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper290/Authors"],"keywords":["Security","Privacy","Model Publication","Generative Adversarial Networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}