{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222775417,"tcdate":1511826336175,"number":3,"cdate":1511826336175,"id":"SkOj779lM","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Review","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/AnonReviewer1"],"readers":["everyone"],"content":{"title":"There are some interesting findings, but not good enough ","rating":"4: Ok but not good enough - rejection","review":"This paper proposes the concept of optimal representation space and suggests that a model should be evaluated in its optimal representation space to get good performance. It could be a good idea if this paper could suggest some ways to find the optimal representation space in general, instead of just showing two cases. It is disappointing, because this paper is named as \"finding optimal representation spaces ...\".\n\nIn addition, one of the contributions claimed in this paper is about introducing the \"formalism\" of an optimal representation space. However, I didn't see any formal definition of this concept or theoretical justification.\n\nAbout FastSent or any other log-linear model, the reason that dot product (or cosine similarity) is a good metric is because the model is trained to optimize the dot product, as shown in equation 5 --- I think this simple fact is missed in this paper.\n\nThe experimental results are not convincing, because I didn't find any consistent pattern that shows the performance is getting better once we evaluated the model in its optimal representation space.\n\nThere are statements in this paper that I didn't agree with\n\n1) Distributional hypothesis from Harris (1954) is about words not sentences.\n2) Not sure the following line makes sense: \"However, these unsupervised tasks are more interesting from a general AI point of view, as they test whether the machine truly understands the human notion of similarity, without being explicitly told what is similar\"","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that shallow bag-of-words models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. While the current work is presented within the context of natural language processing, the insights are applicable to the entire field of representation learning.","pdf":"/pdf/14feddb0d3219ebd7ebe8678c8c668f84b7ee954.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222775457,"tcdate":1511809834742,"number":2,"cdate":1511809834742,"id":"S1GVQk5gG","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Review","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/AnonReviewer2"],"readers":["everyone"],"content":{"title":"some contributions, but more concerns","rating":"5: Marginally below acceptance threshold","review":"This paper is about rethinking how to use encoder-decoder architectures for representation learning when the training objective contains a similarity between the decoder output and the encoding of something else. For example, for the skip-thought RNN encoder-decoder that encodes a sentence and decodes neighboring sentences: rather than use the final encoder hidden state as the representation of the sentence, the paper uses some function of the decoder, since the training objective is to maximize each dot product between a decoder hidden state and the embedding of a context word. If dot product (or cosine similarity) is going to be used as the similarity function for the representation, then it makes more sense, the paper argues, to use the decoder hidden state(s) as the representation of the input sentence. The paper considers both averaging and concatenating hidden states. One difficulty here is that the neighboring sentences are typically not available in downstream tasks, so the paper runs the decoder to produce a predicted sentence one word-at-a-time, using the predicted words as inputs to the decoder RNNs. Then those decoder RNN hidden states are used via averaging or concatenation as the representation of a sentence in downstream tasks. \n\nThis paper is a source of contributions, but I think in its current form it is not yet ready for publication. \n\nPros:\n\nI think it makes sense to pay attention to the training objective when deciding how to use the model for downstream tasks. \n\nI like the empirical investigation of combining RNN and BOW encoders and decoders. \n\nThe experimental results show that a single encoder-decoder model can be trained and then two different functions of it can be used at test time for different kinds of tasks (RNN-RNN for supervised transfer and RNN-RNN-mean for unsupervised transfer). I think this is an interesting result. \n\nCons: \n\nI have several concerns. The first relate to the theoretical arguments and their empirical support. \n\nRegarding the theoretical arguments: \n\nFirst, the paper discusses the notion of an \"optimal representation space\" and describes the argument as theoretical, but I don't see much of a theoretical argument here. \n\nAs far as I can tell, the paper does not formally define its terms or define in what sense the representation space is \"optimal\". I can only find heuristic statements like those in the paragraph in Sec 3.2 that begins \"These observations...\".  What exactly is meant formally by statements like \"any model where the decoder is log-linear with respect to the encoder\" or \"that distance is optimal with respect to the modelâ€™s objective\"?  It seems like the paper may want to start with formal definitions of an encoder and a decoder, then define what is meant by a \"decoder that is log-linear with respect to the encoder\", and define what it means for a distance to be optimal with respect to a training objective. That seems necessary in order to provide the foundation to make any theoretical statement about choices for encoders, decoders, and training objectives.  I am still not exactly sure what that theoretical statement might look like, but maybe defining the terms would help the authors get started in heading toward the goal of defining a statement to prove. \n\nSecond, the paper's theoretical story seems to diverge almost immediately from the choices used in the model and experimental procedure. \n\nFor example, in Sec. 3.2, it is stated that cosine similarity \"is the appropriate similarity measure in the case of log-linear decoders.\"  But the associated footnote (footnote 2) seems to admit a contradiction here by noting that actually the appropriate similarity measure is dot product: \"Evidently, the correct measure is actually the dot product.\"  This is a bit confusing. \nIt also raises a question: If cosine similarity will be used later for computing similarity, then why not try using cosine similarity in place of dot product in the model?  That is, replace \"u_w \\cdot h_i\" in Eq. (2) with \"cos(u_w, h_i)\".  If the paper's story is correct (and if I understand the ideas correctly), training with cosine similarity should work better than training with dot product, because the similarity function used during training is more similar to that used in testing.  This seems like a natural experiment to try.  Other natural experiments would be to vary both the similarity function used in the model during training and the similarity function used at test time.  The authors' claims could be validated if the optimal choices always use the same choice for the training and test-time similarity functions.  That is, if Euclidean distance is used during training, then will Euclidean distance be the best choice at test time?\n\nAnother example of the divergence lies in the use of the skip-thought decoder on downstream tasks. Since the decoder hidden states depend on neighboring sentences and these are considered to be unavailable at test time, the paper \"unrolls\" the decoder for several steps by using it to predict words which are then used as inputs on the next time step. To me, this is a potentially very significant difference between training and testing. Since much of the paper is about reconciling training and testing conditions in terms of the representation space and similarity function, this difference feels like a divergence from the theoretical story. It is only briefly mentioned at the end of Sec. 3.3 and then discussed again later in the experiments section. I think this should be described in more detail in Section 3.3 because it is an important note about how the model will be used in practice. \n\nIt would be nice to be able to quantify the impact (of unrolling the decoder with predicted words) by, for example, using the decoder on a downstream evaluation dataset that has neighboring sentences in it. Then the actual neighboring sentences can be used as inputs to the decoder when it is unrolled, which would be closer to the training conditions and we could empirically see the difference. Perhaps there is an evaluation dataset with ordered sentences so that the authors could empirically compare using real vs predicted inputs to the decoder on a downstream task?\n\nThe above experiments might help to better connect the experiments section with the theoretical arguments. \n\nOther concerns, including more specific points, are below:\n\nSec. 2: \nWhen describing inferior performance of RNN-based models on unsupervised sentence similarity tasks, the paper states: \"While this shortcoming of SkipThought and RNN-based models in general has been pointed out, to the best of our knowledge, it has never been systematically addressed in the literature before.\" \nThe authors may want to check Wieting & Gimpel (2017) (and its related work) which investigates the inferiority of LSTMs compared to word averaging for unsupervised sentence similarity tasks. They found that averaging the encoder hidden states can work better than using the final encoder hidden state; the authors may want to try that as well. \n\nSec. 3.2:\nWhen describing FastSent, the paper includes \"Due to the model's simplicity, it is particularly fast to train and evaluate, yet has shown state-of-the-art performance in unsupervised similarity tasks (Hill et al., 2015).\"\nI don't think it makes much sense to cite the SimLex-999 paper in this context, as that is a word similarity task and that paper does not include any results of FastSent. Maybe the Hill et al (2016) FastSent citation was meant instead? But in that case, I don't think it is quite accurate to make the claim that FastSent is SOTA on unsupervised similarity tasks. In the original FastSent paper (Hill et al., 2016), FastSent is not as good as CPHRASE or \"DictRep BOW+embs\" on average across the unsupervised sentence similarity evaluations. FastSent is also not as good as sent2vec from Pagliardini et al (2017) or charagram-phrase from Wieting et al. (2016).\n\nSec. 3.3:\nIn describing skip-thought, the paper states: \"While computationally complex, it is currently the state-of-the-art model for supervised transfer tasks (Hill et al., 2016).\"\nI don't think it is accurate to state that skip-thought is still state-of-the-art for supervised transfer tasks, in light of recent work (Conneau et al., 2017; Gan et al., 2017). \n\nSec. 3.3:\nWhen discussing averaging the decoder hidden states, the paper states: \"Intuitively, this corresponds to destroying the word order information the decoder has learned.\" I'm not sure this strong language can be justified here. Is there any evidence to suggest that averaging the decoder hidden states will destroy word order information? The hidden states may be representing word order information in a way that is robust to averaging, i.e., in a way such that the average of the hidden states can still lead to the reconstruction of the word order.\n\nSec. 4:\nWhat does it mean to use an RNN encoder and a BOW decoder?  This seems to be a strongly-performing setting and competitive with RNN-mean, but I don't know exactly what this means. \n\n\nMinor things:\n\nSec. 3.1:\nWhen defining v_w, it would be helpful to make explicit that it's in \\mathbb{R}^d.\n\nSec. 4: \nFor TREC question type classification, I think the correct citation should be Li & Roth (2002) instead of Vorhees (2002).\n\nSec. 5:\nI think there's a typo in the following sentence: \"Our results show that, for example, the raw encoder output for SkipThought (RNN-RNN) achieves strong performance on supervised transfer, whilst its mean decoder output (RNN-mean) achieves strong performance on supervised transfer.\"  I think \"unsupervised\" was meant in the latter mention.\n\nReferences:\n\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. EMNLP.\nGan, Z., Pu, Y., Henao, R., Li, C., He, X., & Carin, L. (2017). Learning generic sentence representations using convolutional neural networks. EMNLP.\nLi, X., & Roth, D. (2002). Learning question classifiers. COLING.\nPagliardini, M., Gupta, P., & Jaggi, M. (2017). Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. arXiv preprint arXiv:1703.02507.\nWieting, J., Bansal, M., Gimpel, K., & Livescu, K. (2016). Charagram: Embedding words and sentences via character n-grams. EMNLP.\nWieting, J., & Gimpel, K. (2017). Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings. ACL.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that shallow bag-of-words models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. While the current work is presented within the context of natural language processing, the insights are applicable to the entire field of representation learning.","pdf":"/pdf/14feddb0d3219ebd7ebe8678c8c668f84b7ee954.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1512222775501,"tcdate":1511797248839,"number":1,"cdate":1511797248839,"id":"HJFbzhFeM","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Review","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Gap between theory and results","rating":"5: Marginally below acceptance threshold","review":"The authors provide some theoretical justification for why simple log-linear decoders perform better than RNN decoders for various unsupervised sentence similarity tasks. They also provide a simple method for improving the performance of RNN based models. Please find below my comments/questions/suggestions:\n\n1) I found the theory to be a bit superficial and there is a clearly a gap between what is proven theoretically and demonstrated empirically. For example, as per the theoretical arguments presented in the paper, RNN-concat should do better than RNN-mean. However, the experiments suggest that RNN-mean always does better and in some cases significantly better (referring to Table 1). How does this empirical observation reconcile with the theory ?\n\n2) The authors mention that the results for SkipThought represented in their paper are lower than those presented in the original SkipThought paper. They say that they elaborate on this in Appendix C but there isn't much information provided. In particular, it would be good to mention the original numbers also (of course I can check the original paper but if the authors provide those numbers then it would ensure that there is no misunderstanding)\n\n3) As mentioned in the previous point, the original SkipThought decoder seems to do better than the modified decoder used by the authors. It is not clear, how this can be justified under the theoretical framework presented by the author. I agree that this could be because in the original formulation (referring to equations in Appendix C) the encoder contributes more directly to the decoder. However, it is not clear how this causes it to be \"closer\" to the optimal space.  Can this be proven ?\n\n4) Can you elaborate a bit more on how the model is used at test time? Consider the STS17 benchmark and the following sentence pair from it? \n\n- The bird is bathing in the sink.\n- Birdie is washing itself in the water basin.\n\nHow will you use RNN-mean and RNN-concat to find the similarity between these two sentences.\n\n5) In continuation of the above question, how does the computation of similarity differ when you unroll for 3 steps v/s when you unroll for 5 steps\n\n6) Based on the answers to (4) and (5) above I would like to seek further clarifications on Figure 1.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that shallow bag-of-words models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. While the current work is presented within the context of natural language processing, the insights are applicable to the entire field of representation learning.","pdf":"/pdf/14feddb0d3219ebd7ebe8678c8c668f84b7ee954.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1509739085492,"tcdate":1509135359800,"number":816,"cdate":1509739082816,"id":"Byd-EfWCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that shallow bag-of-words models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. While the current work is presented within the context of natural language processing, the insights are applicable to the entire field of representation learning.","pdf":"/pdf/14feddb0d3219ebd7ebe8678c8c668f84b7ee954.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}