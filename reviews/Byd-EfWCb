{"notes":[{"tddate":null,"ddate":null,"tmdate":1515189266681,"tcdate":1515189266681,"number":4,"cdate":1515189266681,"id":"rkjz4_a7z","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Comment","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper816/Authors"],"content":{"title":"Paper revision - formalised theory and dot-product results","comment":"Many thanks to the reviewers for their extensive and valuable feedback.\n\nWhile the original scope remains the same, the paper itself has changed significantly:\n- Greatly expanded theory section, including a thorough definition of an optimal representation space and details how such an optimal space can be discovered for a given model. \n- Revisited analysis of BOW and RNN decoders to clarify our argumentation.\n- As suggested by the reviewers we now report results using dot-product as similarity metric in the main body of the paper and have moved the results using cosine similarity into the appendix.\n- Expanded presentation and discussion of the performance of unrolled RNN decoders.\n- Results for mean of unrolled decoder states, and a comparison with (variants of) SkipThought have been added to the appendix.\n- Upon further investigation, the SkipThought LayerNorm model, whose results we were comparing against does not directly condition its decoder output on the encoder output at every time step as initially thought. We attribute differences in performance to experimental setup and have thus removed related comments.\n- Literature is now reviewed in the introduction (including references suggested by the reviewers).\n- An additional figure clarifies the proposed unrolling procedure for RNN decoders.\n- Minor changes to the text\n\nWe believe we have thoroughly addressed the bulk of the issues highlighted by the reviewers. In particular we now provide a solid theoretical framework for optimal representation spaces and how they can be obtained for a given model. Further we provide results that are more consistent with our theoretical argument and have revisited the majority of the text in the paper.\n\nAgain we would like to thank the reviewers for the time and care they have put into their reviews, and would like to invite them to reconsider their original ratings."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1513201086677,"tcdate":1513200783522,"number":3,"cdate":1513200783522,"id":"rkwqhGkzz","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Comment","forum":"Byd-EfWCb","replyto":"S1GVQk5gG","signatures":["ICLR.cc/2018/Conference/Paper816/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper816/Authors"],"content":{"title":"Thank you","comment":"Thank you very much for the time and consideration you have taken with your review. We sincerely appreciate your detailed feedback and would like to address some of your questions and concerns below.\n\n> I don't see much of a theoretical argument here.\n\nThe lack of formalism was mentioned by all 3 reviewers. We will reformulate this section, give some clearer definitions as you suggested, and tone down the “theory” aspect in general.\n\n\n> … theoretical story seems to diverge almost immediately… if cosine similarity will be used later, then why not try using cosine similarity in place of dot product in the model?\n\nYou are absolutely correct. While cosine similarity is clearly related to dot product, it is not a drop-in replacement because it is not always consistent with the dot product. Thank you for pointing out this error in our analysis.\nWe will re-run all our experiments with the dot product and will publish the results in the updated manuscript.\nYou are also right that in principle one can use cosine similarity, Euclidean distance or indeed any chosen measure in the model. Due to time and computational restrictions, we are unable to run these additional experiments by the rebuttal deadline but we feel these ideas are definitely worth exploring.\n\n\n> if Euclidean distance is used during training, then will Euclidean distance be the best choice at test time?\n\nOur analysis suggests that trying Euclidean distance is a sensible thing to do in this case. Of course, the downstream task might differ so much that the distance and the model itself are not useful at all.\n\n\n> Another example of the divergence lies in the use of the skip-thought decoder on downstream tasks. Since the decoder hidden states depend on neighboring sentences and these are considered to be unavailable at test time, the paper \"unrolls\" the decoder for several steps by using it to predict words which are then used as inputs on the next time step. To me, this is a potentially very significant difference between training and testing. Since much of the paper is about reconciling training and testing conditions in terms of the representation space and similarity function, this difference feels like a divergence from the theoretical story.\n\nUnfortunately, when the test sentences have no context, the adjacent sentences need to be approximated (e.g. by using the softmax word embeddings or beam search). In those cases the optimal representation is not really attainable but can be “approximated”. However, in other models such as RNN-RNN autoencoders the optimal space is perfectly attainable.\nWe feel our work is more about paying attention to the objective as you mentioned. If the model maximises some similarity between representations, it is sensible to try that similarity on the downstream tasks.\n\n\n> It would be nice to be able to quantify the impact (of unrolling the decoder with predicted words) by, for example, using the decoder on a downstream evaluation dataset that has neighboring sentences in it. \n\nWe absolutely agree and would love to do this but are unaware of such tasks / datasets. Would the Reviewer be able to help us by suggesting one?\n\n\n> ...The hidden states may be representing word order information in a way that is robust to averaging … \n\nThis is a really good point and we totally agree. We will soften or retract our statement.\n\n\n> What does it mean to use an RNN encoder and a BOW decoder? This seems to be a strongly-performing setting and competitive with RNN-mean, but I don't know exactly what this means.\n\nWe use the RNN to encode a sentence into a vector h, and then use the bag-of-words decoder, i.e. softmax(U*h), where U is the logit matrix. Please do let us know if you would like a more detailed description of this or any other model.\n\n\nWe agree with all other points not mentioned here and will fix accordingly.\n\n\nFinally, we will notify you when all of the above are addressed in the new version.\n\nBest wishes,\n\nICLR 2018 Conference Paper816 Authors\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1513198241882,"tcdate":1513198241882,"number":2,"cdate":1513198241882,"id":"Sk9jGf1fG","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Comment","forum":"Byd-EfWCb","replyto":"HJFbzhFeM","signatures":["ICLR.cc/2018/Conference/Paper816/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper816/Authors"],"content":{"title":"Thank you","comment":"We would like to thank you for your time and a detailed assessment of our work.\nWe hope to address some of your questions and concerns below.\n\n\n> I found the theory to be a bit superficial.\n\nThis has been pointed out by all 3 reviewers. We will make our statements more formal and tone down the “theory” aspect in the updated manuscript.\n\n\n> RNN-concat should do better than RNN-mean (but oftentimes the opposite happens)\n\nWe have re-run our experiments with dot product instead of cosine similarity as pointed out by Reviewer 2. We found that RNN-concat works much better than previously reported. We will update the manuscript to reflect the changes.\n\n\n> it would be good to mention the original SkipThought numbers\n\nWe agree and will do so in the updated manuscript.\n\n\n> The original SkipThought decoder seems to do better than the modified decoder used by the authors\n\nThis was definitely a source of concern for us as well. We hypothesise this is because the encoder contributes more directly to the decoder but we cannot say for sure. It is entirely possible the discrepancy is due to variations in experimental setups. E.g. to the best of our knowledge the original paper uses a combination of bidirectional and unidirectional encoders; we use only the latter.\nIn this work we did not aim to compete with existing models, we were only interested in testing our assumptions in a fair setting. To this end, we built our experiments upon a well-known TensorFlow Skip-Thought implementation and will make our codebase public.\n\n\n> Can you elaborate a bit more on how the model is used at test time?\n\nAbsolutely. The encoder RNN encodes a sentence into some vector f and the decoder RNNs produce sequences of hidden states h(1), h(2), …, h(n) for the previous sentence and g(1), g(2), … g(n) for the next sentence. We simply average or concatenate all those hidden states to form the final sentence representation.\nImportantly, since there are no adjacent sentences during test time, the decoder input for the current step is just the softmax of the previous step multiplied by the word embedding matrix, i.e. w(n) = W*p(n-1).\nPlease let us know if that answers your question. We are always happy to elaborate further.\n\n\n> how does the computation of similarity differ when you unroll for 3 steps v/s when you unroll for 5 steps\n\nWe would unroll N hidden states of each decoder, so in total we have 2*N vectors of dimension d.\nIn RNN-mean, we just average all of them and get 1 vector of dimension d.\nIn RNN-concat, we concatenate all of them and get 1 vector of dimension 2*N*d.\nIn either case, we use dot product to compare the resulting representations.\n\n\n> I would like to seek further clarifications on Figure 1.\n\nThis illustrates the performance on the STS14 task as a function of N, where N is the number of unrolled hidden states of the decoder.\n\n\nFinally, we will notify you when all of the above are addressed in the new version.\n\nBest wishes,\n\nICLR 2018 Conference Paper816 Authors\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1513196499688,"tcdate":1513195536601,"number":1,"cdate":1513195536601,"id":"B1Fzd-yzM","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Comment","forum":"Byd-EfWCb","replyto":"SkOj779lM","signatures":["ICLR.cc/2018/Conference/Paper816/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper816/Authors"],"content":{"title":"Thank you","comment":"Thank you very much for your time and assessment of our work.\nWe hope to address some of your concerns below.\n\n> It could be a good idea if this paper could suggest some ways to find the optimal representation space in general, instead of just showing two cases.\n\nWe agree and will describe the general procedure in the updated manuscript.\n\n\n> I didn't see any formal definition of this concept ...\n\nThe lack of formalism was mentioned by all 3 reviewers. We will give some clearer definitions and tone down the “theory” aspect in general.\n\n\n> … the reason that dot product is a good metric is because the log-linear model is trained to optimize the dot product\n\nThis is exactly what we were trying to say. We also argue that dot product is not necessarily appropriate for comparing RNN encoder vectors. Intuitively, due to non-linearities, small changes in the encoder hidden state might lead to big changes in the decoder outputs and vice versa. We are sorry if our core idea was not clear enough, we will present our analysis better in the updated manuscript.\n\n\n> The experimental results are not convincing\n\nCould you perhaps help us by pointing towards the source of your concerns?\nAs our analysis indicates, RNN encoder - RNN decoder is the worst model for similarity because dot product is not appropriate for comparing encoder states when RNN decoder is used. However, dot product is appropriate for BOW encoder - BOW decoder and RNN - RNN-concat. Our experiments show significant and consistent improvements over RNN-RNN.\n\n\n> Distributional hypothesis from Harris (1954) is about words not sentences.\n\nWe had no intention to say otherwise. By “sentence-level version of the distributional hypothesis Harris (1954)” we meant that one can think of a “sentence-level version” of the original word-level hypothesis due to Harris (1954). We will fix our sloppy wording in the updated manuscript.\n\n\n> Not sure the following line makes sense: \"However, these unsupervised tasks are more interesting from a general AI point of view...\n\nWe completely agree with you. In fact, humans are being told what is similar all the time. We will rephrase or retract the statement.\n\n\nFinally, we will notify you when all of the above are addressed in the new version.\n\nBest wishes,\n\nICLR 2018 Conference Paper816 Authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1515642516109,"tcdate":1511826336175,"number":3,"cdate":1511826336175,"id":"SkOj779lM","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Review","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/AnonReviewer1"],"readers":["everyone"],"content":{"title":"There are some interesting findings, but not good enough ","rating":"4: Ok but not good enough - rejection","review":"This paper proposes the concept of optimal representation space and suggests that a model should be evaluated in its optimal representation space to get good performance. It could be a good idea if this paper could suggest some ways to find the optimal representation space in general, instead of just showing two cases. It is disappointing, because this paper is named as \"finding optimal representation spaces ...\".\n\nIn addition, one of the contributions claimed in this paper is about introducing the \"formalism\" of an optimal representation space. However, I didn't see any formal definition of this concept or theoretical justification.\n\nAbout FastSent or any other log-linear model, the reason that dot product (or cosine similarity) is a good metric is because the model is trained to optimize the dot product, as shown in equation 5 --- I think this simple fact is missed in this paper.\n\nThe experimental results are not convincing, because I didn't find any consistent pattern that shows the performance is getting better once we evaluated the model in its optimal representation space.\n\nThere are statements in this paper that I didn't agree with\n\n1) Distributional hypothesis from Harris (1954) is about words not sentences.\n2) Not sure the following line makes sense: \"However, these unsupervised tasks are more interesting from a general AI point of view, as they test whether the machine truly understands the human notion of similarity, without being explicitly told what is similar\"","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1515799178157,"tcdate":1511809834742,"number":2,"cdate":1511809834742,"id":"S1GVQk5gG","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Review","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/AnonReviewer2"],"readers":["everyone"],"content":{"title":"some contributions, but more concerns; updated paper is an improvement","rating":"6: Marginally above acceptance threshold","review":"------ updates to review: ---------\n\nI think the paper is much improved. It is much more clear and the experiments are more focused and more closely connected to the earlier content in the paper. Thanks to the authors for trying to address all of my concerns. \n\nI now better understand in what sense the representation space is optimal.  I had been thinking (or perhaps \"hoping\" is a better word) that the term \"optimal\" implied maximal in terms of some quantifiable measure, but it's more of an empirical \"optimality\".  This makes the paper an empirical paper based on a reasonable and sensible intuition, rather than a theoretical result.  This was a little disappointing to me, but I do still think the paper is marginally above the acceptance threshold and have increased my score accordingly. \n\n------ original review below: --------\nThis paper is about rethinking how to use encoder-decoder architectures for representation learning when the training objective contains a similarity between the decoder output and the encoding of something else. For example, for the skip-thought RNN encoder-decoder that encodes a sentence and decodes neighboring sentences: rather than use the final encoder hidden state as the representation of the sentence, the paper uses some function of the decoder, since the training objective is to maximize each dot product between a decoder hidden state and the embedding of a context word. If dot product (or cosine similarity) is going to be used as the similarity function for the representation, then it makes more sense, the paper argues, to use the decoder hidden state(s) as the representation of the input sentence. The paper considers both averaging and concatenating hidden states. One difficulty here is that the neighboring sentences are typically not available in downstream tasks, so the paper runs the decoder to produce a predicted sentence one word-at-a-time, using the predicted words as inputs to the decoder RNNs. Then those decoder RNN hidden states are used via averaging or concatenation as the representation of a sentence in downstream tasks. \n\nThis paper is a source of contributions, but I think in its current form it is not yet ready for publication. \n\nPros:\n\nI think it makes sense to pay attention to the training objective when deciding how to use the model for downstream tasks. \n\nI like the empirical investigation of combining RNN and BOW encoders and decoders. \n\nThe experimental results show that a single encoder-decoder model can be trained and then two different functions of it can be used at test time for different kinds of tasks (RNN-RNN for supervised transfer and RNN-RNN-mean for unsupervised transfer). I think this is an interesting result. \n\nCons: \n\nI have several concerns. The first relate to the theoretical arguments and their empirical support. \n\nRegarding the theoretical arguments: \n\nFirst, the paper discusses the notion of an \"optimal representation space\" and describes the argument as theoretical, but I don't see much of a theoretical argument here. \n\nAs far as I can tell, the paper does not formally define its terms or define in what sense the representation space is \"optimal\". I can only find heuristic statements like those in the paragraph in Sec 3.2 that begins \"These observations...\".  What exactly is meant formally by statements like \"any model where the decoder is log-linear with respect to the encoder\" or \"that distance is optimal with respect to the model’s objective\"?  It seems like the paper may want to start with formal definitions of an encoder and a decoder, then define what is meant by a \"decoder that is log-linear with respect to the encoder\", and define what it means for a distance to be optimal with respect to a training objective. That seems necessary in order to provide the foundation to make any theoretical statement about choices for encoders, decoders, and training objectives.  I am still not exactly sure what that theoretical statement might look like, but maybe defining the terms would help the authors get started in heading toward the goal of defining a statement to prove. \n\nSecond, the paper's theoretical story seems to diverge almost immediately from the choices used in the model and experimental procedure. \n\nFor example, in Sec. 3.2, it is stated that cosine similarity \"is the appropriate similarity measure in the case of log-linear decoders.\"  But the associated footnote (footnote 2) seems to admit a contradiction here by noting that actually the appropriate similarity measure is dot product: \"Evidently, the correct measure is actually the dot product.\"  This is a bit confusing. \nIt also raises a question: If cosine similarity will be used later for computing similarity, then why not try using cosine similarity in place of dot product in the model?  That is, replace \"u_w \\cdot h_i\" in Eq. (2) with \"cos(u_w, h_i)\".  If the paper's story is correct (and if I understand the ideas correctly), training with cosine similarity should work better than training with dot product, because the similarity function used during training is more similar to that used in testing.  This seems like a natural experiment to try.  Other natural experiments would be to vary both the similarity function used in the model during training and the similarity function used at test time.  The authors' claims could be validated if the optimal choices always use the same choice for the training and test-time similarity functions.  That is, if Euclidean distance is used during training, then will Euclidean distance be the best choice at test time?\n\nAnother example of the divergence lies in the use of the skip-thought decoder on downstream tasks. Since the decoder hidden states depend on neighboring sentences and these are considered to be unavailable at test time, the paper \"unrolls\" the decoder for several steps by using it to predict words which are then used as inputs on the next time step. To me, this is a potentially very significant difference between training and testing. Since much of the paper is about reconciling training and testing conditions in terms of the representation space and similarity function, this difference feels like a divergence from the theoretical story. It is only briefly mentioned at the end of Sec. 3.3 and then discussed again later in the experiments section. I think this should be described in more detail in Section 3.3 because it is an important note about how the model will be used in practice. \n\nIt would be nice to be able to quantify the impact (of unrolling the decoder with predicted words) by, for example, using the decoder on a downstream evaluation dataset that has neighboring sentences in it. Then the actual neighboring sentences can be used as inputs to the decoder when it is unrolled, which would be closer to the training conditions and we could empirically see the difference. Perhaps there is an evaluation dataset with ordered sentences so that the authors could empirically compare using real vs predicted inputs to the decoder on a downstream task?\n\nThe above experiments might help to better connect the experiments section with the theoretical arguments. \n\nOther concerns, including more specific points, are below:\n\nSec. 2: \nWhen describing inferior performance of RNN-based models on unsupervised sentence similarity tasks, the paper states: \"While this shortcoming of SkipThought and RNN-based models in general has been pointed out, to the best of our knowledge, it has never been systematically addressed in the literature before.\" \nThe authors may want to check Wieting & Gimpel (2017) (and its related work) which investigates the inferiority of LSTMs compared to word averaging for unsupervised sentence similarity tasks. They found that averaging the encoder hidden states can work better than using the final encoder hidden state; the authors may want to try that as well. \n\nSec. 3.2:\nWhen describing FastSent, the paper includes \"Due to the model's simplicity, it is particularly fast to train and evaluate, yet has shown state-of-the-art performance in unsupervised similarity tasks (Hill et al., 2015).\"\nI don't think it makes much sense to cite the SimLex-999 paper in this context, as that is a word similarity task and that paper does not include any results of FastSent. Maybe the Hill et al (2016) FastSent citation was meant instead? But in that case, I don't think it is quite accurate to make the claim that FastSent is SOTA on unsupervised similarity tasks. In the original FastSent paper (Hill et al., 2016), FastSent is not as good as CPHRASE or \"DictRep BOW+embs\" on average across the unsupervised sentence similarity evaluations. FastSent is also not as good as sent2vec from Pagliardini et al (2017) or charagram-phrase from Wieting et al. (2016).\n\nSec. 3.3:\nIn describing skip-thought, the paper states: \"While computationally complex, it is currently the state-of-the-art model for supervised transfer tasks (Hill et al., 2016).\"\nI don't think it is accurate to state that skip-thought is still state-of-the-art for supervised transfer tasks, in light of recent work (Conneau et al., 2017; Gan et al., 2017). \n\nSec. 3.3:\nWhen discussing averaging the decoder hidden states, the paper states: \"Intuitively, this corresponds to destroying the word order information the decoder has learned.\" I'm not sure this strong language can be justified here. Is there any evidence to suggest that averaging the decoder hidden states will destroy word order information? The hidden states may be representing word order information in a way that is robust to averaging, i.e., in a way such that the average of the hidden states can still lead to the reconstruction of the word order.\n\nSec. 4:\nWhat does it mean to use an RNN encoder and a BOW decoder?  This seems to be a strongly-performing setting and competitive with RNN-mean, but I don't know exactly what this means. \n\n\nMinor things:\n\nSec. 3.1:\nWhen defining v_w, it would be helpful to make explicit that it's in \\mathbb{R}^d.\n\nSec. 4: \nFor TREC question type classification, I think the correct citation should be Li & Roth (2002) instead of Vorhees (2002).\n\nSec. 5:\nI think there's a typo in the following sentence: \"Our results show that, for example, the raw encoder output for SkipThought (RNN-RNN) achieves strong performance on supervised transfer, whilst its mean decoder output (RNN-mean) achieves strong performance on supervised transfer.\"  I think \"unsupervised\" was meant in the latter mention.\n\nReferences:\n\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. EMNLP.\nGan, Z., Pu, Y., Henao, R., Li, C., He, X., & Carin, L. (2017). Learning generic sentence representations using convolutional neural networks. EMNLP.\nLi, X., & Roth, D. (2002). Learning question classifiers. COLING.\nPagliardini, M., Gupta, P., & Jaggi, M. (2017). Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. arXiv preprint arXiv:1703.02507.\nWieting, J., Bansal, M., Gimpel, K., & Livescu, K. (2016). Charagram: Embedding words and sentences via character n-grams. EMNLP.\nWieting, J., & Gimpel, K. (2017). Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings. ACL.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1515642516190,"tcdate":1511797248839,"number":1,"cdate":1511797248839,"id":"HJFbzhFeM","invitation":"ICLR.cc/2018/Conference/-/Paper816/Official_Review","forum":"Byd-EfWCb","replyto":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference/Paper816/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Gap between theory and results","rating":"5: Marginally below acceptance threshold","review":"The authors provide some theoretical justification for why simple log-linear decoders perform better than RNN decoders for various unsupervised sentence similarity tasks. They also provide a simple method for improving the performance of RNN based models. Please find below my comments/questions/suggestions:\n\n1) I found the theory to be a bit superficial and there is a clearly a gap between what is proven theoretically and demonstrated empirically. For example, as per the theoretical arguments presented in the paper, RNN-concat should do better than RNN-mean. However, the experiments suggest that RNN-mean always does better and in some cases significantly better (referring to Table 1). How does this empirical observation reconcile with the theory ?\n\n2) The authors mention that the results for SkipThought represented in their paper are lower than those presented in the original SkipThought paper. They say that they elaborate on this in Appendix C but there isn't much information provided. In particular, it would be good to mention the original numbers also (of course I can check the original paper but if the authors provide those numbers then it would ensure that there is no misunderstanding)\n\n3) As mentioned in the previous point, the original SkipThought decoder seems to do better than the modified decoder used by the authors. It is not clear, how this can be justified under the theoretical framework presented by the author. I agree that this could be because in the original formulation (referring to equations in Appendix C) the encoder contributes more directly to the decoder. However, it is not clear how this causes it to be \"closer\" to the optimal space.  Can this be proven ?\n\n4) Can you elaborate a bit more on how the model is used at test time? Consider the STS17 benchmark and the following sentence pair from it? \n\n- The bird is bathing in the sink.\n- Birdie is washing itself in the water basin.\n\nHow will you use RNN-mean and RNN-concat to find the similarity between these two sentences.\n\n5) In continuation of the above question, how does the computation of similarity differ when you unroll for 3 steps v/s when you unroll for 5 steps\n\n6) Based on the answers to (4) and (5) above I would like to seek further clarifications on Figure 1.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]}},{"tddate":null,"ddate":null,"tmdate":1515189044066,"tcdate":1509135359800,"number":816,"cdate":1509739082816,"id":"Byd-EfWCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Byd-EfWCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks","abstract":"Experimental evidence indicates that simple models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. Even though this work is presented within the context of natural language processing, the insights are readily applicable to other domains that rely on distributed representations for transfer tasks.","pdf":"/pdf/b6463b36667c3b309055c09cd730e6eb944f54cb.pdf","TL;DR":"By introducing the notion of an optimal representation space, we provide a theoretical argument and experimental validation that an unsupervised model for sentences can perform well on both supervised similarity and unsupervised transfer tasks.","paperhash":"anonymous|decoding_decoders_finding_optimal_representation_spaces_for_unsupervised_similarity_tasks","_bibtex":"@article{\n  anonymous2018decoding,\n  title={Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Byd-EfWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper816/Authors"],"keywords":["distributed representations","sentence embedding","representation learning","unsupervised learning","encoder-decoder","RNN"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}