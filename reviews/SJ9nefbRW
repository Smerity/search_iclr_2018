{"notes":[{"tddate":null,"ddate":null,"tmdate":1512347990063,"tcdate":1512347681053,"number":3,"cdate":1512347681053,"id":"ByDXOzzWf","invitation":"ICLR.cc/2018/Conference/-/Paper773/Official_Review","forum":"SJ9nefbRW","replyto":"SJ9nefbRW","signatures":["ICLR.cc/2018/Conference/Paper773/AnonReviewer2"],"readers":["everyone"],"content":{"title":"More a project report","rating":"3: Clear rejection","review":"This submission does not fit ICLR.  \n\n- The center topic does not fit ICLR. The main novelty is about using word pair embedding to improve the Topic model. The word-pair was generated by the Standford dependency parser. \n\n- Many citation errors exist\n\n- No clear novelty\n\n- The experimental setup is problematic. The authors filtered the number of words and word-pairs to very small. It is hard to justify any of the results after these strategies. \n\n- The baselines are not thorough and lack proper justifications. \n\n- The experimental results are not properly presented, with many overlapping figures. No insights can be derived from the presented results. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Learning Topics using Semantic Locality","abstract":"The topic modeling discovers the latent topic probability of given the text documents. To generate the more meaningful topic that better represents the given document, we proposed a universal method which can be used in the data preprocessing stage. The method consists of three steps. First, it generates the word/word-pair from every single document. Second, it applies a two way parallel TF-IDF algorithm to word/word-pair for semantic filtering. Third, it uses the k-means algorithm to merge the word pairs that have the similar semantic meaning.\n\nExperiments are carried out on the Open Movie Database (OMDb), Reuters Dataset and 20NewsGroup Dataset and use the mean Average Precision score as the evaluation metric. Comparing our results with other state-of-the-art topic models, such as Latent Dirichlet allocation and traditional Restricted Boltzmann Machines. Our proposed data preprocessing can improve the generated topic accuracy by up to 12.99\\%. How the number of clusters and the number of word pairs should be adjusted for different type of text document is also discussed.\n","pdf":"/pdf/10402841c59463097b7c5c707797b6fb138155e8.pdf","TL;DR":"We proposed a universal method which can be used in the data preprocessing stage to generate the more meaningful topic that better represents the given document","paperhash":"anonymous|learning_topics_using_semantic_locality","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Topics using Semantic Locality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nefbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper773/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222762951,"tcdate":1511765228786,"number":2,"cdate":1511765228786,"id":"SySlBNFxz","invitation":"ICLR.cc/2018/Conference/-/Paper773/Official_Review","forum":"SJ9nefbRW","replyto":"SJ9nefbRW","signatures":["ICLR.cc/2018/Conference/Paper773/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper applies the word pairs, instead to bag of words, to current RBM models. However, I think this paper has limited contribution and novelty, and the experiments also need to be improved. ","rating":"4: Ok but not good enough - rejection","review":"This paper applies the word pairs, instead to bag of words, to current RBM models. The word pairs are extracted using Stanford parser. The word pairs are further filtered and clustered to improve the representation. The experiments show improvement over baselines.\n\nHowever, I think this paper has limited contribution and novelty, and the experiments also need to be improved. The detailed comments are as follows:\n\n- The main contribution of this paper is to apply word pairs instead of words to RBM models. However, the main techniques such as RBM, parser to extract word pairs, tf-idf for filtering, and k-means for clustering, are all existing standard techniques. It is more like an application of these methods, and has limited contribution and novelty.\n\n- For experiments, they apply k-means clustering in the process so k is one parameter to tune. K needs to be tuned on validation set instead of testing set. This paper simply presents the results of different parameter k on testing set directly.\n\n- The structure of Section 3 needs to be improved. Instead of listing each step in each subsection, a general introduction picture should be introduced first. More intuition is also needed for each step. \n\n- Some figures and tables are overlapping in the experiments. Just keep one is enough.\n\n- The format of reference should be fixed in this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Topics using Semantic Locality","abstract":"The topic modeling discovers the latent topic probability of given the text documents. To generate the more meaningful topic that better represents the given document, we proposed a universal method which can be used in the data preprocessing stage. The method consists of three steps. First, it generates the word/word-pair from every single document. Second, it applies a two way parallel TF-IDF algorithm to word/word-pair for semantic filtering. Third, it uses the k-means algorithm to merge the word pairs that have the similar semantic meaning.\n\nExperiments are carried out on the Open Movie Database (OMDb), Reuters Dataset and 20NewsGroup Dataset and use the mean Average Precision score as the evaluation metric. Comparing our results with other state-of-the-art topic models, such as Latent Dirichlet allocation and traditional Restricted Boltzmann Machines. Our proposed data preprocessing can improve the generated topic accuracy by up to 12.99\\%. How the number of clusters and the number of word pairs should be adjusted for different type of text document is also discussed.\n","pdf":"/pdf/10402841c59463097b7c5c707797b6fb138155e8.pdf","TL;DR":"We proposed a universal method which can be used in the data preprocessing stage to generate the more meaningful topic that better represents the given document","paperhash":"anonymous|learning_topics_using_semantic_locality","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Topics using Semantic Locality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nefbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper773/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222762995,"tcdate":1511742412570,"number":1,"cdate":1511742412570,"id":"BkHAsAOgf","invitation":"ICLR.cc/2018/Conference/-/Paper773/Official_Review","forum":"SJ9nefbRW","replyto":"SJ9nefbRW","signatures":["ICLR.cc/2018/Conference/Paper773/AnonReviewer1"],"readers":["everyone"],"content":{"title":"More rigorous experimentation is required before acceptance","rating":"3: Clear rejection","review":"The authors propose first applying dependency parsing to documents, then using pairs of words connected via dependency as features in a similarity metric.\n\nWhile intriguing, a lot more work would be required to publish this at ICLR. Namely, the following questions need to be answered:\n\n1. Does using linked-word-pairs truly raise the state of the art? Unlike what is stated in the abstract, the experimental results only compare RBMs with and without this feature. RBMs are not state-of-the-art in topic modeling, therefore it’s difficult to assess whether this is helpful.\n2. If linked words does improve topic modeling, why does it do so? There needs to be some sort of error analysis to show why this idea improves, rather than simply stating metrics.\n3. Are words that are linked via a dependency better than commonly co-occuring words? Experiments need to be done to show that a full dependency parse is actually required, rather than simply looking for co-occuring words.\n4. How is this work related to the extensive work in NLP in applying parsing to various tasks? A quick search reveals [1] (probabilistic modeling of dependency parses to create Bayesian topic models directly) and [2] (creating a semantic vector space from a dependency parse) I suspect there are others. Citations in [2] could be a good place to start.\n5. Can the selection of word pairs be done automatically, from data, rather than pre-computed with a known dependency parser? After all, this is submitted to the International Conference on Learning Representations --- feature engineering papers can easily be published at EMNLP, ICML, etc. An excellent ICLR paper would show some way to either (a) use dependency parsing only at training time (to provide a hint), or (b) not require dependency parsing at all.\n\nA few suggestions for experiments:\nA. I would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation, just using log(tf)-idf as a distance metric. By stripping away more advanced modeling, that could reveal whether the dependency bi-gram has utility.\nB. The authors may wish to consider applying LSA to both bag of words and dependency-bigrams, using log(tf)-idf weighting for both. From what I’ve seen, log(tf)-idf LSA seems to perform about as well as LDA. Plain LSA takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on LSA at all.\nC. Reiterating point (3) above, to really show whether the power of the dependency parse is being used, I would strongly suggest doing a null experiment with co-occuring nearby words.\n\n\nReferences:\n[1] Boyd-Graber, J. L., & Blei, D. M. (2009). Syntactic topic models. In Advances in neural information processing systems(pp. 185-192).\n[2] Padó, S. and Lapata, M., 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2), pp.161-199.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Topics using Semantic Locality","abstract":"The topic modeling discovers the latent topic probability of given the text documents. To generate the more meaningful topic that better represents the given document, we proposed a universal method which can be used in the data preprocessing stage. The method consists of three steps. First, it generates the word/word-pair from every single document. Second, it applies a two way parallel TF-IDF algorithm to word/word-pair for semantic filtering. Third, it uses the k-means algorithm to merge the word pairs that have the similar semantic meaning.\n\nExperiments are carried out on the Open Movie Database (OMDb), Reuters Dataset and 20NewsGroup Dataset and use the mean Average Precision score as the evaluation metric. Comparing our results with other state-of-the-art topic models, such as Latent Dirichlet allocation and traditional Restricted Boltzmann Machines. Our proposed data preprocessing can improve the generated topic accuracy by up to 12.99\\%. How the number of clusters and the number of word pairs should be adjusted for different type of text document is also discussed.\n","pdf":"/pdf/10402841c59463097b7c5c707797b6fb138155e8.pdf","TL;DR":"We proposed a universal method which can be used in the data preprocessing stage to generate the more meaningful topic that better represents the given document","paperhash":"anonymous|learning_topics_using_semantic_locality","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Topics using Semantic Locality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nefbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper773/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739110902,"tcdate":1509134514418,"number":773,"cdate":1509739108253,"id":"SJ9nefbRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ9nefbRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Topics using Semantic Locality","abstract":"The topic modeling discovers the latent topic probability of given the text documents. To generate the more meaningful topic that better represents the given document, we proposed a universal method which can be used in the data preprocessing stage. The method consists of three steps. First, it generates the word/word-pair from every single document. Second, it applies a two way parallel TF-IDF algorithm to word/word-pair for semantic filtering. Third, it uses the k-means algorithm to merge the word pairs that have the similar semantic meaning.\n\nExperiments are carried out on the Open Movie Database (OMDb), Reuters Dataset and 20NewsGroup Dataset and use the mean Average Precision score as the evaluation metric. Comparing our results with other state-of-the-art topic models, such as Latent Dirichlet allocation and traditional Restricted Boltzmann Machines. Our proposed data preprocessing can improve the generated topic accuracy by up to 12.99\\%. How the number of clusters and the number of word pairs should be adjusted for different type of text document is also discussed.\n","pdf":"/pdf/10402841c59463097b7c5c707797b6fb138155e8.pdf","TL;DR":"We proposed a universal method which can be used in the data preprocessing stage to generate the more meaningful topic that better represents the given document","paperhash":"anonymous|learning_topics_using_semantic_locality","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Topics using Semantic Locality},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ9nefbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper773/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}