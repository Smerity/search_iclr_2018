{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222536415,"tcdate":1511827381205,"number":3,"cdate":1511827381205,"id":"Syp3P75gz","invitation":"ICLR.cc/2018/Conference/-/Paper1002/Official_Review","forum":"BJgVaG-Ab","replyto":"BJgVaG-Ab","signatures":["ICLR.cc/2018/Conference/Paper1002/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper presents a method to connect truncated linear temporal logic formulas to reinforcement learning policies, but several relevant details are not sufficiently clear.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition.  The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true. The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.  The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho.  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy.  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.\n\nThe main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.  The robustness term rho is essential, but it is not defined.  I was also confused by the notation $D_\\phi^q$, which was described but not defined.  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.  The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored.  This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula. It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals. is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?  In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION","abstract":"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\nAn obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\n","pdf":"/pdf/a97cdce29399daece7fc8fa7a274c8e4613e15fb.pdf","TL;DR":"Combine temporal logic with hierarchical reinforcement learning for skill composition","paperhash":"anonymous|automata_guided_hierarchical_reinforce_ment_learning_for_zeroshot_skill_composi_tion","_bibtex":"@article{\n  anonymous2018automata,\n  title={AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgVaG-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1002/Authors"],"keywords":["Hierarchical Reinforcement learning","temporal logic","skill composition"]}},{"tddate":null,"ddate":null,"tmdate":1512222536468,"tcdate":1511747283646,"number":2,"cdate":1511747283646,"id":"SJnC0yKez","invitation":"ICLR.cc/2018/Conference/-/Paper1002/Official_Review","forum":"BJgVaG-Ab","replyto":"BJgVaG-Ab","signatures":["ICLR.cc/2018/Conference/Paper1002/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good objective but I don't think the primary result is correct","rating":"3: Clear rejection","review":"I very much appreciate the objectives of this paper:  learning compositional structures is critical for scaling and transfer.  \n\nThe first part of the paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula, and reminds us that we can learn within that restricted MDP.  Some previous work is cited, but I would point the authors to much older work of Parr and Russell on HAMs (hierarchies of abstract machines) and later work by Andre and Russell, which did something very similar (though, indeed, not in hybrid domains).  The idea of extracting policies corresponding to individual automaton states and making them into options seems novel, but it would be important to argue that those options are likely to be useful again under some task distribution. \n\nThe second part offers an exciting result:  If we learn policy pi_1 to satisfy objective phi_1 and policy pi_2 to satisfy objective phi_2, then it will be possible to switch between pi_1 and pi_2 in a way that satisfies phi_1 ^ phi_2.   This just doesn't make sense to me.  What if phi_1 is o ((A v B) Until C) and phi_2 is o ((not A v B) Until C).   Let's assume that o(B Until C) is satisfiable, so the conjunction is satisfiable.  However, we may find policy pi_1 that makes A true and B false (in general, there is no single optimal policy) and find pi_2 that makes A false and B false, and it will not be possible to satisfy the phi_1 and phi_2 by switching between the policies.    But, perhaps I am misunderstanding something.\n\nSome other smaller points:\n- \"zero-shot skill composition\" sounds a lot like what used to be called \"planning\" or \"reasoning\"\n- The function rho is originally defined on whole trajectories but in eq 7 it is only on a single s':  I'm not sure exactly what that means.\n- Section 4:  How is \"as soon as possible\" encoded in this objective?\n- How does the fixed horizon interact with conjoining goals?\n- There are many small errors in syntax;  it would be best to have this paper carefully proofread.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION","abstract":"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\nAn obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\n","pdf":"/pdf/a97cdce29399daece7fc8fa7a274c8e4613e15fb.pdf","TL;DR":"Combine temporal logic with hierarchical reinforcement learning for skill composition","paperhash":"anonymous|automata_guided_hierarchical_reinforce_ment_learning_for_zeroshot_skill_composi_tion","_bibtex":"@article{\n  anonymous2018automata,\n  title={AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgVaG-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1002/Authors"],"keywords":["Hierarchical Reinforcement learning","temporal logic","skill composition"]}},{"tddate":null,"ddate":null,"tmdate":1512222536517,"tcdate":1511716662284,"number":1,"cdate":1511716662284,"id":"ryRVwuOeM","invitation":"ICLR.cc/2018/Conference/-/Paper1002/Official_Review","forum":"BJgVaG-Ab","replyto":"BJgVaG-Ab","signatures":["ICLR.cc/2018/Conference/Paper1002/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Makes a connection between LTL task representation and RL subtasks, providing some capability for composing skills.","rating":"5: Marginally below acceptance threshold","review":"The paper argues for structured task representations (in TLTL) and shows how these representations can be used to reuse learned subtasks to decrease learning time.\n\nOverall, the paper is sloppily put together, so it's a little difficult to assess the completeness of the ideas. The problem being solved is not literally the problem of decreasing the amount of data needed to learn tasks, but a reformulation of the problem that makes it unnecessary to relearn subtasks. That's a good idea, but problem reformulation is always hard to justify without returning to a higher level of abstraction to justify that there's a deeper problem that remains unchanged. The paper doesn't do a great job of making that connection.\n\nThe idea of using task decomposition to create intrinsic rewards seems really interesting, but does not appear to be explored in any depth. Are there theorems to be had? Is there a connection to subtasks rewards in earlier HRL papers?\n\nThe lack of completeness (definitions of tasks and robustness) also makes the paper less impactful than it could be.\n\nDetailed comments:\n\n\"learn hierarchical policies\" -> \"learns hierarchical policies\"?\n\n\"n games Mnih et al. (2015)Silver et al. (2016),\": The citations are a mess. Please proof read.\n\n\"and is hardly reusable\" -> \"and are hardly reusable\".\n\n\"Skill composition is the idea of constructing new skills with existing skills (\" -> \"Skill composition is the idea of constructing \nnew skills out of existing skills (\".\n\n\"to synthesis\" -> \"to synthesize\".\n\n\"set of skills are\" -> \"set of skills is\".\n\n\"automatons\" -> \"automata\".\n\n\"with low-level controllers can\" -> \"with low-level controllers that can\".\n\n\"the options policy π o is followed until β(s) > threshold\": I don't think that's how options were originally defined... beta is generally defined as a termination probability.\n\n\"The translation from TLTL formula FSA to\" -> \"The translation from TLTL formula to FSA\"?\n\n\"four automaton states Qφ = {q0, qf , trap}\": Is it three or four?\n\n\"learn a policy that satisfy\" -> \"learn a policy that satisfies\".\n\n\"HRL, We introduce the FSA augmented MDP\" -> \"HRL, we introduce the FSA augmented MDP.\".\n\n\" multiple options policy separately\" -> \" multiple options policies separately\"?\n\n\"Given flat policies πφ1 and πφ2 that satisfies \" -> \"Given flat policies πφ1 and πφ2 that satisfy \".\n\n\"s illustrated in Figure 3 .\" -> \"s illustrated in Figure 2 .\"?\n\n\", we cam simply\" -> \", we can simply\".\n\n\"Figure 4 <newline> .\" -> \"Figure 4.\".\n\n\", disagreement emerge\" -> \", disagreements emerge\"?\n\nThe paper needs to include SOME definition of robustness, even if it just informal. As it stands, it's not even clear if larger \nvalues are better or worse. (It would seem that *more* robustness is better than less, but the text says that lower values are \nchosen.)\n\n\"with 2 hidden layers each of 64 relu\": Missing word? Or maybe a comma?\n\n\"to aligns with\" -> \"to align with\".\n\n\" a set of quadratic distance function\" -> \" a set of quadratic distance functions\".\n\n\"satisfies task the specification)\" -> \"satisfies the task specification)\".\n\nFigure 4: Tasks 6 and 7 should be defined in the text someplace.\n\n\"current frame work i\" -> \"current framework i\".\n\n\" and choose to follow\" -> \" and chooses to follow\".\n\n\" this makes\" -> \" making\".\n\n\"each subpolicies\" -> \"each subpolicy\".\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION","abstract":"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\nAn obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\n","pdf":"/pdf/a97cdce29399daece7fc8fa7a274c8e4613e15fb.pdf","TL;DR":"Combine temporal logic with hierarchical reinforcement learning for skill composition","paperhash":"anonymous|automata_guided_hierarchical_reinforce_ment_learning_for_zeroshot_skill_composi_tion","_bibtex":"@article{\n  anonymous2018automata,\n  title={AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgVaG-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1002/Authors"],"keywords":["Hierarchical Reinforcement learning","temporal logic","skill composition"]}},{"tddate":null,"ddate":null,"tmdate":1510092382527,"tcdate":1509137739698,"number":1002,"cdate":1510092360713,"id":"BJgVaG-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJgVaG-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION","abstract":"An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\nAn obstacle that prevents the wide adoption of (deep) reinforcement learning (RL) in control systems is its need for a large amount of interactions with the environ- ment in order to master a skill. The learned skill usually generalizes poorly across domains and re-training is often necessary when presented with a new task. We present a framework that combines methods in temporal logic (TL) with hierar- chical reinforcement learning (HRL). The set of techniques we provide allows for convenient specification of tasks with complex logic, learn hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards us- ing any RL methods and is able to construct new skills from existing ones without additional learning. We evaluate the proposed methods in a simple grid world simulation as well as simulation on a Baxter robot.\n","pdf":"/pdf/a97cdce29399daece7fc8fa7a274c8e4613e15fb.pdf","TL;DR":"Combine temporal logic with hierarchical reinforcement learning for skill composition","paperhash":"anonymous|automata_guided_hierarchical_reinforce_ment_learning_for_zeroshot_skill_composi_tion","_bibtex":"@article{\n  anonymous2018automata,\n  title={AUTOMATA GUIDED HIERARCHICAL REINFORCE- MENT LEARNING FOR ZERO-SHOT SKILL COMPOSI- TION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJgVaG-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1002/Authors"],"keywords":["Hierarchical Reinforcement learning","temporal logic","skill composition"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}