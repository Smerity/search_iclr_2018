{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222610963,"tcdate":1511831686206,"number":3,"cdate":1511831686206,"id":"SyCKd4clM","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Review","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review for Emergent Complexity via Multi-Agent Competition","rating":"7: Good paper, accept","review":"This paper demonstrates that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself and such environments come with a natural curriculum by introducing several multi-agent tasks with competing goals in a 3D world with simulated physics. It utilizes a decentralized training approach and use distributed implementation of PPO for very large scale multiagent training. This paper addresses the challenges in applying distributed PPO to train multiple competitive agents, including the problem of exploration with sparse reward by using full roll-outs and use the dense exploration reward which is gradually annealed to zero in favor of the sparse competition reward. It makes training more stable by selecting random old parameters for the opponent. \n \nAlthough the technical contributions seem to be not quite significant, this paper is well written and introduces a few new domains which are useful for studying problems in multiagent reinforcement learning. The paper also makes it clear regarding the connections and distinctions to many existing work. \n\nMinor issues:\n\nE[Loss] in table 1 is undefined.\n\nIn the notation section, the observation model is missing, and the policy is restricted to be reactive.\n \nUniform (v, \\deta v) -> Uniform (\\deta v, v)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/03a45f4dcebc84fbf7b268f77c38961bf73b3856.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222611004,"tcdate":1511675698354,"number":2,"cdate":1511675698354,"id":"By9EwRPxG","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Review","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Impressive results; insights on opponent sampling","rating":"9: Top 15% of accepted papers, strong accept","review":"Understanding how-and-why complex motion skills emerge is an complex and interesting problem.\nThe method and results of this paper demonstrate some good progress on this problem, and focus on\nthe key point that competition introduces a natural learning curriculum.\nMulti-agent competitive learning has seen some previous work in setting involving physics-based skills\nor actual robots. However, the results in this paper are compelling in taking this another good step forward.\nOverall the paper is clearly written and I believe that it will have impact.\n\nlist of pros & cons\n+ informative and unique experiments that demonstrate emergent complexity coming from the natural curriculum\n  provided by competitive play, for physics-based settings\n+ likely to be of broad interest\n- likely large compute resources needed to replicate or build on the results\n- paper is not anonymous to this reviewer, given the advance publicity for this work when it was released\n==> overall this paper will have impact and advances the state of the art, particular wrt to curriculums\n    In many ways, it is what one might expect. But executing on the idea is very much non-trivial.\n\nother comments\n\nCan you comment on the difficulty of designing the \"games\" themselves?\nIt is often difficult to decide apriori when a game is balanced; game designers of any kind\nspend significant time on this. Perhaps it is easier for some of the types of games investigated in\nthis paper, but if you did have any issues with games becoming unbalanced, that would be worthwhile commenting on.\nGame design is also the next level of learning in many ways.  :-)\n\nThe opponent sampling strategy is one of the key results of the paper.\nIt could be brought to the fore earlier, i.e., in the abstract.\n\nHow much do the exploration rewards matter?\nIf two classes of agents are bootstrapped with different flavours of exploration rewards, how much would it matter?\n\nIt would be generally interesting to describe when during the learning various \"strategies\" emerged,\nand in what order.\n\nAdding sensory delays might enable richer decoy strategies.\n\nThe paper could comment on the further additional complexity that might result from situations\nthat allow for collaboration as well as competition. (ok, I now see that this is mentioned in the conclusions)\n\nThe Robocup tournaments for robot soccer (real and simulated) have for a long time provided\na path to growing skills and complexity, although under different constraints, and perhaps less interesting\nin terms of one-on-one movement skills.\n\nSection 2, \"Notation\"\nwhy are the actions described as being discrete here, when the paper uses continuous actions?\nAlso, \"$\\pi_{\\theta}$ can be Gaussian\":   better to say that it *is* Gaussian in this paper.\n\n\"lead to different algorithm*s*\"\n\nAre there torque limits, and if so, what are they?\n\nsec 4: \"We do multiple rollouts for each agent *pair*\" (?)\n\n\"Such rewards have been previously researched for simple tasks like walking forward and standing up\"\nGiven the rather low visual quality and overly-powerful humanoids of the many of the published \"solutions\", \nperhaps \"simple\" is the wrong qualifer.\n\nFigure 2:  curve legend?\n\n\"exiting work\" (sic)\n\n4.2 Opponent sampling:\n\"simultaneously training\"  should add \"in opponent pairs\" (?)\n\n5.1 \"We use both MLP and LSTM\"\nshould be \"We compare MLP and LSTM ...\" (?)\n\nFor \"kick and defend\" and \"you shall not pass\", are there separate attack and defend policies?\nIt seems that these are unique in that the goals are not symmetric, whereas for the other tasks they are.\nWould be worthwhile to comment on this aspect.\n\nepisodic length T, eqn (1)\nIt's not clear at this point in the paper if T is constant or not.\n\nObservations: \"we also give the centre-of-mass based inertia *tensor*\" (?)\n\n\"distance from the edge of the ring\"\nHow is this defined?\n\n\"none of the agents observe the complete global state\"\nDoes this really make much of a difference?  Most of the state seems visible.\n\n\"But these movement strategies\" -> \"These movement strategies ...\"\n\nsec 5.4  suggest to use $\\mathrm{Uniform}(...)$\n\n\"looses by default\" (sic)\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/03a45f4dcebc84fbf7b268f77c38961bf73b3856.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222611050,"tcdate":1511281392976,"number":1,"cdate":1511281392976,"id":"SkFemC-lz","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Review","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper showing nice results lacks a serious scientific analysis and contains several issues","rating":"3: Clear rejection","review":"In this paper, the authors produced quite cool videos showing the acquisition of highly complex skills, and they are happy about it. If you read the conclusion, this is the only message they put forward, and to me this is not a scientific message.\n\nA more classical summary is that the authors use PPO, a state-of-the-art deep RL method, in a context where two agents are trained to perform competitive games against each other. They reuse a very recent \"dense reward\" technique to bootstrap the agent skills, and then anneal it to zero so that the competitive rewards obtained from defeating the opponent takes the lead. They study the effect of this annealing process (considered as a curriculum) and of various strategies for sampling the opponents. The main outcome is the acquisition of a large variety of useful skills, just observed from videos of the competitions.\n\nThe main issue with this paper is the lack of scientific analysis of the results, together with many local issues in the presentation of these results.\nBelow, I talk directly to the authors.\n\n---------------------------------\n\nThe related work subsection is just a list of works, it should explain how the proposed work position itself with respect to these works.\n\n\nIn Section 5.2, you are just describing \"cool\" behaviors observed from your videos.\nScience is about producing quantitative results, analyzing them and discussing them.\nI would be glad to read more science about these cool behaviors. Can you define a repertoire of such behaviors?\nDetermine how often they are discovered? Study how the are represented in the networks?\nAnything beyond \"look, that's great!\" would make the paper better...\n\nBy the end of Section 5.2, you allude to transfer learning phenomena.\nIt would be nice to study these transfer effects in your results with a quantitative methodology.\n\nSection 5.3 is more scientific, but it has serious issues.\n\nIn all subfigures in Figure 3, the performance of opponents should be symmetric around 50%. This is not the case for subfigures (b) and (c-1). Why?\nDo they correspond to non-zero sum game? The x-label is \"version\". Don't you mean \"number of epochs\", or something like this? Why do the last 2 images\nshare the same caption?\n\nI had a hard time understanding the message from Table 1. It really needs a line before the last row and a more explicative caption.\n\nStill in 5.3, \"These results echo\"...: can you characterize this echo? What is the relationship to this other work?\n\nAgain, \"These results shed further light\": further with respect to what? Can you be more explicit about what we learn?\n\nAlso, I find that annealing a kind of reward with respect to another is a weak form of curriculum learning. This should be further discussed.\n\nIn Section 5.4, the idea of using many opponents from many stages of learning in not new.\nIf I'm correct, the same was done in evolutionary method to escape the \"arms race\" dead-end in prey-predator races quite a while ago  (see e.g. \"Coevolving predator and prey robots: Do “arms races” arise in artificial evolution?\" Nolfi and Floreano, 1998)\n\nSection 5.5.1 would deserve a more quantitative presentation of the effect of randomization.\nActually, in Fig5: the axes are not labelled. I don't believe it shows a win-rate. So probably the caption (or the image) is wrong.\n\nIn Section 5.5.2, you \"suspect this is because...\".\nThe role of a scientific paper is to clearly establish results and explanation from solid quantitative analysis. \n\n-------------------------------------------\nMore local comments:\n\nAbstract:\n\n\"Normally, the complexity of the trained agent is closely related to the complexity of the environment.\" Here you could cite Herbert Simon (1962).\n\n\"In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.\"\nWell, for an agent, the other agent(s) are part of its environment, aren't they? So I don't like this perspective that the environment itself is \"simple\".\n\nIntro:\n\n\"RL is exciting because good RL exists.\" I don't believe this is a strong argument. There are many good things that exist which are not exciting.\n\n\"In general, training an agent to perform a highly complex task requires a highly complex environment, and these can be difficult to create.\" Well, the standard perspective is the other way round: in general, you face a complex problem, then you need to design a complex agent to solve it, and this is difficult. \n\n\"This happens because no matter how weak or strong an agent is, an environment populated with other agents of comparable strength provides the right challenge to the agent, facilitating maximally rapid learning and avoiding getting stuck.\" This is not always true. The literature is full of examples where two-players competition end-up with oscillations between to solutions rather than ever-increasing skill performance. See the prey-predator literature pointed above.\n\n\"in the domain of continuous control, where balance, dexterity, and manipulation are the key skills.\" In robotics, dexterity, and manipulation usually refer to using the robot's hand(s), a capability which is not shown here.\n\nIn preliminaries, notation, what you describe corresponds to the framework of Dec-POMDPs, you should position yourself with respect to this framework (see e.g. Memory-Bounded Dynamic Programming for DEC-POMDPs. S Seuken, S Zilberstein)\n\nIn PPO description : Let l_t(\\theta) ... denote the likelihood ratio: of what?\n\np5:\nwould train on the dense reward for about 10-15% of the trainig epochs. So how much is \\alpha_t? How did you tune it? Was it hard?\n\np6:\n\nyou give to the agent the mass: does the mass change over time???\n\nIn observations: Are both agents given different observations? Could you specify which is given what?\n\nIn Algorithms parameters: why do you have to anneal longer for kick-and-defend? What is the underlying phenomenon?\n\nIn Section 5, the text mentions Fig5 before Fig4.\n\n-------------------------------------------------\nTypos:\n\np4:\nresearch(Andrychowicz => missing space\nstraight forward => straightforward\n\np5:\nagent like humanoid(s)\nfrom exi(s)ting work\n\np6:\neq. 1 => Eq. (1) (you should use \\eqref{})\nIn section 4.1 => In Section 4.1 (same p7 for Section 4.2)\n\n\"One question that arises is the extent to which the outcome of learning is affected by this exploration reward and to explore the benefit of this exploration reward. As already argued, we found the exploration reward to be crucial for learning as otherwise the agents are unable to explore the sparse competition reward.\" => One question that arises is the extent to which the outcome of learning is affected by this exploration reward and to explore its benefit. As already argued, we found it to be crucial for learning as otherwise the agents are unable to explore the sparse competition reward.\n\np8:\nin a local minima => minimum\n\np9:\nin references, you have Jakob Foerster and Jakob N Foerster => try to be more consistent.\n\np10, In Laetitia Matignon et al.  ... markov => Markov\n\np11, I would rename C_{alive} as C_{standing}","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/03a45f4dcebc84fbf7b268f77c38961bf73b3856.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739389962,"tcdate":1509088278066,"number":277,"cdate":1509739387303,"id":"Sy0GnUxCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/03a45f4dcebc84fbf7b268f77c38961bf73b3856.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}