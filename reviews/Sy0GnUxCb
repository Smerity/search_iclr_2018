{"notes":[{"tddate":null,"ddate":null,"tmdate":1516171314872,"tcdate":1516171314872,"number":8,"cdate":1516171314872,"id":"BJjEeunNf","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Comment","forum":"Sy0GnUxCb","replyto":"rJI9veRQz","signatures":["ICLR.cc/2018/Conference/Paper277/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper277/Authors"],"content":{"title":"Response to further questions by Reviewer","comment":"We respond to the three points about the paper raised by the reviewer:\n\n1) There are two questions here, one about games being zero sum and another about the plots in Figure 3 being symmetric about 50%. For the first question the answer is games are not zero sum and a draw results in a “negative” reward for both agents not 0 (considering it as a loss for both) and these rewards are very clearly defined in section 3 where the environments are introduced. Now, for the question about plots being symmetric, let's consider a simple example to understand why plot need not be symmetric. Say we are taking averages over 10 games, in the first set agent 1 wins 5 and 5 are draws (considered as losses for both), in the second set agent 1 wins 6 and and rest are draws. Then a curve representing average win-rates of agents over these sets of games will have two points for agent 1 at 50% and 60%, thus giving an increasing curve for agent 1 whereas for agent 2 the curve will be 0. It is easily seen in this synthetic example that the curve for two agents is not symmetric.\n\n2) The legend for the plots clarifies this, where the curves are labelled as kicker:no-curriculum, keeper: curriculum and kicker: curriculum, keeper:no-curriculum respectively for the two plots. Since only one of the reviewer seemed to have a problem understanding this plot we didn't add any additional clarification. We will add a line clarifying this further.\n\n3) Note that we have not highlighted this as a contribution of the paper anywhere in the abstract or introduction. This is also in section 5.2 which we have explained earlier is the qualitative evaluation part of the paper. The evaluation of transfer success (falling or not) is performed by computer code that does returns objective result of the episode. We do have quantitative evaluation for experiment reviewer asks, but do not report the numbers because the difference between using our approach and not is very stark. If other reviewers agree, we are happy to include these numbers in the final manuscript.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515223428324,"tcdate":1515222926063,"number":6,"cdate":1515222926063,"id":"rJI9veRQz","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Comment","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper277/AnonReviewer2"],"content":{"title":"Lack of evaluation rigor","comment":"There is a strong disagreement among reviewers (top paper according to one, clear rejection according to me), so I believe some discussion is necessary.\n\nI completely agree that the videos are awesome, but in itself a nice video does not make a strong scientific paper. Engineering (and most of time a good deal of science) is about producing new interesting phenomena, but science is also about rigorously analyzing these results (rigorously generally means in a quantitative way) and extracting scientific messages from these analyses.\n\nQuoting this excellent blog post \nhttp://www.inference.vc/my-thoughts-on-alchemy/\n\"It's Okay to use non-rigorous methods, but... it's not okay to use non-rigorous evaluation\".\n\nLet me just take 3 examples to show you that this paper lacks rigor everywhere (the first two are admittedly minor points, but they feed my general feeling about this paper. Most other points are present in my review below, and the authors did not make the effort to address all of them):\n\nTaken from their reply to my review:\n\n1) \"“In all subfigures in Figure 3, the performance of opponents should be symmetric around 50%. This is not the case for subfigures (b) and (c-1). Why? Do they correspond to non-zero sum game?”\nReply:\nThe games are not zero-sum, there is some chance of a draw as described in Section 3.\"\n\nWell, in case of a draw, both players should get 0, so this cannot be a correct explanation of why the game is not zero sum. If I cannot trust the answer, can I trust the results themselves?\n\n2) 4. \"“Why do the last 2 images share the same caption?”\nReply:\nBecause the kick-and-defend game is asymmetric, so there are two plots -- one where keeper is trained with curriculum and another where kicker is trained with curriculum. \"\n\nFine. I checked in the new version. The only sentence about Fig3 is \"We plot the average win-rates over 800 games at various intervals during training in Fig. 3, for the sumo and kick-and-defend environments.\" Not a word about the above fact. The reader has to guess that. By the way, why don't we get any result for the \"run to goal\" case? What are the \"various intervals\"? Etc.\n\n3) This one is much more serious, about transfer learning. Any rigorous transfer learning paper will precisely define a source domain and a target domain and will measure (quantitatively) the performance in the target domain with and without training first in the source domain. Here we just get \"The agent trained in a non competitive setting falls over immediately (with episodes lasting less than 10 steps on average), whereas the competitive agent is able to withstand strong forces for hundreds of steps. This difference (in terms of length of episode till agent falls over) can be quantified easily over many random episodes, we only included the qualitative results as the difference is huge and easily visible in the videos.\"\nOK, this is awesome. But to me, the evaluation methodology cannot just be \"visual\". This would be perfectly okay for the \"scientific american\" magazine, but this is not OK for a strong scientific conference. The evaluation itself has to be performed by the computer, because this makes it necessary to rigorously define all the conditions of this evaluation (define fall and stand, define the initial posture, define the number of steps you evaluate, define the wind variations, define everything). The computer will also be able to perform a statistical analysis, which is completely lacking here: did this phenomenon appear every time? If not, what are the chances? For instance, given the lack of quantitative analysis, one cannot determine if another method to come will perform better or worse. \n\nTo me, this paper is representative of what Ali Rahimi's talk was about (the talk was given after I wrote my review). So sorry guys, but though I like the videos and I must admit that my review is a little too harsh because the paper irritated me, I'm still considering that this paper should be rejected.\n\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515188683477,"tcdate":1515188683477,"number":5,"cdate":1515188683477,"id":"B1XRb_6mG","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Comment","forum":"Sy0GnUxCb","replyto":"SyCKd4clM","signatures":["ICLR.cc/2018/Conference/Paper277/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper277/Authors"],"content":{"title":"Thank You","comment":"We thank the reviewer for carefully reading the paper and their positive feedback. We have taken care of the appropriate minor changes in the updated draft. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515188600318,"tcdate":1515188600318,"number":4,"cdate":1515188600318,"id":"r1gtW_p7G","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Comment","forum":"Sy0GnUxCb","replyto":"By9EwRPxG","signatures":["ICLR.cc/2018/Conference/Paper277/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper277/Authors"],"content":{"title":"Response to Review","comment":"We thank the reviewer for carefully reading the paper and their encouraging feedback. We have taken care of the appropriate minor changes in the updated draft. We answer specific questions below.\n\n`1. \"Can you comment on the difficulty of designing the \"games\" themselves?\"\nReply:\nWe picked a few simple competitive games for these results. It is important to appropriately set termination conditions for the games. We did observe some difficulty in games becoming unbalanced, for example in kick-and-defend it was important to use a longer horizon for annealing exploration reward as the defender takes a longer time to learn and adding termination penalties for moving beyond the goal area as well as additional rewards for defending and not falling over on termination lead to more balanced win-rates. In future we will explore more complex game designs where the agents can make use of environmental resources to compete or have to overcome environmental obstacles in addition to competing. \n\n2. \"How much do the exploration rewards matter?\"\nReply:\nWe analyzed the extreme cases where agents have or do not have an exploration reward, as well as the case when the exploration reward is never annealed. The summary is to use exploration reward but anneal it. We also experimented with additional reward terms for interaction with opponent in Sumo environment initially but didn’t observe any significant benefits and chose the simplest form of exploration rewards. \n\n3. \"Are there torque limits, and if so, what are they?\"\nReply:\nWe used the default limits in the gym environment for ant and humanoid body, which is bounded between [-0.4, 0.4]\n\n4. \"For \"kick and defend\" and \"you shall not pass\", are there separate attack and defend policies? It seems that these are unique in that the goals are not symmetric, whereas for the other tasks they are.\"\nReply:\nYes, that is correct, we have noted this is section 5.1\n\n5. \"distance from the edge of the ring. How is this defined?\"\nReply:\nIt is the radial distance of the agent from the edge. So if R is ring radius and r is the agent's distance from center then we give (R-r) as input.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515188188188,"tcdate":1515188188188,"number":3,"cdate":1515188188188,"id":"S1VyeOTQG","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Comment","forum":"Sy0GnUxCb","replyto":"SkFemC-lz","signatures":["ICLR.cc/2018/Conference/Paper277/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper277/Authors"],"content":{"title":"Response to Review","comment":"Thank you for taking the time to review our work. We have taken care of the typos and appropriate minor changes in the updated draft. We answer specific questions below.\n\n1. “In Section 5.2, you are just describing \"cool\" behaviors observed from your videos ..... Study how the are represented in the networks? Anything beyond \"look, that's great!\" would make the paper better…”\nReply:\nThere are four main contributions put forth in this paper: (1) Identifying that a competitive multi-agent interaction can serve as a proxy for complexity of the environment and allows for a natural curriculum for training RL agents (2) Developing 4 new simulated environments which can serve as a test-bed for future research on training competitive agents (Section 3) (3) Developing opponent-sampling strategies and a simple (yet effective) strategy for dealing with sparse rewards (Section 4) -- both of which lead to effective training in the competitive environments (as evaluated through quantitative ablation studies in section 5.3 and 5.4) (4) Demonstrating compelling results on the four competitive 3D environments with two different agent morphologies -- showing remarkable dexterity in the agents without explicit supervision.\nWe believe qualitative evaluation through observing agents’ behavior is an important part of evaluating the success of the proposed contributions, and in section 5.2 we analyze qualitatively many random episodes. Section 4 discusses the parallel implementation of PPO (a recent policy optimization algorithm, see section 2), opponent sampling and exploration curriculum which are crucial technical ideas making the results possible. Section 5.3, 5.4, 5.5 contain rigorous quantitative analysis of the main ideas which make these results possible.\nThe results might be what one may expect, but executing on the idea is very much non-trivial -- as also noted by other reviewers. This paper is an exposé of the potential of competitive multi-agent training and we agree there is a lot of potential for more future work in this area. \n\n2. “By the end of Section 5.2, you allude to transfer learning phenomena. It would be nice to study these transfer effects in your results with a quantitative methodology.”\nReply:\nWe studied a particular case of transfer in the sumo environment, where an agent trained in a competitive setting demonstrated robust standing behavior in a single-agent setting without any modifications or fine-tuning of the policies. The agent trained in a non competitive setting falls over immediately (with episodes lasting less than 10 steps on average), whereas the competitive agent is able to withstand strong forces for hundreds of steps. This difference (in terms of length of episode till agent falls over) can be quantified easily over many random episodes, we only included the qualitative results as the difference is huge and easily visible in the videos.\n\n3. “In all subfigures in Figure 3, the performance of opponents should be symmetric around 50%. This is not the case for subfigures (b) and (c-1). Why? Do they correspond to non-zero sum game?”\nReply:\nThe games are not zero-sum, there is some chance of a draw as described in Section 3.\n\n4. “Why do the last 2 images share the same caption?”\nReply:\nBecause the kick-and-defend game is asymmetric, so there are two plots -- one where keeper is trained with curriculum and another where kicker is trained with curriculum. \n\n5. “I had a hard time understanding the message from Table 1. It really needs a line before the last row and a more explicative caption.”\nReply:\nAdded. It is also described in detail in Section 5.4\n\n6. “Also, I find that annealing a kind of reward with respect to another is a weak form of curriculum learning. This should be further discussed.”\nThis is discussed in section 4.1\n\n7. “Actually, in Fig5: the axes are not labelled. I don't believe it shows a win-rate. So probably the caption (or the image) is wrong.”\nReply:\nThe y-label is the fractional win-rate (and not %), we have clarified this.\n\n8. “would train on the dense reward for about 10-15% of the training epochs. So how much is \\alpha_t? How did you tune it? Was it hard?”\nReply:\nNote that \\alpha_t is an annealing factor, so it’s value starts from 1 and is annealed to 0 over some number of epochs. 10-15% of training epochs is the typical window for annealing \\alpha_t and the exact values are given in the experiments section 5.1. There is no direct tuning of \\alpha_t required, instead the horizon for annealing was tuned in {250, 500, 750, 1000} epochs and the value giving highest win-rate was selected. More quantitative analysis of annealing is in section 5.3\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642423856,"tcdate":1511831686206,"number":3,"cdate":1511831686206,"id":"SyCKd4clM","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Review","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review for Emergent Complexity via Multi-Agent Competition","rating":"7: Good paper, accept","review":"This paper demonstrates that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself and such environments come with a natural curriculum by introducing several multi-agent tasks with competing goals in a 3D world with simulated physics. It utilizes a decentralized training approach and use distributed implementation of PPO for very large scale multiagent training. This paper addresses the challenges in applying distributed PPO to train multiple competitive agents, including the problem of exploration with sparse reward by using full roll-outs and use the dense exploration reward which is gradually annealed to zero in favor of the sparse competition reward. It makes training more stable by selecting random old parameters for the opponent. \n \nAlthough the technical contributions seem to be not quite significant, this paper is well written and introduces a few new domains which are useful for studying problems in multiagent reinforcement learning. The paper also makes it clear regarding the connections and distinctions to many existing work. \n\nMinor issues:\n\nE[Loss] in table 1 is undefined.\n\nIn the notation section, the observation model is missing, and the policy is restricted to be reactive.\n \nUniform (v, \\deta v) -> Uniform (\\deta v, v)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642423893,"tcdate":1511675698354,"number":2,"cdate":1511675698354,"id":"By9EwRPxG","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Review","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Impressive results; insights on opponent sampling","rating":"9: Top 15% of accepted papers, strong accept","review":"Understanding how-and-why complex motion skills emerge is an complex and interesting problem.\nThe method and results of this paper demonstrate some good progress on this problem, and focus on\nthe key point that competition introduces a natural learning curriculum.\nMulti-agent competitive learning has seen some previous work in setting involving physics-based skills\nor actual robots. However, the results in this paper are compelling in taking this another good step forward.\nOverall the paper is clearly written and I believe that it will have impact.\n\nlist of pros & cons\n+ informative and unique experiments that demonstrate emergent complexity coming from the natural curriculum\n  provided by competitive play, for physics-based settings\n+ likely to be of broad interest\n- likely large compute resources needed to replicate or build on the results\n- paper is not anonymous to this reviewer, given the advance publicity for this work when it was released\n==> overall this paper will have impact and advances the state of the art, particular wrt to curriculums\n    In many ways, it is what one might expect. But executing on the idea is very much non-trivial.\n\nother comments\n\nCan you comment on the difficulty of designing the \"games\" themselves?\nIt is often difficult to decide apriori when a game is balanced; game designers of any kind\nspend significant time on this. Perhaps it is easier for some of the types of games investigated in\nthis paper, but if you did have any issues with games becoming unbalanced, that would be worthwhile commenting on.\nGame design is also the next level of learning in many ways.  :-)\n\nThe opponent sampling strategy is one of the key results of the paper.\nIt could be brought to the fore earlier, i.e., in the abstract.\n\nHow much do the exploration rewards matter?\nIf two classes of agents are bootstrapped with different flavours of exploration rewards, how much would it matter?\n\nIt would be generally interesting to describe when during the learning various \"strategies\" emerged,\nand in what order.\n\nAdding sensory delays might enable richer decoy strategies.\n\nThe paper could comment on the further additional complexity that might result from situations\nthat allow for collaboration as well as competition. (ok, I now see that this is mentioned in the conclusions)\n\nThe Robocup tournaments for robot soccer (real and simulated) have for a long time provided\na path to growing skills and complexity, although under different constraints, and perhaps less interesting\nin terms of one-on-one movement skills.\n\nSection 2, \"Notation\"\nwhy are the actions described as being discrete here, when the paper uses continuous actions?\nAlso, \"$\\pi_{\\theta}$ can be Gaussian\":   better to say that it *is* Gaussian in this paper.\n\n\"lead to different algorithm*s*\"\n\nAre there torque limits, and if so, what are they?\n\nsec 4: \"We do multiple rollouts for each agent *pair*\" (?)\n\n\"Such rewards have been previously researched for simple tasks like walking forward and standing up\"\nGiven the rather low visual quality and overly-powerful humanoids of the many of the published \"solutions\", \nperhaps \"simple\" is the wrong qualifer.\n\nFigure 2:  curve legend?\n\n\"exiting work\" (sic)\n\n4.2 Opponent sampling:\n\"simultaneously training\"  should add \"in opponent pairs\" (?)\n\n5.1 \"We use both MLP and LSTM\"\nshould be \"We compare MLP and LSTM ...\" (?)\n\nFor \"kick and defend\" and \"you shall not pass\", are there separate attack and defend policies?\nIt seems that these are unique in that the goals are not symmetric, whereas for the other tasks they are.\nWould be worthwhile to comment on this aspect.\n\nepisodic length T, eqn (1)\nIt's not clear at this point in the paper if T is constant or not.\n\nObservations: \"we also give the centre-of-mass based inertia *tensor*\" (?)\n\n\"distance from the edge of the ring\"\nHow is this defined?\n\n\"none of the agents observe the complete global state\"\nDoes this really make much of a difference?  Most of the state seems visible.\n\n\"But these movement strategies\" -> \"These movement strategies ...\"\n\nsec 5.4  suggest to use $\\mathrm{Uniform}(...)$\n\n\"looses by default\" (sic)\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642423932,"tcdate":1511281392976,"number":1,"cdate":1511281392976,"id":"SkFemC-lz","invitation":"ICLR.cc/2018/Conference/-/Paper277/Official_Review","forum":"Sy0GnUxCb","replyto":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference/Paper277/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper showing nice results lacks a serious scientific analysis and contains several issues","rating":"3: Clear rejection","review":"In this paper, the authors produced quite cool videos showing the acquisition of highly complex skills, and they are happy about it. If you read the conclusion, this is the only message they put forward, and to me this is not a scientific message.\n\nA more classical summary is that the authors use PPO, a state-of-the-art deep RL method, in a context where two agents are trained to perform competitive games against each other. They reuse a very recent \"dense reward\" technique to bootstrap the agent skills, and then anneal it to zero so that the competitive rewards obtained from defeating the opponent takes the lead. They study the effect of this annealing process (considered as a curriculum) and of various strategies for sampling the opponents. The main outcome is the acquisition of a large variety of useful skills, just observed from videos of the competitions.\n\nThe main issue with this paper is the lack of scientific analysis of the results, together with many local issues in the presentation of these results.\nBelow, I talk directly to the authors.\n\n---------------------------------\n\nThe related work subsection is just a list of works, it should explain how the proposed work position itself with respect to these works.\n\n\nIn Section 5.2, you are just describing \"cool\" behaviors observed from your videos.\nScience is about producing quantitative results, analyzing them and discussing them.\nI would be glad to read more science about these cool behaviors. Can you define a repertoire of such behaviors?\nDetermine how often they are discovered? Study how the are represented in the networks?\nAnything beyond \"look, that's great!\" would make the paper better...\n\nBy the end of Section 5.2, you allude to transfer learning phenomena.\nIt would be nice to study these transfer effects in your results with a quantitative methodology.\n\nSection 5.3 is more scientific, but it has serious issues.\n\nIn all subfigures in Figure 3, the performance of opponents should be symmetric around 50%. This is not the case for subfigures (b) and (c-1). Why?\nDo they correspond to non-zero sum game? The x-label is \"version\". Don't you mean \"number of epochs\", or something like this? Why do the last 2 images\nshare the same caption?\n\nI had a hard time understanding the message from Table 1. It really needs a line before the last row and a more explicative caption.\n\nStill in 5.3, \"These results echo\"...: can you characterize this echo? What is the relationship to this other work?\n\nAgain, \"These results shed further light\": further with respect to what? Can you be more explicit about what we learn?\n\nAlso, I find that annealing a kind of reward with respect to another is a weak form of curriculum learning. This should be further discussed.\n\nIn Section 5.4, the idea of using many opponents from many stages of learning in not new.\nIf I'm correct, the same was done in evolutionary method to escape the \"arms race\" dead-end in prey-predator races quite a while ago  (see e.g. \"Coevolving predator and prey robots: Do “arms races” arise in artificial evolution?\" Nolfi and Floreano, 1998)\n\nSection 5.5.1 would deserve a more quantitative presentation of the effect of randomization.\nActually, in Fig5: the axes are not labelled. I don't believe it shows a win-rate. So probably the caption (or the image) is wrong.\n\nIn Section 5.5.2, you \"suspect this is because...\".\nThe role of a scientific paper is to clearly establish results and explanation from solid quantitative analysis. \n\n-------------------------------------------\nMore local comments:\n\nAbstract:\n\n\"Normally, the complexity of the trained agent is closely related to the complexity of the environment.\" Here you could cite Herbert Simon (1962).\n\n\"In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.\"\nWell, for an agent, the other agent(s) are part of its environment, aren't they? So I don't like this perspective that the environment itself is \"simple\".\n\nIntro:\n\n\"RL is exciting because good RL exists.\" I don't believe this is a strong argument. There are many good things that exist which are not exciting.\n\n\"In general, training an agent to perform a highly complex task requires a highly complex environment, and these can be difficult to create.\" Well, the standard perspective is the other way round: in general, you face a complex problem, then you need to design a complex agent to solve it, and this is difficult. \n\n\"This happens because no matter how weak or strong an agent is, an environment populated with other agents of comparable strength provides the right challenge to the agent, facilitating maximally rapid learning and avoiding getting stuck.\" This is not always true. The literature is full of examples where two-players competition end-up with oscillations between to solutions rather than ever-increasing skill performance. See the prey-predator literature pointed above.\n\n\"in the domain of continuous control, where balance, dexterity, and manipulation are the key skills.\" In robotics, dexterity, and manipulation usually refer to using the robot's hand(s), a capability which is not shown here.\n\nIn preliminaries, notation, what you describe corresponds to the framework of Dec-POMDPs, you should position yourself with respect to this framework (see e.g. Memory-Bounded Dynamic Programming for DEC-POMDPs. S Seuken, S Zilberstein)\n\nIn PPO description : Let l_t(\\theta) ... denote the likelihood ratio: of what?\n\np5:\nwould train on the dense reward for about 10-15% of the trainig epochs. So how much is \\alpha_t? How did you tune it? Was it hard?\n\np6:\n\nyou give to the agent the mass: does the mass change over time???\n\nIn observations: Are both agents given different observations? Could you specify which is given what?\n\nIn Algorithms parameters: why do you have to anneal longer for kick-and-defend? What is the underlying phenomenon?\n\nIn Section 5, the text mentions Fig5 before Fig4.\n\n-------------------------------------------------\nTypos:\n\np4:\nresearch(Andrychowicz => missing space\nstraight forward => straightforward\n\np5:\nagent like humanoid(s)\nfrom exi(s)ting work\n\np6:\neq. 1 => Eq. (1) (you should use \\eqref{})\nIn section 4.1 => In Section 4.1 (same p7 for Section 4.2)\n\n\"One question that arises is the extent to which the outcome of learning is affected by this exploration reward and to explore the benefit of this exploration reward. As already argued, we found the exploration reward to be crucial for learning as otherwise the agents are unable to explore the sparse competition reward.\" => One question that arises is the extent to which the outcome of learning is affected by this exploration reward and to explore its benefit. As already argued, we found it to be crucial for learning as otherwise the agents are unable to explore the sparse competition reward.\n\np8:\nin a local minima => minimum\n\np9:\nin references, you have Jakob Foerster and Jakob N Foerster => try to be more consistent.\n\np10, In Laetitia Matignon et al.  ... markov => Markov\n\np11, I would rename C_{alive} as C_{standing}","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516177370197,"tcdate":1509088278066,"number":277,"cdate":1509739387303,"id":"Sy0GnUxCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy0GnUxCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Emergent Complexity via Multi-Agent Competition","abstract":"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment.  This suggests that a highly capable agent requires a complex environment for training.  In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.  We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.\nThis work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","pdf":"/pdf/2bc8a478eb8856e7d86e3236c5ea44277bdac6bc.pdf","paperhash":"anonymous|emergent_complexity_via_multiagent_competition","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Complexity via Multi-Agent Competition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy0GnUxCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper277/Authors"],"keywords":["multi-agent systems","multi-agent competition","self-play","deep reinforcement learning"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}