{"notes":[{"tddate":null,"ddate":null,"tmdate":1511879658840,"tcdate":1511879658840,"number":1,"cdate":1511879658840,"id":"BJ7lVlolf","invitation":"ICLR.cc/2018/Conference/-/Paper1109/Public_Comment","forum":"By4Nxm-CW","replyto":"By4Nxm-CW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Experiments","comment":"In your experiments on \"Labels to Facades\", did you train Pix2Pix on the 400 samples yourself or used a pretrained model? If a pretrained model is used, and since Nearest Neighbour algorithms doesn't need training, what is then used for the evaluation? Were the training procedures as described in \"Nearest-Neighbor Embeddings of Interior Layers\" or \"Pixel-wise Nearest-Neighbor Embeddings\"?\n\nThank you"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?","abstract":"We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. We aim to understand and uncover the synthesis/prediction mechanisms of state-of-the-art convolutional neural networks. To this end, we primarily analyze the synthesis process of generative models and the prediction mechanism of discriminative models. The main hypothesis of this work is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function. Our experiments on semantic segmentation and image-to-image translation show qualitative and quantitative evidence supporting this hypothesis.","pdf":"/pdf/b49d8a762bee482378730e0af6ccd7f32f9a0fec.pdf","TL;DR":"Convolutional Neural Networks behave as Compositional Nearest Neighbors!","paperhash":"anonymous|do_convolutional_neural_networks_act_as_compositional_nearest_neighbors","_bibtex":"@article{\n  anonymous2018do,\n  title={Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By4Nxm-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1109/Authors"],"keywords":["interpreting convolutional neural networks","nearest neighbors","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222556393,"tcdate":1511803236622,"number":2,"cdate":1511803236622,"id":"Hy6vY6Fez","invitation":"ICLR.cc/2018/Conference/-/Paper1109/Official_Review","forum":"By4Nxm-CW","replyto":"By4Nxm-CW","signatures":["ICLR.cc/2018/Conference/Paper1109/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review: lacking novelty and actual insight","rating":"3: Clear rejection","review":"This paper purports to analyze whether convolutional neural networks (CNNs) behave as compositional nearest neighbors, in the context of pixelwise prediction tasks such as semantic segmentation or Pix2Pix [Isola et al, 2016].\n\nBut, nearest neighbors in what feature space?\n\nIf this paper showed that CNNs emulated nearest neighbors according to L2 distance between patches in pixel space, that would be quite a result.  Alternatively, if it showed CNNs emulated nearest neighbors in some other human-engineered feature space, or even any feature space not obviously related to neural networks, it would also be quite informative and impressive.\n\nHowever, no such connections are made.  Rather, the feature space in which CNNs emulate nearest neighbors is the feature space defined by the embedding produced by the penultimate layer of the CNN.  This offers no novel insight.  Rather, it hides the real action of the CNN that goes into producing that embedding.  The production of the embedding may have absolutely nothing to do with nearest neighbors at all!\n\nMoreover, it was already well established that CNNs produce useful generic feature representations that can be ported to a variety of visual tasks [Donahue et al, DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition, 2013].\n\nLooking at the penultimate layer representations, this paper experiments with replacing systems that, as trained, feed them into a single additional softmax classification or regression layer.  The replacement is to feed these features into a nearest neighbor classification layer.  It is not surprising that, given the same descriptive features as input, switching to nearest neighbors produces a similar result as the original linear classifiers.  This seems likely to be true in many settings, regardless of whether or not one is using a CNN.\n\nIndeed, if patches x and y have feature representations phi(x) and phi(y), and for some classifier F, we approximate F(phi(x)) by F(phi(y)) where phi(y) is close to phi(x), how is it surprising that this approximation is good?\n\nThe fact, as the paper points out, that internal CNN layers could also be approximated this way, does not prove that the CNN is performing one big nearest neighbor operation.  The approximation seems to hold locally for the trivial reason that it would hold for almost any reasonable function F().\n\nFinally, the paper claims (page 6), that its results suggest \"one novel interpretation of neural networks as learning hierarchies of feature embeddings\".  I believe virtually everyone in the computer vision community was already aware of this \"novel\" interpretation.  Since [Zeiler and Fergus, 2014], there has significant research in understanding how object and part representations emerge in intermediate layers of CNNs.\n\nNote: Devlin et al, 2015 is duplicated in the references list.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?","abstract":"We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. We aim to understand and uncover the synthesis/prediction mechanisms of state-of-the-art convolutional neural networks. To this end, we primarily analyze the synthesis process of generative models and the prediction mechanism of discriminative models. The main hypothesis of this work is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function. Our experiments on semantic segmentation and image-to-image translation show qualitative and quantitative evidence supporting this hypothesis.","pdf":"/pdf/b49d8a762bee482378730e0af6ccd7f32f9a0fec.pdf","TL;DR":"Convolutional Neural Networks behave as Compositional Nearest Neighbors!","paperhash":"anonymous|do_convolutional_neural_networks_act_as_compositional_nearest_neighbors","_bibtex":"@article{\n  anonymous2018do,\n  title={Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By4Nxm-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1109/Authors"],"keywords":["interpreting convolutional neural networks","nearest neighbors","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222556434,"tcdate":1511657834861,"number":1,"cdate":1511657834861,"id":"HJQ_W5weG","invitation":"ICLR.cc/2018/Conference/-/Paper1109/Official_Review","forum":"By4Nxm-CW","replyto":"By4Nxm-CW","signatures":["ICLR.cc/2018/Conference/Paper1109/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Correct and non-trivial, but not at the same time","rating":"4: Ok but not good enough - rejection","review":"To paraphrase a well-known review, the paper is in part correct and in part non-trivial. Unfortunately, these parts do not overlap.\n\nThe chief correct statement is that training deep networks for classification or regression yields embeddings in which the intended classification or regression task can be performed adequately via nearest neighbors in the learned embedding space. That is, after training, we can strip out the final linear classifier or regressor and replace it by nearest neighbor classification or regression in the same embedding space in which the linear classifier or regressor operates. In other words, the embedding learned via backprop is so good that linear classification or regression in this space can be replaced by nearest-neighbor interpolation. This is well-known and is not surprising. This observation predates the recent resurgence of interest in neural networks, but is well documented in recent work as well. See, for example, Razavian et al., \"CNN Features off-the-shelf: an Astounding Baseline for Recognition\" (1.5K citations). The authors try to claim some novelty in that they apply this well-known observation to pixel-level prediction networks, but the reviewer fails to see what in this application was non-trivial or surprising.\n\nThe paper then suggests that this effectiveness of the Euclidean metric in the final embedding space learned by backprop is in a way the underlying operating principle of the learned model, that in some deep sense the learned network “really is” a nonparametric classifier or regressor in a more pertinent sense than it is a parametric model. That the model is functionally and ontologically a nearest neighbor machine rather than a parametric model. Depending on how one interprets this proposition, it is either trivial or unsubstantiated.\n\nLet’s begin with the trivial: Of course a deep network can be viewed as a nearest neighbor machine. The network was trained to minimize a regression or classification loss on the training samples. It must minimize the loss over the samples. The training samples are the only data it has. Of course the output of the model over the training data and nearby points in the embedding space will be identical or similar to the training labels and can thus be regarded as “memorization”. Since the training data is the only thing that is given, everything the training procedure does can be interpreted as “memorization”. But that is a trivial perspective that does not contain a novel insight.\n\nNow to the unsubstantiated implication. As discussed earlier, training a compositional nonlinear embedding with a linear classification or regression loss in the final layer also yields an embedding in which the Euclidean distance works well. From this the authors appear to conclude that the network is in essence a nearest neighbor machine more so than it is a classifier or a regressor. If we try to go beyond the sense in which this statement is trivial, we reach a false dichotomy: why can’t a model trained for classification or regression also construct an intermediate representation in which the Euclidean metric is effective? This may just be a useful thing to do if one needs to apply a linear classifier or regressor in the embedding space. Arranging the data so that the Euclidean distance works well is a perfectly sensible thing for backprop to do. Why does that make the model more of a nearest neighbor machine than a compositional nonlinear classifier or regressor? Why can’t a parametric model trained to construct layers of nonlinear embeddings and perform linear classification or regression on top also yield a final embedding in which the Euclidean distance works? This can be regarded as a “spandrel”, to borrow a term from evolutionary biology: a property that emerges (as a byproduct) while optimizing for a different objective. The fact that the Euclidean distance works in the final embedding space doesn’t make the model a nearest neighbor machine in any sense beyond the obvious one discussed earlier. Yes, training a deep network for classification or regression yields an embedding in which the Euclidean distance works well, but the network is still a parametric model trained for classification or regression.\n\n\nThe issues covered so far are a sufficient basis for my rating. I will now cover some other issues:\n\n- The term “generative” is misused in the paper. For example, the pix2pix model is taken as an example of a generative model. It's not. pix2pix is a discriminative model. It is trained in a supervised fashion, on input-output pairs, to perform discriminative regression. The models studied in the paper are discriminative models, not discriminative and generative models.\n\n- The term “compositional” is misused in the paper, at least in relation to the general use of this term in the literature. “Compositionality” in the literature generally refers to hierarchical composition with representations of increasing scope. This is the sense in which language is said to be compositional. It is also the sense in which “compositionality” was used in the work of Zeiler and Fergus, which is cited in this context in the submission. But the paper uses “compositional” in a different and rather trivial sense. The analogue in the language domain would be to say that language is compositional because it is composed of letters. This is strictly speaking true, but it is not the compositionality that people generally care to discuss in the representation learning field. This trivial application robs the term “compositional” of its richer and more important meaning.\n\n- The paper contains phrases such as “novel interpretation of neural networks as learning hierarchies of embeddings” (page 6). This interpretation is not novel. It is one of the central guiding intuitions in deep learning and has been with the field from its earliest days.\n\n\nOther comments:\n\n- SegNet is referred to as “a recent state-of-the-art network for image segmentation. SegNet is a fine model, but it’s not state-of-the-art, at least in terms of standard benchmarks and leaderboards. SegNet can be a convenient model to use in experiments because it’s fast. This is a perfectly good motivation, but in this case it should be stated upfront, without hiding behind the misapplied “state-of-the-art” term.\n\n- For the quantitative evaluation (Table 1), pix2pix is used as a semantic segmentation baseline for the Cityscapes dataset. This is very strange. Why not use a semantic segmentation model to perform semantic segmentation? The authors have already set up SegNet and are using it for semantic segmentation. Why not use SegNet as the semantic segmentation baseline on Cityscapes?\n\n- For quantitative evaluation of semantic segmentation (Table 1), the authors report overall pixel accuracy (fraction of correctly classified pixels). This is a terrible and misleading measure that is generally avoided in the literature. The issue is that the measure is dominated by large classes that are easy to classify, such as sky, road, and building. This means that simplistic models perform well and that differences between poor models and good ones can be quantitatively insignificant. For these reasons, experienced researchers don’t trust this measure. Mean per-class accuracy and mean IoU are more reliable.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?","abstract":"We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. We aim to understand and uncover the synthesis/prediction mechanisms of state-of-the-art convolutional neural networks. To this end, we primarily analyze the synthesis process of generative models and the prediction mechanism of discriminative models. The main hypothesis of this work is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function. Our experiments on semantic segmentation and image-to-image translation show qualitative and quantitative evidence supporting this hypothesis.","pdf":"/pdf/b49d8a762bee482378730e0af6ccd7f32f9a0fec.pdf","TL;DR":"Convolutional Neural Networks behave as Compositional Nearest Neighbors!","paperhash":"anonymous|do_convolutional_neural_networks_act_as_compositional_nearest_neighbors","_bibtex":"@article{\n  anonymous2018do,\n  title={Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By4Nxm-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1109/Authors"],"keywords":["interpreting convolutional neural networks","nearest neighbors","generative adversarial networks"]}},{"tddate":null,"ddate":null,"tmdate":1510092380664,"tcdate":1509138479769,"number":1109,"cdate":1510092359953,"id":"By4Nxm-CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By4Nxm-CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?","abstract":"We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. We aim to understand and uncover the synthesis/prediction mechanisms of state-of-the-art convolutional neural networks. To this end, we primarily analyze the synthesis process of generative models and the prediction mechanism of discriminative models. The main hypothesis of this work is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function. Our experiments on semantic segmentation and image-to-image translation show qualitative and quantitative evidence supporting this hypothesis.","pdf":"/pdf/b49d8a762bee482378730e0af6ccd7f32f9a0fec.pdf","TL;DR":"Convolutional Neural Networks behave as Compositional Nearest Neighbors!","paperhash":"anonymous|do_convolutional_neural_networks_act_as_compositional_nearest_neighbors","_bibtex":"@article{\n  anonymous2018do,\n  title={Do Convolutional Neural Networks act  as Compositional Nearest Neighbors?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By4Nxm-CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1109/Authors"],"keywords":["interpreting convolutional neural networks","nearest neighbors","generative adversarial networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}