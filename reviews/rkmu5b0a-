{"notes":[{"tddate":null,"ddate":null,"tmdate":1516630064257,"tcdate":1516630064257,"number":11,"cdate":1516630064257,"id":"rynmx_XHf","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"ByiOfTVNz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"More on multimodal priors/ untying the weights","comment":"We gratefully thank the reviewer for the insightful response!\n\nComment 1: If we view this as graphical model where hidden units and the output G(z) are random variables, then the implementation in the paper can be seen as a multimodal prior. However, hidden units and the output G(z) are learned deterministic functions of z, so each G_k(z) implies a different distribution. Therefore, it is still appropriate to see the implementation as a mixture.\n\nComment 4: We will follow your suggestion and add the experiment result to the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1515668082809,"tcdate":1515668082809,"number":9,"cdate":1515668082809,"id":"ByiOfTVNz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"rycjHcGMz","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/AnonReviewer3"],"content":{"title":"relation to multimodal priors/ untying the weights ","comment":"Thanks for your reply.\n\nComment 1: if we take z standard gaussian. Let (W_j,b_j) be the first Layer that is untied. we have W_jz remains gaussian with mean b_j and covariance W_jW_j^{\\top}. We can think of W_jz as a multimodal prior that feed to the shared generator. so the implementation given in the paper is indeed a multimodal prior (degenerate multivariate gaussians in R^{8192}). It is true that this not standard multimodal prior in low dimension, but since the gaussians in R^{8192} are degenerate, they are still supported on a low dimensional subspace. \n\nComment 4: The experiment on untying the weights and the effect of regularization of having smaller bottleneck is interesting, and maybe worth adding to the paper. \n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513431063460,"tcdate":1513431063460,"number":8,"cdate":1513431063460,"id":"r117lsfGG","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Revision","comment":"A revision has been posted with some minor changes. We added the definition of symmetric Kullback-Leibler in Section 5.1, clarified in Table 1's caption that all models in the table are trained in a unsupervised manner, and changed the Figure 6 so that data generated by each generator have a different color."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460606165,"tcdate":1513429486408,"number":7,"cdate":1513429486408,"id":"BJUxcqfzG","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"Sy9Uo3Ygz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response to comment 1","comment":"We gratefully thank the reviewer for the detailed and valuable comments and notes. It took us a while to thoughtfully answer all the comments, and the following are our answers. Due to the limited number of characters per comment, we will answer in several posts:\n\n**** Comment 1: All told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before.\n\n==== Answer: As discussed in related work, there are previous attempts following the multi-generators approach, but they are different from our proposed method. Mix+GAN is totally different as it's based on the min-max theorem and set up mixed strategies for both generators and discriminators. AdaGAN train generators sequentially in a manner similar to AdaBoost, thus having some disadvantages as we discussed. MAD-GAN, at a first glance, looks somewhat similar to our proposed method in terms of model design, but there are some key differences. First, it uses a multi-class discriminator, which outputs D_k(x) as the probability that x generated by G_k for k = 1, 2, … K, and D_{K+1}(x) as the probability that x came from the training data. The gradient signal for each generator k comes from the loss function E_{x~p_G_k}[log (1 - D_{k+1}(x)], which is similar to that in a standard GAN. So, it might be vulnerable to the issue discussed in the Improved GAN paper: “Because the discriminator processes each example independently, there is no coordination between its gradients, and thus no mechanism to tell the outputs of the generator to become more dissimilar to each other.” Our proposed method is distinguished in the use of a classifier to enforce JSD divergence among generators. In addition, the use of a separate classifier makes our method easier to integrate with other single-generator GAN models. There is also extension to our method that do not apply to MAD-GAN. We can use the classifier to cluster the train data, and then further train each generator in a different cluster.\n\nIn terms of performance, our method is far superior than Mix+GAN both in terms of Inception Scores and sample quality. The AdaGAN only presents experiment on MNIST. MAD-GAN mostly performed experiment on narrow-domain datasets, and they did not report any quantitative data on diverse datasets and did not release code as well."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460662734,"tcdate":1513429421526,"number":6,"cdate":1513429421526,"id":"ByShYqMMM","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"Sy9Uo3Ygz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response to Comment 2","comment":"**** Comment 2: The Inception scores are good but it's widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception network's p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score.\n\n==== Answer: We chose Inception Score because at the time we set up our experiment, it was the most widely accepted metrics, so it would be easier for us to compare with many baselines. We did acknowledge that any quantitative metric has its weakness and Inception Score is no exception. Therefore, we included a lot of samples in the paper and looked at them from different angle. It can be noticed that our samples, in terms of quality, are far better than those shown in previously published papers. In addition, we looked at samples generated by each of the generators to check whether they trap Inception Score by memorizing a few examples from each class. We saw no sign of trapping as samples generated by each generator were diverse, especially on diverse datasets such as STL-10 or ImageNet. Therefore, we believe that our method achieved higher Inception Score than single-GAN methods not because it trapped the score, but because each of the generators learned to model a different subset of the training data. As a result, our generated samples are more diverse and at the same time more visually appealing. For the mentioned reasons, we strongly believe the use of Inception Score in our experiment to evaluate our proposed method is valid and plausible.\n\nAs per your suggestion, we looked for GAN baselines using the AIS loglikelihood, but we found no GAN baseline. Regarding Frechet Inception (FID) distance, our model got an FID of 26.7 for Cifar-10. Some baselines we collected from (Heusel et al., 2017) are 37.7 for the original DCGAN, 36.9 for DCGAN using Two Time-scale Update rule (DCGAN + TTUR), 29.3 for WGAN-GP (Gulrajani, 2017) FID of 29.3, and 24.8 for WGAN-GP using TTUR. It is noteworthy that lower FID is better, and that the base model for MGAN is DCGAN. Therefore, in terms of FID, MGAN (26.7) is 28% better than DCGAN (37.7) and DCGAN using TTUR (36.9) and is 9% better than WGAN-GP (29.3), which uses ResNet architecture. This example further shows evidence that our proposed method helps to address the mode collapsing problem."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460689553,"tcdate":1513429348668,"number":5,"cdate":1513429348668,"id":"HkTPt5zMM","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"Sy9Uo3Ygz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response to Comment 3 and Note 6","comment":"**** Comment 3: The mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious.\n\n**** Note 6: The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi). - \"... which further minimizes the objective value\" – it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it.\n\n==== Answer: Our theorem 3 shows that by means of maximizing the divergence among the generated distributions p_G_k ( ⋅ ) , in an ideal case, our proposed MGAN can recover the true data distribution wherein each p_G_k describes a mixture component in this data distribution. Although this theorem gives more insightful understanding of our MGAN as well as its behavior, it requires a strict setting wherein we need to specify the number of mixtures and the mixing proportions a priori. Stating this theorem, we want to emphasize that maximizing the divergence among the generated distributions p_G_k is an efficient way to encourage the generators to produce diverse data that can occupy multiple modes in the real data. Moreover, since GAN requires training a single generator that can cover multiple data modes, it is much harder to train, and always ends up with missing of data modes. In contrast, our MGAN aims at training each generator to cover one or a few data modes, hence being easier to train, and reducing the missed data modes. In addition, due to the fact that each generator can cover some data modes, the number of generators K can be less than the number of data modes as shown in Figure 6 wherein samples generated from 3 or 4 generators can well cover a mixture of 8 Gaussians.\n\nGiven the fact that we are learning from the empirical data distribution, we develop a further theorem to clarify that if we wish to learn the mixing proportion π, the optimal solution is the uniform distribution. The idea is that the optimal generators will learn to partition the empirical data into K disjoint sets of roughly equal size, and each generator approximates a set. In addition, due to the fact that the discrete distribution p_A_k is well-approximated by a continuous generator G_k, the data points in each A_k occupies several groups or clusters. Again, Figure 6 illustrates this point. In Figure 6b, each of the 2 generators (yellow and blue) covers 4 modes. In Figure 6c, one generator (dark green) covers 2 modes and the other two generators (yellow and blue) covers 3 modes. In Figure 6d, each of the four generators (yellow, blue, dark green and dodger blue) cover 2 modes.\n\nFor details of our theorem, please refer to this link: https://app.box.com/s/jjr5kt69uxbr0aikrm0d9cdp2jj95wa0"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460752492,"tcdate":1513428939595,"number":4,"cdate":1513428939595,"id":"BkNAv9fMz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"Sy9Uo3Ygz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response to Comment 4 and 5","comment":"**** Comment 4: Finally, their own qualitative results indicate that they've simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing.\n\n==== Answer: If we look carefully at samples shown in previously published papers (such as Figure 4 of the Improved GAN paper that showed samples generated by semi-supervised GAN trained on CIFAR-10 with feature matching), there are often broken samples that look similar.\n\nSolving mode collapse for a single-generator GAN is out of scope of this paper. As discussed in Introduction, we acknowledged the challenges of training a single generator, and therefore we took the multi-generator approach. We did not seek to improve within-generator diversity but instead improve among-generator diversity. The intuition is that GAN can be pretty good for narrow-domain datasets, so if a group of generators learns to partition the data space, and each of them focuses on a region of the data space, then they together can do a good job too. Finally, the use of a classifier to enforce divergence among generators makes our method relatively easy to integrate with other single-generator models that achieved improvement regarding the mode collapsing problem.\n\n**** Comment 5: Uncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if I'm not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted.\n\n==== Answer: Generative Adversarial Parallelization (GAP) trains many pairs of GAN, periodically swap the discriminators (generators) randomly, and finally selects the best GAN based on GAM evaluation. When we discussed methods in the multi-generator approach, we focused on mixture GAN and as a result neglected GAP. It is fair to discuss GAP as an approach to reduce the mode collapsing problem.\n\nIn AC-GAN, the label information and the noise are concatenated and then fed into the generator network. In our model, generators have different weights in the first layer, so they are mapped to the first hidden layer differently. MGAN and AC-GAN both add the log-likelihood of the correct class to the objective function, but the motivation is very different. Our idea started by asking how to force generators to generate different data, while AC-GAN's motivation is to leverage the label information from training data. So, the two works are totally independent and happens to share some similarities. Our paper focuses on unsupervised GAN, so we did not discuss semi-supervised methods."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460868564,"tcdate":1513428778714,"number":3,"cdate":1513428778714,"id":"HkXND5MMM","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"Sy9Uo3Ygz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response to other Notes","comment":"**** Note 1: The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice.\n\n==== Answer: We do not understand exactly what you meant by ill-posedness. Can please you further clarify this note? \n\n**** Note 2: \"As a result, the optimization order in 1 can be reversed\" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) \"On distinguishability criteria...\".\n\n==== Answer: Here, we simply mentioned the issue discussed in The GAN tutorial (Goodfellow, 2016): “Simultaneous gradient descent does not clearly privilege min max over max min or vice versa. We use it in the hope that it will behave like min max but it often behaves like max min.”\n\n**** Note 3: Section 3: the second last sentence of the third paragraph is vague and doesn't really say anything. Of course parameter sharing leverages common informaNtion. How does this help to train the model effectively?\n\n==== Answer: We discussed in Section 5.2, Model Architectures that our experiment showed that when the parameters are not tied between the classifier and discriminator, the model learns slowly and eventually yields lower performance.\n\n**** Note 4: Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it's not clear what the steps are; it looks like a regular KL divergence to me.\n\n==== Answer: The general definition of JSD is:\nJSD_pi(P_1, P_2, …P_n) = H(sum_{i=1..n} (pi_i * P_i)) -  sum_{i=1..n}(pi_i * H(P_i)\nWhere H(P) is the Shannon entropy for distribution P. Due to limited space, we showed more details of the derivation of L(G_1:K) in Appendix B.\n\n**** Note 5: Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter.\n\n==== Answer: It is the latter. Based on Eq. 2, the classifier is trained to minimize its softmax loss, and based on the optimal solution for the classifier, the generators, by minimizing their objective function, will maximize the JSD divergence.\n\n**** Note 6: The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi). - \"... which further minimizes the objective value\" – it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it.\n\n==== Answer: Please refer to our answer to comment 3.\n\n**** Note 7: There's no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which I'm not sure was quite right in the first place)\n\n==== Answer: We said in the end of Section 3: “In addition, we adopt the non-saturating heuristic proposed in (Goodfellow et al., 2014) to train G_{1:K} by maximizing log D(G_k (z)) instead of minimizing log D(1 - G_k (z)).”\n\n**** Note 8: Section 4: does the DAE introduced in DFM really introduce that much of a computational burden?\n\n==== Answer: It was stated in Section 5.3, paragraph 2 in (Warde-Farley & Bengio, 2017) that: “we achieve a higher Inception score using denoising feature matching, using denoiser with 10 hidden layers of 2,048 rectified linear units each.” That means the DAE adds more than 40 million parameters.\n\n**** Note 9: “Symmetric Kullback Liebler divergence” is not a well-known measure. The standard KL is asymmetric. Please define it. - Figure 2 is illegible in grayscale.\n\n==== Answer: Symmetric Kullback Liebler is the average of the KL and reverse KL divergence. As per your suggestion, we will define it in the paper. Regarding Figure 2, we tried different shapes for the real and generated data points, but due the small size if figure, they are just clusters of red and blue points. We will try different approaches to make the figure more legible.\n\n**** Note 10: Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. It's fine to include it but indicate it as such.\n\n==== Answer: We will take your advice and make it clear that Improve-GAN score in Table 1 is for the unsupervised version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460477515,"tcdate":1513428386293,"number":2,"cdate":1513428386293,"id":"rycjHcGMz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"rJAgO6KlM","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response","comment":"We gratefully thank the reviewer for the thoughtful and insightful comments. It took us a while to answer all the reviews as well as to run additional experiments as suggested. Our answers are the following:\n\n**** Comment 1: when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z!\n\n==== Answer: The first hidden layer actually has 4x4x512 = 8,192 dimensions (for Cifar-10). So, untying weights in the first layer effectively maps the noise prior to a different distribution in R^8192 (with a different mean and covariances) for each generator. So, our proposed method is different from a GAN with a multimodal prior.\n\n**** Comment 2: taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps?\n\n==== Answer: We tried to learn the mean and covariance of the prior for each generator, but the result was not much different from the standard GAN.\n\n**** Comment 3: in the tied weight case, in the synthetic example, can you show what each \"generator\" of the mixture learn? are they really learning modes of the data?\n\n==== Answer: Following your suggestion, we revised figure 6 so that data points generated by different generators have different colors. As you can see, generators learned different modes of the data.\n\n**** Comment 4: the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior. would be good to have some experiments and see how much we loose for example in term of inception scores, between tied and untied weights of generators.\n\n==== Answer: In theory, tying weights will add constraints to the optimization of the objective function for G_{1:K} in Eq. 4. For example, if we tie weights in all layers and generators differ only in the mean and variance of the noise prior, the result was similar to the standard GAN like we reported in comment 2. Untying weights in the first layer, however, achieved good results like we discussed in the paper. Finally, as per your request, we conducted experiments without parameter sharing. Surprisingly, when we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer, the model failed to learn. The model even failed to learn when we set beta to 0. When we reduced the number of feature maps in the penultimate layer for each generator to 32, they managed to learn and achieved an Inception Score of 7.42. So, we hypothesize that added benefit of our parameter sharing scheme is to balance the capacity of generators and that of the discriminator/classifier."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513460527232,"tcdate":1513428218390,"number":1,"cdate":1513428218390,"id":"HJGWrcGMG","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Comment","forum":"rkmu5b0a-","replyto":"Hkib3t2lz","signatures":["ICLR.cc/2018/Conference/Paper84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper84/Authors"],"content":{"title":"Response","comment":"We gratefully thank reviewers for the insightful comments. We have endeavored to address as much as we can, including running additional experiments as suggested, thus it has taken us a while.\n\n**** Comment 1: Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various.\n\n==== Answer: We agree that we don’t have any principle to choose the correct number of generators for our proposed model, as choosing the correct number of clusters for Gaussian mixture model (GMM) and other clustering methods. If we wish to specify an appropriate number of generators automatically, we would need to go for a Bayesian nonparametric extension, similarly to going from GMM to Dirichlet Process Mixtures. Within the scope of this work, our motivation is that GAN works pretty well on narrow-domain dataset but poorly on diverse dataset; So, if we can efficiently train many generators while enforcing divergence among them, they can work well too. In general, more generators tend to work better.\n\n**** Comment 2: Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing.\n\n==== Answer: We did experiment without parameters sharing among generators and found an interesting behavior. When we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer, the model failed to learn. The model even failed to learn when we set beta to 0. When we reduced the number of feature maps in the penultimate layer for each generator to 32, they managed to learn and achieved an Inception Score of 7.42. So, we hypothesize that added benefit of parameter sharing is to help balance the capacity of generators and that of the discriminator/classifier.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1515642519319,"tcdate":1511984131444,"number":3,"cdate":1511984131444,"id":"Hkib3t2lz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Review","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen-Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state-of-art results.\n\nThe paper is easy to follow.\n\nComment:\n\n1. Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various.\n2. Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1515642519353,"tcdate":1511802869885,"number":2,"cdate":1511802869885,"id":"rJAgO6KlM","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Review","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review for MGAN","rating":"7: Good paper, accept","review":"Summary:\n\nThe paper proposes a mixture of  generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added.\n\n\nQuality/clarity:\n\nThe paper is well written and easy to follow.\n\nclarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text.\n\nOriginality:\n\n Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator.\n\nGeneral review:\n\n- when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z!\n\n- Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps?  \n \n- in the tied weight case, in the synthetic example, can you show what each \"generator\" of the mixture learn? are they really learning modes of the data? \n\n- the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior.  would be good to have some experiments and  see how much we loose for example in term of inception scores, between tied and untied weights of generators.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1515642519389,"tcdate":1511799634517,"number":1,"cdate":1511799634517,"id":"Sy9Uo3Ygz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Review","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Attempt at solving mode collapse just moves the problem.","rating":"4: Ok but not good enough - rejection","review":"The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components.\n\nAll told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before. The Inception scores are good but it's widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception network's p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score.\n\nThe mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious.\n\nFinally, their own qualitative results indicate that they've simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing.\n\nUncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if I'm not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted.\n\nOther notes:\n- The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice.\n- \"As a result, the optimization order in 1 can be reversed\" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) \"On distinguishability criteria...\".\n- Section 3: the second last sentence of the third paragraph is vague and doesn't really say anything. Of course parameter sharing leverages common information. How does this help to train the model effectively?\n- Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it's not clear what the steps are; it looks like a regular KL divergence to me.\n- Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter.\n- The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi).\n- \"... which further minimizes the objective value\" -- it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it.\n- There's no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which I'm not sure was quite right in the first place)\n- Section 4: does the DAE introduced in DFM really introduce that much of a computational burden? \n- \"Symmetric Kullback Liebler divergence\" is not a well-known measure. The standard KL is asymmetric. Please define it.\n- Figure 2 is illegible in grayscale.\n- Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. It's fine to include it but indicate it as such.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1513427904662,"tcdate":1508936299476,"number":84,"cdate":1509739493523,"id":"rkmu5b0a-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/7f47f1fe84480eb1a9c02b06d20b05832e42ba80.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]},"nonreaders":[],"replyCount":13,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}