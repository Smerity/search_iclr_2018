{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222737905,"tcdate":1511851958851,"number":3,"cdate":1511851958851,"id":"By16DK9xM","invitation":"ICLR.cc/2018/Conference/-/Paper736/Official_Review","forum":"H1mCp-ZRZ","replyto":"H1mCp-ZRZ","signatures":["ICLR.cc/2018/Conference/Paper736/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good empirical study of modifications to action-dependent baselines","rating":"7: Good paper, accept","review":"The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein’s identity and control functionals. The method relates closely to prior work on action-dependent baselines, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance. \n\nA criticism of the paper is that it does not require Stein’s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate. The derivation through Stein’s identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick.\n\nThe empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning. However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:\n\n-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action. A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.\n\n-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice. Similar comparison should be done with off-policy fitting in Q-Prop.\n\nI wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations. \n\nThe paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop. As discussed above, it is encouraged to elaborate other potential causes that led to performance differences. The experimental results are presented well for a range of Mujoco tasks. \n\nPros:\n\n-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time\n\n-Good empirical evaluation\n\nCons:\n\n-The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein’s identity etc. and does not inherit novel insights due to this derivation.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]}},{"tddate":null,"ddate":null,"tmdate":1512222737947,"tcdate":1511725854564,"number":2,"cdate":1511725854564,"id":"ryP7s5Oxz","invitation":"ICLR.cc/2018/Conference/-/Paper736/Official_Review","forum":"H1mCp-ZRZ","replyto":"H1mCp-ZRZ","signatures":["ICLR.cc/2018/Conference/Paper736/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice Application of Stein's identity","rating":"7: Good paper, accept","review":"This paper proposed a class of control variate methods based on Stein's identity. Stein's identity has been widely used in classical statistics and recently in statistical machine learning literature. Nevertheless, applying Stein's identity to estimating policy gradient is a novel approach in reinforcement learning community. To me, this approach is the right way of constructing control variates for estimating policy gradient. The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies. The experimental results also look promising.\n\nIt would be nice to include some theoretical analyses like under what conditions, the proposed method can achieve smaller sample complexity than existing works.    \n\nOverall this is a strong paper and I recommend to accept.\n \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]}},{"tddate":null,"ddate":null,"tmdate":1512222737985,"tcdate":1511696901564,"number":1,"cdate":1511696901564,"id":"SkRWcmOgz","invitation":"ICLR.cc/2018/Conference/-/Paper736/Official_Review","forum":"H1mCp-ZRZ","replyto":"H1mCp-ZRZ","signatures":["ICLR.cc/2018/Conference/Paper736/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice work; some suggestions in order to improve the paper are provided.","rating":"7: Good paper, accept","review":"In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework. The authors also introduce a specific control variate technique based on the so-called Stein’s identity. The paper is interesting and well-written.\n\nI have some question and some consideration that can be useful for improving the appealing of the paper.\n\n- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq. (1), as also suggested in this work. Are there other alternatives in the literature? Please, please discuss and cite some papers if required.  \n\n- I suggest to divide Section 3.1 in two subsections. The first one introducing Stein’s identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title “Stein Control Variate”.\n\n-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as\n\nM. U. Gutmann and J. Corander, “Bayesian optimization for likelihood-free inference of simulator-based statistical mod- els,” Journal of Machine Learning Research, vol. 16, pp. 4256– 4302, 2015. \n\nG. da Silva Ferreira and D. Gamerman, “Optimal design in geostatistics under preferential sampling,” Bayesian Analysis, vol. 10, no. 3, pp. 711–735, 2015. \n\nL. Martino, J. Vicent, G. Camps-Valls, \"Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models\", IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017.\n\n-  Please also discuss the dependence of your algorithm with respect to the starting baseline function \\phi_0.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]}},{"tddate":null,"ddate":null,"tmdate":1510284481227,"tcdate":1510284481227,"number":2,"cdate":1510284481227,"id":"H1Fan9z1G","invitation":"ICLR.cc/2018/Conference/-/Paper736/Public_Comment","forum":"H1mCp-ZRZ","replyto":"S17GnS-1G","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Good to have a discussion","comment":"So, just to emphasise the similarity, and note that since these are parallel submissions by no means I intend to diminish your contributions, just I think the connection is interesting. Equation (8) from your paper is exactly equivalent to Equation (6) (LAX estimator) of the paper, specifically by setting:\npi(a|theta) = p(b|theta)\nQ(s,a) = f(b)\nphi(s,a) = c_phi(b)\nf(s, eps|theta) = T(eps, theta)\nSimilar to your remark in Equation (13) the other authors just below their Equation (6) mention that taking the \"control\" function equal the original one (if that is differentiable) we recover the path gradient. Additionally, they also suggest optimizing the \"control\" function by minimizing the variance.\n\nRegarding, equation (18) and the Gaussian policy indeed it is an interesting observation that we can apply this a second time and get a potentially lower variance estimator. This in fact I think is a more general result that if the derivatives depend on epsilon you can reapply the procedure, but don't cite me on that. Potentially, investigating/generalizing that might be interesting. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]}},{"tddate":null,"ddate":null,"tmdate":1510198282733,"tcdate":1510198282733,"number":1,"cdate":1510198282733,"id":"S17GnS-1G","invitation":"ICLR.cc/2018/Conference/-/Paper736/Official_Comment","forum":"H1mCp-ZRZ","replyto":"H1siIPk1M","signatures":["ICLR.cc/2018/Conference/Paper736/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper736/Authors"],"content":{"title":"Thank you for comment","comment":"Thank you for pointing us to this independent ICLR submission. It is highly relevant. Their estimator in the RL setting (their Eq 11) is mathematically equivalent to ours. However, our paper is derived from a different perspective and give more comprehensive results on reinforcement learning specifically.  \n\n1) Our work focuses on RL. By combining with PPO and TRPO, we obtain significant improvement on challenging RL tasks such as Humanoid-v1 and HumanoidStandup-v1. We also proposed and tested different architectures and optimization methods for the control variates, providing guidance on what may work best in practice. We explicitly establish the connection with  Q-prop(Gu et al., 2016b), which can be viewed as our method with linear control variates. \n\n2)  Our work was motivated by Stein’s identity and control functionals (Oates et al. 2017), and hence develops a connection between Stein’s identity and reparameterization trick which can be itself useful. For example, for Gaussian policy, we further derive a different update rule with lower variance by utilizing Stein’s Identity twice."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]}},{"tddate":null,"ddate":null,"tmdate":1510074019507,"tcdate":1510074019507,"number":1,"cdate":1510074019507,"id":"H1siIPk1M","invitation":"ICLR.cc/2018/Conference/-/Paper736/Public_Comment","forum":"H1mCp-ZRZ","replyto":"H1mCp-ZRZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comment","comment":"I'm wondering if in fact what is suggested as Stein Control Varite is not indeed similar (if not the same) with the technique proposed here: https://arxiv.org/abs/1711.00123 ? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]}},{"tddate":null,"ddate":null,"tmdate":1509739132932,"tcdate":1509133770877,"number":736,"cdate":1509739130266,"id":"H1mCp-ZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1mCp-ZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE","abstract":"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein’s identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches.\n","pdf":"/pdf/876b006f2c8f0d2967b0e520f9184a87ebd07f68.pdf","paperhash":"anonymous|sampleefficient_policy_optimization_with_stein_control_variate","_bibtex":"@article{\n  anonymous2018sample-efficient,\n  title={SAMPLE-EFFICIENT POLICY OPTIMIZATION WITH STEIN CONTROL VARIATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1mCp-ZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper736/Authors"],"keywords":["reinforcement learning","control variates","sample efficiency","variance reduction"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}