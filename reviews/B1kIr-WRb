{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222717942,"tcdate":1511754534951,"number":3,"cdate":1511754534951,"id":"rkyEiZKef","invitation":"ICLR.cc/2018/Conference/-/Paper673/Official_Review","forum":"B1kIr-WRb","replyto":"B1kIr-WRb","signatures":["ICLR.cc/2018/Conference/Paper673/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper approaches the important problem of word embeddings via factorization of a pointwise mutual information tensor. The idea is not novel and the proposed algorithm lacks theoretical guarantees, despite this is one of the main motivations. Although extensive experimental evaluation with good results is provided, it is hard to assess due to the lack of details about the experiments and missing comparisons with other important algorithms.","rating":"5: Marginally below acceptance threshold","review":"The paper presents the word embedding technique which consists of: (a) construction of a positive (i.e. with truncated negative values) pointwise mutual information order-3 tensor for triples of words in a sentence and (b) symmetric tensor CP factorization of this tensor. The authors propose the CP-S (stands for symmetric CP decomposition) approach which tackles such factorization in a \"batch\" manner by considering small random subsets of the original tensor. They also consider the JCP-S approach, where the ALS (alternating least squares) objective is represented as the joint objective of the matrix and order-3 tensor ALS objectives. The approach is evaluated experimentally on several tasks such as outlier detection, supervised analogy recovery, and sentiment analysis tasks.\n\nCLARITY: The paper is very well written and is easy to follow. However, some implementation details are missing, which makes it difficult to assess the quality of the experimental results.\n\nQUALITY: I understand that the main emphasis of this work is on developing faster computational algorithms, which would handle large scale problems, for factorizing this tensor. However, I have several concerns about the algorithms proposed in this paper:\n\n  - First of all, I do not see why using small random subsets of the original tensor would give a desirable factorization. Indeed, a CP decomposition of a tensor can not be reconstructed from CP decompositions of its subtensors. Note that there is a difference between batch methods in stochastic optimization where batches are composed of a subset of observations (which then leads to an approximation of desirable quantities, e.g. the gradient, in expectation) and the current approach where subtensors are considered as batches. I would expect some further elaboration of this question in the paper. Although similar methods appeared in the tensor literature before, I don't see any theoretical ground for their correctness.\n\n  - Second, there is a significant difference between the symmetric CP tensor decomposition and the non-negative symmetric CP tensor decomposition. In particular, the latter problem is well posed and has good properties (see, e.g., Lim, Comon. Nonengative approximations of nonnegative tensors (2009)). However, this is not the case for the former (see, e.g., Comon et al., 2008 as cited in this paper). Therefore, (a) computing the symmetric and not non-negative symmetric decomposition does not give any good theoretical guarantees (while achieving such guarantees seems to be one of the motivations of this paper) and (b) although the tensor is non-negative, its symmetric factorization is not guaranteed to be non-negative and further elaboration of this issue seem to be important to me.\n\n  - Third, the authors claim that one of their goals is an experimental exploration of tensor factorization approaches with provable guarantees applied to the word embedding problem. This is an important question that has not been addressed in the literature and is clearly a pro of the paper. However, it seems to me that this goal is not fully implemented. Indeed, (a) I mentioned in the previous paragraph the issues with the symmetric CP decomposition and (b) although the paper is motivated by the recent algorithm proposed by Sharan&Valiant (2017), the algorithms proposed in this paper are not based on this or other known algorithms with theoretical guarantees. This is therefore confusing and I would be interested in the author's point of view to this issue.\n\n  - Further, the proposed joint approach, where the second and third order information are combined requires further analysis. Indeed, in the current formulation the objective is completely dominated by the order-3 tensor factor, because it contributes O(d^3) terms to the objective vs O(d^2) terms contributed by the matrix part. It would be interesting to see further elaboration of the pros and cons of such problem formulation.\n\n  - Minor comment. In the shifted PMI section, the authors mention the parameter alpha and set specific values of this parameter based on experiments. However, I don't think that enough information is provided, because, given the author's approach, the value of this parameter most probably depends on other parameters, such as the bach size.\n\n  - Finally, although the empirical evaluation is quite extensive and outperforms the state-of the art, I think it would be important to compare the proposed algorithm to other tensor factorization approaches mentioned above. \n\nORIGINALITY: The idea of using a pointwise mutual information tensor for word embeddings is not new, but the authors fairly cite all the relevant literature. My understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evaluation. However, such batch approaches for tensor factorization are not new and I am quite skeptical about their correctness (see above). The experimental evaluation presents indeed interesting results. However, I think it would also be important to compare to other tensor factorization approaches. I would also be quite interested to see the performance of the proposed algorithm for different values of parameters (such as the butch size).\n\nSIGNIFICANCE: I think the paper addresses very interesting problem and significant amount of work is done towards the evaluation, but there are some further important questions that should be answered before the paper can be published.  To summarize, the following are the pros of the paper:\n\n  - clarity and good presentation;\n  - good overview of the related literature;\n  - extensive experimental comparison and good experimental results.\n\nWhile the following are the cons:\n\n  - the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees;\n  - lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm;\n  - lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem);\n  - the novelty of the approach is somewhat limited, although the idea of the extensive experimental comparison is good.\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION","abstract":"Many state-of-the-art word embedding techniques involve factorization of a cooccurrence\nbased matrix. We aim to extend this approach by studying word embedding\ntechniques that involve factorization of co-occurrence based tensors (N-\nway arrays). We present two new word embedding techniques based on tensor\nfactorization and show that they outperform common methods when used for several\nsemantic NLP tasks when trained with the same data. To train one of the\nembeddings, we present a new joint tensor factorization problem and an approach\nfor solving it. Furthermore, we modify the performance metrics for the Outlier\nDetection Camacho-Collados & Navigli (2016) task to measure the quality\nof higher-order relationships that a word embedding captures. Our tensor-based\nmethods significantly outperform existing methods at this task when using our\nnew metric. Finally, we demonstrate that vectors in our embeddings can be composed\nmultiplicatively to create different vector representations for each meaning\nof a polysemous word in a way that cannot be done with other common embeddings.\nWe show that this property stems from the higher order information that\nthe vectors contain, and thus is unique to our tensor based embeddings.","pdf":"/pdf/3fd209e14824345a3f14a7cd9375b8ee378b7789.pdf","paperhash":"anonymous|learning_semantic_word_respresentations_via_tensor_factorization","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1kIr-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper673/Authors"],"keywords":["Word Embeddings","Tensor Factorization","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222717985,"tcdate":1511736476027,"number":2,"cdate":1511736476027,"id":"Hy4sVp_lf","invitation":"ICLR.cc/2018/Conference/-/Paper673/Official_Review","forum":"B1kIr-WRb","replyto":"B1kIr-WRb","signatures":["ICLR.cc/2018/Conference/Paper673/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A tensor extension of the familiar PPMI factorization for learning word embeddings","rating":"5: Marginally below acceptance threshold","review":"The paper proposes to extend the usual PPMI matrix factorization (Levy and Goldberg, 2014) to a (3rd-order) PPMI tensor factorization. The paper chooses symmetric CP decomposition so that word representations are tied across all three views. The MSE objective (optionally interpolated with a 2nd-order tensor) is optimized incrementally by SGD. \n\nThe paper's most clear contribution is the observation that the objective results in multiplicative compositionality of vectors, which indeed does not seem to hold in CBOW. \n\nWhile the paper reports superior performance, the empirical claims are not well substantiated. It is *not* true that given CBOW, it's not important to compare with SGNS and GloVe. In fact, in certain cases such as unsupervised word analogy, SGNS is clearly and vastly superior to other techniques (Stratos et al., 2015). The word similarity scores are also generally low: it's easy to achieve >0.76 on MEN using the plain PPMI matrix factorization on Wikipedia. So it's hard to tell if it's real improvement. \n\nQuality: Borderline. The proposed approach is simple and has an appealing compositional feature, but the work is not adequately validated and the novelty is somewhat limited. \n\nClarity: Clear.\n\nOriginality: Low-rank tensors have been used to derive features in many prior works in NLP (e.g., Lei et al., 2014). The paper's particular application to learning word embeddings (PPMI factorization), however, is new although perhaps not particularly original. The observation on multiplicative compositionality is the main strength of the paper.\n\nSignificance: Moderate. For those interested in word embeddings, this work suggests an alternative training technique, but it has some issues (described above).  ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION","abstract":"Many state-of-the-art word embedding techniques involve factorization of a cooccurrence\nbased matrix. We aim to extend this approach by studying word embedding\ntechniques that involve factorization of co-occurrence based tensors (N-\nway arrays). We present two new word embedding techniques based on tensor\nfactorization and show that they outperform common methods when used for several\nsemantic NLP tasks when trained with the same data. To train one of the\nembeddings, we present a new joint tensor factorization problem and an approach\nfor solving it. Furthermore, we modify the performance metrics for the Outlier\nDetection Camacho-Collados & Navigli (2016) task to measure the quality\nof higher-order relationships that a word embedding captures. Our tensor-based\nmethods significantly outperform existing methods at this task when using our\nnew metric. Finally, we demonstrate that vectors in our embeddings can be composed\nmultiplicatively to create different vector representations for each meaning\nof a polysemous word in a way that cannot be done with other common embeddings.\nWe show that this property stems from the higher order information that\nthe vectors contain, and thus is unique to our tensor based embeddings.","pdf":"/pdf/3fd209e14824345a3f14a7cd9375b8ee378b7789.pdf","paperhash":"anonymous|learning_semantic_word_respresentations_via_tensor_factorization","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1kIr-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper673/Authors"],"keywords":["Word Embeddings","Tensor Factorization","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1512222718028,"tcdate":1511300821560,"number":1,"cdate":1511300821560,"id":"r1aR0zMgf","invitation":"ICLR.cc/2018/Conference/-/Paper673/Official_Review","forum":"B1kIr-WRb","replyto":"B1kIr-WRb","signatures":["ICLR.cc/2018/Conference/Paper673/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good start, but more analysis and experiments needed.","rating":"5: Marginally below acceptance threshold","review":"In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S. Additionally, they propose an extension JCP-S, for n-order tensor decompositions. This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods. The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis. Additionally, it is evaluated over the MEN and Mturk datasets.\n\n\nFor the JCP-S model, the loss function is unclear to me. L is defined for 3rd order tensors only;  how is the extended to n > 3? Intuitively it seems that L is redefined, and for, say, n = 4, the model is M(i,j,k,n) = \\sum_1^R u_ir u_jr u_kr u_nr. However, the statement \"since we are using at most third order tensors in this work\" I am further confused. Is it just that JCP-S also incorporates 2nd order embeddings? I believe this requires clarification in the manuscript itself.\n\nFor the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:\n\nPengfei Liu, Xipeng Qiuâˆ— and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015\n\nJingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo. Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014\n\nMo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors\n\nto name a few via quick googling.\n\nAdditionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated. There are many such methods, see for example \n\nJiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding?\n\nand citations within, plus quick googling for more recent works.\n\nI am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3-way outlier detection). \n\nObserve also that in table 2, NNSE gets the highest performance in both MEN and MTurk. Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well-suited for matrix factorization. So, statements like \"as we can see, our embeddings very clearly outperform the random embedding at this task\" is  an unnecessary inflation of a result that 1) is not good and 2) is reasonable to not be good. \n\nOverall, I think for a more sincere evaluation, the authors need to better pick tasks that clearly exploit 3-way information and compare against other methods proposed to do the same.\n\nThe multiplicative relation analysis is interesting, but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model. \n\nIn conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION","abstract":"Many state-of-the-art word embedding techniques involve factorization of a cooccurrence\nbased matrix. We aim to extend this approach by studying word embedding\ntechniques that involve factorization of co-occurrence based tensors (N-\nway arrays). We present two new word embedding techniques based on tensor\nfactorization and show that they outperform common methods when used for several\nsemantic NLP tasks when trained with the same data. To train one of the\nembeddings, we present a new joint tensor factorization problem and an approach\nfor solving it. Furthermore, we modify the performance metrics for the Outlier\nDetection Camacho-Collados & Navigli (2016) task to measure the quality\nof higher-order relationships that a word embedding captures. Our tensor-based\nmethods significantly outperform existing methods at this task when using our\nnew metric. Finally, we demonstrate that vectors in our embeddings can be composed\nmultiplicatively to create different vector representations for each meaning\nof a polysemous word in a way that cannot be done with other common embeddings.\nWe show that this property stems from the higher order information that\nthe vectors contain, and thus is unique to our tensor based embeddings.","pdf":"/pdf/3fd209e14824345a3f14a7cd9375b8ee378b7789.pdf","paperhash":"anonymous|learning_semantic_word_respresentations_via_tensor_factorization","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1kIr-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper673/Authors"],"keywords":["Word Embeddings","Tensor Factorization","Natural Language Processing"]}},{"tddate":null,"ddate":null,"tmdate":1509739168148,"tcdate":1509131591546,"number":673,"cdate":1509739165484,"id":"B1kIr-WRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1kIr-WRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION","abstract":"Many state-of-the-art word embedding techniques involve factorization of a cooccurrence\nbased matrix. We aim to extend this approach by studying word embedding\ntechniques that involve factorization of co-occurrence based tensors (N-\nway arrays). We present two new word embedding techniques based on tensor\nfactorization and show that they outperform common methods when used for several\nsemantic NLP tasks when trained with the same data. To train one of the\nembeddings, we present a new joint tensor factorization problem and an approach\nfor solving it. Furthermore, we modify the performance metrics for the Outlier\nDetection Camacho-Collados & Navigli (2016) task to measure the quality\nof higher-order relationships that a word embedding captures. Our tensor-based\nmethods significantly outperform existing methods at this task when using our\nnew metric. Finally, we demonstrate that vectors in our embeddings can be composed\nmultiplicatively to create different vector representations for each meaning\nof a polysemous word in a way that cannot be done with other common embeddings.\nWe show that this property stems from the higher order information that\nthe vectors contain, and thus is unique to our tensor based embeddings.","pdf":"/pdf/3fd209e14824345a3f14a7cd9375b8ee378b7789.pdf","paperhash":"anonymous|learning_semantic_word_respresentations_via_tensor_factorization","_bibtex":"@article{\n  anonymous2018learning,\n  title={LEARNING SEMANTIC WORD RESPRESENTATIONS VIA TENSOR FACTORIZATION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1kIr-WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper673/Authors"],"keywords":["Word Embeddings","Tensor Factorization","Natural Language Processing"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}