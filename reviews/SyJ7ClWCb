{"notes":[{"tddate":null,"ddate":null,"tmdate":1515792162645,"tcdate":1515792162645,"number":12,"cdate":1515792162645,"id":"HksQPiUVM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"B1gREqS4f","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thank you for the clarification","comment":"I believe that clarifies the statement made. The claim being made is not that the adversary does not have any knowledge of the defense, but just that the adversary doesn't know the exact choices of randomness that the defender makes."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515721928170,"tcdate":1515721928170,"number":10,"cdate":1515721928170,"id":"B1gREqS4f","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"HyulgtSVz","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Unreasonable threat model assumptions","comment":"We agree that security through obscurity is inherently weak and ineffective. However, the defenses that we evaluated, in particular TV minimization and image quilting, are stochastic in nature. This allows the defense to randomize its transformation, and hence prevents the adversary from knowing the exact transformation being applied even if the defense strategy is known. The image quilting defense enjoys the additional property that the patch database used to construct the quilted images can be considered as the secret key, which obeys Kerckhoff's principle: even if the defense strategy is known, the adversary cannot apply the same quilting transformation if he does not have access to the patch database."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515716592204,"tcdate":1515716592204,"number":11,"cdate":1515716592204,"id":"HyulgtSVz","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Unreasonable threat model assumptions","comment":"It appears that in the revised manuscript, the authors completely change their threat model. In the new draft of the paper, the authors add the sentence \"our defenses assume that part of the defense strategy (viz., the input transformation) is unknown to the adversary\".\n\nThis is a completely unreasonable assumption. Any algorithm which hopes to be secure must allow the adversary to, at the very least, understand what the defense is that's being used. Consider a world where the defense here is implemented in practice: any attacker in the world could just go look up the paper, read the description of the algorithm, and know how it works.\n\nThe authors mention Kerckhoffs's Principle in a comment below, and that's exactly what is being violated here. It is perfectly reasonable to assume the adversary does not have the model parameters or training data. But declaring the entire defense process a secret is exactly what Kerckhoffs's Principle says is *not* okay.\n\nThe current threat model of this paper appears to be arguing for security through obscurity, and that is not a reasonable defense."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515621026384,"tcdate":1515621026384,"number":10,"cdate":1515621026384,"id":"rk5sqWNNG","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"HkKhk67Nf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks","comment":"Wow, that was a quick response. Thanks!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515601841084,"tcdate":1515601841084,"number":9,"cdate":1515601841084,"id":"HkKhk67Nf","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"rkqFThmEM","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"The overlap size is 2 pixels","comment":"Apologies for leaving out this detail! We used an overlap of 2 pixels between the 5x5 patches. We will update the paper with this information."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515601281674,"tcdate":1515601281674,"number":9,"cdate":1515601281674,"id":"rkqFThmEM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Parameter settings used in experiments","comment":"I'm trying to reproduce the experimental results shown in this paper. Thank you for giving a detailed description of parameter settings in Section 5.1 -- this makes it a lot more straightforward.\n\nOne thing that I noticed was missing was the tile overlap for the image quilting defense. You mention that you use 5x5 tiles, with a database of 1M tiles, and randomly select from the 10 nearest neighbors. In Section 4.3 and Section 6, you mention using minimum graph cuts in boundary regions, but this only applies when you have overlapping tiles. What was the overlap parameter used in the experiments?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1513969595053,"tcdate":1513969595053,"number":8,"cdate":1513969595053,"id":"rJX6P09fz","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"S1wXIrVgM","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Valuable idea but immature contribution.","comment":"Thank you for your insightful comments on our work, which have been very helpful in improving the paper!\n\n* The black-box versus white-box terminology is not appropriate...\n\nAs several public comments have pointed out, the white-box terminology can be misleading. Some of our experiments are performed in a \"gray-box\" setting in which the adversary has access to the network parameters, but not to the quilting database that acts as a kind of \"secret key\". We believe that this gray-box setting is of practical interest because the quilting process is stochastic and because the adversary never directly observes the quilted images themselves: this makes it very difficult for the adversary to exactly reproduce the quilted images that the defender produces. Per your suggestion, we have clarified the learning-setting terminology in the revised version of the paper.\n\n* Using block diagrams would definitely help in presenting the training/testing and attack/defense schemes investigated in Figure 3, 4, and 5.\n\nPer your suggestion, we have added block diagrams clarifying the workflow of our attack/defense schemes in the revised version of the paper.\n\n* The paper does not discuss the impact of the defense strategy on the classification performance in absence of adversity.\n\nThe first row of Tables 1 and 2 present the accuracy of various defenses on non-adversarial images (\"no attack\"). In Figures 3, 4 and 5, the y-axis value corresponding to normalized L2-dissimilarity of 0 corresponds to the accuracy on non-adversarial images. We have emphasized this point in the table and figure captions in the revised version of the paper.\n\n* The paper lacks of positioning with respect to recent related works, e.g. 'Adversary Resistant Deep Neural Networks with an Application to Malware Detection' in KDD 2017, or 'Building Adversary-Resistant Deep Neural Networks without Security through Obscurity' at https://arxiv.org/abs/1612.01401.\n\nThank you for pointing out these references, which we were unaware of at the time of submission. Both approaches are similar to our defenses in the sense that they focus on non-differentiable, stochastic transformations. Having said that, there are also substantial differences between our study and those related works. The first paper relies on LLE to represent data points as a linear combination of nearest neighbors: this approach may certainly be suitable for certain kinds of data, but is unlikely to work very well in extremely high-dimensional spaces such as the ImageNet pixel space. The second paper's approach of randomly removing blocks of pixels is related to our image-cropping baseline defense, which is one of our baselines. We have included positioning with respect to these works in the revised version of the paper.\n\n* In a white-box scenario, the adversary knows about the transformation and the classification model. Hence, an effective and realistic attack should exploit this knowledge...\n\nIn white-box settings, it may, indeed, be possible to devise attacks that are tailored towards a particular defense. In our work, we have tried to make the development of such attacks non-trivial by making our defenses non-differentiable and stochastic. Having said that, it may certainly be possible to devise attack strategies that are successful nevertheless (such as the strategy sketched in our response to AnonReviewer3). We leave the investigation of attacks that are tailored to our defenses to future work.\n\n* If I understand correctly, the classification model considered in Figure 3 has been trained on original images, while the one in Figure 4 has been trained on transformed images. However, in absence of attack, they both achieve 76% accuracy. Is it correct? Does it mean that the transformation does not affect the classification accuracy at all?\n\nThe 76% accuracy is obtained by a convolutional network that is trained and tested on images on which no defense (i.e., input transformation) is applied. The \"no defense\" baseline is this exactly the same in both Figures 3 and 4. For defenses such as TV minimization and quilting, the accuracy on non-adversarial images is lower (both in Figure 3 and 4), which shows that the transformations, indeed, do negatively impact classification accuracy on non-adversarial images."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1513969450752,"tcdate":1513969450752,"number":7,"cdate":1513969450752,"id":"r1XVPRqzM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"SJzYnEqef","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Review","comment":"Thank you for your insightful comments, and positive evaluation of work! \n\nRegarding model-based approaches for attacking our quilting defense: we agree this may the most viable option for attacking our defense. As you suggest, it may be possible for the adversary to construct its own patch database, and use it to construct quilted images that may be sufficiently similar to the quilted image created using our \"secret database\". The remaining issue for the adversary is then to backpropagate gradients through the quilting transformation: the adversary may be able to do this by training a pixel-to-pixel network that learns to produce the quilted image given an original image, and using this network to approximate gradients. We intend to investigate such attack approaches in future work. We have updated our paragraph describing future work to reflect this.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1513969388100,"tcdate":1513969388100,"number":6,"cdate":1513969388100,"id":"SyNlDC9GM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"Sk47YIYlM","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Well argumented, solid work","comment":"Thanks for your positive evaluation of our paper! Per your suggestion, we have updated the bibliography entries to make them uniform."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1511935289107,"tcdate":1511935289107,"number":8,"cdate":1511935289107,"id":"r1-ST6olG","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SJp9ze61G","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: Re: The \"huge drop\"","comment":"This work considers ImageNet which is a far more challenging dataset. After reading the other submission (which is quite interesting too), I humbly think that this work is the most sensible ICLR submission on defending against adversarial attacks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515642483464,"tcdate":1511832698264,"number":3,"cdate":1511832698264,"id":"SJzYnEqef","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Review","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference/Paper636/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":" The paper investigates using input transformation techniques as a defence against adversarial examples. The authors evaluate a number of simple defences that are based on input transformations such TV minimization and image quilting and compare it against previously proposed ideas of JPEG compression and decompression and random crops.  The authors have evaluated their defences against four main kinds of adversarial attacks.\n\nThe main takeaways of the paper are to incorporate transformations that are non-differentiable and randomised. Both TV minimisation and image quilting have that property and show good performance in withstanding adversarial attacks in various settings. \n\nOne argument that I am not sure would be applicable perhaps and could be used by adversarial attacks is as follows: If the defence uses image quilting for instance and obtains an image $P$ that approximates the original observation $X$, it could be possible to use a model based approach that obtains an observation $Q$ that is close to $P$ which can be attacked using adversarial attacks. Would this observation then be vulnerable to such attacks? This could perhaps be explored in future.\n\nThe paper provides useful contributions in forming model agnostic defences that could be further investigated. The authors show that the simple input transformations advocated work against the major kind of attacks. The input transformations of TV minimization and image quilting share varying characteristics in terms of being sensitive to various kinds of attacks and therefore can be combined. The evaluation is carried out on ImageNet dataset with large number of examples.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515642483507,"tcdate":1511774491624,"number":2,"cdate":1511774491624,"id":"Sk47YIYlM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Review","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference/Paper636/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well argumented, solid work","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary: This works proposes strategies to make neural networks less sensitive to adversarial attacks. They consist into applying different transformations to the images, such as quantization, JPEG compression, total variation minimization and image quilting. Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images.\nExperiments are conducted in a black box setting (when the model to attack is unknown by the adversary) or white box setting (the model and defense strategy are known by the adversary).\n60% of attacks are countered in this last most difficult setting.\nThe previous best approach for this task consists in ensemble training and is attack specific. It is therefore pretty robust to the attack it was trained on but is largely outperformed by the authors methods that manage to reduce the classifier error drop below 25%.  \n\nComments: The paper is well written, the proposed methods are well adapted to the task and lead to satisfying results.\n \nThe discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice.\nMinor: the bibliography should be uniformed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1515642483544,"tcdate":1511441950604,"number":1,"cdate":1511441950604,"id":"S1wXIrVgM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Review","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference/Paper636/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Valuable idea but immature contribution.","rating":"4: Ok but not good enough - rejection","review":"To increase robustness to adversarial attacks, the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier. The purpose of the transformation is to erase the high-frequency signals potentially embedded by an adversarial attack.\n\nStrong points:\n\n* To my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). \n\n* The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work.\n\n* The proposed approach really helps in a black-box scenario (Figure 4). As explained below, the presented investigation is however insufficient to assess whether the proposed defense helps in a true white-box scenario. \n\n\nWeak points:\n\n* The black-box versus white-box terminology is not appropriate, and confusing. In general, black-box means that the adversary ignores everything from the decision process. Hence, in this case, the adversary does not know about the classification model, nor the defensive method, when used. This corresponds to Figure 3. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. Assimilating the parameters of the transform to a secret key is not correct because those parameters could be inferred by presenting many image samples to the transform and looking at the outcome of the transformation (which is supposed to be available in a 'white-box' paradigm) for those samples. \n\n* Using block diagrams would definitely help in presenting the training/testing and attack/defense schemes investigated in Figure 3, 4, and 5.\n\n* The paper does not discuss the impact of the denfense strategy on the classification performance in absence of adversity.\n\n* The paper lacks of positioning with respect to recent related works, e.g. 'Adversary Resistant Deep Neural Networks with an Application to Malware Detection' in KDD 2017, or 'Building Adversary-Resistant Deep Neural Networks without\nSecurity through Obscurity' at https://arxiv.org/abs/1612.01401. \n\n* In a white-box scenario, the adversary knows about the transformation and the classification model. Hence, an effective and realistic attack should exploit this knowledge. Designing an attack in case of a non differentiable transformation is obviously not trivial since back-propagation can not be used. However, since the proposed transformation primarily aim at removing the high frequency pattern induced by the attack, one could for example design an attack that account for a (linear and differentiable) low-pass filter transformation. Another example of attack that account for transformation knowledge (and would hopefully be more robust than the attacks considered in the manuscript) could be one that alternates between a conventional attack and the transformation.\n\n* If I understand correctly, the classification model considered in Figure 3 has been trained on original images, while the one in Figure 4 has been trained on transformed images. However, in absence of attack, they both achieve 76% accuracy. Is it correct? Does it mean that the transformation does not affect the classification accuracy at all?\n\n\nOverall, the works investigates an interesting idea, but lacks maturity to be accepted. Therefore, I would only recommend acceptation if room.\n\nMinor issues:\n\nTypo on p7: to change*s*\nClarify poor formulations:\n* p1: 'enforce model-specific strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization schemes'. \n* p1: 'too simple to remove adversarial perturbations from input images sufficiently'","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510961812999,"tcdate":1510961812999,"number":7,"cdate":1510961812999,"id":"SJp9ze61G","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"H1mJgiqJG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: The \"huge drop\"","comment":"You might want to check out https://openreview.net/forum?id=S18Su--CW where the authors show that simple quantization (depth reduction) leads to a loss of accuracy on clean examples, but you can get around it by discretizing your input. Also simple quantization maintains a roughly linear response of the classifier to it's input (see Figure 1), which is hypothesized as the cause of non-robustness to adversarial attacks (Goodfellow, 2014). The attacks proposed are also not \"grey-box\" like in this paper, i.e. the attacker has knowledge of the transformations and can attack using \"discretized\" versions of various iterated algorithms. The defense of this paper is essentially defense by \"obfuscation\", under the assumption that the attacker does not know what \"obfuscation\" has happened."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510809563479,"tcdate":1510809563479,"number":6,"cdate":1510809563479,"id":"H1mJgiqJG","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"HyOUaNzkz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The \"huge drop\"","comment":"The authors' result is not surprising at all. It is actually one of the few works on the defense side which makes sense to me.\nPersonally, I have not seen any strong evidence suggesting that we can generally maintain a high accuracy on clean and adversarial samples simultaneously. We have to take any such idealistic results on ImageNet with a grain of salt. In most of these works, either the defense is super weak (i.e., easily attackable) or something is hidden under the carpet."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510414603838,"tcdate":1510414603838,"number":5,"cdate":1510414603838,"id":"ry4zt5Vyf","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"HyOUaNzkz","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Performance on clean examples","comment":"The transformations we studied, indeed, all have some hyper-parameter that controls how lossy the transformation is, and can be used to trade off clean accuracy and adversarial accuracy. These hyper-parameters are: crop ratio for the crop-rescale transform, pixel drop rate and regularization parameter for total variation minimization, and patch size for image quilting. For instance, using a larger patch size in image quilting will remove more of the adversarial perturbation (which likely leads to higher adversarial accuracy), but also affects clean images which deteriorates clean accuracy. \n\nWe selected hyper-parameter that achieve high adversarial accuracy, but one may choose to set them differently depending on the user's needs. We surmise it may be possible to achieve high clear and adversarial accuracy by ensembling predictions over multiple hyper-parameter settings, but further experimentation is needed to confirm this hypothesis."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510260215579,"tcdate":1510260048162,"number":5,"cdate":1510260048162,"id":"HyOUaNzkz","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Performance on clean examples","comment":"The proposed several transformation methods are quite effective in recognizing the adversarial examples. For most existing works, people are trying to increase the performance on adversarial examples while maintaining the accuracy of clean examples. However, in table 2, I found there is a huge drop in accuracy for all your methods on clean examples. \nI wonder can you solve this problem by tuning some hyper-parameters to balance the performance of no attack and with attack?"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092430875,"tcdate":1509891866807,"number":4,"cdate":1509891866807,"id":"ByXm1shC-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"ryuYj65C-","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Three comments","comment":"1) We have performed experiments with our defenses against PGD; see our previous comment for the results of those experiments, which we will add to the paper. Our results do not suggest substantial differences in the effectiveness of our defenses between PGD and I-FGSM. \n\n2) Our proposed defenses are not intended to compete with adversarial-training-based defenses: the two defenses can be used together and are likely to be complementary. We chose ImageNet to conduct our experiment for the following two reasons:\n\n- The interest in adversarial examples mainly stems from the concern of use of computer vision models in real-world applications such as self-driving cars and image-classification services. In these settings, the input to the model has high resolution and diverse content; ImageNet more closely resembles this scenario than MNIST or CIFAR.\n\n- Defending a model that performs classification on ImageNet is inherently more difficult than defending a MNIST or CIFAR classification model, since the model must output very diverse class labels and the model's prediction is often uncertain. Moreover, the input dimensionality for ImageNet is much higher (~150000 compared to 768 for MNIST and 3072 for CIFAR-10), which gives the attacker much more maneuverability.\n\n3) We agree that the terminology of white-box is ambiguous, in particular, in the context of randomized defenses (is the adversary allowed access to the random seed?). In cryptography, Kerckhoffs's principle prescribes the use of a \"secret key\". One could make the argument that the patch database used by image quilting is an implementation of such a secret key. To the best of our knowledge, there is no consensus yet in the community yet on what a \"secret key\" may and may not contain in the context of defenses against adversarial examples. In our current paper, \"white-box\" refers to access the model parameters; other parameters (random seed, patch database) are considered to be part of the secret key. We will add a section to our paper clarifying the terminology."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092430922,"tcdate":1509891831321,"number":3,"cdate":1509891831321,"id":"HJkZJjhAb","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"Hk1TeF9RZ","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: White-box attacks with knowledge of defense transformation","comment":"We agree that the terminology of white-box is ambiguous, in particular, in the context of randomized defenses (is the adversary allowed access to the random seed?). In cryptography, Kerckhoffs's principle prescribes the use of a \"secret key\". One could make the argument that the patch database used by image quilting is an implementation of such a secret key. To the best of our knowledge, there is no consensus yet in the community yet on what a \"secret key\" may and may not contain in the context of defenses against adversarial examples.\n\nIn our current paper, \"white-box\" refers to access the model parameters; other parameters (random seed, patch database) are considered to be part of the secret key. We will add a section to our paper clarifying the terminology."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509772159993,"tcdate":1509772159993,"number":4,"cdate":1509772159993,"id":"ryuYj65C-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"HktFIuqC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Three comments","comment":"1) Projecting after every step would make a lot of difference if the number of iterations and step length is high, since typically gradient signal from outside the epsilon ball is not that useful in finding adversarial examples.\n\n2) I am wondering what kind of accuracy your defense would get on MNIST/CIFAR-10, so one could compare to vanilla adversarial training as in Madry et al. It is not clear at the moment whether it would improve on adversarial training with PGD.\n\n3) Since some of the transformations are non-differentiable you could also have settings where the attacker knows your transformations and attack your transformed inputs (rather than the original image) - as the other commenter says, what you study is the \"grey-box\" case and not truly the \"white-box\" case. I would expect that in the truly \"white-box case\", it is no more robust against such attacks than just adversarial training with PGD."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509753015381,"tcdate":1509753015381,"number":3,"cdate":1509753015381,"id":"Hk1TeF9RZ","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"H1dO8OqC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: White-box attacks with knowledge of defense transformation","comment":"Thanks for these clarifications! It might make sense to introduce a new term to characterize the threat model here, as \"white-box\" typically refers to full information about the defense. I think \"grey-box\" has been used before although it isn't very evocative either."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092430965,"tcdate":1509750400591,"number":2,"cdate":1509750400591,"id":"HktFIuqC-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"SJ67F79R-","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: PGD evaluation missing","comment":"For our experiments, we selected four attacks that we believe are representative of the large number of attacks that people have proposed. Since PGD is related to I-FGSM (the main difference between the two is the projection step after every iteration), we expect that our defenses will have similar performance against PGD. \n\nWe performed a small experiment with PGD to confirm this. We created attacks with an average L2-dissimilarity of 0.06, and find that the accuracy of our defenses against white-box I-FGSM/PGD attacks are: no defense 0%/0%, crop ensemble 44%/48%, TVM 29%/36%, and image quilting 35%/37%. These results suggest that the effectiveness of our defenses against PGD is similar to their effectiveness against I-FGSM. We will add these results to the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092431009,"tcdate":1509750384434,"number":1,"cdate":1509750384434,"id":"H1dO8OqC-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"BkDPS_tAb","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: White-box attacks with knowledge of defense transformation","comment":"Thank you for your comment! We were unaware of [1] at the time of submission. We will include it as a citation. Based on our understanding, [2] applies Carlini-Wagner's attack against a target loss that averages the prediction over multiple fixed models with random network weight dropout rather than random pixel dropout, but the two techniques are certainly related.\n\nIn regards to attacking the transformation defenses, independent of [1], we have observed that it is possible to produce adversarial examples that are invariant to crop location and scale. By randomly selecting a crop of size 135x135 each iteration to compute the loss function for CW-L2, the resulting adversarial examples can reduce the accuracy of crop defense to 30% at an average L2-dissimilarity of 0.045. \n\nEnhancing the attack with random pixel dropping can, indeed, reduce the effectiveness of TV minimization significantly. Using a pixel dropout mask with drop probability 0.1 each iteration, CW-L2 can reduce the accuracy of TV minimization defense to 9% at an average L2-dissimilarity of 0.06. However, we do not have a good idea of how to backpropagate through the quilting transformation, as the construction is stochastic and non-differentiable in nature. Nevertheless, section 5.5 of our paper does show that it is possible to successfully attack the quilting defense in some cases even without knowledge of this transformation; we presume because the convolutional filters reveal some information about the patch database used.\n\nThe attacks we use for the white-box setting do not have knowledge of the defense mechanism used. \n\nWe will clarify these points in the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509730597194,"tcdate":1509730597194,"number":2,"cdate":1509730597194,"id":"SJ67F79R-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"PGD evaluation missing","comment":"I am curious why the authors have not evaluated their defense methods against the PGD attack of Madry et al https://arxiv.org/abs/1706.06083, which is the strongest first order adversary possible."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509684574939,"tcdate":1509684574939,"number":1,"cdate":1509684574939,"id":"BkDPS_tAb","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"White-box attacks with knowledge of defense transformation","comment":"Interesting paper! It wasn't clear to me whether you evaluated attacks (white-box or black-box) with knowledge of the defensive technique being used? For JPEG or TV-minimization for instance, this would mean back-propagating through the transformation step. Analogously, one could incorporate randomized procedures such as cropping, pixel dropout or quilting into the adversarial example generation procedure, either in a white-box setting or in a black-box setting over a locally trained model with a similar defense.\n\nSome prior works [1,2] seem to indicate that various types of transformations do actually remain vulnerable to adversaries with such tailored attacks. ([1] considered random crops and found that an attack tailored to a particular cropping mechanism was still effective. [2] looked at random pixel dropout and found that computing white-box or black-box attacks for multiple randomly chosen dropout masks generalized well to unseen random masks).\n\n[1] Foveation-based Mechanisms Alleviate Adversarial Examples, https://arxiv.org/abs/1511.06292\n[2] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods, https://arxiv.org/abs/1705.07263"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1513972693177,"tcdate":1509129751066,"number":636,"cdate":1509739185961,"id":"SyJ7ClWCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/ff7140f60666ac0cc92e576b2be0c7519bf0d7e9.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]},"nonreaders":[],"replyCount":25,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}