{"notes":[{"tddate":null,"ddate":null,"tmdate":1515695721918,"tcdate":1515687943703,"number":4,"cdate":1515687943703,"id":"rklzlzBVf","invitation":"ICLR.cc/2018/Conference/-/Paper596/Public_Comment","forum":"Hk6kPgZA-","replyto":"rJBbuPTmz","signatures":["~Aleksander_Madry1"],"readers":["everyone"],"writers":["~Aleksander_Madry1"],"content":{"title":"Re: [Part I] Our goal is to defend against imperceptible perturbations. More empirical evaluations available.","comment":"[I put my reply here as the threads below are now a bit hard to follow.]\n\nThank you for responding to my comments and making the effort to provide more data. This indeed helps me understand this work better.\n\nI agree that studying the regime of small adversarial perturbation budget epsilon is a very valid research goal. I think, however, that it is important to explicitly mention in the paper that this is the target. Especially, as the proposed methods seem to be inherently restricted to apply to only such a small epsilon regime. \n\nI am not sure though that I agree with the argument why the regime of larger values of epsilon might be less interesting. Yes, some of the larger perturbations will be clearly visible to a human, but some (e.g., the ones that correspond to a change of the background color or its pattern) will not - and we still would like to be robust to them. After all, security guarantees are about getting \"for all\", not \"for some\" guarantees. \n\nNow, regarding being explicit about the constants in the bounds, I agree that many optimization and statistical learning guarantees do not provide not provide explicit constants. However, I think the situation in the context considered here is fundamentally different. \n\nAfter all, for example, in the context of generalization bounds, we always have a meaningful way of checking if a given bound \"triggered\" for a given model and dataset by testing its performance on a validation/test set. When we talk about robustness guarantee, the whole point is to have it hold even against attacks that we are not able to produce ourselves (but the adversary might). Then, we really need a very concrete guarantee of the form \"(With high probability) the model classifies correctly 90% of the test set against perturbation budget of epsilon <= 0.1”. \n\nIn the light of this, providing a guarantee of the form \"(With high probability) the model correctly classifies 90% of the test set against perturbation budget of some positive epsilon\", which is what the proposed guarantees seem to provide, is somewhat less meaningful. (One could argue that, after all, there is always some positive epsilon for which the model is robust.)\n\nIt might be worth noting that, e.g., for MNIST, we currently are able to deliver guarantees of the former (explicit) type. For instance, there is a recent work of Kolter and Wong (https://arxiv.org/abs/1711.00851). Although they provide such guarantees via verification techniques and not by proving an explicit generalization bound.\n\nFinally, I am not sure how much bearing the formal NP-hardness of certifying the robustness has here. (I assume you are referring to the result in Appendix B.) Could you elaborate?"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515186674787,"tcdate":1515186674787,"number":5,"cdate":1515186674787,"id":"rkix5PTQf","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Comment","forum":"Hk6kPgZA-","replyto":"Hk2kQP3Qz","signatures":["ICLR.cc/2018/Conference/Paper596/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper596/Authors"],"content":{"title":"Addressing concerns","comment":"Thank you for your interest in our paper. We appreciate your detailed feedback.\n\n1. This is a fair criticism; it seems to apply generally to most learning-theoretic guarantees on deep learning (though see the recent work of Dziugaite and Roy, https://arxiv.org/abs/1703.11008 and Bartlett, Foster, Telgarsky https://arxiv.org/pdf/1706.08498.pdf). We believe that our statistical guarantees in Theorems 3 and 4 are steps towards a principled understanding of adversarial training. Replacing our current covering number arguments with more intricate notions such as margin based-bounds (Bartlett et al. 2017)) would extend the scope of our theoretical guarantees; as Bartlett et al. provide covering number bounds, it seems likely that we could massage them into applying in Theorem 3 (Eqs. (11)-(12)). This is a meaningful future research direction.\n\n\n2. In Figure 2, we plot our certificate of robustness on two datasets (omitting the statistical error term) and observe that our data-dependent upper bound on the worst-case performance is reasonable. This roughly implies that our adversarial training procedure generalizes, allowing us to learn to defend against attacks on the test set.\n\n“In the experimental sections, good performance is achieved at test time. But it would be more convincing if the performance for training data is also shown. The current experiments don't seem to evaluate generalization of the proposed WRM. Furthermore, analysis of other classification problems (cifar10, cifar 100, imagenet) is highly desired.“\n\nThese are both great suggestions. We are currently working on experiments with subsets of Imagenet and will include them in a revision (soon we hope).\n\n3. Our adversarial training algorithm has intimate connections with other previously proposed heuristics. Our main theoretical contribution is that for small adversarial perturbations, we can show both computational and statistical guarantees for our procedure. More specifically, the computational guarantees for our algorithm are indeed based on the curvature of the L2-norm; provably efficient computation of attacks based on infinity-norms remains open."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515186612695,"tcdate":1515186612695,"number":4,"cdate":1515186612695,"id":"rJ63YwTQM","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Comment","forum":"Hk6kPgZA-","replyto":"BJVnpJPXM","signatures":["ICLR.cc/2018/Conference/Paper596/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper596/Authors"],"content":{"title":"[Part I] Our goal is to defend against imperceptible perturbations. More empirical evaluations available. ","comment":"Thank you for your interest in our paper. We appreciate the detailed feedback and probing questions.\n\nUpon your suggestions during our meeting at NIPS, we have included a more extensive empirical evaluation of our algorithm. Most notably, we trained and tested our method—alongside other baselines, including [MMSTV17]—on large values of adversarial budgets. We further compared our algorithm trained against L2-norm Lagrangian attacks against other heuristic methods trained against infinity-norm attacks. Lastly, we proposed a (heuristic) proximal variant of our algorithm that learns to defend against infinity-norm attacks. See Appendices A.4, A.5, and E for the relevant exposition and figures.\n\n1. Empirical Evaluation on Large Adversarial Budgets\n\nOur primary motivation of this paper is to provide a theoretically principled algorithm that can defend against small adversarial perturbations. In particular, we are concerned with provable procedures against small adversarial perturbations that can fool deep nets but are imperceptible to humans. Our main finding in the original empirical experiments in Section 4 was that for such small adversarial perturbations, our principled algorithm matches or outperforms other existing heuristics. (See also point 2 below.)\n\nThe adversarial budget epsilon = .3 in the infinity-norm you suggest allows attacks that are highly visible to the human eye. For example, one can construct hand-tuned perturbations that look significantly different from the original image (see https://www.dropbox.com/sh/c6789iwhnooz5po/AABBpU_mg-FRRq7PT1LzI0GAa?dl=0). Defending against such attacks is certainly interesting, but was not our main goal. This probably warrants a longer discussion, but it is not clear to us that infinity-norm-bounded attacks are most appropriate if one allows perceptible image modifications. An L1-budgeted adversary might be able to make small changes in some part of the image, which yields a different set of attacks.\n\nIn spite of the departure of large perturbations from our nominal goal of protection against small changes, we test our algorithm on attacks with large adversarial budgets in Appendix A.4. In this case, our algorithm is a heuristic—as are other methods for large adversarial budgets—but we nevertheless match the performance of other methods (FGM, IFGM, PGM) trained against L2-norm adversaries.\n\nSince our computational guarantees are based on strong concavity w.r.t. Lp-norms for p \\in (1, 2], our robustly-fitted network defends against L2-norm attacks. Per the suggestion to compare against networks trained to defend against infinity-norm attacks—and we agree, this is an important comparison that we did not perform originally (though we should have)—we compared our method with other heuristics in Appendix A.5.1. On imperceptible L2 and infinity-norm attacks, our algorithm outperforms other heuristics trained to defend against infinity-norm attacks (Figures 11 and 12). On larger attacks, particularly infinity-norm attacks, we observe that other heuristics trained on infinity-norm attacks outperform our method (Figure 12). In this sense, the conclusions we reached from the main figures in our paper—where we considered imperceptible perturbations—are still valid: we match or outperform other heuristic methods for small perturbations.\n\n(Continued in Part II)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"ddate":null,"tddate":1515186511699,"tmdate":1515186576938,"tcdate":1515186497472,"number":3,"cdate":1515186497472,"id":"HyFBKPp7z","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Comment","forum":"Hk6kPgZA-","replyto":"BJVnpJPXM","signatures":["ICLR.cc/2018/Conference/Paper596/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper596/Authors"],"content":{"title":"[Part II] Our goal is to defend against imperceptible perturbations. More empirical evaluations available.","comment":"2. Theoretical Guarantees\n\nThe motivation for our work is that computing the worst-case perturbation of a deep network under norm-constraints is typically intractable. As we state in the introduction, we simply give up on computing worst-case perturbations at arbitrary budget levels, instead considering small adversarial perturbations. Our theoretical guarantees are concerned with imperceptible changes; we give computational and statistical guarantees for such small (adversarial) perturbations. This is definitely a limit of the approach; given that it is NP hard to certify robustness for larger perturbations this may be challenging to get around.\n\nOur main theoretical guarantee is the certificate of robustness—a data-dependent upper bound on the worst-case performance—given in Theorem 3. This upper bound applies in general, although its efficient computation is only guaranteed for large penalty parameters \\gamma and smooth losses. Similarly, as you note, Theorems 2 and 4 only apply in such regimes. To address this, we augment our theoretical guarantees for small adversarial budgets with empirical evaluations in Section 4 and Appendix A. We empirically checked if our level of \\gamma = .385 (=.04 * C_2) is above the estimated smoothness parameter at the adversarially trained model and observed that this condition is satisfied on 98% of the training data points.\n\nOur guarantees indeed depend on the problem-dependent smoothness parameter. As with most optimization and statistical learning guarantees, this value is often unknown. This limitation applies to most learning-theoretic results, and we believe that being adaptive to such problem-dependent constants is a meaningful future research direction. With that said, it seems likely (though we have not had time to verify this) that the recent work of Bartlett et al. (https://arxiv.org/pdf/1706.08498.pdf) should apply--it provides covering number bounds our Theorem 3 (Eq. (11-12)) can use.\n\nWe hope that our theoretical guarantees are a step towards understanding the performance of these adversarial training procedures. Gaps still remain; we hope future work will close this gap."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515186243502,"tcdate":1515186201974,"number":2,"cdate":1515186201974,"id":"Hkzmdv67G","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Comment","forum":"Hk6kPgZA-","replyto":"H1wDpaNbM","signatures":["ICLR.cc/2018/Conference/Paper596/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper596/Authors"],"content":{"title":"Comparison with Roy et al. (2017)","comment":"Thank you for bringing our attention to Roy et al. (2017). In Section 4.3, we adapted our adversarial training algorithm in the supervised learning setting to reinforcement learning; this approach shares similar motivations as Roy et al. (2017)—and more broadly, the robust MDP literature—where we also solve approximations of the worst-case Bellman equation. Compared to our Wasserstein ball, Roy et al. (2017) uses more simple and tractable worst-case regions. While they give convergence guarantees for their algorithm, the empirical performance of these different worst-case regions remains open.\n\nAnother key difference in our experiments is that we assumed access to the simulator for updating the underlying state. This allows us to explore bad regions better. Nevertheless, our adversarial state update in Eqn (20) can be replaced with an adversarial reward update for settings where the simulator cannot be accessed."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515186172886,"tcdate":1515186172886,"number":1,"cdate":1515186172886,"id":"rJBbuPTmz","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Comment","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper596/Authors"],"content":{"title":"Response to Reviews","comment":"We thank the reviewers for their time and positive feedback. We will use the comments and suggestions to improve the quality and presentation the paper. In addition to cleaning up our exposition, we added some content to make our main points more clear. We address these main revisions below.\n\nOur formulation (2) is general enough to include a number of different adversarial training scenarios. In Section 2 (and more thoroughly in Appendix D), we detail how our general theory can be modified in the supervised learning setting so that we learn to defend against adversarial perturbations to only the feature vectors (and not the labels). By suitably modifying the cost function that defines the Wasserstein distance, our formulation further encompasses other variants such as adversarial perturbations only to a fixed small region of an image.\n\nWe emphasize that our certificate of robustness given in Theorem 3 applies for any level of robustness \\rho. Our results imply that the output of our principled adversarial training procedure has worst-case performance no worse than this data-dependent certificate. Our certificate is efficiently computable, and we plot it in Figure 2 for our experiments. We see that in practice, the bound indeed gives a meaningful performance guarantee against attacks on the unseen test sets.\n\nWhile the primary focus of our paper is on providing provable defenses against imperceptible adversarial perturbations, we supplement our previous results with a more extensive empirical evaluation. In Appendix A.4, we augment our results by evaluating performance against L2-norm adversarial attacks with larger adversarial budgets (higher values of \\rho or \\epsilon). Our method also becomes a heuristic for such large values of adversarial budgets, but we nevertheless match the performance of other methods (FGM, IFGM, PGM) trained against L2-norm adversaries. In Appendix A.5.1, we further compare our method——which is trained to defend against L2-norm attacks——with other adversarial training algorithms trained against inf-norm attacks. We also propose a new (heuristic) proximal algorithm for solving our Lagrangian problem with inf-norms, and test its performance against other methods in Appendix A.5.2. In both sections, we observe that our method is competitive with other methods against imperceptible adversarial attacks, and performance starts to degrade as the attacks become visible to the human eye.\n\nAgain, we appreciate the reviewers' close reading and thoughtful comments."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515119332259,"tcdate":1515119332259,"number":3,"cdate":1515119332259,"id":"Hk2kQP3Qz","invitation":"ICLR.cc/2018/Conference/-/Paper596/Public_Comment","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Rigorous approach to Adversarial training but some concerns","comment":"The problems are very well formulated (although only the L2 case is discussed). Identifying a concave surrogate in this mini-max problem is illuminating. The interplay between optimal transport, robust statistics, optimization and learning theory make the work a fairly thorough attempt at this difficult problem. Thanks to the authors for turning many intuitive concepts into rigorous maths. There are some potential concerns, however: \n\n1. The generalization bounds in THM 3, Cor 1, THM 4 for deep neural nets appear to be vacuous, since they scale like \\sqrt (d/n), but d > n for deep learning. This is typical, although such generalization bounds are not common in deep adversarial training. So establishing such bounds is still interesting.\n\n2. Deep neural nets generalize well in practice, despite the lack of non-vacuous generalization bounds. Does the proposed WRM adversarial training procedure also generalize despite the vacuous bounds? \n\nIn the experimental sections, good performance is achieved at test time. But it would be more convincing if the performance for training data is also shown. The current experiments don't seem to evaluate generalization of the proposed WRM. Furthermore, analysis of other classification problems (cifar10, cifar 100, imagenet) is highly desired. \n\n3. From an algorithmic viewpoint, the change isn't drastic. It appears that it controls the growth of the loss function around the L2 neighbourhood of the data manifold (thanks to the concavity identified). Since L2 geometry has good symmetry, it makes the decision surface more symmetrical between data (Fig 1). \n\nIt seems to me that this is the reason for the performance gain at test time, and the size of such \\epsilon tube is the robust certificate. So it is unclear how much success is due to the generalization bounds claimed. \n\nI think there is enough contribution in the paper, but I share the opinion of Aleksander Madry, and would like to be corrected for missing some key points."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1514761644468,"tcdate":1514761644468,"number":2,"cdate":1514761644468,"id":"BJVnpJPXM","invitation":"ICLR.cc/2018/Conference/-/Paper596/Public_Comment","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["~Aleksander_Madry1"],"readers":["everyone"],"writers":["~Aleksander_Madry1"],"content":{"title":"An interesting attempt but some of the key claims seem to be inaccurate and miss comparison to proper baselines. ","comment":"Developing principled approaches to training adversarially robust models is an important (and difficult) challenge. This is especially the case if such an approach is to offer provable guarantees and outperform state of the art methods. \n\nHowever, after reading this submission, I am confused by some of the key claims and find them to be inaccurate and somewhat exaggerated. In particular, I believe that the following points should be addressed and clarified:\n\n1. The authors claim their methods match or outperform existing methods. However, their evaluations seem to miss some key baselines and parameter regimes. \n    \nFor example, when reporting the results for l_infty robustness - a canonical evaluation setting in most previous work - the authors plot (in Figure 2b) the robustness only for the perturbations whose size eps (as measured in the l_infty norm) is between 0 and 0.2. (Note that in Figure 2b the x-axis is scaled as roughly 2*eps.) However, in order to properly compare against prior work, one needs to be able to see the scaling for larger perturbations.\n\nIn particular, [MMSTV’17] https://arxiv.org/abs/1706.06083 gives a model that exhibits high robustness even for perturbations of l_infty size 0.3. What robustness does the approach proposed in this work offer in that regime? \n\nAs I describe below, my main worry is that the theorems in this work only apply for very small perturbations (and, in fact, this seems to be an inherent limitation of the whole approach). Hence, it would be good to see if this is true in practice as well.  \nIn particular, Figure 2b suggests that this method will indeed not work for larger perturbations. I thus wonder in what sense the presented results outperform/match previous work?\n\nAfter a closer look, it seems that this discrepancy occurs because the authors are reproducing the results of [MMSTV’17] using l_2 based adversarial training.  [MMSTV’17] uses l_infity based training and achieves much better results than those reported in this submission. This artificially handicaps the baseline from [MMSTV’17]. That is, there is a significantly better baseline that is not reflected in Figure 2b. I am not sure why the authors decided to do that.\n\n2. It is hard to properly interpret what actual provable guarantees the proposed techniques offer. More concretely, what is the amount of perturbation that models trained using these techniques are provably robust to? \n\nBased on the presented theorems, it is unclear why they should yield any non-vacuous generalization bounds. \n\nIn particular, as far as I can understand, there might be no uniform bound on the amount of perturbation that the trained model will be robust to. This seems to be so as the provided guarantees (see Theorem 4) might give different perturbation resistance for different regions of the underlying distribution. In fact, it could be that for a significant fraction of points we have a (provable) robustness guarantee only for vanishingly small perturbations. \n\nMore precisely, note that the proposed approach uses adversarial training that is based on a Lagrangian formulation of finding the worst case perturbation, as opposed to casting this primitive as optimization over an explicitly defined constraint set. These two views are equivalent as long as one has full flexibility in setting the Lagrangian penalization parameter gamma. In particular, for some instances, one needs to set gamma to be *small enough*, i.e., sufficiently small so as it does not exclude norm-eps vectors from the set of considered perturbations. (Here, eps denotes the desired robustness measured in a specific norm such as l_infty, i.e., the prediction of our model should not change under perturbations of magnitude up to eps.)\n\nHowever, the key point of the proposed approach is to ensure that gamma is always set to be *large enough* so as the optimized function (i.e., the loss + the Lagrangian penalization) becomes concave (and thus provably tractable). Specifically, the authors need gamma to be large enough to counterbalance the (local) smoothness parameter of the loss function. \n\nThere seems to be no global (and sufficiently small) bound on this smoothness and, as a result, it is unclear what is the value of the eps-based robustness guarantee offered once gamma is set to be as large as the proposed approach needs it to be.\n\nFor the same reason (i.e., the dependence on the smoothness parameter of the loss function that is not explicitly well bounded), the provided generalization bounds - and thus the resulting robustness guarantees - might be vacuous for actual deep learning models. \n\nIs there something I am missing here? If not, what is the exact nature of the provable guarantees that are offered in the proposed work?\n"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1512525151114,"tcdate":1512525151114,"number":1,"cdate":1512525151114,"id":"H1wDpaNbM","invitation":"ICLR.cc/2018/Conference/-/Paper596/Public_Comment","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Very cool!","comment":"Very interesting work! I was wondering how the robust MDP/RL setup compares to http://papers.nips.cc/paper/6897-reinforcement-learning-under-model-mismatch.pdf ? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642476541,"tcdate":1512147964590,"number":3,"cdate":1512147964590,"id":"rkx-2-y-f","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Review","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Very interesting principled analysis of robust learning","rating":"9: Top 15% of accepted papers, strong accept","review":"In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\n\nSince the min-max problem is intractable in general, what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem. If the duality parameter is large enough, the functions become convex given that the initial losses are smooth. \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions. Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642476577,"tcdate":1511887853232,"number":2,"cdate":1511887853232,"id":"HySlNfjgf","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Review","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). I think the idea is intuitive and reasonable, the result is nice. Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642476616,"tcdate":1511800281907,"number":1,"cdate":1511800281907,"id":"HJ-1AnFlM","invitation":"ICLR.cc/2018/Conference/-/Paper596/Official_Review","forum":"Hk6kPgZA-","replyto":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference/Paper596/AnonReviewer2"],"readers":["everyone"],"content":{"title":"a very interesting approach to adversarial training based on robustness over Wasserstein balls","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution. In particular, the authors adopt the Wasserstein distance to define the ambiguity sets. This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost. The theoretical results in the paper are supported by experiments.\n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515186030150,"tcdate":1509127909457,"number":596,"cdate":1509739208417,"id":"Hk6kPgZA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk6kPgZA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Certifiable Distributional Robustness with Principled Adversarial Training","abstract":"  Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We take the principled view of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbation of the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization.  Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n","pdf":"/pdf/ff384027794a435cf6231dd154054baf9fb7e2e3.pdf","TL;DR":"We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.","paperhash":"anonymous|certifiable_distributional_robustness_with_principled_adversarial_training","_bibtex":"@article{\n  anonymous2018certifiable,\n  title={Certifiable Distributional Robustness with Principled Adversarial Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6kPgZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper596/Authors"],"keywords":["adversarial training","distributionally robust optimization","deep learning","optimization","learning theory"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}