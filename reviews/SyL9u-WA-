{"notes":[{"tddate":null,"ddate":null,"tmdate":1515088729839,"tcdate":1514579684795,"number":3,"cdate":1514579684795,"id":"rk61wXVQM","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Comment","forum":"SyL9u-WA-","replyto":"H1Z5gRJZf","signatures":["ICLR.cc/2018/Conference/Paper699/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper699/Authors"],"content":{"title":"Rebuttal 3","comment":"Thank you for your comments. We have carefully reorganized the paper to take into account your suggestions.\n\nSince our parameterization works for any real matrix, any weight matrix in CNN (e.g. filter matrix) can also be SVD parametrized.\nSince the gradient exploding/vanishing problem is generally considered more significant in RNN/MLP models than in CNN, we have not implement our algorithm as yet for CNNs.\n\n(Please also note that the state-of-the-art neural networks for image processing consists of both convolutional layers and fully-connected layers, like LeNet, AlexNet, ResNet, or Inception, etc. We are already able to parameterize the fully connected layers using our scheme. )\n\nWe will explicitly explain in section 6.2 that we are talking about non-singular critical points. Our theory effectively states that our update rule avoids singular critical points, and thus spurious local minimum.\n\nIn Appendix C (experimental results), we have added experiments on widely used benchmarking RNN tasks like Adding and Copying tasks. In all experiments, svdRNN outperforms others expecially in capturing long-range dependencies. Also from the plots of (first layer) gradient magnitude, we can observe that svdRNN has much more stable gradient than that of RNN and LSTM, which could be the reason why it converges much faster.\n         \nThe efficiency and GPU utilization is indeed an important aspect.\nHowever, with efficient linear algebra algorithms, GPU utilization of our method can actually be quite high. The main part in our algorithm, where we multiply m Householder reflectors to the hidden matrix (stacking of hidden vector h's within a minibatch), can be done using blocked (BLAS3) algorithm widely used in QR decomposition.\nFor example, in LAPACK library, the corresponding subroutine is called 'DLARFT', while in MAGMA (LAPACK with GPU) it is called 'magma_cunmqr'. We plan to implement these blocked BLAS3 algorithms as part of our software in the near future."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1515088484201,"tcdate":1514579425009,"number":2,"cdate":1514579425009,"id":"HJtJL7Emz","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Comment","forum":"SyL9u-WA-","replyto":"Syi9ojdgf","signatures":["ICLR.cc/2018/Conference/Paper699/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper699/Authors"],"content":{"title":"Rebuttal 2","comment":"Thank you for your insightful comments. We'll consider your suggestions and make modifications to the paper.\n\nIt is certainly important for the community to study the spectral properties of different types of stationary points of linear RNN. Nevertheless in our work we focus on how our proposed svdRNN helps to avoid local minimum, which is guaranteed by theorem 5 and its corollary."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1515088611448,"tcdate":1514579328566,"number":1,"cdate":1514579328566,"id":"BJttB7EXf","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Comment","forum":"SyL9u-WA-","replyto":"Bkyxj89lM","signatures":["ICLR.cc/2018/Conference/Paper699/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper699/Authors"],"content":{"title":"Rebuttal 1","comment":"Thank you for your insightful comments. We have carefully reorganized the paper to take into account your suggestions.\n\nWe actually have done experiments on Adding and Copying tasks -- these classical benchmarking tasks can test the model's ability in learning long term dependencies. In the new version, we have included experimental result of Adding/Copying problem in Appendix C. On short sequnences, all models performs similarly well. However, svdRNN outperforms other models when sequence length is large.\nFor example, on the addition task, when length of the sequence is 300, svdRNN reached almost 0 loss after 5k examples while RNN/LSTM failed to converge within 20k examples. From the plot of first layer gradient magnitude on different models, we can observe that svdRNN has much more stable gradients than RNN/LSTM.\nThese experiments support our claim that svdRNN is much better at capturing long-term dependencies than vanilla RNNs.\n\nIn most experiments we run all models for the same number of iterations. In the experiment on MNIST, RNN with random initialization encountered gradient explosion and stopped early. Implementing gradient clipping might be able to solve this. However, to have a fair comparison among the algorithms, we did not use huristics like gradient clipping on any of these models.\n\nSorry for the small font in figures. We have enlarged the fonts in figures, and added more details in the figure captions."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1515642493897,"tcdate":1512198282662,"number":3,"cdate":1512198282662,"id":"H1Z5gRJZf","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Review","forum":"SyL9u-WA-","replyto":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference/Paper699/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN. \n\nMy concerns with the paper include: \n\na) the paper says that the same method works for convolutional neural networks but I couldn't find anything about convolution. \n\nb) the theoretical analysis might be misleading --- clearly section 6.2 shouldn't have title ALL CRITICAL POINTS ARE GLOBAL MINIMUM because 0 is a critical point but it's not a global minimum. Theorem 5 should be phrased as \n\nall critical points of the population risk that is non-singular are global minima.\n\nc) the paper should run some experiments on language applications where RNN is widely used\n\nd) I might be wrong on this point, but it seems that the GPU utilization of the method would be very poor so that it's kind of impossible to scale to large datasets? \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1515642493935,"tcdate":1511840487132,"number":2,"cdate":1511840487132,"id":"Bkyxj89lM","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Review","forum":"SyL9u-WA-","replyto":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference/Paper699/AnonReviewer1"],"readers":["everyone"],"content":{"title":"SVD reparametrization of the transition matrix","rating":"5: Marginally below acceptance threshold","review":"This paper suggests a reparametrization of the transition matrix. The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.\n\nThe paper is well-written and authors explain related work adequately. The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary. The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact. \n\nI have two comments on the experiment section:\n\n- Choice of experiments. Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common. For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs. For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported.\n\n- Stopping condition. The plots suggest that the optimization has stopped earlier for some models. Is this because of some stopping condition or because of gradient explosion? Is there a way to avoid this?\n\n- Quality of figures. Figures are very hard to read because of small font. Also, the captions need to describe more details about the figures.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1515642493974,"tcdate":1511730067422,"number":1,"cdate":1511730067422,"id":"Syi9ojdgf","invitation":"ICLR.cc/2018/Conference/-/Paper699/Official_Review","forum":"SyL9u-WA-","replyto":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference/Paper699/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training. The proposed method improved two previous papers:\n1) stronger expressive power than Mahammedi et al. (2017),\n2) faster gradient update than Vorontsov et al. (2017).\nThe proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power. The experimental results also look promising. \n\nIt would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe). If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum. \n\nOverall this is a strong paper and I recommend to accept.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]}},{"tddate":null,"ddate":null,"tmdate":1514758612021,"tcdate":1509132429987,"number":699,"cdate":1509739150659,"id":"SyL9u-WA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyL9u-WA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization","abstract":"Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed svdRNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it potentially makes the optimization process easier. Our extensive  experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially when the depth is large. \n","pdf":"/pdf/81da65b73bcf90d4a586105160478ee60845f12c.pdf","TL;DR":"To solve the gradient vanishing/exploding problems, we proprose an efficient parametrization of the transition matrix of RNN that loses no expressive power, converges faster and has good generalization.","paperhash":"anonymous|stabilizing_gradients_for_deep_neural_networks_via_efficient_svd_parameterization","_bibtex":"@article{\n  anonymous2018stabilizing,\n  title={Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyL9u-WA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper699/Authors"],"keywords":["Recurrent Neural Network","Vanishing Gradient","Exploding Gradient","Linear Algebra","Householder Reflections"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}