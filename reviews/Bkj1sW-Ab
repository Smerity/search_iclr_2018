{"notes":[{"tddate":null,"ddate":null,"tmdate":1515138607018,"tcdate":1515138607018,"number":1,"cdate":1515138607018,"id":"BywVCshQf","invitation":"ICLR.cc/2018/Conference/-/Paper707/Public_Comment","forum":"Bkj1sW-Ab","replyto":"Bkj1sW-Ab","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Authors' response","comment":"The authors thank the reviewers for their constructive suggestions, comments, and corrections. We have studied your recommendations carefully and will use them as a guide to improve future versions of the paper. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Weight Sparsity for Training Deep Neural Networks","abstract":"We introduce adaptive weight sparsity, an algorithm that allows a neural network to learn a sparse connection pattern during training. We demonstrate that the proposed algorithm shows performance benefits across a wide variety of tasks and network structures, improving state-of-the-art results for recurrent networks of comparable size. We show that adaptive weight sparsity outperforms traditional pruning-based approaches to learning sparse configurations on convolutional and recurrent networks.  We offer insights into the algorithm's behavior, demonstrating that training-time adaptivity is crucial to the success of the method and uncovering an interpretable evolution toward small-world network structures.","pdf":"/pdf/f3c912c1f178daaacb409191d7d2549263bf0eae.pdf","paperhash":"anonymous|adaptive_weight_sparsity_for_training_deep_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Weight Sparsity for Training Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkj1sW-Ab}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper707/Authors"],"keywords":["deep learning","sparsity","adaptive methods"]}},{"tddate":null,"ddate":null,"tmdate":1515642495040,"tcdate":1512114862305,"number":3,"cdate":1512114862305,"id":"ryI25tCez","invitation":"ICLR.cc/2018/Conference/-/Paper707/Official_Review","forum":"Bkj1sW-Ab","replyto":"Bkj1sW-Ab","signatures":["ICLR.cc/2018/Conference/Paper707/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The paper propose a way to learn sparse connection by introducing a binary mask to deactivate and activate neural during training. The results shows outperform simple pruning method on several tasks. At the end the papers discussed the learned structure is related to small-world network. \n\nOverall, the paper is very hard to understand. For example, Figure 1 is never been referred in the whole paper. Does (d) and (e) is the reactivate procedure? There are several big table in the experimental session but barely mentioned in the text so I cannot understand what these table means. For example, \"The results are detailed below and are illustrated in Table 2 and\nFigure 2.\" and from section 4.1 to section 4.4, table 2 never been mentioned. I cannot link the text to the items in the table. \n\nLots of description is vague in this paper, e.g. \"The combination of increased clustering and constant shortest path length indicates, by definition, an evolution toward what is known as a small-world graph \", is there any quantitive results to prove that the learned structure is a small-world network? The LSTM experiments seems not a standard benchmark. Why not just use PTB for a LM? It's a bit surprised random sparse is better than the baseline for a LSTM.\n\nWhat is the final size of the network after the \"sparsity\"? I know alpha is related but it would be great can have a column to specify the number of activate weights and the accuracy.\n\nI would suggest the author revising the paper and I'm happy to adjust my score if I can have a better understanding of the content.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Weight Sparsity for Training Deep Neural Networks","abstract":"We introduce adaptive weight sparsity, an algorithm that allows a neural network to learn a sparse connection pattern during training. We demonstrate that the proposed algorithm shows performance benefits across a wide variety of tasks and network structures, improving state-of-the-art results for recurrent networks of comparable size. We show that adaptive weight sparsity outperforms traditional pruning-based approaches to learning sparse configurations on convolutional and recurrent networks.  We offer insights into the algorithm's behavior, demonstrating that training-time adaptivity is crucial to the success of the method and uncovering an interpretable evolution toward small-world network structures.","pdf":"/pdf/f3c912c1f178daaacb409191d7d2549263bf0eae.pdf","paperhash":"anonymous|adaptive_weight_sparsity_for_training_deep_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Weight Sparsity for Training Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkj1sW-Ab}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper707/Authors"],"keywords":["deep learning","sparsity","adaptive methods"]}},{"tddate":null,"ddate":null,"tmdate":1515642495077,"tcdate":1511833357045,"number":2,"cdate":1511833357045,"id":"S1Szkr5xM","invitation":"ICLR.cc/2018/Conference/-/Paper707/Official_Review","forum":"Bkj1sW-Ab","replyto":"Bkj1sW-Ab","signatures":["ICLR.cc/2018/Conference/Paper707/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper attempts to learn a spare connectivity pattern for the weights in a neural network. The major problem is the lack of experimental comparison against similar approaches in the past. It also makes numerous claims without proper empirical evidence. ","rating":"3: Clear rejection","review":"Detailed review and questions below:\n1. As mentioned, there is no comparison with [1,2,3]. Please explain how your framework is different from these, what are its advantages, and have an empirical comparison. \n2. A possible validation of this framework would be that starting from a fully connected network, it recovers sparse connected patterns that we know work well (e.g. sparse convolutions). Have you tried such an experiment?\n3. Another question is how does this relate to other sparse recovery algorithms like graphical lasso which also tries to learn a sparse precision matrix (network) in the Gaussian case. \n4. It seems that a lot depends on the initialized set of weights. What is the relation between the magnitudes of these weights to the hyper-parameters \\beta_0, \\beta_1. \n5. It seems quite easy to converge to a trivial local minima early in the training process. How and why is this avoided?\n6. Please explain the rationale and intuition behind the design decisions - form of equations for the activation and deactivation of weights. It is easy to imagine many variations of this. Why did you choose this particular formulation? \n7. On a related note, in the sentence \"Activated weights are initialized with small values according to gradients\". It is not clear to me why this is necessary, what does the c_{g} parameter mean and how is r chosen? In your experiments, c_{g} is zero, implying that the gradients are computed only for the deactivation of some weights? How is the loss function optimized i.e. where is the gradient descent (or a similar step)? \n8. In the sentence, \"We also keep the number of nonzero parameters constant fixed in the course of single experiment...\". Please justify this?\n9. I assumed that all the hyper-parameters are being chosen by cross-validation. Is this true? If not, why? If so, please explicitly say it in the paper.\n10. Please include experiments on larger datasets such as ImageNet, CIFAR-100. \n11. Where you claim, \"it trains more quickly\", please include wall-clock runtimes.\n12. In the sentence, \"One notable exception to this pattern is our LSTM\nmodel which performs slightly worse than the baseline on the training set but significantly better on\nthe validation set\". Why is this the case? Do you have any intuition? Did you conduct ablation studies to figure this out?\n13. \"We conclude that adaptive weight sparsity succeeds as an optimization method for other reasons in addition to the topology it learns.\" Again, there isn't sufficient evidence for this claim. \n\n[1] AdaNet: Adaptive Structural Learning of Artificial Neural Networks, ICML'17. \n[2] meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting, ICML'17. \n[3] Neural architecture search with reinforcement learning","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Weight Sparsity for Training Deep Neural Networks","abstract":"We introduce adaptive weight sparsity, an algorithm that allows a neural network to learn a sparse connection pattern during training. We demonstrate that the proposed algorithm shows performance benefits across a wide variety of tasks and network structures, improving state-of-the-art results for recurrent networks of comparable size. We show that adaptive weight sparsity outperforms traditional pruning-based approaches to learning sparse configurations on convolutional and recurrent networks.  We offer insights into the algorithm's behavior, demonstrating that training-time adaptivity is crucial to the success of the method and uncovering an interpretable evolution toward small-world network structures.","pdf":"/pdf/f3c912c1f178daaacb409191d7d2549263bf0eae.pdf","paperhash":"anonymous|adaptive_weight_sparsity_for_training_deep_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Weight Sparsity for Training Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkj1sW-Ab}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper707/Authors"],"keywords":["deep learning","sparsity","adaptive methods"]}},{"tddate":null,"ddate":null,"tmdate":1515642495117,"tcdate":1511824493993,"number":1,"cdate":1511824493993,"id":"Hk8unf5gG","invitation":"ICLR.cc/2018/Conference/-/Paper707/Official_Review","forum":"Bkj1sW-Ab","replyto":"Bkj1sW-Ab","signatures":["ICLR.cc/2018/Conference/Paper707/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A heuristic is developed to prune or activate neural network links during learning. Although interesting, I felt that the algorithm was not justified in terms of a principled optimisation procedure and the empirical validation was not completely convincing.  ","rating":"5: Marginally below acceptance threshold","review":"Summary:\n\nA simple heuristic is introduced for adaptively learning sparse neural networks during training. A pruning step probabilistically removes weights according to a sigmoid function with smaller magnitude weights more likely to be removed. A weight activation step introduces new weights with a probability dependent on the gradient of the cost function with respect to the introduced weights. These pruning and activation steps are combined with parameter learning to learn the structure and weights simultaneously. The methods is compared to a simple pruning approach, dense networks and randomly sparse networks in a number of architectures and tasks. \n\nPros: \n\nAdapting structure and weights simultaneously appears to improve learning in some cases and is a promising direction for research. \n\nThe algorithm is quite simple and should therefore be straightforward to implement. \n\nStudying the network structure after learning is an interesting line of research. \n\nCons:\n\nThe pruning and activation steps are heuristic and there is no explicit optimisation problem stated. Therefore it is difficult to justify this particular choice of pruning/activation algorithm in a principled manner. For example, the cost function is not used in the pruning step but it is used in the activation step. Some intuition as to why this is the best choice would be useful. \n\nThere is no consideration of treating sets of weights together as in drop-out approaches to sparsity, for example. \n\nBecause of the heuristic nature of the algorithm it would be important to consider a very extensive empirical study. In the study presented the approach provides good results in some cases, and is less good in other cases. It’s not really clear to me how general the conclusions can be here – perhaps the performance on different datasets with similar architectures is different? Only one other simple pruning algorithm is compared to so other methods for introducing sparsity are not thoroughly benchmarked. \n\nIn the learned connectivity patterns section it would be interesting to also compare to methods based on pruning after learning a dense network. Is it learning the sparsity during optimisation that determines these patterns or is a general sparse network with good performance going to have these network properties? \n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Weight Sparsity for Training Deep Neural Networks","abstract":"We introduce adaptive weight sparsity, an algorithm that allows a neural network to learn a sparse connection pattern during training. We demonstrate that the proposed algorithm shows performance benefits across a wide variety of tasks and network structures, improving state-of-the-art results for recurrent networks of comparable size. We show that adaptive weight sparsity outperforms traditional pruning-based approaches to learning sparse configurations on convolutional and recurrent networks.  We offer insights into the algorithm's behavior, demonstrating that training-time adaptivity is crucial to the success of the method and uncovering an interpretable evolution toward small-world network structures.","pdf":"/pdf/f3c912c1f178daaacb409191d7d2549263bf0eae.pdf","paperhash":"anonymous|adaptive_weight_sparsity_for_training_deep_neural_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Weight Sparsity for Training Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bkj1sW-Ab}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper707/Authors"],"keywords":["deep learning","sparsity","adaptive methods"]}}],"limit":2000,"offset":0}