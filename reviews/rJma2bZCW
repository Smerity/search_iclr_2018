{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222735655,"tcdate":1511816210660,"number":3,"cdate":1511816210660,"id":"H19fnlceG","invitation":"ICLR.cc/2018/Conference/-/Paper722/Official_Review","forum":"rJma2bZCW","replyto":"rJma2bZCW","signatures":["ICLR.cc/2018/Conference/Paper722/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Theory not particularly novel, experiments okay.","rating":"5: Marginally below acceptance threshold","review":"The authors study SGD as a stochastic differential equation and use the Fokker planck equation from statistical physics to derive the stationary distribution under standard assumptions. Under a (somewhat strong) local convexity assumption, they derive the probability of arriving at a local minimum, in terms of the batchsize, learning rate and determinant of the hessian.\n\nThe theory in section 3 is described clearly, although it is largely known. The use of the Fokker Planck equation for stationary distributions of stochastic SDEs has seen wide use in the machine learning literature over the last few years, and this paper does not add any novel insights to that. For example, the proof of Theorem 1 in Appendix C is boilerplate. Also, though it may be relatively new to the deep learning/ML community, I don't see the need to derive the F-P equation in Appendix A.\n\nTheorem 2 uses a fairly strong locally convex assumption, and uses a straightforward taylor expansion at a local minimum. It should be noted that the proof in Appendix D assumes that the covariance of the noise is constant in some interval around the minimum; I think this is again a strong assumption and should be included in the statement of Theorem 2.\n\nThere are some detailed experiments showing the effect of the learning rate and batchsize on the noise and therefore performance of SGD, but the only real insight that the authors provide is that the ratio of learning rate to batchsize controls the noise, as opposed to the that of l.r. to sqrt(batchsize). I wish this were analyzed in more detail.\n\nOverall I think the paper is borderline; the lack of real novelty makes it marginally below threshold in my view.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Three factors influencing minima in SGD","abstract":"We focus on the importance of noise in stochastic gradient descent (SGD) based training of deep neural networks (DNNs). We develop theory that studies SGD training as a stochastic differential equation and show that its stationary distribution is related to the loss surface. Our analysis suggests that the combination of batch size, learning rate, and the variance of the true loss gradients acts as a hyper- parameter steering the behavior of SGD and determines the trade-offs between the depth and width of the minima that SGD converges to. In a nutshell, a higher ratio of learning rate to batch size leads to wider minima. We validate our theory by examining the correlation between these three factors and the final performance and sharpness of the minimum found. As a verification of our theory, we empirically demonstrate that the learning dynamics is similar between experiments with different learning rates and batch sizes in SGD if the ratio of learning rate to batch size is the same.","pdf":"/pdf/a2fdeb772e9f17e08aaf0b0a80261b0021df3feb.pdf","TL;DR":"Three factors (batch size, learning rate, gradient noise) change in predictable way properties (e.g. sharpness) of found minima by SGD.","paperhash":"anonymous|three_factors_influencing_minima_in_sgd","_bibtex":"@article{\n  anonymous2018three,\n  title={Three factors influencing minima in SGD},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJma2bZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper722/Authors"],"keywords":["SGD","Deep Learning","Generalization"]}},{"tddate":null,"ddate":null,"tmdate":1512222735704,"tcdate":1511814405769,"number":2,"cdate":1511814405769,"id":"BkC-HgcxG","invitation":"ICLR.cc/2018/Conference/-/Paper722/Official_Review","forum":"rJma2bZCW","replyto":"rJma2bZCW","signatures":["ICLR.cc/2018/Conference/Paper722/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper, however not convincing theoretical results","rating":"3: Clear rejection","review":"In this paper, the authors present an analysis of SGD within an SDE framework. The ideas and the presented results are interesting and are clearly of interest to the deep learning community. The paper is well-written overall.\n\nHowever, the paper has important problems. \n\n1) The analysis is widely based on the recent paper by Mandt et al. While being an interesting work on its own, the assumptions made in that paper are very strict and not very realistic. For instance, the assumption that the stochastic gradient noise being Gaussian is very restrictive and trying to justify it just by the usual CLT is not convincing especially when the parameter space is extremely large, the setting that is considered in the paper.\n\n2) There is a mistake in the proof Theorem 1. Even with the assumption that the gradient of sigma is bounded, eq 20 cannot be justified and the equality can only be \"approximately equal to\". The result will only hold if sigma does not depend on theta. However, letting sigma depend on theta is the only difference from Mandt et al. On the other hand, with constant sigma the result is very trivial and can be found in any text book on SDEs (showing the Gibbs distribution). Therefore, presenting it as a new result is misleading.  \n\n3) Even if the sigma is taken constant and theorem 1 is corrected, I don't think theorem 2 is conclusive. Theorem 2 basically assumes that the distribution is locally a proper Gaussian (it is stated as locally convex, however it is taken as quadratic) and the result just boils down to computing some probability under a Gaussian distribution, which is still quite trivial. Apart from this assumption not being very realistic, the result does not justify the claims on \"the probability of ending in a certain minimum\" -- which is on the other hand a vague statement. First of all \"ending in\" a certain area depends on many different factors, such as the structure of the distribution, the initial point, the distance between the modes etc. Also it is not very surprising that the inverse image of a wider Gaussian density is larger than of a pointy one. This again does not justify the claims. For instance consider a GMM with two components, where the means of the individual components are close to each other, but one component having a very large variance and a smaller weight, and the other one having a lower variance and higher weight. With authors' claim, the algorithm should spend more time on the wider one, however it is evident that this will not be the case. \n\n4) There is a conceptual mistake that the authors assume that SGD will attain the exact stationary distribution even when the SDE is simulated by the fixed step-size Euler integrator. As soon as one uses eta>0 the algorithm will never attain the stationary distribution of the continuous-time process, but will attain a stationary distribution that is close to the ideal one (of course with several smoothness, growth assumptions). The error between the ideal distribution and the empirical distribution will be usually O(eta) depending on the assumption and therefore changing eta will result in a different distribution than the ideal one. With this in mind the stationary distributions for (eta/S) and (2eta/2S) will be clearly different. \n\n\nThe experiments are very interesting and I do not underestimate their value. However, the current analysis unfortunately does not properly explain the rather strong claims of the authors, which is supposed to be the main contribution of this paper. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Three factors influencing minima in SGD","abstract":"We focus on the importance of noise in stochastic gradient descent (SGD) based training of deep neural networks (DNNs). We develop theory that studies SGD training as a stochastic differential equation and show that its stationary distribution is related to the loss surface. Our analysis suggests that the combination of batch size, learning rate, and the variance of the true loss gradients acts as a hyper- parameter steering the behavior of SGD and determines the trade-offs between the depth and width of the minima that SGD converges to. In a nutshell, a higher ratio of learning rate to batch size leads to wider minima. We validate our theory by examining the correlation between these three factors and the final performance and sharpness of the minimum found. As a verification of our theory, we empirically demonstrate that the learning dynamics is similar between experiments with different learning rates and batch sizes in SGD if the ratio of learning rate to batch size is the same.","pdf":"/pdf/a2fdeb772e9f17e08aaf0b0a80261b0021df3feb.pdf","TL;DR":"Three factors (batch size, learning rate, gradient noise) change in predictable way properties (e.g. sharpness) of found minima by SGD.","paperhash":"anonymous|three_factors_influencing_minima_in_sgd","_bibtex":"@article{\n  anonymous2018three,\n  title={Three factors influencing minima in SGD},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJma2bZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper722/Authors"],"keywords":["SGD","Deep Learning","Generalization"]}},{"tddate":null,"ddate":null,"tmdate":1512222735747,"tcdate":1511730908761,"number":1,"cdate":1511730908761,"id":"ByBJy2Oef","invitation":"ICLR.cc/2018/Conference/-/Paper722/Official_Review","forum":"rJma2bZCW","replyto":"rJma2bZCW","signatures":["ICLR.cc/2018/Conference/Paper722/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A stability analysis of local optima in constant-rate SGD","rating":"6: Marginally above acceptance threshold","review":"The paper investigates how the learning rate and mini-batch size in SGD impacts the optima that the SGD algorithm finds.\nEmpirically, the authors argue that it was observed that larger learning rates converge to minima which are more wide,\nand that smaller learning rates more often lead to convergence to minima which are narrower, i.e. where the Hessian has large Eigenvalues. In this paper, the authors derive an analytical theory that aims at explaining this phenomenon.\n\nPoint of departure is an analytical theory proposed by Mandt et al., where SGD is analyzed in a continuous-time stochastic\nformalism. In more detail, a stochastic differential equation is derived which mimicks the behavior of SGD. The advantage of\nthis theory is that under specific assumptions, analytic stationary distributions can be derived. While Mandt et al. focused\non the vicinity of a local optima, the authors of the present paper assumed white diagonal gradient noise, which allows to\nderive an analytic, *global* stationary distribution (this is similar as in Langevin dynamics).\n\nThen, the authors focus again on individual local optima and \"integrate out\" the stationary distribution around a local optimum, using again a Gaussian assumption. As a result, the authors obtain un-normalized probabilities of getting trapped in a given local optimum. This un-normalized probability depends on the strength of the value of the loss function in the vicinity of the optimum, the gradient noise, and the width of the optima. In the end, these un-normalized probabilities are taken as\nprobabilities that the SGD algorithm will be trapped around the given optimum in finite time.\n\n\nOverall assessment:\nI find the analytical results of the paper very original and interesting. The experimental part has some weaknesses. The paper could be drastically improved when focusing on the experimental part.\n\nDetailed comments:\n\nRegarding the analytical part, I think this is all very nice and original. However, I have some comments/requests:\n\n1. Since the authors focus around Gaussian regions around the local minima, perhaps the diagonal white noise assumption could be weakened. This is again the multivariate Ornstein-Uhlenbeck setup examined in Mandt et al., and probably possesses an analytical solution for the un-normalized probabilities (even if the noise is multivariate Gaussian). Would the authors to consider generalizing the proof for the camera-ready version perhaps?\n\n2. It would be nice to sketch the proof of theorem 2 in the main paper, rather than to just refer to the appendix. In my opinion, the theorem results from a beautiful and instructive calculation that should provide the reader with some intuition.\n\n3. Would the authors comment on the underlying theoretical assumptions a bit more? In particular, the stationary distribution predicted by the Ornstein-Uhlenbeck formalism is never reached in practice. When using SGD in practice, one is in the initial mode-seeking phase. So, why is it a reasonable assumption to still use results obtained from the stationary (equilibrated) distribution which is never reached?\n\n\nRegarding the experiments: here I see a few problems. First, the writing style drops in quality. Second, figures 2 and 3 are cryptic. Why do the authors focus on two manually selected optima? In which sense is this statistically significant? How often were the experiments repeated? The figures are furthermore hard to read. I would recommend overhauling the entire experiments section.\n\nDetails:\n\n- Typo in Figure 2: ”with different with different”.\n- “the endpoint of SGD with a learning rate schedule η → η/a, for some a > 0, and a constant batch size S, should be the same\n  as the endpoint of SGD with a constant learning rate and a batch size schedule S → aS.” This is clearly wrong as there are many local minima, and running teh algorithm twice results in different local optima.  Maybe add something that this only true on average, like “the characteristics of these minima ... should be the same”.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Three factors influencing minima in SGD","abstract":"We focus on the importance of noise in stochastic gradient descent (SGD) based training of deep neural networks (DNNs). We develop theory that studies SGD training as a stochastic differential equation and show that its stationary distribution is related to the loss surface. Our analysis suggests that the combination of batch size, learning rate, and the variance of the true loss gradients acts as a hyper- parameter steering the behavior of SGD and determines the trade-offs between the depth and width of the minima that SGD converges to. In a nutshell, a higher ratio of learning rate to batch size leads to wider minima. We validate our theory by examining the correlation between these three factors and the final performance and sharpness of the minimum found. As a verification of our theory, we empirically demonstrate that the learning dynamics is similar between experiments with different learning rates and batch sizes in SGD if the ratio of learning rate to batch size is the same.","pdf":"/pdf/a2fdeb772e9f17e08aaf0b0a80261b0021df3feb.pdf","TL;DR":"Three factors (batch size, learning rate, gradient noise) change in predictable way properties (e.g. sharpness) of found minima by SGD.","paperhash":"anonymous|three_factors_influencing_minima_in_sgd","_bibtex":"@article{\n  anonymous2018three,\n  title={Three factors influencing minima in SGD},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJma2bZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper722/Authors"],"keywords":["SGD","Deep Learning","Generalization"]}},{"tddate":null,"ddate":null,"tmdate":1509739140559,"tcdate":1509133498744,"number":722,"cdate":1509739137888,"id":"rJma2bZCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJma2bZCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Three factors influencing minima in SGD","abstract":"We focus on the importance of noise in stochastic gradient descent (SGD) based training of deep neural networks (DNNs). We develop theory that studies SGD training as a stochastic differential equation and show that its stationary distribution is related to the loss surface. Our analysis suggests that the combination of batch size, learning rate, and the variance of the true loss gradients acts as a hyper- parameter steering the behavior of SGD and determines the trade-offs between the depth and width of the minima that SGD converges to. In a nutshell, a higher ratio of learning rate to batch size leads to wider minima. We validate our theory by examining the correlation between these three factors and the final performance and sharpness of the minimum found. As a verification of our theory, we empirically demonstrate that the learning dynamics is similar between experiments with different learning rates and batch sizes in SGD if the ratio of learning rate to batch size is the same.","pdf":"/pdf/a2fdeb772e9f17e08aaf0b0a80261b0021df3feb.pdf","TL;DR":"Three factors (batch size, learning rate, gradient noise) change in predictable way properties (e.g. sharpness) of found minima by SGD.","paperhash":"anonymous|three_factors_influencing_minima_in_sgd","_bibtex":"@article{\n  anonymous2018three,\n  title={Three factors influencing minima in SGD},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJma2bZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper722/Authors"],"keywords":["SGD","Deep Learning","Generalization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}