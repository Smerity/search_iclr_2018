{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222633941,"tcdate":1511821696948,"number":3,"cdate":1511821696948,"id":"B1KY-MqgG","invitation":"ICLR.cc/2018/Conference/-/Paper391/Official_Review","forum":"SkfNU2e0Z","replyto":"SkfNU2e0Z","signatures":["ICLR.cc/2018/Conference/Paper391/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of \"STATESTREAM: A TOOLBOX TO EXPLORE LAYERWISE-PARALLEL DEEP NEURAL NETWORKS\"","rating":"5: Marginally below acceptance threshold","review":"In this paper, the authors present an open-source toolbox to explore layerwise-parallel deep neural networks. They offer an interesting and detailed comparison of the temporal progression of layerwise-parallel and layerwise-sequential networks, and differences that can emerge in the results of these two computation strategies.\n\nWhile the open-source toolbox introduced in this paper can be an excellent resource for the community interested in exploring these networks, the present submission offers relatively few results actually using these networks in practice. In order to make a more compelling case for these networks, the present submission could include more detailed investigations, perhaps demonstrating that they learn differently or better than other implementations on standard training sets.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Statestream: A toolbox to explore layerwise-parallel deep neural networks","abstract":"Building deep neural networks to control autonomous agents which have to interact in real-time with the physical world, such as robots or automotive vehicles, requires a seamless integration of time into a network’s architecture. The central question of this work is, how the temporal nature of reality should be reflected in the execution of a deep neural network and its components. Most artificial deep neural networks are partitioned into a directed graph of connected modules or layers and the layers themselves consist of elemental building blocks, such as single units. For most deep neural networks, all units of a layer are processed synchronously and in parallel, but layers themselves are processed in a sequential manner. In contrast, all elements of a biological neural network are processed in parallel. In this paper, we define a class of networks between these two extreme cases. These networks are executed in a streaming or synchronous layerwise-parallel manner, unlocking the layers of such networks for parallel processing. Compared to the standard layerwise-sequential deep networks, these new layerwise-parallel networks show a fundamentally different temporal behavior and flow of information, especially for networks with skip or recurrent connections. We argue that layerwise-parallel deep networks are better suited for future challenges of deep neural network design, such as large functional modularized and/or recurrent architectures as well as networks allocating different network capacities dependent on current stimulus and/or task complexity. We layout basic properties and discuss major challenges for layerwise-parallel networks. Additionally, we provide a toolbox to design, train, evaluate, and online-interact with layerwise-parallel networks.","pdf":"/pdf/bdaddd8d4654234c4a3d112de09a655ee0733636.pdf","TL;DR":"We define a concept of layerwise model-parallel deep neural networks, for which layers operate in parallel, and provide a toolbox to design, train, evaluate, and on-line interact with these networks.","paperhash":"anonymous|statestream_a_toolbox_to_explore_layerwiseparallel_deep_neural_networks","_bibtex":"@article{\n  anonymous2018statestream:,\n  title={Statestream: A toolbox to explore layerwise-parallel deep neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkfNU2e0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper391/Authors"],"keywords":["model-parallel","parallelization","software platform"]}},{"tddate":null,"ddate":null,"tmdate":1512222633982,"tcdate":1511778616475,"number":2,"cdate":1511778616475,"id":"HkeBFwYgf","invitation":"ICLR.cc/2018/Conference/-/Paper391/Official_Review","forum":"SkfNU2e0Z","replyto":"SkfNU2e0Z","signatures":["ICLR.cc/2018/Conference/Paper391/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A potentially interesting toolbox not supported by enough examples","rating":"5: Marginally below acceptance threshold","review":"This paper introduces a new toolbox for deep neural networks learning and evaluation. The central idea is to include time in the processing of all the units in the network. For this, the authors propose a paradigm switch: form layerwise-sequential networks, where at every time frame the network is evaluated by updating each layer – from bottom to top – sequentially; to layerwise-parallel networks, where all the neurons are updated in parallel. The new paradigm implies that the layer update is achieved by using the stored previous state and the corresponding previous state of the previous layer. This has three consequences. First, every layer now use memory, a condition that already applies for RNNs in layerwise-sequential networks. Second, in order to have a consistent output, the information has to flow in the network for a number of time frames equal to the number of layers. In Neuroscience, this concept is known as reaction time. Third, since the network is not synchronized in terms of the information that is processed in a specific time frame, there are discrepancies w.r.t. the layerwise-sequential networks computation: all the techniques used to train deep NNs have to be reconsidered. \n\nOverall, the concept is interesting and timely especially for the rising field of spiking neural networks or for large and distributed architectures. The paper, however, should probably provide more examples and results in terms of architectures that can been implemented with the toolbox in comparison with other toolboxes. The paper presents a single example in which either the accuracy and the training time are not reported. While I understand that the main result of this work is the toolbox itself, more examples and results would improve the clarity and the implications for such paradigm switch. Another concern comes from the choice to use Theano as back-end, since it's known that it is going to be discontinued. Finally I suggest to improve the clarity and description of Figure 2, which is messy and confusing especially if printed in B&W. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Statestream: A toolbox to explore layerwise-parallel deep neural networks","abstract":"Building deep neural networks to control autonomous agents which have to interact in real-time with the physical world, such as robots or automotive vehicles, requires a seamless integration of time into a network’s architecture. The central question of this work is, how the temporal nature of reality should be reflected in the execution of a deep neural network and its components. Most artificial deep neural networks are partitioned into a directed graph of connected modules or layers and the layers themselves consist of elemental building blocks, such as single units. For most deep neural networks, all units of a layer are processed synchronously and in parallel, but layers themselves are processed in a sequential manner. In contrast, all elements of a biological neural network are processed in parallel. In this paper, we define a class of networks between these two extreme cases. These networks are executed in a streaming or synchronous layerwise-parallel manner, unlocking the layers of such networks for parallel processing. Compared to the standard layerwise-sequential deep networks, these new layerwise-parallel networks show a fundamentally different temporal behavior and flow of information, especially for networks with skip or recurrent connections. We argue that layerwise-parallel deep networks are better suited for future challenges of deep neural network design, such as large functional modularized and/or recurrent architectures as well as networks allocating different network capacities dependent on current stimulus and/or task complexity. We layout basic properties and discuss major challenges for layerwise-parallel networks. Additionally, we provide a toolbox to design, train, evaluate, and online-interact with layerwise-parallel networks.","pdf":"/pdf/bdaddd8d4654234c4a3d112de09a655ee0733636.pdf","TL;DR":"We define a concept of layerwise model-parallel deep neural networks, for which layers operate in parallel, and provide a toolbox to design, train, evaluate, and on-line interact with these networks.","paperhash":"anonymous|statestream_a_toolbox_to_explore_layerwiseparallel_deep_neural_networks","_bibtex":"@article{\n  anonymous2018statestream:,\n  title={Statestream: A toolbox to explore layerwise-parallel deep neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkfNU2e0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper391/Authors"],"keywords":["model-parallel","parallelization","software platform"]}},{"tddate":null,"ddate":null,"tmdate":1512222634026,"tcdate":1511770999029,"number":1,"cdate":1511770999029,"id":"SJ1tsSFgf","invitation":"ICLR.cc/2018/Conference/-/Paper391/Official_Review","forum":"SkfNU2e0Z","replyto":"SkfNU2e0Z","signatures":["ICLR.cc/2018/Conference/Paper391/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper describes a toolbox for parallel neuron updating written in Theano.","rating":"3: Clear rejection","review":"Quality and clarity\n\nThe paper goes to some length to explain that update order in a neural network matters in the sense that different update orders give different results. While standard CNN like architectures are fine with the layer parallel updating process typically used in standard tools, for recurrent networks and also for networks with connections that skip layers, different update orders may be more natural, but no GPU-accelerated toolboxes exist that support this. The authors provide such a toolbox, statestream, written Theano.\n\nThe paper's structure is reasonably clear, though the text has very poor \"flow\": the english could use a native speaker straightening out the text. For example, a number of times there are phrases like \"previously mentioned\", which is ugly. \n\nMy main issue is with the significance of the work. There are no results in the paper that demonstrate a case where it is useful to apply fully parallel updates. As such, it is hard to see the value of the contribution, also since the toolbox is written in Theano for which support has been discontinued. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Statestream: A toolbox to explore layerwise-parallel deep neural networks","abstract":"Building deep neural networks to control autonomous agents which have to interact in real-time with the physical world, such as robots or automotive vehicles, requires a seamless integration of time into a network’s architecture. The central question of this work is, how the temporal nature of reality should be reflected in the execution of a deep neural network and its components. Most artificial deep neural networks are partitioned into a directed graph of connected modules or layers and the layers themselves consist of elemental building blocks, such as single units. For most deep neural networks, all units of a layer are processed synchronously and in parallel, but layers themselves are processed in a sequential manner. In contrast, all elements of a biological neural network are processed in parallel. In this paper, we define a class of networks between these two extreme cases. These networks are executed in a streaming or synchronous layerwise-parallel manner, unlocking the layers of such networks for parallel processing. Compared to the standard layerwise-sequential deep networks, these new layerwise-parallel networks show a fundamentally different temporal behavior and flow of information, especially for networks with skip or recurrent connections. We argue that layerwise-parallel deep networks are better suited for future challenges of deep neural network design, such as large functional modularized and/or recurrent architectures as well as networks allocating different network capacities dependent on current stimulus and/or task complexity. We layout basic properties and discuss major challenges for layerwise-parallel networks. Additionally, we provide a toolbox to design, train, evaluate, and online-interact with layerwise-parallel networks.","pdf":"/pdf/bdaddd8d4654234c4a3d112de09a655ee0733636.pdf","TL;DR":"We define a concept of layerwise model-parallel deep neural networks, for which layers operate in parallel, and provide a toolbox to design, train, evaluate, and on-line interact with these networks.","paperhash":"anonymous|statestream_a_toolbox_to_explore_layerwiseparallel_deep_neural_networks","_bibtex":"@article{\n  anonymous2018statestream:,\n  title={Statestream: A toolbox to explore layerwise-parallel deep neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkfNU2e0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper391/Authors"],"keywords":["model-parallel","parallelization","software platform"]}},{"tddate":null,"ddate":null,"tmdate":1509739329067,"tcdate":1509111337606,"number":391,"cdate":1509739326401,"id":"SkfNU2e0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkfNU2e0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Statestream: A toolbox to explore layerwise-parallel deep neural networks","abstract":"Building deep neural networks to control autonomous agents which have to interact in real-time with the physical world, such as robots or automotive vehicles, requires a seamless integration of time into a network’s architecture. The central question of this work is, how the temporal nature of reality should be reflected in the execution of a deep neural network and its components. Most artificial deep neural networks are partitioned into a directed graph of connected modules or layers and the layers themselves consist of elemental building blocks, such as single units. For most deep neural networks, all units of a layer are processed synchronously and in parallel, but layers themselves are processed in a sequential manner. In contrast, all elements of a biological neural network are processed in parallel. In this paper, we define a class of networks between these two extreme cases. These networks are executed in a streaming or synchronous layerwise-parallel manner, unlocking the layers of such networks for parallel processing. Compared to the standard layerwise-sequential deep networks, these new layerwise-parallel networks show a fundamentally different temporal behavior and flow of information, especially for networks with skip or recurrent connections. We argue that layerwise-parallel deep networks are better suited for future challenges of deep neural network design, such as large functional modularized and/or recurrent architectures as well as networks allocating different network capacities dependent on current stimulus and/or task complexity. We layout basic properties and discuss major challenges for layerwise-parallel networks. Additionally, we provide a toolbox to design, train, evaluate, and online-interact with layerwise-parallel networks.","pdf":"/pdf/bdaddd8d4654234c4a3d112de09a655ee0733636.pdf","TL;DR":"We define a concept of layerwise model-parallel deep neural networks, for which layers operate in parallel, and provide a toolbox to design, train, evaluate, and on-line interact with these networks.","paperhash":"anonymous|statestream_a_toolbox_to_explore_layerwiseparallel_deep_neural_networks","_bibtex":"@article{\n  anonymous2018statestream:,\n  title={Statestream: A toolbox to explore layerwise-parallel deep neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkfNU2e0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper391/Authors"],"keywords":["model-parallel","parallelization","software platform"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}