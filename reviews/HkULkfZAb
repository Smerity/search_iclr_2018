{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222743823,"tcdate":1511813124902,"number":2,"cdate":1511813124902,"id":"S1T-ex5eM","invitation":"ICLR.cc/2018/Conference/-/Paper753/Official_Review","forum":"HkULkfZAb","replyto":"HkULkfZAb","signatures":["ICLR.cc/2018/Conference/Paper753/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Potentially interesting idea. Writing and experiment are very weak.","rating":"4: Ok but not good enough - rejection","review":"The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system. Because the layer-wise decoupling, it can easily be applied for distributed training of the model. The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results.\n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly, it is a shame to see the overall quality of the paper very weak. The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP). The experiments are also very weak. Important information on the experiment settings are missing, e.g., how the model is parallelized.\n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD. It needs to be better defined.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Propagation: Layer-Parallel Training","abstract":"Continuous propagation is a parallel technique for training deep neural networks with batch size one at full utilization of a multiprocessor system. It enables spatially distributed computations on emerging deep learning hardware accelerators that do not impose programming limitations of contemporary GPUs. The algorithm achieves model parallelism along the depth of a deep network. The method is based on the continuous representation of the optimization process and enables sustained gradient generation during all phases of computation. We demonstrate that in addition to its increased concurrency, continuous propagation improves the convergence rate of state of the art methods while matching their accuracy. ","pdf":"/pdf/5c86c5ec8452c4b05988f396d7a353a6c190d66d.pdf","paperhash":"anonymous|continuous_propagation_layerparallel_training","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Propagation: Layer-Parallel Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkULkfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper753/Authors"],"keywords":["Deep Learning","Model parallelism","Learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1512222743864,"tcdate":1511360636338,"number":1,"cdate":1511360636338,"id":"HJ4t_WmlM","invitation":"ICLR.cc/2018/Conference/-/Paper753/Official_Review","forum":"HkULkfZAb","replyto":"HkULkfZAb","signatures":["ICLR.cc/2018/Conference/Paper753/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Promising idea, but needs more demonstration of practicality","rating":"5: Marginally below acceptance threshold","review":"Pros:\n* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models\n* the paper is clearly written and easy to understand\n\nCons:\n* weak experiments: performance of algorithms are not analyzed in terms of wall-clock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm\n* weak theory: although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent, neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice; the proof is straightforward corollary of Lin et al.\n\nSummary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously. Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers. While asynchronous updates of parameters in stochastic gradient descent has been explored (dating back to [1] in 1986, and authors should also be referring to [2]), to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studied. Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine.\n\nUnfortunately, the practical usefulness of the algorithm has not been demonstrated. It remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures, or in which situations this algorithm will be more useful than existing algorithms. Experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm. What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs. Also, authors should compare against mini-batch gradient descent, because this is the most popular way of training deep neural networks; authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method.\n\nAuthors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent, but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm. The algorithm operates in discrete time horizon, and continuous time is not used anywhere. Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon.\n\nAuthors argue in page 1 that Continuous Propagation is statistically superior to mini-batch gradient descent, but I cannot find statistical superiority of the method. Also, the upper bound of the time-delay T slows down the convergence rate (Proposition in the appendix), so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient descent. I think which algorithm is faster depends on values of L, T and M.\n\nAuthors do not provide enough citations. Continuous-time characterization of gradient descent has a long history, and authors should provide citation of it, for example when (5) is introduced. Authors should provide more discussion of the history of model-parallel asynchronous SGD (such as [1] and [2]), and when mentioning alternatives like Czarnecki et al (2017), authors should discuss what advantages and disadvantages the proposed algorithm has against these alternatives.\n\n\n[1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986)\n[2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Propagation: Layer-Parallel Training","abstract":"Continuous propagation is a parallel technique for training deep neural networks with batch size one at full utilization of a multiprocessor system. It enables spatially distributed computations on emerging deep learning hardware accelerators that do not impose programming limitations of contemporary GPUs. The algorithm achieves model parallelism along the depth of a deep network. The method is based on the continuous representation of the optimization process and enables sustained gradient generation during all phases of computation. We demonstrate that in addition to its increased concurrency, continuous propagation improves the convergence rate of state of the art methods while matching their accuracy. ","pdf":"/pdf/5c86c5ec8452c4b05988f396d7a353a6c190d66d.pdf","paperhash":"anonymous|continuous_propagation_layerparallel_training","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Propagation: Layer-Parallel Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkULkfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper753/Authors"],"keywords":["Deep Learning","Model parallelism","Learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1509739122540,"tcdate":1509134158027,"number":753,"cdate":1509739119885,"id":"HkULkfZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkULkfZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Continuous Propagation: Layer-Parallel Training","abstract":"Continuous propagation is a parallel technique for training deep neural networks with batch size one at full utilization of a multiprocessor system. It enables spatially distributed computations on emerging deep learning hardware accelerators that do not impose programming limitations of contemporary GPUs. The algorithm achieves model parallelism along the depth of a deep network. The method is based on the continuous representation of the optimization process and enables sustained gradient generation during all phases of computation. We demonstrate that in addition to its increased concurrency, continuous propagation improves the convergence rate of state of the art methods while matching their accuracy. ","pdf":"/pdf/5c86c5ec8452c4b05988f396d7a353a6c190d66d.pdf","paperhash":"anonymous|continuous_propagation_layerparallel_training","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Propagation: Layer-Parallel Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkULkfZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper753/Authors"],"keywords":["Deep Learning","Model parallelism","Learning theory"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}