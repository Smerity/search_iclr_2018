{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642503290,"tcdate":1512267655607,"number":3,"cdate":1512267655607,"id":"HJgcJyWWM","invitation":"ICLR.cc/2018/Conference/-/Paper753/Official_Review","forum":"HkULkfZAb","replyto":"HkULkfZAb","signatures":["ICLR.cc/2018/Conference/Paper753/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Poorly described, lack of precision and mathematical rigor, crucial mistakes in the proof.","rating":"3: Clear rejection","review":"# Summary of paper\nThe authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples.\n\n# Summary of review\nThe idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem. However, this contribution is far from correctly addressing the problem. The Algorithm is poorly described and crucial parts of the algorithm are very confusing. Mathematical rigor in the proof and discussion is lacking. Proof has mathematical errors. \n\n# Detailed comments\n* Key definitions are scattered across the paper, making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a variable. To make things worse, some variables are simply not defined. For example, I can't find the definition of D. From the context it seems to be the number of layers in the network (I shouldn't need to guess). \n\n* From Algorithm 1, the bracket notation is used for both indexing and specifying the size of the variables? This is nonstandard and confusing.\n\n* Again, from Algorithm 1, it is not clear which parts can be performed asynchronously. It is even not clear to me if the algorithm can be run asynchronously (as some of the other reviewers seem to imply) or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their \"continuous-propagation\" factorization? \n\n* Eq. (3) and (4): I doubt this is true without some assumptions on the distribution of the data generating process. \n\n* The proof, despite being a trivial application of existing work, has obvious flaws. After equation (17) it is stated that \"the left-hand side is independent of x_{k, m, l}\" which is not true since Theta_{k+1} is computed **precisely** using x_{k, m, l} and so is not independent (this is actually done correctly in Lian 2015, where the expectation is correctly carried on that term). \n\n* The proof relies on an inequality (16) in which key quantities are not defined (what is L? is L = L_d?) and which is impossible to verify in practice (T is not known). This crucial detail is only mentioned in the appendix, giving the impression in the main text that the algorithm is always convergent. It should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quantities.\n\n* As mentioned in the other reviews, key references are lacking, e.g., for ODE interpretation, Eq. (3) and (4).\n\nIn appendix:\n\n * Assumption 3, 4: Why is upper superindex d? In any case, be consistent, most of the time these are used but then its stated \"for all Theta\" (whithout superindex)\n * Proposition: what is L? is L = L_d? \n\n\nOther\n\n  * Assumption 5: decay -> delay?\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Continuous Propagation: Layer-Parallel Training","abstract":"Continuous propagation is a parallel technique for training deep neural networks with batch size one at full utilization of a multiprocessor system. It enables spatially distributed computations on emerging deep learning hardware accelerators that do not impose programming limitations of contemporary GPUs. The algorithm achieves model parallelism along the depth of a deep network. The method is based on the continuous representation of the optimization process and enables sustained gradient generation during all phases of computation. We demonstrate that in addition to its increased concurrency, continuous propagation improves the convergence rate of state of the art methods while matching their accuracy. ","pdf":"/pdf/5c86c5ec8452c4b05988f396d7a353a6c190d66d.pdf","paperhash":"anonymous|continuous_propagation_layerparallel_training","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Propagation: Layer-Parallel Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkULkfZAb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper753/Authors"],"keywords":["Deep Learning","Model parallelism","Learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642503327,"tcdate":1511813124902,"number":2,"cdate":1511813124902,"id":"S1T-ex5eM","invitation":"ICLR.cc/2018/Conference/-/Paper753/Official_Review","forum":"HkULkfZAb","replyto":"HkULkfZAb","signatures":["ICLR.cc/2018/Conference/Paper753/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Potentially interesting idea. Writing and experiment are very weak.","rating":"4: Ok but not good enough - rejection","review":"The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system. Because the layer-wise decoupling, it can easily be applied for distributed training of the model. The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results.\n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly, it is a shame to see the overall quality of the paper very weak. The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP). The experiments are also very weak. Important information on the experiment settings are missing, e.g., how the model is parallelized.\n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD. It needs to be better defined.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Propagation: Layer-Parallel Training","abstract":"Continuous propagation is a parallel technique for training deep neural networks with batch size one at full utilization of a multiprocessor system. It enables spatially distributed computations on emerging deep learning hardware accelerators that do not impose programming limitations of contemporary GPUs. The algorithm achieves model parallelism along the depth of a deep network. The method is based on the continuous representation of the optimization process and enables sustained gradient generation during all phases of computation. We demonstrate that in addition to its increased concurrency, continuous propagation improves the convergence rate of state of the art methods while matching their accuracy. ","pdf":"/pdf/5c86c5ec8452c4b05988f396d7a353a6c190d66d.pdf","paperhash":"anonymous|continuous_propagation_layerparallel_training","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Propagation: Layer-Parallel Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkULkfZAb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper753/Authors"],"keywords":["Deep Learning","Model parallelism","Learning theory"]}},{"tddate":null,"ddate":null,"tmdate":1515642503364,"tcdate":1511360636338,"number":1,"cdate":1511360636338,"id":"HJ4t_WmlM","invitation":"ICLR.cc/2018/Conference/-/Paper753/Official_Review","forum":"HkULkfZAb","replyto":"HkULkfZAb","signatures":["ICLR.cc/2018/Conference/Paper753/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Promising idea, but needs more demonstration of practicality","rating":"5: Marginally below acceptance threshold","review":"Pros:\n* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models\n* the paper is clearly written and easy to understand\n\nCons:\n* weak experiments: performance of algorithms are not analyzed in terms of wall-clock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm\n* weak theory: although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent, neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice; the proof is straightforward corollary of Lin et al.\n\nSummary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously. Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers. While asynchronous updates of parameters in stochastic gradient descent has been explored (dating back to [1] in 1986, and authors should also be referring to [2]), to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studied. Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine.\n\nUnfortunately, the practical usefulness of the algorithm has not been demonstrated. It remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures, or in which situations this algorithm will be more useful than existing algorithms. Experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm. What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs. Also, authors should compare against mini-batch gradient descent, because this is the most popular way of training deep neural networks; authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method.\n\nAuthors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent, but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm. The algorithm operates in discrete time horizon, and continuous time is not used anywhere. Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon.\n\nAuthors argue in page 1 that Continuous Propagation is statistically superior to mini-batch gradient descent, but I cannot find statistical superiority of the method. Also, the upper bound of the time-delay T slows down the convergence rate (Proposition in the appendix), so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient descent. I think which algorithm is faster depends on values of L, T and M.\n\nAuthors do not provide enough citations. Continuous-time characterization of gradient descent has a long history, and authors should provide citation of it, for example when (5) is introduced. Authors should provide more discussion of the history of model-parallel asynchronous SGD (such as [1] and [2]), and when mentioning alternatives like Czarnecki et al (2017), authors should discuss what advantages and disadvantages the proposed algorithm has against these alternatives.\n\n\n[1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986)\n[2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous Propagation: Layer-Parallel Training","abstract":"Continuous propagation is a parallel technique for training deep neural networks with batch size one at full utilization of a multiprocessor system. It enables spatially distributed computations on emerging deep learning hardware accelerators that do not impose programming limitations of contemporary GPUs. The algorithm achieves model parallelism along the depth of a deep network. The method is based on the continuous representation of the optimization process and enables sustained gradient generation during all phases of computation. We demonstrate that in addition to its increased concurrency, continuous propagation improves the convergence rate of state of the art methods while matching their accuracy. ","pdf":"/pdf/5c86c5ec8452c4b05988f396d7a353a6c190d66d.pdf","paperhash":"anonymous|continuous_propagation_layerparallel_training","_bibtex":"@article{\n  anonymous2018continuous,\n  title={Continuous Propagation: Layer-Parallel Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkULkfZAb}\n}","withdrawal":"Confirmed","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper753/Authors"],"keywords":["Deep Learning","Model parallelism","Learning theory"]}}],"limit":2000,"offset":0}