{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222654418,"tcdate":1512015575909,"number":3,"cdate":1512015575909,"id":"H1gkDZaeM","invitation":"ICLR.cc/2018/Conference/-/Paper446/Official_Review","forum":"SkFqf0lAZ","replyto":"SkFqf0lAZ","signatures":["ICLR.cc/2018/Conference/Paper446/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Nice contribution to memory augmented recurrent neural network ","rating":"8: Top 50% of accepted papers, clear accept","review":"The main contribution of this paper are:\n(a) a proposed extension to continuous stack model to allow multiple pop operation,\n(b) on a language model task, they demonstrate that their model gives better perplexity than comparable LSTM and attention model, and \n(c) on a syntactic task (non-local subject-verb agreement), again, they demonstrate better performance than comparable LSTM and attention model.\n\nAdditionally, the paper provides a nice introduction to the topic and casts the current models into three categories -- the sequential memory access, the random memory access and the stack memory access models. \n\nTheir analysis in section (3.4) using the Venn diagram and illustrative figures in (3), (4) and (5) provide useful insight into the performance of the model.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Architectures in Recurrent Neural Network Language Models","abstract":"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.","pdf":"/pdf/aca2fc020369672734d80003b459eda5ff483c79.pdf","paperhash":"anonymous|memory_architectures_in_recurrent_neural_network_language_models","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Architectures in Recurrent Neural Network Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFqf0lAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper446/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222654454,"tcdate":1511858740958,"number":2,"cdate":1511858740958,"id":"SkTEzjqgf","invitation":"ICLR.cc/2018/Conference/-/Paper446/Official_Review","forum":"SkFqf0lAZ","replyto":"SkFqf0lAZ","signatures":["ICLR.cc/2018/Conference/Paper446/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors propose a new stack augmented recurrent neural network, which supports continuous push, stay and a variable number of pop operations at each time step. They need to test the model on large corpora.","rating":"5: Marginally below acceptance threshold","review":"The authors propose a new stack augmented recurrent neural network, which supports continuous push, stay and a variable number of pop operations at each time step. They thoroughly compare several typical neural language models (LSTM, LSTM+attention mechanism, etc.), and demonstrate the power of the stack baed recurrent neural network language model in the similar parameter scale with other models, and especially show the superiority when the long-range dependencies are more complex in NLP area.\n\nHowever the corpora they choose to test the ideas, are PTB and Wikitext-2, they're quite small, so the variance of the estimate is high, similar conclusions might not be valid on large corpora such as 1B token benchmark corpus. \n\nTable 1 only gives results with the same level of parameters, the ppls are worse than some other models. Another angle might be the proposed model use the similar size of hidden layer 1500 plus the stack, and see how much ppl reductions it could get.\n\nFinally the authors should do some experiments on machine translation or speech recognition and see whether the model could get performance improvement.\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Architectures in Recurrent Neural Network Language Models","abstract":"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.","pdf":"/pdf/aca2fc020369672734d80003b459eda5ff483c79.pdf","paperhash":"anonymous|memory_architectures_in_recurrent_neural_network_language_models","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Architectures in Recurrent Neural Network Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFqf0lAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper446/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222654497,"tcdate":1511820259322,"number":1,"cdate":1511820259322,"id":"SkiJh-5lM","invitation":"ICLR.cc/2018/Conference/-/Paper446/Official_Review","forum":"SkFqf0lAZ","replyto":"SkFqf0lAZ","signatures":["ICLR.cc/2018/Conference/Paper446/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Official Review","rating":"6: Marginally above acceptance threshold","review":"The authors propose to compare three different memory architecture for recurrent neural network language models:\nvanilla LSTM, random access based on attention and continuous stack. The second main contribution of the paper is to propose an extension of continuous stacks, which allows to perform multiple pop operations at a single time step.\nThe way to do that is to use a similar mechanism as the adaptive computation time from Graves (2016): all the pop operations are performed, and the final state of the continuous stack is weighted average of all the intermediate states. The different memory models are evaluated on two standard language modeling tasks: PTB and WikiText-2, as well as on the verb number prediction dataset from Linzen et al (2016). On the language modeling tasks, the stack model performs slightly better than the attention models (0-2 ppl points) which performs slightly better than the plain LSTM (2-3 ppl). On the verb number prediction tasks, the stack model tends to outperforms the two other models (which get similar results) for hard examples (2 or more attractors).\n\nOverall, I enjoy reading this paper: it is clearly written, and contains interesting analysis of different memory architecture for recurrent neural networks. As far as I know, it is the first thorough comparison of the different memory architecture for recurrent neural network applied to language modeling. The experiments on the Linzen et al. (2016) dataset is also interesting, as it shows that for hard examples, the different models do have different behavior (even when the difference are not noticeable on the whole test set).\n\nOne small negative aspect of the paper is that the substance might be a bit limited. The only technical contribution is to merge the ideas from the continuous stack with the adaptive computation time to obtain the \"multi-pop\" model. In the experimental section, which I believe is the main contribution of the paper, I would have liked to see more \"in-depth\" analysis of the different models. I found the experiments performed on the Linzen et al. (2016) dataset (Table 2) to be quite interesting, and would have liked more analysis like that. On the other hand, I found Figures 2 or 3 not very informative, as it is (would like to see more). For example, from Fig. 2, it would be interesting to get a better understanding of what errors are made by the different models (instead of just the distribution).\n\nFinally, I have a few questions for the authors:\n- In Figure 1. shouldn't there be an arrow from h_{t-1} to m_t instead of x_{t-1} to m_t?\n- What are the equations to update the stack? I assume something similar to Joulin & Mikolov (2015)?\n- Do you have any ideas why there is a sharp jump between 4 and 5 attractors (Table 2)?\n- Why no \"pop\" operations in Figure 3 and 4?\n\npros/cons:\n+ clear and easy to read\n+ interesting analysis\n- not very original\n\nOverall, while not groundbreaking, this is a serious paper with interesting analysis. Hence, I am weakly recommending to accept this paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Architectures in Recurrent Neural Network Language Models","abstract":"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.","pdf":"/pdf/aca2fc020369672734d80003b459eda5ff483c79.pdf","paperhash":"anonymous|memory_architectures_in_recurrent_neural_network_language_models","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Architectures in Recurrent Neural Network Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFqf0lAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper446/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511384007393,"tcdate":1511384007393,"number":1,"cdate":1511384007393,"id":"SJyC7vQgM","invitation":"ICLR.cc/2018/Conference/-/Paper446/Official_Comment","forum":"SkFqf0lAZ","replyto":"HJhJDmfxM","signatures":["ICLR.cc/2018/Conference/Paper446/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper446/Authors"],"content":{"title":"re: REINFORCE reward","comment":"The reward used is the log probability of the sequence generated, conditional on the sampled stack control decisions. This is thus optimizing an EM-like bound on the marginal likelihood.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Architectures in Recurrent Neural Network Language Models","abstract":"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.","pdf":"/pdf/aca2fc020369672734d80003b459eda5ff483c79.pdf","paperhash":"anonymous|memory_architectures_in_recurrent_neural_network_language_models","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Architectures in Recurrent Neural Network Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFqf0lAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper446/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511302883677,"tcdate":1511302883677,"number":1,"cdate":1511302883677,"id":"HJhJDmfxM","invitation":"ICLR.cc/2018/Conference/-/Paper446/Public_Comment","forum":"SkFqf0lAZ","replyto":"SkFqf0lAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"REINFORCE reward","comment":"Could you please elaborate on the reward that was used for REINFORCE in the Single Computation Discrete Stack?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory Architectures in Recurrent Neural Network Language Models","abstract":"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.","pdf":"/pdf/aca2fc020369672734d80003b459eda5ff483c79.pdf","paperhash":"anonymous|memory_architectures_in_recurrent_neural_network_language_models","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Architectures in Recurrent Neural Network Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFqf0lAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper446/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739299571,"tcdate":1509118608757,"number":446,"cdate":1509739296908,"id":"SkFqf0lAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkFqf0lAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Memory Architectures in Recurrent Neural Network Language Models","abstract":"We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that stack-based memory architectures consistently achieve the best performance in terms of held out perplexity. We also propose a generalization to existing continuous stack models (Joulin & Mikolov,2015; Grefenstette et al., 2015)  to allow a variable number of pop operations more naturally that further improves performance. We further evaluate these language models in terms of their ability to capture non-local syntactic dependencies on a subject-verb agreement dataset  (Linzen et al., 2016) and establish new state of the art results using memory augmented language models. Our results demonstrate the value of stack-structured memory for explaining the distribution of words in natural language, in line with linguistic theories claiming a context-free backbone for natural language.","pdf":"/pdf/aca2fc020369672734d80003b459eda5ff483c79.pdf","paperhash":"anonymous|memory_architectures_in_recurrent_neural_network_language_models","_bibtex":"@article{\n  anonymous2018memory,\n  title={Memory Architectures in Recurrent Neural Network Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkFqf0lAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper446/Authors"],"keywords":[]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}