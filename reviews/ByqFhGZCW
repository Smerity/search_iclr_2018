{"notes":[{"tddate":null,"ddate":null,"tmdate":1515173078409,"tcdate":1515173078409,"number":2,"cdate":1515173078409,"id":"HkCRNN6XM","invitation":"ICLR.cc/2018/Conference/-/Paper969/Public_Comment","forum":"ByqFhGZCW","replyto":"HJRfymTXM","signatures":["~Seong_Joon_Oh1"],"readers":["everyone"],"writers":["~Seong_Joon_Oh1"],"content":{"title":"Makes sense now","comment":"That does clarify my misunderstanding. For a moment I thought the perturbations are scalar multiples of raw gradients, but yes it is FGSM indeed.\nThanks for your response!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515167509791,"tcdate":1515167509791,"number":5,"cdate":1515167509791,"id":"HJRfymTXM","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Comment","forum":"ByqFhGZCW","replyto":"SkfZVe6mz","signatures":["ICLR.cc/2018/Conference/Paper969/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper969/Authors"],"content":{"title":"eta vs Lp norm","comment":"Thanks again for the comment.\nWe don't clearly understand the question since the Lp norm of the perturbation is always normalized to 1 in this paper. \nAn adversarial sample z is z = x + eta*q, where x is the original image, eta is the perturbation strength, and q is the perturbation pattern with \\|q\\|_p=1. For FGSM, the pattern q = sign(grad loss) has a unit L-inf norm. After sensitivity training, the new q=sign(grad new loss) with a unit L-inf norm, cannot affect the new classifier as much as the old q (with a unit L-inf norm) could affect the undefended classifier using the same eta value. It would have to use a larger eta value.\nWe hope this clarifies the issue.\n\n\n\n\n  \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515156474547,"tcdate":1515156474547,"number":1,"cdate":1515156474547,"id":"SkfZVe6mz","invitation":"ICLR.cc/2018/Conference/-/Paper969/Public_Comment","forum":"ByqFhGZCW","replyto":"H1tsY-hQz","signatures":["~Seong_Joon_Oh1"],"readers":["everyone"],"writers":["~Seong_Joon_Oh1"],"content":{"title":"Remaining question","comment":"Thanks a lot for your kind explanations.\n\nI find it hard to agree with the last sentence though -- \"A classifier with a lower sensitivity is certainly more robust to gradient-based attacks since it causes an attacker to use a stronger (eta) perturbation to fool the classifier. \"\n\nTo me, eta does not tell how strong the perturbation is -- the Lp norm of the perturbation does. If a network has been trained with the sensitivity penalty, then the image gradient itself will be downscaled. Then, even if a larger value of eta is applied, the Lp norm of the perturbation could still be smaller. My question is, does the sensitivity penalty improve the robustness of a network against attacks with the same Lp norm, rather than the same eta value (which is done in this work; please correct me if I'm wrong). If the robustness in this case remains the same, then I wouldn't say the sensitivity penalty works as a defence strategy."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515096481505,"tcdate":1515096481505,"number":4,"cdate":1515096481505,"id":"H1tsY-hQz","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Comment","forum":"ByqFhGZCW","replyto":"rygOMUuCb","signatures":["ICLR.cc/2018/Conference/Paper969/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper969/Authors"],"content":{"title":"Game-theoretic view of adversarial examples","comment":"Thank you for your interest and comments. The ICCV'17 paper is certainly interesting and is in line with the main idea of the current paper that the adversarial example problems should be viewed as an attacker-defender game. \nThe main difference between the ICCV'17 paper and the current paper is that we propose continuous minimax problems and new optimization algorithms as opposed to the classic discrete minimax problems on probability simplices used in the ICCV'17 paper. We will update our related work in the final version. \n\nRegarding local optima and the guarantee: We cannot at present expect to have the flexibility of non-convex models and the global optimality of convex-concave models at the same time. But in practice, we often observe that a local optimum of a neural network performs nearly as well as any other local optimum. It would be very desirable to have rigorous and tight bounds on the performance of deep neural networks. \n\nRegarding the formulation in eq7: We haven't tested it, but we believe that we can cause misclassification eventually by increasing eta. However, it also increases detectability of the presence of perturbation which defeats the purpose.\nA classifier with a lower sensitivity is certainly more robust to gradient-based attacks since it causes an attacker to use a stronger (eta) perturbation to fool the classifier. \n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513141126824,"tcdate":1513141126824,"number":3,"cdate":1513141126824,"id":"SyJ9m4CWz","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Comment","forum":"ByqFhGZCW","replyto":"B1kbvzdxz","signatures":["ICLR.cc/2018/Conference/Paper969/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper969/Authors"],"content":{"title":"The revision has clear conclusions that the initial submission was missing. ","comment":"<Common>\n\nWe thank all the reviewers for important suggestions.\nWe could see where the submitted version was unclear or has caused confusions. \nFollowing the comments, we EXTENSIVELY revised the paper, re-ran the experiments and reported additional results to answer the questions. \nIn particular, we show how the proposed minimax algorithm gives us better results than alternating descent/ascent used in GAN training, and how the class of neural-net based attacks is more general than the class of gradient-based attacks.\n\nSince we believe most of the questions are now addressed in the submitted revision, we politely ask the reviewers for updating their evaluations.\n\n\n<Reviewer 3>\n\n\"Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims.\"\n\nMathematically they are straightforward, although they have not been applied in this domain before.\nWe agree this is not the essence of the experiments, and the new Table 5 now has more conclusive experimental results.\n\n\n\"...  the CW attack gives the most powerful attack and this should be considered for comparison.\"\n\nAFAIK, there is no particularly effective method to optimization-based attacks [Huang'15, CW'15] when eta is large. In the revision, we discuss how the neural-network based attacks is an approximation of a much larger class of attacks such as CW, and how the approximation allows us to practically find minimax defenses. Direct adversarial training against optimization-based attacks [Huang'15] does not work, as shown in the new Table 3 (LWA FGSM). \n\n\"The results with MNIST and CIFAR-10 are different. In some cases, MNIST is too easy to consider the complex structure of deep architectures. I prefer to have discussions on experimental results with both datasets.\"\n\nWe understand but the paper is already 12 pages without the CIFAR-10 results. As one can see, our conclusions on the MNIST results are also applicable to the corresponding CIFAR-10 results, except that the error rates of different defenses/attacks are not as much spread as MNIST. \n\n\n\"The main takeaway ... is not clear\"\n\nWe extensively revised the paper as well as reported important missing results. The key messages are 1) optimal defense-attack has to be studied as a dynamic problem, 2) we provide analytical and numerical tools to study them, and 3) the minimax defense is empirically better than previous adversarially-trained classifeirs or the results of optimization without sensitivity terms. \n\n\n\"The paragraph after eq. 17 is duplicated with a paragraph introduced before\"\n\nThe paragraph (about maximin) is not the same as the previous paragraph (about minimax). They are exactly the opposite.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513140858608,"tcdate":1513140858608,"number":2,"cdate":1513140858608,"id":"rymYGVR-f","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Comment","forum":"ByqFhGZCW","replyto":"B1oPNGYez","signatures":["ICLR.cc/2018/Conference/Paper969/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper969/Authors"],"content":{"title":"Our minimax algorithm finds a more robust classifier than GAN-type alternating optimization","comment":"<Common>\n\nWe thank all the reviewers for important suggestions.\nWe could see where the submitted version was unclear or has caused confusions. \nFollowing the comments, we EXTENSIVELY revised the paper, re-ran the experiments and reported additional results to answer the questions. \nIn particular, we show how the proposed minimax algorithm gives us better results than alternating descent/ascent used in GAN training, and how the class of neural-net based attacks is more general than the class of gradient-based attacks.\n\nSince we believe most of the questions are now addressed in the submitted revision, we politely ask the reviewers for updating their evaluations.\n\n\n\n<Reviewer 2>\n\n\"The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different \ninitializations, for instance) would be a nice addition to Table 5.\"\n\nWe are in the process of repeating all the experiments and will report them as soon as they are available.\n\n\n\n\"There is no exact evaluation of the impact of the sensitivity loss vs. the minimax/maximin algorithm.\"\n\nAs for adversarial training against gradient-type attacks (Sec 3), the new Table 3 compares the classifiers trained with (Sens-FGSM) and without (LWA-FGSM) the sensitivity term, where the latter procedure is similar to Huang et al.'15. Sens-FGSM performs slightly better than LWA-FGSM.\nAs for training against learning-based attacks (Sec 4), the new Table 5 compares the classifiers trained with (Minimax) and without (Alt) sensitivity term. The minimax solutions are shown to be more robust than the alt solutions. Fig 3 also shows that the solutions under the two methods converge to very different values.\n\n\n\"...hard to follow ... Lemma 1 and experimental analysis.\"\n\nLemma 1 follows simply from the definition and it was not the essence of the experiments. We replaced it with Table 5 which has more conclusive experimental results.\n\n\n\"It is unclear (from Figures 3 and 7) that \"alternative optimization\" and \"minimax\" converged fully, and/or that the sets of hyperparameters were optimal.\"\n\nWe tested the algorithms with different hyperparameters which did not improve the convergence speed. Instead, we now report the results with a 3-4 times larger number of iterations than before. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513140558053,"tcdate":1513140558053,"number":1,"cdate":1513140558053,"id":"HkLIb4Cbz","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Comment","forum":"ByqFhGZCW","replyto":"B1VlMxjlG","signatures":["ICLR.cc/2018/Conference/Paper969/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper969/Authors"],"content":{"title":"We added more experiments and revised the results.","comment":"<Common>\n\nWe thank all the reviewers for important suggestions.\nWe could see where the submitted version was unclear or has caused confusions. \nFollowing the comments, we EXTENSIVELY revised the paper, re-ran the experiments and reported additional results to answer the questions. \nIn particular, we show how the proposed minimax algorithm gives us better results than alternating descent/ascent used in GAN training, and how the class of neural-net based attacks is more general than the class of gradient-based attacks.\n\nSince we believe most of the questions are now addressed in the submitted revision, we politely ask the reviewers for updating their evaluations.\n\n\n<Reviewer 1>\n\n\"Sens-FGSM ... does not appear to perform particularly well with error rates well above 20%.\"\n\nYes, that is true. With large eta's, hardening a classifier against all FGSM attacks by adversarial training is difficult, regardless of whether sensitivity norm is used or not. \n\n\n\"https://arxiv.org/pdf/1705.09064.pdf\"\n\nIt is now included in the revision along with two other papers:\n\"...A few researchers have also proposed using a detector to detect and reject adversarial examples \\citep{meng2017magnet,lu2017safetynet,metzen2017detecting}. While we do not use detectors in this work, the minimax idea can be applied to train the detectors similarly.\"\n\n\n\"Why ... does Sens-FGSM provide a consistently better defense aginst FGSM-81?\"\n\nIt's a misunderstanding. FGSM-curr for Sens-FGSM is the attack on the current parameter and not the same as FGSM-81.\nAnyway, Sens-FGSM is consistently better because it is trained so that the loss gradient is small. \n\n\n\"... using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.)\"\n\nIt is not true. Alternating descent/ascent used in GAN cannot find minimax solutions but only local saddle points.\nSaddle points can be minimax, maximin or neither. They are the same only when f(u,v) is convex in u and concave in v.\nEmpirically, the minimax and the alternating methods converge to very different values (Fig 3), and the minimax solutions are shown to be more robust than the alt solutions in the new Table 5.\n\n\n\"it’s unlikely that the defender would ever know the attack network utilized by an attacker.\"\n\nYes. The maximin case is the other extreme case which is more hypothetical than realistic. However it gives the lower bound.\n\n \n\"How robust is the defense against samples generated by a different attack network?\" \n\"The authors ... state that the minimax solution is not meaningful for other network classes\"\n\"Is ... the defenses fall flat against samples generated by different architectures?\"\n\nSorry for the confusion. We unnecessarily overstated the limitations of minimax defense. They can certainly be evaluated against any other attack. We show in Table 5 that minimax-trained classifiers are still moderately robust to out-of-class FGSM attacks, whereas FGSM-trained classifiers fails utterly against neural-net based attacks. Evaluation with a different neural network architecture is underway.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642536447,"tcdate":1511879147689,"number":3,"cdate":1511879147689,"id":"B1VlMxjlG","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Review","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference/Paper969/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Well-written, but experiments could be more thorough. ","rating":"5: Marginally below acceptance threshold","review":"The authors describe a mechanism for defending against adversarial learning attacks on classifiers. They first consider the dynamics generated by the following procedure. They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating.  \n\nThey next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent. They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach. \n\nThey then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples). In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps. They conclude with empirical observations about the performance of this algorithm.\n\nThe paper is well-written and easy to follow. However, I found the empirical results to be a little underwhelming. Sens-FGSM outperforms the adversarial training defenses tuned for the “wrong” iteration, but it does not appear to perform particularly well with error rates well above 20%. How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)? Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM? It is my understanding that Sens-FGSM is not trained to a particular iteration of the “cat-and-mouse” game. Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81? With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.), thus I would liked to see more thorough experiments here as well. For example, it’s unlikely that the defender would ever know the attack network utilized by an attacker. How robust is the defense against samples generated by a different attack network? The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying. Any defense can be *evaluated* against samples generated by any attacker strategy. Is it the case that the defenses fall flat against samples generated by different architectures? \n\n\nMinor Comments:\nSection 3.1, First Line. ”f(ul(g(x),y))” appears to be a mistake.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642536483,"tcdate":1511756899094,"number":2,"cdate":1511756899094,"id":"B1oPNGYez","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Review","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference/Paper969/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting extension and empirical study of GANs (Goodfellow et al. 2014)","rating":"6: Marginally above acceptance threshold","review":"This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses. It has a lemma which claims that the \"minimax and the maximin solutions provide the best worst-case defense and attack models, respectively\", without proof, although that statement is supported experimentally.\n\n+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain.\n- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5.\n- There is no exact (\"alternating optimization\" could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm.\n- The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis.\n- It is unclear (from Figures 3 and 7) that \"alternative optimization\" and \"minimax\" converged fully, and/or that the sets of hyperparameters were optimal.\n+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims.\n\n\nNitpicks:\nthe gradient descent -> gradient descent or the gradient descent algorithm\nseeming -> seemingly\narbitrary flexible -> arbitrarily flexible\ncan name \"gradient descent that maximizes\": gradient ascent.\nThe mini- max or the maximin solution is defined -> are defined\nis the follow -> is the follower\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642536517,"tcdate":1511692022773,"number":1,"cdate":1511692022773,"id":"B1kbvzdxz","invitation":"ICLR.cc/2018/Conference/-/Paper969/Official_Review","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference/Paper969/AnonReviewer2"],"readers":["everyone"],"content":{"title":"MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS","rating":"5: Marginally below acceptance threshold","review":"The game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study. \n\nLemma 1 summarizes properties of the solutions that are expected to have after reaching equilibria. Important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave w.r.t. to the target variable.  In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1.　Nonetheless, this type of analysis can be useful under appropriate solutions if non-trivial claims are derived; however, Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims.\n\nAs long as the analysis is experimental, the state of the art should be considered. As long as the reviewer knows, the CW attack gives the most powerful attack and this should be considered for comparison. The results with MNIST and CIFAR-10 are different. In some cases, MNIST is too easy to consider the complex structure of deep architectures. I prefer to have discussions on experimental results with both datasets.\n\nThe main takeaway from the entire paper is not clear very much. It contains a game-theoretic framework of adversarial examples/training, novel attack method, and many experimental results.\n\nMinor:\nDefinition of g in the beginning of Sec 3.1 seems to be a typo. What is u? This is revealed in the latter sections but should be specified here.\n\nIn Section 3.1, \n>This is in stark contrast with the near-perfect misclassification of the undefended classifier in Table 1.\nThe results shown in the table seems to indicate the “perfect” misclassification.\n\nSentence after eq. 15 seems to contain a grammatical error\n\nThe paragraph after eq. 17 is duplicated with a paragraph introduced before\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509610088487,"tcdate":1509610088487,"number":1,"cdate":1509610088487,"id":"rygOMUuCb","invitation":"ICLR.cc/2018/Conference/-/Paper965/Public_Comment","forum":"ByqFhGZCW","replyto":"ByqFhGZCW","signatures":["~Seong_Joon_Oh1"],"readers":["everyone"],"writers":["~Seong_Joon_Oh1"],"content":{"title":"untitled","comment":"Thanks a lot for your great work! I think game theory is really one of the few valid ways to study attacks & defenses regarding adversarial examples, as opposed to the \"cat-and-mouse game\" we see in this field these days. Honestly, it is really becoming harder to trust papers saying \"we have a great defense mechanism\" or \"we have a great attack method\". \n\nAlong a similar line of reasoning, we have published a paper at ICCV'17, \"Adversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective\". We have also proposed a game theoretic framework to find the equilibrium in the dynamics between user and recogniser, trying to thwart/re-enable recognition. Perhaps this paper should also be mentioned in the related work!\n\nI'd like to point out some issues that I'd like to hear your response. First one is the term \"best worst-case defense and attack\". I feel this is contradictory to the fact that \"we can only find local solutions in practice for complex loss functions such as deep networks-based defenders and attackers\" (sec4.2). And this is also to me the biggest hurdle for using game theory with non-convex rewards under this security/privacy setup -- the equilibria, or the saddle points, do not guarantee anything, making the game theoretic analysis inconclusive.\n\nMaybe a minor issue: while I like the cleanness of the formulation in eq7 (\"sensitivity penality\"), it eventually just tries to scale down the image gradients around the training data points (or hopefully around the entire data distribution). So, when FGSM is applied again to sensitivity penalised networks, wouldn't FGSM with larger step size (eta) re-enable high original error rate? Do you have any preliminary results?\n\nWhile game theory has limitations (that it's hard to guarantee upper/lower bounds in non-convex setup), I still think game theory is great in spelling out assumptions explicitly (as we have argued in our ICCV'17 paper). I appreciate that the authors have really discussed the limitations in sec5.1. Overall, I really enjoyed the paper!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513140083806,"tcdate":1509137542888,"number":969,"cdate":1510092361078,"id":"ByqFhGZCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByqFhGZCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"MACHINE VS MACHINE: MINIMAX-OPTIMAL DEFENSE AGAINST ADVERSARIAL EXAMPLES","abstract":"Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small perturbations in the input unnoticeable to human eyes.  It is known that an attacker can generate strong adversarial examples if she knows the classifier parameters. Conversely, a defender can robustify the classifier by retraining if she has the adversarial examples. \nThe cat-and-mouse game nature of attacks and defenses raises the question of the presence of equilibria in the dynamics.\nIn this paper, we present a neural-network based attack class to approximate a larger but intractable class of attacks, and \nformulate the attacker-defender interaction as a zero-sum leader-follower game. We present sensitivity-penalized optimization algorithms to find minimax solutions, which are the best worst-case defenses against whitebox attacks. Advantages of the learning-based attacks and defenses compared to gradient-based attacks and defenses are demonstrated with MNIST and CIFAR-10.","pdf":"/pdf/854f4df485c0750a787829732f0768dc2fc84e6e.pdf","TL;DR":"A game-theoretic solution to adversarial attacks and defenses.","paperhash":"anonymous|machine_vs_machine_minimaxoptimal_defense_against_adversarial_examples","_bibtex":"@article{\n  anonymous2018machine,\n  title={MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByqFhGZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper969/Authors"],"keywords":[]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}