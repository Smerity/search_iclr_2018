{"notes":[{"tddate":null,"ddate":null,"tmdate":1512437665222,"tcdate":1512437065128,"number":3,"cdate":1512437065128,"id":"ry-Ird7Zf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"ryJCJQXZf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response to Public Comment 1","comment":"Actually, we just obtained the codes from the authors of FastText.zip and finished the comparison a few weeks ago.\n\nTheir idea is based on normalized product quantization (NPQ), which split a vector into K parts and quantize each part. For each word, an extra byte is used to quantize the norm of the embedding vector. We found one drawback of this approach is that it produces very long codes in order to achieve good model performance. Here are the results of IMDB sentiment analysis task:\n\n------------------------------------------------------\n                                           code len     total size        accuracy\nGloVe baseline                     -                78 MB               87.18\nNPQ (K=60)                      480 bits       4.26 MB            87.11\nOur Model(16x32)           80 bits        1.23 MB            87.37\n------------------------------------------------------\n\nI think it's a nice idea to separate the vector norm from quantization and it may also work in our approach to achieve higher compression rate. We will upload the revised paper once we are allowed to add revision. \n\nFor Matrin's paper, their method is based on sparsification. I will try to get the codes from him or find a way to compare with his model."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512429209828,"tcdate":1512429209828,"number":2,"cdate":1512429209828,"id":"HybsIIQbz","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"rk0hvx5xf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response for AnonReviewer1","comment":"Thank you for spending the time to review our paper. Hope our response can answer your question.\n\n1. About compressing the Softmax layer\n\nWe spent a significant period of time trying to apply the proposed method to the softmax layer, though without a successful result. It can be caused by the code sharing problem (multiple words get the same code) or the loss function. However, we are still optimistic that the compositional coding approach can also be applied to the softmax, which should be our future work. We will refer the readers to some recent papers that reduce the size of the softmax layer using pruning techniques.\n\n2. About the total model size\n\nThe full sizes of the baseline models are summarized in the following table:\n\nTask                          Embed      Full size   Ratio of embed\nIMDB                         78 MB        79.1 MB         98.6%\nIWSLT De-En           35 MB         94   MB         37.2%\nASPEC En-Ja            274 MB       506 MB         54.1%\n\nWe will put the information of full model sizes into the tables in the experiment section.\n\n3. Experiment with random code assignment\n\nWe tried to initialize a set of 32 x 16 codes to be random numbers and see the performance in the \u0010IMDB task. With the random code assignment, the accuracy is much lower than the baseline.\n\n------------------------------------------------------------------------------------\nModel                                                            Accuracy\nBaseline                                                        87.18\nRandom code + trained codebooks           84.19\nRandom code + random codebooks          84.72\n---------------------------------------------------------------\n\n4. Analysis of information distributed in the codes\n\nAs the codes are learned by a neural net, the interpretability is not guaranteed. However, we found some interesting relations in the codes.\n\nFor animal names, the 3rd subcode is normally a \"5\" for the plural nouns. For the verbs, we found the 2nd subcode is normally a \"0\" if the verb is in the past tense. Although there are also violations, we believe the model learned to arrange the codes in an efficient way.\n\n----\ndog        7 7 0 1 7 3 7 0\ndogs      4 7 5 1 7 3 4 0\n\ncat        0 7 0 1 7 3 7 0\ncats      4 7 5 1 7 3 4 0\n\npig        7 3 6 1 7 3 4 7\npigs      7 3 5 1 7 3 4 0\n\nfish       7 7 6 1 4 3 4 7\nfishes    7 2 5 0 7 3 4 6\n\nfox        6 5 7 1 4 3 0 0\nfoxes    6 2 5 1 7 3 4 6\n----\nbuy          0 7 2 1 4 3 3 1\nbought    0 0 2 1 4 3 3 1\n\nkick        7 6 1 1 4 3 0 0\nkicked    7 0 1 1 4 3 2 0\n\ngo         7 7 0 6 4 3 3 0\nwent    4 0 7 6 4 3 2 0\n\npick        7 6 7 1 4 3 3 0\npicked    7 0 7 1 4 0 3 0\n\ncatch       7 7 1 6 4 3 6 0\ncaught     7 0 7 4 4 3 2 0\n----"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512415911939,"tcdate":1512415175245,"number":1,"cdate":1512415175245,"id":"ryJCJQXZf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comparaison with related word embeddings compression methods and literature","comment":"I am curious to know if this method performs better than existing word embeddings compression techniques such as Product Quantizers (which also exploits the idea of compositional codes) [1] or WTA autoencoders [2].\n\n[1] FastText.zip: Compressing text classification models https://arxiv.org/pdf/1612.03651.pdf\n[2] ANDREWS, Martin. Compressing word embeddings. In : International Conference on Neural Information Processing. Springer International Publishing, 2016. p. 413-422."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512404878373,"tcdate":1512404878373,"number":3,"cdate":1512404878373,"id":"ryIqDgXbz","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple yet effective work","rating":"7: Good paper, accept","review":"The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick. The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss. \n\nThis paper is well-written and easy to follow.  The motivation is clear and the idea is simple and effective.\n\nIt would be better to provide deeper analysis in Subsection 6.1. The current analysis is too simple. It may be interesting to explain the meanings of individual components. Does each component is related to a certain topic? Is it meaningful to perform ADD or SUBSTRACT on the leaned code? \n\nIt may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222621429,"tcdate":1512167949124,"number":2,"cdate":1512167949124,"id":"SyrG5UJ-G","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings. It also adopts an interesting multicodebook approach for encoding than binary embeddings. \n\nThe paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy.\n\nThe experiments are convincing and solid. Overall I am weakly inclined to accept this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222621468,"tcdate":1511815094006,"number":1,"cdate":1511815094006,"id":"rk0hvx5xf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Effective work","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices. The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks.\n\nOverall, the paper is well-written. The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation.\n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer. For example, the model does not guarantee to be able to convert resulting \"indices\" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector. As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task.\nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well.\nAlso, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer. There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies.\n\nFurthermore, I would like to see two additional analysis. First, if we trained the proposed model with starting from \"zero\" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739359887,"tcdate":1509097989677,"number":331,"cdate":1509739357229,"id":"BJRZzFlRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}