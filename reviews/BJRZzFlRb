{"notes":[{"tddate":null,"ddate":null,"tmdate":1514962322225,"tcdate":1514962322225,"number":7,"cdate":1514962322225,"id":"Sk5cTlq7f","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"rJL04SDXf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response to Martin Renqiang Min","comment":"Hi, the authors of the NIPS 2017 Workshop paper have already contacted us. We share a similar idea but the work is conducted independently. They have already cited our paper in their Arxiv version. We will upload the revised version of our paper as soon as we are allowed to upload a revision. (Currently I cannot find the upload button in the revision page.)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1514784924167,"tcdate":1514783950318,"number":6,"cdate":1514783950318,"id":"rJL04SDXf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["~Martin_Renqiang_Min1"],"readers":["everyone"],"writers":["~Martin_Renqiang_Min1"],"content":{"title":"Related work missing","comment":"Besides that this work is conceptually related to the product quantification method (https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)  mentioned by a public comment, this work is also highly related to the linear version of our recent work presented at NIPS 2017 Workshop on Machine Learning in Discrete Structures, entitled \"Learning K-way D-dimensional Discrete Code For Compact Embedding Representations\" (https://arxiv.org/abs/1711.03067). \n\nWe hope that the authors could mention our work in future revision of this concurrent ICLR submission. Thanks."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1513127865414,"tcdate":1513127865414,"number":5,"cdate":1513127865414,"id":"SyW6J-AWM","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"By5oXODZG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response to Others (Area Chair)","comment":"Thank you for spending the time to give us a feedback.\n\nDo you mean the paper [1]? I have read both [1] and [2], they are applying sparse coding to the word embeddings. Although they have a different purpose, I'm impressed by the strong interpretability they can gain through the process. I will cite the related papers in the sparse coding. As two reviewers are interesting in the interpretability of the codes, I'm also doing some experiments trying to find some ways to find the topics of learned codes.\n\n[1] Sparse Overcomplete Word Vector Representations (Faruqui et al., 2015)\n[2] Learning Word Representations with Hierarchical Sparse Coding (Yogatama et al., 2014)\n\nFor (See et al., 2016) and the compression part of (Zhang et al., 2017), we also compare with the same pruning techniques and report the results in the experiment sections. As we only compress word embeddings, we don't have the class weighting problem as discussed in (See et al., 2016). Both of the papers report a compression rate of 80% with a small performance loss, which is identical to our results. Actually, with the pruning technique, we achieved a 90% compression rate for word embeddings in the translation tasks as shown in Table 4. However, it does not work well on the sentiment analysis task.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512698786560,"tcdate":1512698786560,"number":1,"cdate":1512698786560,"id":"By5oXODZG","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Comment","forum":"BJRZzFlRb","replyto":"ryJCJQXZf","signatures":["ICLR.cc/2018/Conference/Paper331/Area_Chair"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper331/Area_Chair"],"content":{"title":"Others","comment":"There are also several papers using sparse coding directly on word embeddings (Yogatama et al 2015?), using optimization tools like SPAMS, instead of an autoencoder. These models are not \"deep\" but certainly worth citing and understanding the benefits of this approach.  (Also worth comparing to See et al and Kim et al 2016? who both run pruning on the same dataset.)  "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512474009400,"tcdate":1512473813880,"number":4,"cdate":1512473813880,"id":"Bk0C4WEZz","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"ryIqDgXbz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response for AnonReviewer3","comment":"Thank you for spending the time to review our paper. \n\nAs multiple reviewers are asking for us to analyze the information learned by each component, we did some extra experiments and found some interesting results.\n\nWe tried to learn a set of codes using a 3 x 256 coding scheme, this will force the model to decompose each embedding into 3 vectors. In order to maximize the compression rate, the model must make these 3 vectors as independent as possible. So we can think that they represent 3 concepts.\n\nThen we extracted the codes of some related words:\n------------------------------\nman\t    210 153 153\nwoman\t    232 153 153\n\nking\t     210 180 39\nqueen\t     232 180 39\n\nbritish\t    118 132 142\nlondon\t    185 126 142\n\njapan\t    118 56 21\ntokyo\t    185 36 21\n------------------------------\n\nWe can transform a \"man/king\" to \"woman/queen\" by change the subcode \"210\" in the first component to \"232\".\nSo we can think \"210\" must be a \"male\" code, and \"232\" must be a \"female\" code.\n\nSimilarly, when we look at the country and city names, we can find \"185\" in the first component to be a \"city\" code.\n\nWe uploaded the 3x256 and 8x8 codes of 10,000 most frequent words to anonymous gists, so the those who are interested in the codes can have a look.\n\n------\n3 x 256 codes of 10k most frequent words:\nhttps://gist.github.com/anonymous/aa6c03f871900a3c4e5d7f65d74361fe\n\n8 x 8 codes of 10k most frequent words:\nhttps://gist.github.com/anonymous/584d64a28c3bb7c421eee8450cae823a"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512437665222,"tcdate":1512437065128,"number":3,"cdate":1512437065128,"id":"ry-Ird7Zf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"ryJCJQXZf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response to Public Comment 1","comment":"Actually, we just obtained the codes from the authors of FastText.zip and finished the comparison a few weeks ago.\n\nTheir idea is based on normalized product quantization (NPQ), which split a vector into K parts and quantize each part. For each word, an extra byte is used to quantize the norm of the embedding vector. We found one drawback of this approach is that it produces very long codes in order to achieve good model performance. Here are the results of IMDB sentiment analysis task:\n\n------------------------------------------------------\n                                           code len     total size        accuracy\nGloVe baseline                     -                78 MB               87.18\nNPQ (K=60)                      480 bits       4.26 MB            87.11\nOur Model(16x32)           80 bits        1.23 MB            87.37\n------------------------------------------------------\n\nI think it's a nice idea to separate the vector norm from quantization and it may also work in our approach to achieve higher compression rate. We will upload the revised paper once we are allowed to add revision. \n\nFor Matrin's paper, their method is based on sparsification. I will try to get the codes from him or find a way to compare with his model."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512429209828,"tcdate":1512429209828,"number":2,"cdate":1512429209828,"id":"HybsIIQbz","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"rk0hvx5xf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response for AnonReviewer1","comment":"Thank you for spending the time to review our paper. Hope our response can answer your question.\n\n1. About compressing the Softmax layer\n\nWe spent a significant period of time trying to apply the proposed method to the softmax layer, though without a successful result. It can be caused by the code sharing problem (multiple words get the same code) or the loss function. However, we are still optimistic that the compositional coding approach can also be applied to the softmax, which should be our future work. We will refer the readers to some recent papers that reduce the size of the softmax layer using pruning techniques.\n\n2. About the total model size\n\nThe full sizes of the baseline models are summarized in the following table:\n\nTask                          Embed      Full size   Ratio of embed\nIMDB                         78 MB        79.1 MB         98.6%\nIWSLT De-En           35 MB         94   MB         37.2%\nASPEC En-Ja            274 MB       506 MB         54.1%\n\nWe will put the information of full model sizes into the tables in the experiment section.\n\n3. Experiment with random code assignment\n\nWe tried to initialize a set of 32 x 16 codes to be random numbers and see the performance in the \u0010IMDB task. With the random code assignment, the accuracy is much lower than the baseline.\n\n------------------------------------------------------------------------------------\nModel                                                            Accuracy\nBaseline                                                        87.18\nRandom code + trained codebooks           84.19\nRandom code + random codebooks          84.72\n---------------------------------------------------------------\n\n4. Analysis of information distributed in the codes\n\nAs the codes are learned by a neural net, the interpretability is not guaranteed. However, we found some interesting relations in the codes.\n\nFor animal names, the 3rd subcode is normally a \"5\" for the plural nouns. For the verbs, we found the 2nd subcode is normally a \"0\" if the verb is in the past tense. Although there are also violations, we believe the model learned to arrange the codes in an efficient way.\n\n----\ndog        7 7 0 1 7 3 7 0\ndogs      4 7 5 1 7 3 4 0\n\ncat        0 7 0 1 7 3 7 0\ncats      4 7 5 1 7 3 4 0\n\npig        7 3 6 1 7 3 4 7\npigs      7 3 5 1 7 3 4 0\n\nfish       7 7 6 1 4 3 4 7\nfishes    7 2 5 0 7 3 4 6\n\nfox        6 5 7 1 4 3 0 0\nfoxes    6 2 5 1 7 3 4 6\n----\nbuy          0 7 2 1 4 3 3 1\nbought    0 0 2 1 4 3 3 1\n\nkick        7 6 1 1 4 3 0 0\nkicked    7 0 1 1 4 3 2 0\n\ngo         7 7 0 6 4 3 3 0\nwent    4 0 7 6 4 3 2 0\n\npick        7 6 7 1 4 3 3 0\npicked    7 0 7 1 4 0 3 0\n\ncatch       7 7 1 6 4 3 6 0\ncaught     7 0 7 4 4 3 2 0\n----"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512415911939,"tcdate":1512415175245,"number":1,"cdate":1512415175245,"id":"ryJCJQXZf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Public_Comment","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comparaison with related word embeddings compression methods and literature","comment":"I am curious to know if this method performs better than existing word embeddings compression techniques such as Product Quantizers (which also exploits the idea of compositional codes) [1] or WTA autoencoders [2].\n\n[1] FastText.zip: Compressing text classification models https://arxiv.org/pdf/1612.03651.pdf\n[2] ANDREWS, Martin. Compressing word embeddings. In : International Conference on Neural Information Processing. Springer International Publishing, 2016. p. 413-422."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642433403,"tcdate":1512404878373,"number":3,"cdate":1512404878373,"id":"ryIqDgXbz","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Simple yet effective work","rating":"7: Good paper, accept","review":"The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick. The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss. \n\nThis paper is well-written and easy to follow.  The motivation is clear and the idea is simple and effective.\n\nIt would be better to provide deeper analysis in Subsection 6.1. The current analysis is too simple. It may be interesting to explain the meanings of individual components. Does each component is related to a certain topic? Is it meaningful to perform ADD or SUBSTRACT on the leaned code? \n\nIt may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642433442,"tcdate":1512167949124,"number":2,"cdate":1512167949124,"id":"SyrG5UJ-G","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings. It also adopts an interesting multicodebook approach for encoding than binary embeddings. \n\nThe paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy.\n\nThe experiments are convincing and solid. Overall I am weakly inclined to accept this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642433479,"tcdate":1511815094006,"number":1,"cdate":1511815094006,"id":"rk0hvx5xf","invitation":"ICLR.cc/2018/Conference/-/Paper331/Official_Review","forum":"BJRZzFlRb","replyto":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference/Paper331/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Effective work","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices. The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks.\n\nOverall, the paper is well-written. The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation.\n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer. For example, the model does not guarantee to be able to convert resulting \"indices\" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector. As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task.\nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well.\nAlso, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer. There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies.\n\nFurthermore, I would like to see two additional analysis. First, if we trained the proposed model with starting from \"zero\" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739359887,"tcdate":1509097989677,"number":331,"cdate":1509739357229,"id":"BJRZzFlRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJRZzFlRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Compressing Word Embeddings via Deep Compositional Code Learning","abstract":"Natural language processing (NLP) models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. Deploying neural NLP models to mobile devices requires compressing the word embeddings without any significant sacrifices in performance. For this purpose, we propose to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code. To maximize the compression rate, we adopt the multi-codebook quantization approach instead of binary coding scheme. Each code is composed of multiple discrete numbers, such as (3, 2, 1, 8), where the value of each component is limited to a fixed range. We propose to directly learn the discrete codes in an end-to-end neural network by applying the Gumbel-softmax trick. Experiments show the compression rate achieves 98% in a sentiment analysis task and 94% ~ 99% in machine translation tasks without performance loss. In both tasks, the proposed method can improve the model performance by slightly lowering the compression rate. Compared to other approaches such as character-level segmentation, the proposed method is language-independent and does not require modifications to the network architecture.","pdf":"/pdf/cf2282943203b8af245d0966da31280a252c1e25.pdf","TL;DR":"Compressing the word embeddings over 94% without hurting the performance.","paperhash":"anonymous|compressing_word_embeddings_via_deep_compositional_code_learning","_bibtex":"@article{\n  anonymous2018compressing,\n  title={Compressing Word Embeddings via Deep Compositional Code Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJRZzFlRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper331/Authors"],"keywords":["natural language processing","word embedding","compression","deep learning"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}