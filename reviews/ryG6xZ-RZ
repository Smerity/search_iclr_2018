{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222708246,"tcdate":1511512147310,"number":2,"cdate":1511512147310,"id":"rJs8O8Bgz","invitation":"ICLR.cc/2018/Conference/-/Paper645/Official_Review","forum":"ryG6xZ-RZ","replyto":"ryG6xZ-RZ","signatures":["ICLR.cc/2018/Conference/Paper645/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper presents a compiler framework for domain-specific languages (DSLs) targeting deep learning systems. The paper describes the overall structure, the different compilation stages, and also provides a small example. This type of frameworks is very useful and can have a significant impact in the community. However, no evaluation of the proposed framework is done in the paper.","rating":"7: Good paper, accept","review":"Deep learning is a technique that has attracted a lot of attention. Typically, when using a deep learning framework we describe the network using some kind of computation graph, e.g., as in TensorFlow. A drawback is that the execution performance can be limited, e.g., due to run-time interpretation of the computation graph. \n\nThis paper takes a different approach and presents a compiler framework that allows definition of domain-specific languages (DSLs) for deep learning system, defines a number of compilation stages that can take advantage of standard compiler optimizations as well as specialized optimizations for neural networks using an intermediate representation, and also a back-end. Thus a computation graph is compiled directly to binary code, which increases the performance. For example, the compiler infrastructure enables optimization over multiple kernels using kernel fusion. \n\nI find the paper very interesting and the findings can have a significant impact on how we develop deep learning systems in the future. The paper addresses an important problem, is very well written, and is easy to follow. The different optimization stages are well describe and also motive why they improve performance over existing techniques. The intention is to provide the framework as open source in the future. \n\nThe main drawback of the paper is the lack of evaluation. Although the framework is well described, its application and use are only demonstrated with a very small code example. No comparison with existing frameworks is done, and no evaluation of the actual performance is done. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DLVM: A modern compiler infrastructure for deep learning systems","abstract":"Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","pdf":"/pdf/257e01fe10187199c27725f06fed2b91133bb0c2.pdf","TL;DR":"We introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.","paperhash":"anonymous|dlvm_a_modern_compiler_infrastructure_for_deep_learning_systems","_bibtex":"@article{\n  anonymous2018dlvm:,\n  title={DLVM: A modern compiler framework for neural network DSLs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryG6xZ-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper645/Authors"],"keywords":["deep learning","automatic differentiation","algorithmic differentiation","domain specific languages","neural networks","programming languages","DSLs"]}},{"tddate":null,"ddate":null,"tmdate":1512222708281,"tcdate":1511334617972,"number":1,"cdate":1511334617972,"id":"ByG1QoMxz","invitation":"ICLR.cc/2018/Conference/-/Paper645/Official_Review","forum":"ryG6xZ-RZ","replyto":"ryG6xZ-RZ","signatures":["ICLR.cc/2018/Conference/Paper645/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Modern-day neural networks require code generation (to generate the backward pass) and a host of other optimizations to be run effectively. The paper proposes to do this using best practices in the compilers and programming languages communities as opposed to the ad hoc manner adopted by current deep learning engines. ","rating":"5: Marginally below acceptance threshold","review":"Modern-day deep learning engines (e.g., tensorflow, caffe) perform code generation (to generated the backward pass) and a host of other optimizations to run today's deep learning workloads. Unfortunately, the manner in which they go about doing this is ad hoc and does not adopt best practices developed in the compilers and programming languages communities. The obvious consequence is missed opportunities for optimizing deep learning workloads. This paper proposes to fix this by re-designing from ground up the deep learning engine placing particular focus on code generation, compilation (e.g., type checking), optimization (e.g., fusion, matrix chain reordering, common subexpression elimination) etc. Unfortunately, the paper falls short in two significant respects: It does not adequately cite related work and it does not present any experiments to quantify the benefits they claim will be achieved by their new compiler.\n\nPros:\n- The paper proposes a very relevant and timely proposal to design a modern day deep learning compiler framework.\n- Their design includes a number of optimizations that are missing from currently available deep learning engines which can lead to significant benefits.\n\nCons:\n- Related work is not adequately referenced. Here are two (others should be easy to find): Apache SystemML (https://systemml.apache.org/), TACO (http://tensor-compiler.org/)\n- Experiments section is conspicuous by its absence. They provide no micro benchmarks or end-to-end deep learning use cases to quantify the benefits of their compiler vs. some of the currently available ones.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DLVM: A modern compiler infrastructure for deep learning systems","abstract":"Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","pdf":"/pdf/257e01fe10187199c27725f06fed2b91133bb0c2.pdf","TL;DR":"We introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.","paperhash":"anonymous|dlvm_a_modern_compiler_infrastructure_for_deep_learning_systems","_bibtex":"@article{\n  anonymous2018dlvm:,\n  title={DLVM: A modern compiler framework for neural network DSLs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryG6xZ-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper645/Authors"],"keywords":["deep learning","automatic differentiation","algorithmic differentiation","domain specific languages","neural networks","programming languages","DSLs"]}},{"tddate":null,"ddate":null,"tmdate":1512315797562,"tcdate":1509130426327,"number":645,"cdate":1509739180997,"id":"ryG6xZ-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryG6xZ-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DLVM: A modern compiler infrastructure for deep learning systems","abstract":"Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.","pdf":"/pdf/257e01fe10187199c27725f06fed2b91133bb0c2.pdf","TL;DR":"We introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks.","paperhash":"anonymous|dlvm_a_modern_compiler_infrastructure_for_deep_learning_systems","_bibtex":"@article{\n  anonymous2018dlvm:,\n  title={DLVM: A modern compiler framework for neural network DSLs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryG6xZ-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper645/Authors"],"keywords":["deep learning","automatic differentiation","algorithmic differentiation","domain specific languages","neural networks","programming languages","DSLs"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}