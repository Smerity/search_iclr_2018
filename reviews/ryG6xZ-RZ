{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222708246,"tcdate":1511512147310,"number":2,"cdate":1511512147310,"id":"rJs8O8Bgz","invitation":"ICLR.cc/2018/Conference/-/Paper645/Official_Review","forum":"ryG6xZ-RZ","replyto":"ryG6xZ-RZ","signatures":["ICLR.cc/2018/Conference/Paper645/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper presents a compiler framework for domain-specific languages (DSLs) targeting deep learning systems. The paper describes the overall structure, the different compilation stages, and also provides a small example. This type of frameworks is very useful and can have a significant impact in the community. However, no evaluation of the proposed framework is done in the paper.","rating":"7: Good paper, accept","review":"Deep learning is a technique that has attracted a lot of attention. Typically, when using a deep learning framework we describe the network using some kind of computation graph, e.g., as in TensorFlow. A drawback is that the execution performance can be limited, e.g., due to run-time interpretation of the computation graph. \n\nThis paper takes a different approach and presents a compiler framework that allows definition of domain-specific languages (DSLs) for deep learning system, defines a number of compilation stages that can take advantage of standard compiler optimizations as well as specialized optimizations for neural networks using an intermediate representation, and also a back-end. Thus a computation graph is compiled directly to binary code, which increases the performance. For example, the compiler infrastructure enables optimization over multiple kernels using kernel fusion. \n\nI find the paper very interesting and the findings can have a significant impact on how we develop deep learning systems in the future. The paper addresses an important problem, is very well written, and is easy to follow. The different optimization stages are well describe and also motive why they improve performance over existing techniques. The intention is to provide the framework as open source in the future. \n\nThe main drawback of the paper is the lack of evaluation. Although the framework is well described, its application and use are only demonstrated with a very small code example. No comparison with existing frameworks is done, and no evaluation of the actual performance is done. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DLVM: A modern compiler framework for neural network DSLs","abstract":" Many current approaches to deep learning make use of high-level toolkits such as TensorFlow, Torch, or Caffe. Toolkits such as Caffe have a layer-based programming framework with hard-coded gradients specified for each layer type, making research using novel layer types problematic. Toolkits such as Torch and TensorFlow define a computation graph in a host language such as Python, where each node represents a linear algebra operation parallelized as a compute kernel on GPU and stores the result of evaluation; some of these toolkits subsequently perform runtime interpretation over that graph, storing the results of forward calculations and reverse-accumulated gradients at each node. This approach is more flexible, but these toolkits take a very limited and ad-hoc approach to performing optimization. Also problematic are the facts that most toolkits lack type safety, and target only a single (usually GPU) architecture, limiting users’ abilities to make use of heterogeneous and emerging hardware architectures. We introduce a novel framework for high-level programming that addresses all of the above shortcomings.","pdf":"/pdf/a51cee5526337888d0ff62dbbc70cdab1e55abf2.pdf","TL;DR":"We introduce a novel compiler framework with a specialized IR that addresses shortcomings of existing deep learning frameworks.","paperhash":"anonymous|dlvm_a_modern_compiler_framework_for_neural_network_dsls","_bibtex":"@article{\n  anonymous2018dlvm:,\n  title={DLVM: A modern compiler framework for neural network DSLs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryG6xZ-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper645/Authors"],"keywords":["deep learning","automatic differentiation","algorithmic differentiation","domain specific languages","neural networks","programming languages","DSLs"]}},{"tddate":null,"ddate":null,"tmdate":1512222708281,"tcdate":1511334617972,"number":1,"cdate":1511334617972,"id":"ByG1QoMxz","invitation":"ICLR.cc/2018/Conference/-/Paper645/Official_Review","forum":"ryG6xZ-RZ","replyto":"ryG6xZ-RZ","signatures":["ICLR.cc/2018/Conference/Paper645/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Modern-day neural networks require code generation (to generate the backward pass) and a host of other optimizations to be run effectively. The paper proposes to do this using best practices in the compilers and programming languages communities as opposed to the ad hoc manner adopted by current deep learning engines. ","rating":"5: Marginally below acceptance threshold","review":"Modern-day deep learning engines (e.g., tensorflow, caffe) perform code generation (to generated the backward pass) and a host of other optimizations to run today's deep learning workloads. Unfortunately, the manner in which they go about doing this is ad hoc and does not adopt best practices developed in the compilers and programming languages communities. The obvious consequence is missed opportunities for optimizing deep learning workloads. This paper proposes to fix this by re-designing from ground up the deep learning engine placing particular focus on code generation, compilation (e.g., type checking), optimization (e.g., fusion, matrix chain reordering, common subexpression elimination) etc. Unfortunately, the paper falls short in two significant respects: It does not adequately cite related work and it does not present any experiments to quantify the benefits they claim will be achieved by their new compiler.\n\nPros:\n- The paper proposes a very relevant and timely proposal to design a modern day deep learning compiler framework.\n- Their design includes a number of optimizations that are missing from currently available deep learning engines which can lead to significant benefits.\n\nCons:\n- Related work is not adequately referenced. Here are two (others should be easy to find): Apache SystemML (https://systemml.apache.org/), TACO (http://tensor-compiler.org/)\n- Experiments section is conspicuous by its absence. They provide no micro benchmarks or end-to-end deep learning use cases to quantify the benefits of their compiler vs. some of the currently available ones.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DLVM: A modern compiler framework for neural network DSLs","abstract":" Many current approaches to deep learning make use of high-level toolkits such as TensorFlow, Torch, or Caffe. Toolkits such as Caffe have a layer-based programming framework with hard-coded gradients specified for each layer type, making research using novel layer types problematic. Toolkits such as Torch and TensorFlow define a computation graph in a host language such as Python, where each node represents a linear algebra operation parallelized as a compute kernel on GPU and stores the result of evaluation; some of these toolkits subsequently perform runtime interpretation over that graph, storing the results of forward calculations and reverse-accumulated gradients at each node. This approach is more flexible, but these toolkits take a very limited and ad-hoc approach to performing optimization. Also problematic are the facts that most toolkits lack type safety, and target only a single (usually GPU) architecture, limiting users’ abilities to make use of heterogeneous and emerging hardware architectures. We introduce a novel framework for high-level programming that addresses all of the above shortcomings.","pdf":"/pdf/a51cee5526337888d0ff62dbbc70cdab1e55abf2.pdf","TL;DR":"We introduce a novel compiler framework with a specialized IR that addresses shortcomings of existing deep learning frameworks.","paperhash":"anonymous|dlvm_a_modern_compiler_framework_for_neural_network_dsls","_bibtex":"@article{\n  anonymous2018dlvm:,\n  title={DLVM: A modern compiler framework for neural network DSLs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryG6xZ-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper645/Authors"],"keywords":["deep learning","automatic differentiation","algorithmic differentiation","domain specific languages","neural networks","programming languages","DSLs"]}},{"tddate":null,"ddate":null,"tmdate":1509739183655,"tcdate":1509130426327,"number":645,"cdate":1509739180997,"id":"ryG6xZ-RZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryG6xZ-RZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DLVM: A modern compiler framework for neural network DSLs","abstract":" Many current approaches to deep learning make use of high-level toolkits such as TensorFlow, Torch, or Caffe. Toolkits such as Caffe have a layer-based programming framework with hard-coded gradients specified for each layer type, making research using novel layer types problematic. Toolkits such as Torch and TensorFlow define a computation graph in a host language such as Python, where each node represents a linear algebra operation parallelized as a compute kernel on GPU and stores the result of evaluation; some of these toolkits subsequently perform runtime interpretation over that graph, storing the results of forward calculations and reverse-accumulated gradients at each node. This approach is more flexible, but these toolkits take a very limited and ad-hoc approach to performing optimization. Also problematic are the facts that most toolkits lack type safety, and target only a single (usually GPU) architecture, limiting users’ abilities to make use of heterogeneous and emerging hardware architectures. We introduce a novel framework for high-level programming that addresses all of the above shortcomings.","pdf":"/pdf/a51cee5526337888d0ff62dbbc70cdab1e55abf2.pdf","TL;DR":"We introduce a novel compiler framework with a specialized IR that addresses shortcomings of existing deep learning frameworks.","paperhash":"anonymous|dlvm_a_modern_compiler_framework_for_neural_network_dsls","_bibtex":"@article{\n  anonymous2018dlvm:,\n  title={DLVM: A modern compiler framework for neural network DSLs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryG6xZ-RZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper645/Authors"],"keywords":["deep learning","automatic differentiation","algorithmic differentiation","domain specific languages","neural networks","programming languages","DSLs"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}