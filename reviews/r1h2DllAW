{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642410029,"tcdate":1511832172663,"number":3,"cdate":1511832172663,"id":"H1S_cEcxM","invitation":"ICLR.cc/2018/Conference/-/Paper213/Official_Review","forum":"r1h2DllAW","replyto":"r1h2DllAW","signatures":["ICLR.cc/2018/Conference/Paper213/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The authors consider the problem of ultra-low precision neural networks motivated by \nlimited computation and bandwidth. Their approach first posits a Bayesian neural network\na discrete prior on the weights followed by central limit approximations to efficiently \napproximate the likelihood. The authors propose several tricks like normalization and cost \nrescaling to help performance. They compare their results on several versions of MNIST. The \npaper is promising, but I have several questions:\n\n1) One major concern is that the experimental results are only on MNIST. It's important \nto have another (larger) dataset to understand how sensitive the approach is to \ncharacteristics of the data. It seems plausible that a more difficulty problem may \nrequire more precision.\n\n2) Likelihood weighting is related to annealing and variational tempering\n\n3) The structure of the paper could be improved:\n - The introduction contains way too many details about the method \n    and related work without a clear boundary.\n - I would add the model up front at the start of section 2\n - Section 2.1 could be reversed or equations 2-5 could be broken with text \n   explaining each choice \n\n4) What does training time look like? Is the Bayesian optimization necessary?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete-Valued Neural Networks Using Variational Inference","abstract":"The increasing demand for neural networks (NNs) being employed on embedded devices has led to plenty of research investigating methods for training low precision NNs. While most methods involve a quantization step, we propose a principled Bayesian approach where we first infer a distribution over a discrete weight space from which we subsequently derive hardware-friendly low precision NNs. To this end, we introduce a probabilistic forward pass to approximate the intractable variational objective that allows us to optimize over discrete-valued weight distributions for NNs with sign activation functions. In our experiments, we show that our model achieves state of the art performance on several real world data sets. In addition, the resulting models exhibit a substantial amount of sparsity that can be utilized to further reduce the computational costs for inference.","pdf":"/pdf/5c610a586fbc2330c4ac85bd02f7226eb8fb2794.pdf","TL;DR":"Variational Inference for infering a discrete distribution from which a low-precision neural network is derived","paperhash":"anonymous|discretevalued_neural_networks_using_variational_inference","_bibtex":"@article{\n  anonymous2018discrete-valued,\n  title={Discrete-Valued Neural Networks Using Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1h2DllAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper213/Authors"],"keywords":["low-precision","neural networks","resource efficient","variational inference","Bayesian"]}},{"tddate":null,"ddate":null,"tmdate":1515642410082,"tcdate":1511827890542,"number":2,"cdate":1511827890542,"id":"HkshYX9xz","invitation":"ICLR.cc/2018/Conference/-/Paper213/Official_Review","forum":"r1h2DllAW","replyto":"r1h2DllAW","signatures":["ICLR.cc/2018/Conference/Paper213/AnonReviewer1"],"readers":["everyone"],"content":{"title":"New approach to train ternary-weight NNs, unclear advantages with respect to previous works","rating":"5: Marginally below acceptance threshold","review":"In this work, discrete-weight NNs are trained using the variational Bayesian framework, achieving similar results to other state-of-the-art models. Weights use 3 bits on the first layer and are ternary on the remaining layers.\n\n\n- Pros:\n\nThe paper is well-written and connections with the literature properly established.\n\nThe approach to training discrete-weights NNs, which is variational inference, is more principled than previous works (but see below).\n\n- Cons:\n\nThe authors depart from the original motivation when the central limit theorem is invoked. Once we approximate the activations with Gaussians, do we have any guarantee that the new approximate lower bound is actually a lower bound? This is not discussed. If it is not a lower bound, what is the rationale behind maximizing it? This seems to place this work very close to previous works, and not in the \"more principled\" regime the authors claim to seek.\n\nThe likelihood weighting seems hacky. The authors claim \"there are usually many more NN weights than there are data samples\". If that is the case, then it seems that the prior dominating is indeed the desired outcome. A different, more flat prior (or parameter sharing), can be used, but the described reweighting seems to be actually breaking a good property of Bayesian inference, which is defecting to the prior when evidence is lacking.\n\nIn terms of performance (Table 1), the proposed method seems to be on par with existing ones. It is unclear then what the advantage of this proposal is.\n\nSparsity figures are provided for the current approach, but those are not contrasted with existing approaches. Speedup is claimed with respect to an NN with real weights, but not with respect existing NNs with binary weights, which is the appropriate baseline.\n\n\n- Minor comments:\n\nPage 3: Subscript t and variable t is used for the targets, but I can't find where it is defined.\n\nOnly the names of the datasets used in the experiments are given, but they are not described, or even better, shown in pictures (maybe in a supplementary).\n\nThe title of the paper says \"discrete-valued NNs\". The weights are discrete, but the activations and outputs are continuous, so I find it confusing. As a contrast, I would be less surprised to hear a sigmoid belief network called a \"discrete-valued NN\", even though its weights are continuous.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete-Valued Neural Networks Using Variational Inference","abstract":"The increasing demand for neural networks (NNs) being employed on embedded devices has led to plenty of research investigating methods for training low precision NNs. While most methods involve a quantization step, we propose a principled Bayesian approach where we first infer a distribution over a discrete weight space from which we subsequently derive hardware-friendly low precision NNs. To this end, we introduce a probabilistic forward pass to approximate the intractable variational objective that allows us to optimize over discrete-valued weight distributions for NNs with sign activation functions. In our experiments, we show that our model achieves state of the art performance on several real world data sets. In addition, the resulting models exhibit a substantial amount of sparsity that can be utilized to further reduce the computational costs for inference.","pdf":"/pdf/5c610a586fbc2330c4ac85bd02f7226eb8fb2794.pdf","TL;DR":"Variational Inference for infering a discrete distribution from which a low-precision neural network is derived","paperhash":"anonymous|discretevalued_neural_networks_using_variational_inference","_bibtex":"@article{\n  anonymous2018discrete-valued,\n  title={Discrete-Valued Neural Networks Using Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1h2DllAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper213/Authors"],"keywords":["low-precision","neural networks","resource efficient","variational inference","Bayesian"]}},{"tddate":null,"ddate":null,"tmdate":1515642410120,"tcdate":1511123078636,"number":1,"cdate":1511123078636,"id":"SJJ5dvJgM","invitation":"ICLR.cc/2018/Conference/-/Paper213/Official_Review","forum":"r1h2DllAW","replyto":"r1h2DllAW","signatures":["ICLR.cc/2018/Conference/Paper213/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This is outside of my areas of expertise. This is an educated guess. Marginal accept. ","rating":"6: Marginally above acceptance threshold","review":"Summary: \nThe paper considers a Bayesian approach in order to infer the distribution over a discrete weight space, from which they derive hardware-friendly low precision NNs. This is an alternative to a standard quantization step, often performed in cases such as emplying NNs on embedded devices.\nThe NN setting considered here contains sign activation functions.\nThe experiments conducted show that the proposed model achieves nice performance on several real world data Comments\n\nDue to an error in the openreview platform, I didn't have the chance to bid on time. This is not within my areas of expertise. Sorry for any inconvenience.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete-Valued Neural Networks Using Variational Inference","abstract":"The increasing demand for neural networks (NNs) being employed on embedded devices has led to plenty of research investigating methods for training low precision NNs. While most methods involve a quantization step, we propose a principled Bayesian approach where we first infer a distribution over a discrete weight space from which we subsequently derive hardware-friendly low precision NNs. To this end, we introduce a probabilistic forward pass to approximate the intractable variational objective that allows us to optimize over discrete-valued weight distributions for NNs with sign activation functions. In our experiments, we show that our model achieves state of the art performance on several real world data sets. In addition, the resulting models exhibit a substantial amount of sparsity that can be utilized to further reduce the computational costs for inference.","pdf":"/pdf/5c610a586fbc2330c4ac85bd02f7226eb8fb2794.pdf","TL;DR":"Variational Inference for infering a discrete distribution from which a low-precision neural network is derived","paperhash":"anonymous|discretevalued_neural_networks_using_variational_inference","_bibtex":"@article{\n  anonymous2018discrete-valued,\n  title={Discrete-Valued Neural Networks Using Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1h2DllAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper213/Authors"],"keywords":["low-precision","neural networks","resource efficient","variational inference","Bayesian"]}},{"tddate":null,"ddate":null,"tmdate":1513601612867,"tcdate":1509062580188,"number":213,"cdate":1509739423480,"id":"r1h2DllAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1h2DllAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discrete-Valued Neural Networks Using Variational Inference","abstract":"The increasing demand for neural networks (NNs) being employed on embedded devices has led to plenty of research investigating methods for training low precision NNs. While most methods involve a quantization step, we propose a principled Bayesian approach where we first infer a distribution over a discrete weight space from which we subsequently derive hardware-friendly low precision NNs. To this end, we introduce a probabilistic forward pass to approximate the intractable variational objective that allows us to optimize over discrete-valued weight distributions for NNs with sign activation functions. In our experiments, we show that our model achieves state of the art performance on several real world data sets. In addition, the resulting models exhibit a substantial amount of sparsity that can be utilized to further reduce the computational costs for inference.","pdf":"/pdf/5c610a586fbc2330c4ac85bd02f7226eb8fb2794.pdf","TL;DR":"Variational Inference for infering a discrete distribution from which a low-precision neural network is derived","paperhash":"anonymous|discretevalued_neural_networks_using_variational_inference","_bibtex":"@article{\n  anonymous2018discrete-valued,\n  title={Discrete-Valued Neural Networks Using Variational Inference},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1h2DllAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper213/Authors"],"keywords":["low-precision","neural networks","resource efficient","variational inference","Bayesian"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}