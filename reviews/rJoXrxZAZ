{"notes":[{"ddate":null,"tddate":1515452898756,"tmdate":1515452917665,"tcdate":1515199659723,"number":3,"cdate":1515199659723,"id":"rJ43n56XM","invitation":"ICLR.cc/2018/Conference/-/Paper580/Public_Comment","forum":"rJoXrxZAZ","replyto":"ByDRVIuZG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Accuracy is not the main goal of this work. This work is orthogonal to other techniques. ","comment":"We really appreciate the reviewer's comments. We also really like Reviewer1's feedback that accuracy is not the main purpose of this paper. We are not trying to outperform SOTA in terms of accuracy but only provide a way to speedup an autoregressive model like WaveNet. We understand that the WaveNet team also have made great progress improving their MOS scores using various techniques (please find their recent paper :) ), but even with those changes, our technique can still be applied to a model that is fundamentally a \"WaveNet\" and still achieve 2-4x speedup.  \n\nLike we explained to Reviewer1, mathematically, the hybrid method can be built on top of other techniques including caching, and still achieve 2x-4x speedup. For instance, caching reduces per sample generation time by k (~20).  The total generation time for a full utterance of n samples would be n*(1/k)*T, where T is the original per sample generation time. With the hybrid method, it can be further reduced to n*(1/k)*T*(1/4). \n\nThe speedup can be beyond 2x. The inference time can be drastically reduced (~2x each time step added) by increasing the number of steps produced by the LSTM. The audio quality will not degrade noticeably until 6-7 steps (~32-64x speed up) compared to base line. We would love to add more evaluation in future version. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1515452962250,"tcdate":1515198500317,"number":2,"cdate":1515198500317,"id":"Sk3XOcp7f","invitation":"ICLR.cc/2018/Conference/-/Paper580/Public_Comment","forum":"rJoXrxZAZ","replyto":"r16uKJ5gG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"We thank the reviewer's great feedback.","comment":"We thank the reviewer's great feedback. In terms of your question: \n* How well do other multi-step decoders do?\nYes, we have the same question at the early stage of the project. We tried a variety of approaches to generate multiple samples, including a transposed convolution, a vanilla RNN, a high-way, etc. None of them get comparable performance to LSTMs. \n\n* Why does the Wavenet output a single timestep? Why not just have the multi-step decoder output all the timesteps? \nWe tried having multi-step decoder to output all timesteps, but unintuitively it is worse than having one sample generated by WaveNet. As pointed out in the result section, LSTM can effectively reduce variance in the output distribution, but this also could reduce the sharpness and naturalness of the audio.  \n\n* How much of a boost does the separate training give over joint training? If you used the idea suggested in the previous point, you wouldn't need this separate training scheme.\nThe audio quality is substantially better with ground-truth training. Thanks for the suggestion, we will try this idea out. \n\n* How does performance vary over changing the number of steps the multi-step decoder outputs?\nThe inference time can be drastically reduced (~2x each time step added) by increasing the number of steps. The audio quality will not degrade noticeably until 6-7 steps (~32-64x speed up) compared to base line. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1515452950793,"tcdate":1515197140678,"number":1,"cdate":1515197140678,"id":"Byp0z9TQz","invitation":"ICLR.cc/2018/Conference/-/Paper580/Public_Comment","forum":"rJoXrxZAZ","replyto":"ryOLIn5lf","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The technique is orthogonal to existing techniques to speedup WaveNet ","comment":"We thank the reviewer's feedback but we do feel the hybrid method has its own merits. It is orthogonal to existing techniques including caching.  Even with caching, the critical path caused by dependencies between samples still exists (You cannot generate the next sample earlier). Caching does not fundamentally address this dependency problem. Also caching is subject to hardware. For a different hardware platform (e.g. mobile), there might not be sufficient cache or memory for this purpose.  \n\nMathematically, the hybrid method can be built on top of caching, and still achieve 2x-4x speedup. For instance, caching reduces per sample generation time by k (~20).  The total generation time for a full utterance of n samples would be n*(1/k)*T, where T is the original per sample generation time. With the hybrid method, it can be further reduced to n*(1/k)*T*(1/4). \n\nWith respect to the evaluation, we do have a figure of comparison of inference time (Figure 6). We feel it is a fair comparison when we fix the accuracy while comparing the inference time.  And yes, we agree that the key point of the paper is not to improve accuracy,  thus the figures should better convey the key point (reference time). \n\nIn terms of the definition of validation error, we partition the training data into 5% validation data and 95% training data and run validation every 250 iterations.  It is not the final test error. Audio quality is measured with MOS as described in the result section. \n\nIn terms of audio quality, yes we feel confident to upload samples. The Mechanical Turkers consistently gives better MOS scores for this hybrid model, compared to a WaveNet. Me, personally, listened the samples many times and can confirm that the scores reflect the quality. \nWe would love to compare with samples posted by van den Oord et al (2017).  \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1515642472755,"tcdate":1512756431359,"number":3,"cdate":1512756431359,"id":"ByDRVIuZG","invitation":"ICLR.cc/2018/Conference/-/Paper580/Official_Review","forum":"rJoXrxZAZ","replyto":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference/Paper580/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Right choice of problem. Introduces significant independence assumptions.","rating":"4: Ok but not good enough - rejection","review":"By generating multiple samples at once with the LSTM, the model is introducing some independence assumptions between samples that are from neighbouring windows and are not conditionally independent given the context produced by Wavenet. This reduces significantly the generality of the proposed technique.\n\nPros:\n- Attempting to solve the important problem of speeding up autoregressive generation.\n- Clarity of the write-up is OK, although it could use some polishing in some parts.\n- The work is in the right direction, but the paucity of results and lack of thoroughness reduces somewhat the work's overall significance.\n\nCons:\n- The proposed technique is not particularly novel and it is not clear whether the technique can be used to get speed-ups beyond 2x - something that is important for real-world deployment of Wavenet.\n- The amount of innovation is on the low side, as it involves mostly just fairly minor architectural changes.\n- The absolute results are not that great (MOS ~3.8 is not close to the SOTA of 4.4 - 4.5)\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1515642472794,"tcdate":1511863887998,"number":2,"cdate":1511863887998,"id":"ryOLIn5lf","invitation":"ICLR.cc/2018/Conference/-/Paper580/Official_Review","forum":"rJoXrxZAZ","replyto":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference/Paper580/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Right name. Low innovation. Samples please!","rating":"4: Ok but not good enough - rejection","review":"This paper presents HybridNet, a neural speech (and other audio) synthesis system (vocoder) that combines the popular and effective WaveNet model with an LSTM with the goal of offering a model with faster inference-time audio generation.\n\nSummary: The proposed model, HybridNet is a fairly straightforward variation of WaveNet and thus the paper offers a relatively low novelty. There is also a lack of detail regarding the human judgement experiments that make the significance of the results difficult to interpret. \n\nLow novelty of approach / impact assessment:\nThe proposed model is based closely on WaveNet, an existing state-of-the-art vocoder model. The proposal here is to extend WaveNet to include an LSTM that will generate samples between WaveNet samples -- thus allowing WaveNet to sample at a lower sample frequency. WaveNet is known for being relatively slow at test-time generation time, thus allowing it to run at a lower sample frequency should decrease generation time. The introduction of a local LSTM is perhaps not a sufficiently significant innovation. \n\nAnother issue that lowers the assessment of the likely impact of this paper is that there are already a number of alternative mechanism to deal with the sampling speed of WaveNet. In particular, the cited method of Ramachandran et al (2017) uses caching and other tricks to achieve a speed up of 21 times over WaveNet (compared to the 2-4 times speed up of the proposed method). The authors suggest that these are orthogonal strategies that can be combined, but the combination is not attempted in this paper. There are also other methods such as sampleRNN (Mehri et al. 2017) that are faster than WaveNet at inference time. The authors do not compare to this model.\n\nInappropriate evaluation:\nWhile the model is motivated by the need to reduce the generation of WaveNet sampling, the evaluation is largely based on the quality of the sampling rather than the speed of sampling. The results are roughly calibrated to demonstrate that HybridNet produces higher quality samples when (roughly) adjusted for sampling time. The more appropriate basis of comparison is to compare sample time as a function of sample quality. \n\nExperiments:\nFew details are provided regarding the human judgment experiments with Mechanical Turkers. As a result it is difficulty to assess the appropriateness of the evaluation and therefore the significance of the findings. I would also be much more comfortable with this quality assessment if I was able to hear the samples for myself and compare the quality of the WaveNet samples with HybridNet samples. I will also like to compare the WaveNet samples generated by the authors' implementation with the WaveNet samples posted by van den Oord et al (2017).  \n\n\nMinor comments / questions:\n\nHow, specifically, is validation error defined in the experiments? \n\nThere are a few language glitches distributed throughout the paper.  \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1515642472833,"tcdate":1511811445029,"number":1,"cdate":1511811445029,"id":"r16uKJ5gG","invitation":"ICLR.cc/2018/Conference/-/Paper580/Official_Review","forum":"rJoXrxZAZ","replyto":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference/Paper580/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good results but lacking details about design decisions","rating":"6: Marginally above acceptance threshold","review":"TL;DR of paper: for sequential prediction, in order to scale up the model size without increasing inference time, use a model that predicts multiple timesteps at once. In this case, use an LSTM on top of a Wavenet for audio synthesis, where the LSTM predicts N steps for every Wavenet forward pass. The main result is being able to train bigger models, by increasing Wavenet depth, without increasing inference time.\n\nThe idea is simple and intuitive. I'm interested in seeing how well this approach can generalize to other sequential prediction domains. I suspect that it's easier in the waveform case because neighboring samples are highly correlated. I am surprised by how much an improvement \n\nHowever, there are a number of important design decisions that are glossed over in the paper. Here are a few that I am wondering about:\n* How well do other multi-step decoders do? For example, another natural choice is using transposed convolutions to upsample multiple timesteps. Fully connected layers? How does changing the number of LSTM layers affect performance?\n* Why does the Wavenet output a single timestep? Why not just have the multi-step decoder output all the timesteps?\n* How much of a boost does the separate training give over joint training? If you used the idea suggested in the previous point, you wouldn't need this separate training scheme.\n* How does performance vary over changing the number of steps the multi-step decoder outputs?\n\nThe paper also reads like it was hastily written, so please go back and fix the rough edges.\n\nRight now, the paper feels too coupled to the existing Deep Voice 2 system. As a research paper, it is lacking important ablations. I'll be happy to increase my score if more experiments and results are provided.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1509739224236,"tcdate":1509127458882,"number":580,"cdate":1509739221571,"id":"rJoXrxZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}