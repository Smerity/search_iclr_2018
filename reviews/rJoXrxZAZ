{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222693771,"tcdate":1511863887998,"number":2,"cdate":1511863887998,"id":"ryOLIn5lf","invitation":"ICLR.cc/2018/Conference/-/Paper580/Official_Review","forum":"rJoXrxZAZ","replyto":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference/Paper580/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Right name. Low innovation. Samples please!","rating":"4: Ok but not good enough - rejection","review":"This paper presents HybridNet, a neural speech (and other audio) synthesis system (vocoder) that combines the popular and effective WaveNet model with an LSTM with the goal of offering a model with faster inference-time audio generation.\n\nSummary: The proposed model, HybridNet is a fairly straightforward variation of WaveNet and thus the paper offers a relatively low novelty. There is also a lack of detail regarding the human judgement experiments that make the significance of the results difficult to interpret. \n\nLow novelty of approach / impact assessment:\nThe proposed model is based closely on WaveNet, an existing state-of-the-art vocoder model. The proposal here is to extend WaveNet to include an LSTM that will generate samples between WaveNet samples -- thus allowing WaveNet to sample at a lower sample frequency. WaveNet is known for being relatively slow at test-time generation time, thus allowing it to run at a lower sample frequency should decrease generation time. The introduction of a local LSTM is perhaps not a sufficiently significant innovation. \n\nAnother issue that lowers the assessment of the likely impact of this paper is that there are already a number of alternative mechanism to deal with the sampling speed of WaveNet. In particular, the cited method of Ramachandran et al (2017) uses caching and other tricks to achieve a speed up of 21 times over WaveNet (compared to the 2-4 times speed up of the proposed method). The authors suggest that these are orthogonal strategies that can be combined, but the combination is not attempted in this paper. There are also other methods such as sampleRNN (Mehri et al. 2017) that are faster than WaveNet at inference time. The authors do not compare to this model.\n\nInappropriate evaluation:\nWhile the model is motivated by the need to reduce the generation of WaveNet sampling, the evaluation is largely based on the quality of the sampling rather than the speed of sampling. The results are roughly calibrated to demonstrate that HybridNet produces higher quality samples when (roughly) adjusted for sampling time. The more appropriate basis of comparison is to compare sample time as a function of sample quality. \n\nExperiments:\nFew details are provided regarding the human judgment experiments with Mechanical Turkers. As a result it is difficulty to assess the appropriateness of the evaluation and therefore the significance of the findings. I would also be much more comfortable with this quality assessment if I was able to hear the samples for myself and compare the quality of the WaveNet samples with HybridNet samples. I will also like to compare the WaveNet samples generated by the authors' implementation with the WaveNet samples posted by van den Oord et al (2017).  \n\n\nMinor comments / questions:\n\nHow, specifically, is validation error defined in the experiments? \n\nThere are a few language glitches distributed throughout the paper.  \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1512222693816,"tcdate":1511811445029,"number":1,"cdate":1511811445029,"id":"r16uKJ5gG","invitation":"ICLR.cc/2018/Conference/-/Paper580/Official_Review","forum":"rJoXrxZAZ","replyto":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference/Paper580/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good results but lacking details about design decisions","rating":"6: Marginally above acceptance threshold","review":"TL;DR of paper: for sequential prediction, in order to scale up the model size without increasing inference time, use a model that predicts multiple timesteps at once. In this case, use an LSTM on top of a Wavenet for audio synthesis, where the LSTM predicts N steps for every Wavenet forward pass. The main result is being able to train bigger models, by increasing Wavenet depth, without increasing inference time.\n\nThe idea is simple and intuitive. I'm interested in seeing how well this approach can generalize to other sequential prediction domains. I suspect that it's easier in the waveform case because neighboring samples are highly correlated. I am surprised by how much an improvement \n\nHowever, there are a number of important design decisions that are glossed over in the paper. Here are a few that I am wondering about:\n* How well do other multi-step decoders do? For example, another natural choice is using transposed convolutions to upsample multiple timesteps. Fully connected layers? How does changing the number of LSTM layers affect performance?\n* Why does the Wavenet output a single timestep? Why not just have the multi-step decoder output all the timesteps?\n* How much of a boost does the separate training give over joint training? If you used the idea suggested in the previous point, you wouldn't need this separate training scheme.\n* How does performance vary over changing the number of steps the multi-step decoder outputs?\n\nThe paper also reads like it was hastily written, so please go back and fix the rough edges.\n\nRight now, the paper feels too coupled to the existing Deep Voice 2 system. As a research paper, it is lacking important ablations. I'll be happy to increase my score if more experiments and results are provided.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]}},{"tddate":null,"ddate":null,"tmdate":1509739224236,"tcdate":1509127458882,"number":580,"cdate":1509739221571,"id":"rJoXrxZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJoXrxZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models","abstract":"This paper introduces HybridNet, a hybrid neural network to speed-up autoregressive\nmodels for raw audio waveform generation. As an example, we propose\na hybrid model that combines an autoregressive network named WaveNet and a\nconventional LSTM model to address speech synthesis. Instead of generating\none sample per time-step, the proposed HybridNet generates multiple samples per\ntime-step by exploiting the long-term memory utilization property of LSTMs. In\nthe evaluation, when applied to text-to-speech, HybridNet yields state-of-art performance.\nHybridNet achieves a 3.83 subjective 5-scale mean opinion score on\nUS English, largely outperforming the same size WaveNet in terms of naturalness\nand provide 2x speed up at inference.","pdf":"/pdf/bebe8314335a4e5cb2fa7d0d42a81b6489764b87.pdf","TL;DR":"It is a hybrid neural architecture to speed-up autoregressive model. ","paperhash":"anonymous|hybridnet_a_hybrid_neural_architecture_to_speedup_autoregressive_models","_bibtex":"@article{\n  anonymous2018hybridnet:,\n  title={HybridNet: A Hybrid Neural Architecture to Speed-up Autoregressive  Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJoXrxZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper580/Authors"],"keywords":["neural architecture","inference time reduction","hybrid model"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}