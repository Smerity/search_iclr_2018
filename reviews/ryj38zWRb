{"notes":[{"tddate":null,"ddate":null,"tmdate":1515189884259,"tcdate":1515189884259,"number":3,"cdate":1515189884259,"id":"ryNFIO6mf","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Comment","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper849/Authors"],"content":{"title":"paper revision","comment":"We updated the draft with the following changes:\n- Added VAE baselines trained on CelebA 128x128 (figures 2, 4, 6).\n- Added image reconstructions of held-out images (figure 10)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515189583306,"tcdate":1515189583306,"number":2,"cdate":1515189583306,"id":"SkvIrda7G","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Comment","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper849/Authors"],"content":{"title":"author's rebuttal 2/2","comment":"\n# R3 “The authors mention in Section 3.3.2 that they leave the careful modeling of Z to future work, however the paper is quite incomplete without this”\n\nAs mentioned above, our primary goal is to tease apart the influence of inductive bias via the convolutional network architecture from the GAN training protocol; as such, we have also restricted ourselves to the simplest sampling methods. Because a GAN is sampled via a simple Gaussian (or uniformly), we do the same here.   Of course we can improve sample quality (and log-likelihood) with more sophisticated model for Z, but this does not serve to help understand how a GAN is working. For example, consider figure 8 which we used to show the effects of moving principal components of the Z. We could easily make this into a method for generating new samples.\n\nNote that DCGAN generations trained on faces will also include many “monsters” (even look at the results in the original DCGAN paper). On the other hand, we agree that on the SUN bedrooms, our model produces less convincing generations than DCGAN; but we also think the discrepancy between the results on two datasets (and indeed, the *way* in which our model's samples are less convincing paired with the way GANs fail to reconstruct training samples) is worth publishing.\n\n\n# baselines (R3, R1):\n\nWe agree that VAE's are reasonable to include for comparison, and we will do so. \n\nW.r.t. better GAN training protocols, most of the improvements have dealt with reliability. In our experiments, we trained hundreds of GAN models and picked the one with the best generations for comparison. As our goal is not to claim a SOTA image generation method, but rather to try to understand the factors in the success of a GAN, using simple techniques with with *standardized implementations* (and running them lots of times and picking the best outcomes) is preferable to getting the bleeding edge of reliable GAN training.\n\n\n# GLO / AE loss (R3):\n\nThe loss from our model (with direct optimization of Z) is lower than that from an auto-encoder (although an auto-encoder fine-tuned with direct optimization of Z is comparable with our model).  However, we do agree with the intuition that random initialization serves as a kind of regularizer; we consistently see that random initialization leads to better generations.\n\n\n# Reconstruction of held out images\n\nWe agree, and we will do this."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515189550266,"tcdate":1515189550266,"number":1,"cdate":1515189550266,"id":"HJ8VH_pmz","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Comment","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper849/Authors"],"content":{"title":"author's rebuttal 1/2","comment":"First of all, we would like to thank the reviewers for their thoughtful comments. \nThe missing references suggested by reviewer R3 are indeed relevant and we will include them in the discussion of the related work in our revised draft. We would also like to thank reviewer R1 for pointing out problems with writing.\n\n# R1: “The results on SUN are a bit underwhelming”\n# R3: “the image generation results (Figure 4 and 5) are worse than GAN”\n\nThe primary focus of this work is shedding light on the success of GANs, as opposed to demonstrating a SOTA generative model for images. In particular, in this work we focused on DCGAN like architectures, without progressive generation or other more sophisticated setups.   Our aim was to understand what part of the success of DCGAN models could be explained by the inductive bias of the architecture, rather than as a result of the GAN training protocol.  We have demonstrated that on celeba, the inductive bias is key, and the GAN training protocol is not crucial, as the GAN generations are at best marginally superior to GLO generations.  On the other hand, with a DCGAN architecture,  the GAN training protocol is important on the bedrooms. We suspect that this is a capacity issue, and that the GAN “solves” the capacity issue by ignoring a large part of the training distribution, as evidenced by the reconstruction results in figure 2 and 3. Even if one does not believe this hypothesis, the discrepancy between the results on the faces and on the bedrooms is interesting as it suggests multiple other avenues for understanding the success of GANs.    \n\nIn short: we fully acknowledge (here and in the text of the paper) that our generations are inferior to GAN on bedrooms and not noticeably superior to GAN on celeb; but this does not invalidate the thesis of the paper or its scientific value. \n\n\n# R1: “ Especially since it is a common practice nowadays to generate much higher dimensional images, i.e. 256x256, the results presented in this paper appear weak.”\n\nGenerating 256x256 images with a DCGAN architecture on the datasets we used is still not common. In future work we will building models with more capacity and use more powerful generation protocols, with sample quality as the primary focus.\n\n\n# R1: “the authors mention that different initializations were used for the z vectors in the case of CelebA and LSUN. Does this lead to significantly different results?” \n# R2 “Is it fair to assume from this that the initialization of z during training matters? If so, why”\n\nFor all of the celeb images in the paper, we initialized Z with Gaussian normal vectors projected to the sphere.  For the bedrooms, we initialized with PCA.  On the faces, we found that initializing with whitened PCA leads to faster convergence, comparable reconstruction, and worse generations; and initializing with the results of an auto-encoder leads to even faster convergence, but still worse generations.   On the bedrooms, because for the models described in the paper, we are capacity limited, and because the data set is so large (and so optimization takes time), we used PCA initializations.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642520469,"tcdate":1511902124122,"number":3,"cdate":1511902124122,"id":"HyE2oHixz","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Review","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Official review","rating":"6: Marginally above acceptance threshold","review":"The paper is well written and easy to follow. I find the results very interesting. In particular the paper shows that many properties of GAN (or generative) models (e.g. interpolation, feature arithmetic) are a in great deal result of the inductive bias of deep CNN’s and can be obtained with simple reconstruction losses. \n\nThe results on CelebA seem quite remarkable for training examples (e.g. interpolation). Samples are quite good but inferior to GANs, but still impressive for the simplicity of the model. The results on SUN are a bit underwhelming, but still deliver the point reasonably well in my view. Naturally, the paper would make a much stronger claim showing good results on different datasets. \n\nThe authors mentioned that the current method can recover all the solutions that could be found by an autoencoder and reach some others. It would be very interesting to empirically explore this statement. Specifically, my intuition is that if we train a traditional autoencoder (with normalization of the latent space to match this setting) and compute the corresponding z vectors for each element in the dataset, the loss function (1) would be lower than that achieved with the proposed model. If that is true, the way of solving the problem is helping find a solution that prevents overfitting. \n\nFollowing with the previous point, the authors mention that different initializations were used for the z vectors in the case of CelebA and LSUN. Does this lead to significantly different results? What would happen if the z values were initialized say with the representations learned by a fully trained deterministic autoencoder (with the normalization as in this work)? It would be good to report and discuss these alternatives in terms of loss function and results (e.g. quality of the samples). \n\nIt seems natural to include VAE baselines (using both of the losses in this work). Also, recent works have used ‘perceptual losses’, for instance for building VAE’s capable of generating sharper images:\n\nLamb, A., et al (2016). Discriminative regularization for generative models. arXiv preprint arXiv:1602.03220.\n\nIt would be good to compare these results with those presented in this work. One could argue that VAE’s are also mainly trained via a regularized reconstruction loss. Conversely, the proposed method can be thought as a form of autoencoder. The encoder could be thought to be implicitly defined by the optimization procedure used to recover the latent vectors in GAN's. Using explicit variables for each image would be a way of solving the optimization problem.\n\nIt would be informative to also shows reconstruction and interpolation results for a set of ‘held out’ images. Where the z values would be found as with GANs. This would test the coverage of the method and might be a way of making the comparison with GANs more relevant. \n\nThe works:\n\nNguyen, Anh, et al. \"Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.\" Advances in Neural Information Processing Systems. 2016.\n\nHan, Tian, et al. \"Alternating Back-Propagation for Generator Network.\" AAAI. 2017.\n\nSeems very related.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642520505,"tcdate":1511802915972,"number":2,"cdate":1511802915972,"id":"SynXdTKeM","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Review","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper is a potentially interesting alternative training procedure to GANs.","rating":"6: Marginally above acceptance threshold","review":"In this paper, the authors propose a new architecture for generative neural networks. Rather than the typical adversarial training procedure used to train a generator and a discriminator, the authors train a generator only. To ensure that noise vectors get mapped to images from the target distribution, the generator is trained to map noise vectors to the set of training images as closely as possible. Both the parameters of the generator and the noise vectors themselves are optimized during training. \n\nOverall, I think this paper is useful. The images generated by the model are not (qualitatively and in my opinion) as high quality as extremely recent work on GANs, but do appear to be better than those produced by DCGANs. More importantly than the images produced, however, is the novel training procedure. For all of their positive attributes, the adversarial training procedure for GANs is well known to be fairly difficult to deal with. As a result, the insight that if a mapping from noise vectors to training images is learned directly, other noise images still result in natural images is interesting.\n\nHowever, I do have a few questions for the authors, mostly centered around the choice of noise vectors.\n\nIn the paper, you mention that you \"initialize the z by either sampling them from a Gaussian distribution or by taking the whitened PCA of the raw image pixels.\" What does this mean? Do you sample them from a Gaussian on some tasks, and use PCA on others? Is it fair to assume from this that the initialization of z during training matters? If so, why?\n\nAfter training, you mention that you fit a full Gaussian to the noise vectors learned during training and sample from this to generate new images. I would be interested in seeing some study of the noise vectors learned during training. Are they multimodal, or is a unimodal distribution indeed sufficient? Does a Gaussian do a good job (in terms of likelihood) of fitting the noise vectors, or would some other model (even something like kernel density estimation) allow for higher probability noise vectors (and therefore potentially higher quality images) to be drawn? Does the choice of distribution even matter, or do you think uniform random vectors from the space would produce acceptable images?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642520541,"tcdate":1511799117545,"number":1,"cdate":1511799117545,"id":"BkILtntlz","invitation":"ICLR.cc/2018/Conference/-/Paper849/Official_Review","forum":"ryj38zWRb","replyto":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference/Paper849/AnonReviewer1"],"readers":["everyone"],"content":{"title":"OPTIMIZING THE LATENT SPACE OF GENERATIVE NETWORKS","rating":"4: Ok but not good enough - rejection","review":"Summary: The authors observe that the success of GANs can be attributed to two factors; leveraging the inductive bias of deep CNNs and the adversarial training protocol. In order to disentangle the factors of success, and they propose to eliminate the adversarial training protocol while maintaining the first factor. The proposed Generative Latent Optimization (GLO) model maps a learnable noise vector to the real images of the dataset by minimizing a reconstruction loss. The experiments are conducted on CelebA and LSUN-Bedroom datasets. \n\nStrengths: \nThe paper is well written and the topic is relevant for the community.\nThe notations are clear, as far as I can tell, there are no technical errors.\nThe design choices are well motivated in Chapter 2 which makes the main idea easy to grasp. \nThe image reconstruction results are good. \nThe experiments are conducted on two challenging datasets, i.e. CelebA and LSUN-Bedroom.\n\nWeaknesses:\nA relevant model is Generative Moment Matching Network (GMMN) which can also be thought of as a “discriminator-less GAN”. However, the paper does not contrast GLO with GMMN either in the conceptual level or experimentally. \n\nAnother relevant model is Variational Autoencoders (VAE) which also learns the data distribution through a learnable latent representation by minimizing a reconstruction loss. The paper would be more convincing if it provided a comparison with VAE.\n\nIn general, having no comparisons with other models proposed in the literature as improvements over GAN such as ACGAN, InfoGAN, WGAN weakens the experimental section.\n\nThe evaluation protocol is quite weak: CelebA images are 128x128 while LSUN images are 64x64. Especially since it is a common practice nowadays to generate much higher dimensional images, i.e. 256x256, the results presented in this paper appear weak. \n\nAlthough the reconstruction examples (Figure 2 and 3) are good, the image generation results (Figure 4 and 5) are worse than GAN, i.e. the 3rd images in the 2nd row in Figure 4 for instance has unrealistic artifacts, the entire Figure 5 results are quite boxy and unrealistic. The authors mention in Section 3.3.2 that they leave the careful modeling of Z to future work, however the paper is quite incomplete without this.\n\nIn Section 3.3.4, the authors claim that the latent space that GLO learns is interpretable. For example, smiling seems correlated with the hair color in Figure 6. This is a strong claim based on one example, moreover the evidence of this claim is not as obvious (based on the figure) to the reader. Moreover, in Figure 8, the authors claim that the principal components of the GLO latent space is interpretable. However it is not clear from this figure what each eigenvector generates. The authors’ observations on Figure 8 and 9 are not clearly visible through manual inspection.  \n\nFinally, as a minor note, the paper has some vague statements such as\n“A linear interpolation in the noise space will generate a smooth interpolation of visually-appealing images”\n“Several works attempt at recovering the latent representation of an image with respect to a generator.”\nTherefore, a careful proofreading would improve the exposition. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515188041439,"tcdate":1509136050552,"number":849,"cdate":1509739065072,"id":"ryj38zWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryj38zWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Optimizing the Latent Space of Generative Networks","abstract":"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most applications, GAN models share two aspects in common. On the one hand, GANs training involves solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions. On the other hand, the generator and the discriminator are parametrized in terms of deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators without using discriminators, thus avoiding the instability of adversarial optimization problems. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: learning from large data, synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors.","pdf":"/pdf/305fd049211de562d6a10bc2abc11952de4632cf.pdf","TL;DR":"Are GANs successful because of adversarial training or the use of ConvNets? We show a ConvNet generator trained with a simple reconstruction loss and learnable noise vectors leads many of the desirable properties of a  GAN.","paperhash":"anonymous|optimizing_the_latent_space_of_generative_networks","_bibtex":"@article{\n  anonymous2018optimizing,\n  title={Optimizing the Latent Space of Generative Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj38zWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper849/Authors"],"keywords":["generative models","latent variable models","image generation","generative adversarial networks","convolutional neural networks"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}