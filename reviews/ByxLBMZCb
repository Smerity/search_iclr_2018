{"notes":[{"tddate":null,"ddate":null,"tmdate":1512367394953,"tcdate":1512367394953,"number":2,"cdate":1512367394953,"id":"rkimHPzbz","invitation":"ICLR.cc/2018/Conference/-/Paper834/Official_Review","forum":"ByxLBMZCb","replyto":"ByxLBMZCb","signatures":["ICLR.cc/2018/Conference/Paper834/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good idea but writing/execution is not upto the mark.","rating":"5: Marginally below acceptance threshold","review":"Summary:\n\nThis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems. The paper studies locally open maps, which preserve the local minima geometry. Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map. Theorem 3 provides conditions under which the multiplication X*Y is a locally open map. For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima.  \n\nComments:\n\nThe locally open maps (Behrends 2017) is an interesting concept. However I am not convinced that the paper is able to use stronger results about the geometry of linear/neural networks. Further the claims all over the paper, comparing with the existing works. are over the top and not justified. I believe the paper needs a significant rewriting.\n\nThe results are not a strict improvement over existing works. For neural networks, Nguyen and Hein (2017) assume the link function is differentiable. This paper assumes the link function is invertible. Both papers can handle sigmoid/tanh, but cannot handle ReLU.\n\nResults for linear networks are not an improvement over existing works. Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point.  Results are also written in a confusing way as stating each critical point is a saddle or a global minima. Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly.\n\nProof of Lemma 7 seems to have typos/mistakes. What is \\bar{W_i}? Why are the first two equations just showing d_i \\leq d_i ? How do you use this to conclude locally openness of \\mathcal{M}?\n\nAuthors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Models: Critical Points and Local Openness","abstract":"With the increasing interest in deeper understanding of  the loss surface of many non-convex deep models, this paper presents a unifying framework to study the local/global equivalence of the optimization problem arising from training of such non-convex models. Using the local openness property of the underlying training models,  we provide sufficient conditions under which any local optimum of the resulting optimization problem is  global. Our result unifies and extends many of the existing results in the literature. For example, our theory shows that when the input data matrix X is full row rank, all  non-degenerate local optima of the optimization problem for training linear deep model with squared loss error are  global minima.  Moreover, for two layer linear models, we show that all degenerate critical points are either global or second order saddles and the non-degenerate local optima are global.  Unlike many existing results in the literature, our result assumes no assumption  on the target data matrix Y. For non-linear deep models having certain pyramidal structure with invertible activation functions, we can show global/local equivalence with no assumption on the differentiability of the activation function. Our results are the direct consequence of our main theorem  that provides necessary and sufficient conditions for the matrix multiplication mapping to be locally open in its range.","pdf":"/pdf/b35ce17ff6dd71d5c76d47ff50ae081efbdc2045.pdf","paperhash":"anonymous|learning_deep_models_critical_points_and_local_openness","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Models: Critical Points and Local Openness},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByxLBMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper834/Authors"],"keywords":["Deep Learning","Local and Global minima","Local Openness"]}},{"tddate":null,"ddate":null,"tmdate":1512222783540,"tcdate":1511010510340,"number":1,"cdate":1511010510340,"id":"BkL0g3a1f","invitation":"ICLR.cc/2018/Conference/-/Paper834/Official_Review","forum":"ByxLBMZCb","replyto":"ByxLBMZCb","signatures":["ICLR.cc/2018/Conference/Paper834/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice presentation, useful results, interesting future directions. Weak accept mostly for the reason that the paper mostly reproduces similar results in the literature with a different methodology. I'm not strongly opinioned until I see also the other reviews.","rating":"6: Marginally above acceptance threshold","review":"Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc. The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks.\nAs the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y).\n\nOriginality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work. In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results. Thus, I would say original.\n\nImportance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points. While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference.\n\nClarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version.\n\nComments:\n1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5).\n\n2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector?\n\n3. In section 5, the authors make a connection with the work of Ge et al. 2016. They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016. However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation). It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work. If that is the case, could the authors describe this a bit further?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Deep Models: Critical Points and Local Openness","abstract":"With the increasing interest in deeper understanding of  the loss surface of many non-convex deep models, this paper presents a unifying framework to study the local/global equivalence of the optimization problem arising from training of such non-convex models. Using the local openness property of the underlying training models,  we provide sufficient conditions under which any local optimum of the resulting optimization problem is  global. Our result unifies and extends many of the existing results in the literature. For example, our theory shows that when the input data matrix X is full row rank, all  non-degenerate local optima of the optimization problem for training linear deep model with squared loss error are  global minima.  Moreover, for two layer linear models, we show that all degenerate critical points are either global or second order saddles and the non-degenerate local optima are global.  Unlike many existing results in the literature, our result assumes no assumption  on the target data matrix Y. For non-linear deep models having certain pyramidal structure with invertible activation functions, we can show global/local equivalence with no assumption on the differentiability of the activation function. Our results are the direct consequence of our main theorem  that provides necessary and sufficient conditions for the matrix multiplication mapping to be locally open in its range.","pdf":"/pdf/b35ce17ff6dd71d5c76d47ff50ae081efbdc2045.pdf","paperhash":"anonymous|learning_deep_models_critical_points_and_local_openness","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Models: Critical Points and Local Openness},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByxLBMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper834/Authors"],"keywords":["Deep Learning","Local and Global minima","Local Openness"]}},{"tddate":null,"ddate":null,"tmdate":1509739076208,"tcdate":1509135688027,"number":834,"cdate":1509739073547,"id":"ByxLBMZCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByxLBMZCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Deep Models: Critical Points and Local Openness","abstract":"With the increasing interest in deeper understanding of  the loss surface of many non-convex deep models, this paper presents a unifying framework to study the local/global equivalence of the optimization problem arising from training of such non-convex models. Using the local openness property of the underlying training models,  we provide sufficient conditions under which any local optimum of the resulting optimization problem is  global. Our result unifies and extends many of the existing results in the literature. For example, our theory shows that when the input data matrix X is full row rank, all  non-degenerate local optima of the optimization problem for training linear deep model with squared loss error are  global minima.  Moreover, for two layer linear models, we show that all degenerate critical points are either global or second order saddles and the non-degenerate local optima are global.  Unlike many existing results in the literature, our result assumes no assumption  on the target data matrix Y. For non-linear deep models having certain pyramidal structure with invertible activation functions, we can show global/local equivalence with no assumption on the differentiability of the activation function. Our results are the direct consequence of our main theorem  that provides necessary and sufficient conditions for the matrix multiplication mapping to be locally open in its range.","pdf":"/pdf/b35ce17ff6dd71d5c76d47ff50ae081efbdc2045.pdf","paperhash":"anonymous|learning_deep_models_critical_points_and_local_openness","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Deep Models: Critical Points and Local Openness},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByxLBMZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper834/Authors"],"keywords":["Deep Learning","Local and Global minima","Local Openness"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}