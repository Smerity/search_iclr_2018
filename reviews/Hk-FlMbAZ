{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222762361,"tcdate":1511815449072,"number":3,"cdate":1511815449072,"id":"S1ZQKlqxM","invitation":"ICLR.cc/2018/Conference/-/Paper770/Official_Review","forum":"Hk-FlMbAZ","replyto":"Hk-FlMbAZ","signatures":["ICLR.cc/2018/Conference/Paper770/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"\n1) Summary\nThis paper proposes a new approach to defending against adversarial attacks based on the manifold assumption of natural data. Specifically, this method takes inputs (possibly coming from an adversarial attack), project their semantic representation into the closest data class manifold. The authors show that adversarial attack techniques can be with their algorithm for attack prevention. In experiments, they show that using their method on top of a base model achieves perfect success rate on attacks that the base model is vulnerable to while retaining generalizability.\n\n\n2) Pros:\n+ Novel/interesting way of defending against adversarial attacks by taking advantage of the manifold assumption.\n+ Well stated formulation and intuition.\n+ Experiments validate the claim, and insightful discussion about the limitations and advantages of the proposed method.\n\n3) Cons:\nNumber of test examples used too small:\nAs mentioned in the paper, the number of testing points is a weakness. There needs to be more test examples to make a strong conclusion about the methodâ€™s performance in the experiments.\n\nComparison against other baselines:\nEven though the method proposes a new approach for dealing with adversarial attacks using Madry et al. as base model, it would be useful to the community to see how this method works with other base models.\n\nAlgorithm generalizability:\nAs mentioned by the authors, their method depends on assumptions of the learned embeddings by the model being used. This makes the method less attractive for people that may be interested in dealing with adversarial examples in, for example, reinforcement learning problems. Can the authors comment on this?\n\nAdditional comments:\nThe writing needs to be polished.\n\n\n4) Conclusion:\nOverall, this is a very interesting work on how to deal with adversarial attacks in deep learning, while at the same time, it shows encouraging results of the application of the proposed method. The experimental section could improve a little bit in terms of baselines and test examples as previously mentioned, and also the authors may give some comments on if there is a simple way to make their algorithm not depend on assumptions of the learned embeddings.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Manifold Assumption and Defenses Against Adversarial Perturbations","abstract":"In the adversarial perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and identifies another point $\\bfx'$, which is \\emph{nearby} $\\bfx$, that $F$ classifies \\emph{incorrectly}. In this paper we consider a defense method that is based on the \\emph{semantics} of $F$. Our starting point is the common \\emph{manifold assumption}, which states that natural data points lie on separate low dimensional manifolds for different classes. We then make a further postulate which states that \\emph{(a good model) $F$ is confident on natural points on the manifolds, but has low confidence on points outside of the manifolds,} where a natural measure of ``confident behavior'' is $\\|F(\\bfx)\\|_\\infty$  (i.e. how confident $F$ is about its prediction). Under this postulate, an adversarial example becomes a point that is outside of the low dimensional manifolds which $F$ has learned, but is still close to at least one manifold under some distance metric. Therefore, defending against adversarial perturbations becomes embedding an adversarial point back to the nearest manifold where natural points are drawn from. We propose algorithms to formalize this intuition and perform a preliminary evaluation. Noting that the effectiveness of our method depends on both how well $F$ satisfies the postulate and how effective we can conduct the embedding, we use a model trained recently by Madry et al., as the \\emph{base model}, and use \\emph{gradient based optimization}, such as the \\emph{Carlini-Wagner attack} (but now they are used for defense), as the \\emph{embedding procedure}. Our preliminary results are encouraging: The base model wrapped with the embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining the good generalization behavior of the base model.\n","pdf":"/pdf/020a5212e955a55a5edbe04c90ab3031f4304a37.pdf","TL;DR":"Defending against adversarial perturbations of neural networks from manifold assumption ","paperhash":"anonymous|manifold_assumption_and_defenses_against_adversarial_perturbations","_bibtex":"@article{\n  anonymous2018manifold,\n  title={Manifold Assumption and Defenses Against Adversarial Perturbations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk-FlMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper770/Authors"],"keywords":["manifold assumption","adversarial perturbation","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222762425,"tcdate":1511653683083,"number":2,"cdate":1511653683083,"id":"HyjN-YPlz","invitation":"ICLR.cc/2018/Conference/-/Paper770/Official_Review","forum":"Hk-FlMbAZ","replyto":"Hk-FlMbAZ","signatures":["ICLR.cc/2018/Conference/Paper770/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting ideas but at the current stage, this seems to be a preliminary work that is not well matured yet!","rating":"4: Ok but not good enough - rejection","review":"The manuscript proposes two objective functions based on the manifold assumption as defense mechanisms against adversarial examples. The two objective functions are based on assigning low confidence values to points that are near or off the underlying (learned) data manifold while assigning high confidence values to points lying on the data manifold. In particular, for an adversarial example that is distinguishable from the points on the manifold and assigned a low confidence by the model, is projected back onto the designated manifold such that the model assigns it a high confidence value. The authors claim that the two objective functions proposed in this manuscript provide such a projection onto the desired manifold and assign high confidence for these adversarial points. These mechanisms, together with the so-called shell wrapper around the model (a deep learning model in this case) will provide the desired defense mechanism against adversarial examples.\n\nThe manuscript at the current stage seems to be a preliminary work that is not well matured yet. The manuscript is overly verbose and the arguments seem to be weak and not fully developed yet. More importantly, the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Manifold Assumption and Defenses Against Adversarial Perturbations","abstract":"In the adversarial perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and identifies another point $\\bfx'$, which is \\emph{nearby} $\\bfx$, that $F$ classifies \\emph{incorrectly}. In this paper we consider a defense method that is based on the \\emph{semantics} of $F$. Our starting point is the common \\emph{manifold assumption}, which states that natural data points lie on separate low dimensional manifolds for different classes. We then make a further postulate which states that \\emph{(a good model) $F$ is confident on natural points on the manifolds, but has low confidence on points outside of the manifolds,} where a natural measure of ``confident behavior'' is $\\|F(\\bfx)\\|_\\infty$  (i.e. how confident $F$ is about its prediction). Under this postulate, an adversarial example becomes a point that is outside of the low dimensional manifolds which $F$ has learned, but is still close to at least one manifold under some distance metric. Therefore, defending against adversarial perturbations becomes embedding an adversarial point back to the nearest manifold where natural points are drawn from. We propose algorithms to formalize this intuition and perform a preliminary evaluation. Noting that the effectiveness of our method depends on both how well $F$ satisfies the postulate and how effective we can conduct the embedding, we use a model trained recently by Madry et al., as the \\emph{base model}, and use \\emph{gradient based optimization}, such as the \\emph{Carlini-Wagner attack} (but now they are used for defense), as the \\emph{embedding procedure}. Our preliminary results are encouraging: The base model wrapped with the embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining the good generalization behavior of the base model.\n","pdf":"/pdf/020a5212e955a55a5edbe04c90ab3031f4304a37.pdf","TL;DR":"Defending against adversarial perturbations of neural networks from manifold assumption ","paperhash":"anonymous|manifold_assumption_and_defenses_against_adversarial_perturbations","_bibtex":"@article{\n  anonymous2018manifold,\n  title={Manifold Assumption and Defenses Against Adversarial Perturbations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk-FlMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper770/Authors"],"keywords":["manifold assumption","adversarial perturbation","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512222762477,"tcdate":1511239064210,"number":1,"cdate":1511239064210,"id":"rkgspXWgf","invitation":"ICLR.cc/2018/Conference/-/Paper770/Official_Review","forum":"Hk-FlMbAZ","replyto":"Hk-FlMbAZ","signatures":["ICLR.cc/2018/Conference/Paper770/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Not convincing.","rating":"3: Clear rejection","review":"The authors argue that \"good\" classifiers naturally represent the classes in a classification as well-separated manifolds, and that adversarial examples are low-confidence examples lying near to one of these manifolds. The authors suggest \"fixing\" adversarial examples by projecting them back to the manifold, essentially by finding a point near the adversarial example that has high confidence.\n\nThere are numerous issues here, which taken together, make the whole story pretty unconvincing.\n\nThe term \"manifold\" is used very sloppily. To be fair, this is unfortunately common in modern machine learning. An actual manifold is a specific mathematical structure with specific properties. In ML, what is generally hypothesized is that the data (often per class) lives \"near\" to some \"low-dimensional\" structure. In this paper, even the low-dimensionality isn't used --- the \"manifold assumption\" is used as a stand-in for \"the regions associated with different classes are well-separated.\" (This is partially discussed in Section 6, where the authors point out correctly that the same defense as used here could be used with a 1-nn model.) This is fine as far as it goes, but the paper refs Basri & Jacobs 2016 multiple times as if it says anything relevant about this paper: Basri & Jacobs is specifically about the ability of deep nets to fit data that falls on (actual, mathematical) manifolds. This reference doesn't add much to the present story.\n\nThe essential argument of the paper rests on the \"Postulate: (A good model) F is confident on natural points drawn from the manifolds, but has low confidence on points outside of the manifolds.\" \n\nThis postulate is sloppy and speculative. For instance, taken in its strong form, if believe the postulate, then a good model:\n1. Can classify all \"natural points\" from all classes with 100% accuracy.\n2. Can detect adversarial points with 100% accuracy because all high-confidence points are correct classifications and all low-confidence points are adversarial.\n3. All adversarial examples will be low-confidence.\n\nPoint 1 makes it clear that no good model F fully satisfying the postulate exists --- models never achieve 100% accuracy on difficult real-world distributions. But the method for dealing with adversarial examples seems to require Points 2 and 3 being true.\n\nTo be fair, the paper more-or-less admits that how true these points are is not known and is important. Nevertheless, I think this paper comes pretty close to arguing something that I *think* is not true, and doesn't do much to back up its argument. Because of the quality of the writing (generally sloppy), it's hard to tell, but I believe the authors are basically arguing that:\na. You can generally easily detect adversarial points because they are low confidence.\nb. If you go through a procedure to find a point near your adversarial point that is high-confidence, you'll get the \"correct\" (or perhaps \"original\") class back.\n\nI think b follows from a, but a is extremely suspect. I do not personally work in adversarial examples, and briefly looking at the literature, it seems that most authors *do* focus on how something is classified and not its confidence, but I don't think it's *that* hard to generate high-confidence adversarial examples. Early work by Goodfellow et al. (\"Explaining and Harnessing Adversarial Examples\", Figure 1, shows an example where the incorrect classification has very high confidence. The present paper only uses Carlini-Wagner attacks. From a read of Carlini-Wagner, it seems they are heavily concerned with finding *minimal* perturbations to achieve a given misclassification; this will of course produce low-confidence adversaries, but I see no reason why this is a general property of all adversarial examples.\n\nThe experiments are weak. I applaud the authors for mentioning the experiments are very preliminary, but that doesn't make them any less weak. \n\nWhat are we to make of the one image discussed at the end of Section 5 and shown in Figure 1? The authors note that the original image gives low-confidence for the correct class. (Does this mean that the classifier isn't \"good\"? Is it evidence against some kind of manifold assumption?) The authors note the adversarial category has significantly higher confidence, and say \"in this case, it seems that it is the vagueness of the signals/data that lead to a natural difficulty.\" But the signals and data are ALWAYS vague. If they weren't, machine learning would be easy. This paper proposes something, looks at a tiny number of examples, and already finds a counterexample to the theory. What's the evidence *for* the theory? \n\nA lot of writing is given over to how this method is \"semantic\", and I just don't buy it. The connection to manifolds is weak. The basic argument here is really \"(1) If our classifiers produce smooth well-separated high-confidence regions, (2) then we can detect adversaries because they're low-confidence, and (3) we can correct adversaries by projecting them back to high-confidence.\" (1) seems vastly unlikely to me based on all my experience: neural nets often get things wrong, they often get things wrong with high confidence, and when they're right, the confidence is at least sometimes low. The authors use a sloppy postulate about good models and so could perhaps argue I've never seen a good model, but the methods of this paper require a good model. (2) seems to follow logically from (1). (3) is also suspect --- perturbations which are *minimal* can be corrected as this paper does (and Carlini-Wagner attacks are minimal by design), but there's no reason to expect general perturbations to be minimal.\n\nThe writing is poor throughout. It's generally readable, but the wordings are often odd, and sometimes so odd it's hard to tell what was meant. For instance, I spent awhile trying to decide whether the authors assumed common classifiers are \"good\" (according to the postulate) or whether this paper was about a way to *make* classifiers good (I eventually decided the former).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Manifold Assumption and Defenses Against Adversarial Perturbations","abstract":"In the adversarial perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and identifies another point $\\bfx'$, which is \\emph{nearby} $\\bfx$, that $F$ classifies \\emph{incorrectly}. In this paper we consider a defense method that is based on the \\emph{semantics} of $F$. Our starting point is the common \\emph{manifold assumption}, which states that natural data points lie on separate low dimensional manifolds for different classes. We then make a further postulate which states that \\emph{(a good model) $F$ is confident on natural points on the manifolds, but has low confidence on points outside of the manifolds,} where a natural measure of ``confident behavior'' is $\\|F(\\bfx)\\|_\\infty$  (i.e. how confident $F$ is about its prediction). Under this postulate, an adversarial example becomes a point that is outside of the low dimensional manifolds which $F$ has learned, but is still close to at least one manifold under some distance metric. Therefore, defending against adversarial perturbations becomes embedding an adversarial point back to the nearest manifold where natural points are drawn from. We propose algorithms to formalize this intuition and perform a preliminary evaluation. Noting that the effectiveness of our method depends on both how well $F$ satisfies the postulate and how effective we can conduct the embedding, we use a model trained recently by Madry et al., as the \\emph{base model}, and use \\emph{gradient based optimization}, such as the \\emph{Carlini-Wagner attack} (but now they are used for defense), as the \\emph{embedding procedure}. Our preliminary results are encouraging: The base model wrapped with the embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining the good generalization behavior of the base model.\n","pdf":"/pdf/020a5212e955a55a5edbe04c90ab3031f4304a37.pdf","TL;DR":"Defending against adversarial perturbations of neural networks from manifold assumption ","paperhash":"anonymous|manifold_assumption_and_defenses_against_adversarial_perturbations","_bibtex":"@article{\n  anonymous2018manifold,\n  title={Manifold Assumption and Defenses Against Adversarial Perturbations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk-FlMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper770/Authors"],"keywords":["manifold assumption","adversarial perturbation","neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1509739112501,"tcdate":1509134457384,"number":770,"cdate":1509739109840,"id":"Hk-FlMbAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk-FlMbAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Manifold Assumption and Defenses Against Adversarial Perturbations","abstract":"In the adversarial perturbation problem of neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly, and identifies another point $\\bfx'$, which is \\emph{nearby} $\\bfx$, that $F$ classifies \\emph{incorrectly}. In this paper we consider a defense method that is based on the \\emph{semantics} of $F$. Our starting point is the common \\emph{manifold assumption}, which states that natural data points lie on separate low dimensional manifolds for different classes. We then make a further postulate which states that \\emph{(a good model) $F$ is confident on natural points on the manifolds, but has low confidence on points outside of the manifolds,} where a natural measure of ``confident behavior'' is $\\|F(\\bfx)\\|_\\infty$  (i.e. how confident $F$ is about its prediction). Under this postulate, an adversarial example becomes a point that is outside of the low dimensional manifolds which $F$ has learned, but is still close to at least one manifold under some distance metric. Therefore, defending against adversarial perturbations becomes embedding an adversarial point back to the nearest manifold where natural points are drawn from. We propose algorithms to formalize this intuition and perform a preliminary evaluation. Noting that the effectiveness of our method depends on both how well $F$ satisfies the postulate and how effective we can conduct the embedding, we use a model trained recently by Madry et al., as the \\emph{base model}, and use \\emph{gradient based optimization}, such as the \\emph{Carlini-Wagner attack} (but now they are used for defense), as the \\emph{embedding procedure}. Our preliminary results are encouraging: The base model wrapped with the embedding procedure achieves almost perfect success rate in defending against attacks that the base model fails on, while retaining the good generalization behavior of the base model.\n","pdf":"/pdf/020a5212e955a55a5edbe04c90ab3031f4304a37.pdf","TL;DR":"Defending against adversarial perturbations of neural networks from manifold assumption ","paperhash":"anonymous|manifold_assumption_and_defenses_against_adversarial_perturbations","_bibtex":"@article{\n  anonymous2018manifold,\n  title={Manifold Assumption and Defenses Against Adversarial Perturbations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk-FlMbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper770/Authors"],"keywords":["manifold assumption","adversarial perturbation","neural networks"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}