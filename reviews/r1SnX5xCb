{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222623970,"tcdate":1512020219797,"number":3,"cdate":1512020219797,"id":"Bk4-YGplf","invitation":"ICLR.cc/2018/Conference/-/Paper350/Official_Review","forum":"r1SnX5xCb","replyto":"r1SnX5xCb","signatures":["ICLR.cc/2018/Conference/Paper350/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Intriguing angle on an under-studied problem in health+ML with strong empirical results","rating":"6: Marginally above acceptance threshold","review":"This is a very interesting submission that takes an interesting angle on clinical time series modeling, namely, actively choosing when to measure while simultaneously attempting to impute missing measurements and predict outcomes of interest. The proposed solution formulates everything as a giant learning problem that involves learning (a) an interpolation function that predicts a missing measurement from its past and present, (b) an imputation function that predicts a missing measurement from other variables at the same time step, (c) a prediction function that predicts outcomes of interest, including forecasting future measurements, (d) an error estimation function that estimates error of the forecasts in (c). These four pieces are then used in combination with a heuristic to decide when certain variables should be measured. This framework is used with a GRU-RNN architecture and in experiments with two datasets, outperforms a number of strong baselines.\n\nI am inclined toward accepting this paper due to the significance of the problem, the ingenuity of their proposed approach, and the strength of the empirical results. However, I think that there is a lot of room for improvement in the current manuscript, which is difficult to read and fully grasp. This will lessen its impact in the long run, so I encourage the authors to strive to make it clearer. If they succeed in improving it during the review period, I will gladly raise my score.\n\nNOTE: please do a thorough editorial pass for the next version -- I found at least one typo in the references (Yu, et al. \"Active sensin.\")\n\nQUALITY\n\nThis is solid research, and I have few complaints about the work itself (most of my feedback will focus on clarity). I will list some strengths (+) and weaknesses (-) below and try to provide actionable feedback:\n\n+ Very important problem that receives limited attention from the community\n+ I like the formulation of active sensing as a prediction loss optimization problem\n+ The learning problem is pretty intuitive and is well-suited to deep learning architectures since it yields a differentiable (albeit complex) loss function\n+ The results speak for themselves -- for adverse event prediction in the MIMIC-III task, DS improves upon the nearest baseline by almost 9 points in AUC! More interestingly, using Deep Sensing to create a \"resampled\" version of the data set improves the performance of the baselines. It also achieves much more accurate imputation than standard approaches.\n\n- The proposed approach is pretty complex, and it's unclear what is the relative contribution of each component. I think it is incumbent to do an ablation study where different components are removed to see how performance degrades, if at all. For example, how would the model perform with interpolation but not imputation? Is bidirectional interpolation necessary, or would forward interpolation work sufficiently well (the obvious disadvantage of the bidirectional approach is the need to rerun inference at each new time step). Is it necessary to use both the actual AND predicted measurements as inputs (what if we instead used actual measurements when available and predicted otherwise)?\n- The experiments are thorough with a nice selection of baselines, but I wonder if perhaps Futoma, et al. [1] would be a stronger baseline than Choi, Che, or Lipton. They showed improvements over similar magnitude over baselines for predicting sepsis, and their approach (a differentiable GP-approximating layer) is conceptually simpler and has other benefits. I think it could be combined with the active sensing framework in this paper.\n- The one question this framework appears incapable of answering in a straightforward manner is WHEN the next set of measurements should be made. One could imagine a heuristic in which predictive loss/gain are assessed at different points in the future, but the search space will be huge, particularly if one wants to optimize over measurements at different points, e.g., maybe the optimal strategy is to take roughly hourly vitals but no labs until 12 hours from now. Indeed, it might be impossible to train such a model properly since the sampling times in the available training data are highly biased.\n- One thing potentially missing from this paper is a theoretical analysis to understand and analyze its behavior and performance. My very superficial analysis is that the prediction loss/gain framework is related to minimizing entropy and that the heuristic for choosing which variables to measure is a greedy search. A theoretical treatment to understand whether and how this approach might be sub-optimal would be very desirable.\n- Are the measurement and prediction \"confidence intervals\" proper confidence intervals (in the formal statistical sense)? I don't think so -- I wonder if there are alternatives for measuring uncertainty (formal CIs or maybe a Bayesian approach?).\n\nCLARITY\n\nMy main complaint about this paper is clarity -- it is not difficult to read per se, but it is difficult to fully grok the details of the approach and the experimental setup. From the current manuscript, I do not feel confident that I could re-implement Deep Sensing or reproduce the experiments. This is especially important in healthcare research, where there is a minor reproducibility crisis, even for resarch using MIMIC (see [2]). Of course, this can be alleviated by publishing the code and using a public benchmark [3], but it can't hurt to clarify these details in the paper itself (and to add an appendix if length is an issue).\n\nHere are some potential areas for improvement:\n\n- The structure of the paper is a bit weird. In particular section 2 (pages 2-4) seems to be a grab bag of miscellaneous topics, at least by the headers. I think the content is fine -- perhaps section 2 can be renamed as \"Background,\" subsection 2.1 renamed as \"Notation,\" and subsection 2.2 renamed as \"Problem Formulation\" (or similar). I'd just combine subsection 2.3 with the previous one and explain that Figure 1 illustrates the problem formulation.\n- The active sensing procedure (subsection 2.2, page 3, equation 1 and the equations just above) is unclear. How are the minimization and maximization performed (gradient descent, line search, etc.)? How is the search for the subset of measurement variables performed (greedy search)? The latter is a discrete search, and I doubt it's, e.g., submodular, so it must be a nontrivial optimization.\n- Related, I'm a little confused about equation 1: C_T is the set of variables that should be measured, but C_T is being used to index prediction targets -- is this a typo?\n- The related work section is pretty extensive, but I wonder if it should also include work on active learning (Bayesian active learning, in particular, has been applied to sensing), submodular optimization (for sensor placement, which can be thought of as a spatial version of active sensing), and reinforcement learning.\n- I don't understand how the training data for the interpolation and imputation functions are constructed. I *think* that is what is described in the Adaptive Sampling subsection on page 8, but that is unclear. The word \"representations\" is used here, but that's an overloaded term in machine learning, and its meaning here is unclear from context. It appears that maybe there's an iterative procedure in which we alternate between training a model and then resampling the data using the model -- starting with the full data set.\n- The distinction between training and inference is not clear to me, at least with respect to the active sensing component. Is selective sampling performed during training? If so, what happens if the model elects to sample a variable at time t that is not actually measured in the data?\n- I don't follow subsection 4.2 (pages 8-9) at all -- what is it describing? If by \"runtime\" the authors refer to the computational complexity of the algorithm, then I would expect a Big-O analysis (none is provided -- it's just a rather vague discussion of what happens). I'd recommend removing this entire subsection and replacing it with, e.g., an Algorithm figure with pseudocode, as a more succinct description.\n- For the experiments, the authors provide insufficient detail about the data and task setup. Since MIMIC is publicly available, then readers ought (hypothetically) to be able to reproduce the experiments, but that is not currently possible. As an example, what adverse events are being predicted? How are they defined?\n- Figure 4 is nice, but it's not immediately obvious what the connection between observation rate and sampling cost. The authors should explain how a given observation rate is encoded as cost in the loss function.\n\nORIGINALITY\n\nWhile active sensing is not a new research topic per se, there has been very limited research into the specific question of choosing what clinical variables to measure when in the context of a given prediction problem. This is a topic that (in my experience) is frequently discussed but rarely studied in clinical informatics circles. Hence, this is a very original line of inquiry, and the prediction loss/gain framing is a unique angle.\n\nSIGNIFICANCE\n\nI anticipate this paper will generate significant interest and follow-up work, at least among clinical informaticists and machine learning + health researchers. The main blockers to a significant impact are the clarity of writing issues listed above -- and if the authors fail to publish their code.\n\nREFERENCES\n\n[1] Futoma, et al. An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection. MLHC 2017.\n[2] Johnson, et al. Reproducibility in critical care: a mortality prediction case study. MLHC 2017\n[3] Harutyunyan, et al. Multitask Learning and Benchmarking with Clinical Time Series Data. arXiv.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks","abstract":"For every prediction we might wish to make, we must decide what to observe (what source of information) and when to observe it. Because making observations is costly, this decision must trade off the value of information against the cost of observation. Making observations (sensing) should be an active choice. To solve the problem of active sensing we develop a novel deep learning architecture: Deep Sensing. At training time, Deep Sensing learns how to issue predictions at various cost-performance points. To do this, it creates multiple representations at various performance levels associated with different measurement rates (costs). This requires learning how to estimate the value of real measurements vs. inferred measurements, which in turn requires learning how to infer missing (unobserved) measurements.  To infer missing measurements, we develop a Multi-directional Recurrent Neural Network (M-RNN).  An M-RNN differs from a bi-directional RNN in that it sequentially operates across streams in addition to within streams, and because the timing of inputs into the hidden layers is both lagged and advanced.  At runtime, the operator prescribes a performance level or a cost constraint, and Deep Sensing determines what measurements to take and what to infer from those measurements, and then issues predictions. To demonstrate the power of our method, we apply it to two real-world medical datasets with significantly improved performance.","pdf":"/pdf/714bbe26098d0e35553a3f84188be102486594a1.pdf","paperhash":"anonymous|deep_sensing_active_sensing_using_multidirectional_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SnX5xCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper350/Authors"],"keywords":["Active Sensing","Timely Prediction","Irregular Sampling","Missing Data"]}},{"tddate":null,"ddate":null,"tmdate":1512222624009,"tcdate":1511971287931,"number":2,"cdate":1511971287931,"id":"HJg15Lhgz","invitation":"ICLR.cc/2018/Conference/-/Paper350/Official_Review","forum":"r1SnX5xCb","replyto":"r1SnX5xCb","signatures":["ICLR.cc/2018/Conference/Paper350/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper proposes a novel method to solve the problem of active sensing","rating":"7: Good paper, accept","review":"This paper proposes a novel method to solve the problem of active sensing from a new angle (Essentially, the active sensing is a kind of method that decides when (or where) to take new measurements and what measurements we should conduct at that time or (place)). By taking advantage of the characteristics of long-term memory and Bi-directionality of Bi-RNN and M-RNN, deep sensing can model multivariate time-series signals for predicting future labels and estimating the values of new measurements. The architecture of Deep Sensing basically consists of three components: \n1. Interpolation and imputation for each of channels where missing points exist;\n2. Prediction for the future labels in terms of the whole multivariate signals (The signal is a time-series data and made up of multiple channels, there is supposed to be a measured label for each moment of the signal); \n3. Active sensing for the future moments of each of the channels. \n\nPros\n\nThe novelty of this paper lies in using a neural network structure to solve a traditional statistical problem which was usually done by a Bayesian approach or using the idea of the stochastic process. \n\nA detailed description of the network architecture is provided and each of the configurations has been fully illustrated.  The explanation of the structure of the combined RNNs is rigorous but clear enough of understanding. \n\nThe method was tested on a large real dataset and got a really promising result based several rational assumptions (such as assuming some of the points are missing for evaluating the error of the interpolation & imputation).\n\nCons\n\nHow and why the architecture is designed in this way should be further discussed or explained. Some of the details of the design could be inferred indirectly. But somewhere like the structure of the interpolation in Fig.3 doesn't have any further discussion. For example, why using GRU based RNN, and how Bi-RNN benefits here. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks","abstract":"For every prediction we might wish to make, we must decide what to observe (what source of information) and when to observe it. Because making observations is costly, this decision must trade off the value of information against the cost of observation. Making observations (sensing) should be an active choice. To solve the problem of active sensing we develop a novel deep learning architecture: Deep Sensing. At training time, Deep Sensing learns how to issue predictions at various cost-performance points. To do this, it creates multiple representations at various performance levels associated with different measurement rates (costs). This requires learning how to estimate the value of real measurements vs. inferred measurements, which in turn requires learning how to infer missing (unobserved) measurements.  To infer missing measurements, we develop a Multi-directional Recurrent Neural Network (M-RNN).  An M-RNN differs from a bi-directional RNN in that it sequentially operates across streams in addition to within streams, and because the timing of inputs into the hidden layers is both lagged and advanced.  At runtime, the operator prescribes a performance level or a cost constraint, and Deep Sensing determines what measurements to take and what to infer from those measurements, and then issues predictions. To demonstrate the power of our method, we apply it to two real-world medical datasets with significantly improved performance.","pdf":"/pdf/714bbe26098d0e35553a3f84188be102486594a1.pdf","paperhash":"anonymous|deep_sensing_active_sensing_using_multidirectional_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SnX5xCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper350/Authors"],"keywords":["Active Sensing","Timely Prediction","Irregular Sampling","Missing Data"]}},{"tddate":null,"ddate":null,"tmdate":1512222624047,"tcdate":1511807767428,"number":1,"cdate":1511807767428,"id":"HJyXsRtef","invitation":"ICLR.cc/2018/Conference/-/Paper350/Official_Review","forum":"r1SnX5xCb","replyto":"r1SnX5xCb","signatures":["ICLR.cc/2018/Conference/Paper350/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting approach, but limited engagement with related work","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new approach to determining what to measure and when to measure it, using a novel deep learning architecture. The problem addressed is important and timely and advances here may have an impact on many application areas outside medicine. The approach is evaluated on real-world medical datasets and has increased accuracy over the other methods compared against.\n\n+ A key advantage of the approach is that it continually learns from the collected data, using new measurements to update the model, and that it runs efficiently even on large real-world datasets.\n\n-However, the related work section is significantly underdeveloped, making it difficult to really compare the approach to the state of the art. The paper is ambitious and claims to address a variety of problems, but as a result each segment of related work seems to have been shortchanged. In particular, the section on missing data is missing a large amount of recent and related work. Normally, methods for handling missing data are categorized based on the missingness model (MAR/MCAR/MNAR). The paper seems to assume all data are missing at random, which is also a significant limitation of the methods.\n\n-The paper is organized in a nonstandard way, with the methods split across two sections, separated by the related work. It would be easier to follow with a more common intro/related work/methods structure.\n\nQuestions:\n-One of the key motivations for the approach is sensing in medicine. However, many tests come as a group (e.g. the chem-7 or other panels). In this case, even if the only desired measurement is glucose, others will be included as well. Is it possible to incorporate this? It may change the threshold for the decision, as a combination of measures can be obtained for the same cost.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks","abstract":"For every prediction we might wish to make, we must decide what to observe (what source of information) and when to observe it. Because making observations is costly, this decision must trade off the value of information against the cost of observation. Making observations (sensing) should be an active choice. To solve the problem of active sensing we develop a novel deep learning architecture: Deep Sensing. At training time, Deep Sensing learns how to issue predictions at various cost-performance points. To do this, it creates multiple representations at various performance levels associated with different measurement rates (costs). This requires learning how to estimate the value of real measurements vs. inferred measurements, which in turn requires learning how to infer missing (unobserved) measurements.  To infer missing measurements, we develop a Multi-directional Recurrent Neural Network (M-RNN).  An M-RNN differs from a bi-directional RNN in that it sequentially operates across streams in addition to within streams, and because the timing of inputs into the hidden layers is both lagged and advanced.  At runtime, the operator prescribes a performance level or a cost constraint, and Deep Sensing determines what measurements to take and what to infer from those measurements, and then issues predictions. To demonstrate the power of our method, we apply it to two real-world medical datasets with significantly improved performance.","pdf":"/pdf/714bbe26098d0e35553a3f84188be102486594a1.pdf","paperhash":"anonymous|deep_sensing_active_sensing_using_multidirectional_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SnX5xCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper350/Authors"],"keywords":["Active Sensing","Timely Prediction","Irregular Sampling","Missing Data"]}},{"tddate":null,"ddate":null,"tmdate":1509739350069,"tcdate":1509102508755,"number":350,"cdate":1509739347407,"id":"r1SnX5xCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1SnX5xCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks","abstract":"For every prediction we might wish to make, we must decide what to observe (what source of information) and when to observe it. Because making observations is costly, this decision must trade off the value of information against the cost of observation. Making observations (sensing) should be an active choice. To solve the problem of active sensing we develop a novel deep learning architecture: Deep Sensing. At training time, Deep Sensing learns how to issue predictions at various cost-performance points. To do this, it creates multiple representations at various performance levels associated with different measurement rates (costs). This requires learning how to estimate the value of real measurements vs. inferred measurements, which in turn requires learning how to infer missing (unobserved) measurements.  To infer missing measurements, we develop a Multi-directional Recurrent Neural Network (M-RNN).  An M-RNN differs from a bi-directional RNN in that it sequentially operates across streams in addition to within streams, and because the timing of inputs into the hidden layers is both lagged and advanced.  At runtime, the operator prescribes a performance level or a cost constraint, and Deep Sensing determines what measurements to take and what to infer from those measurements, and then issues predictions. To demonstrate the power of our method, we apply it to two real-world medical datasets with significantly improved performance.","pdf":"/pdf/714bbe26098d0e35553a3f84188be102486594a1.pdf","paperhash":"anonymous|deep_sensing_active_sensing_using_multidirectional_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1SnX5xCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper350/Authors"],"keywords":["Active Sensing","Timely Prediction","Irregular Sampling","Missing Data"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}