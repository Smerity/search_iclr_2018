{"notes":[{"tddate":null,"ddate":null,"tmdate":1515187536929,"tcdate":1515187536929,"number":5,"cdate":1515187536929,"id":"rktIavaQz","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Comment","forum":"r1gs9JgRZ","replyto":"r1gs9JgRZ","signatures":["ICLR.cc/2018/Conference/Paper203/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper203/Authors"],"content":{"title":"New Revision of the paper","comment":"We've added a new revision of the paper that addresses the following points:\n\n- Corrected the typos pointed out by reviewer #1\n- Added a reference to Inception v3, modified the classification CNN table to include references and both names for googlenet.  - Added improved accuracy numbers for ResNet-50\n- Added a paragraph to the conclusion on operation speedups, etc.\n\n\nWe thank the reviewers for their helpful comments and feedback. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515123813840,"tcdate":1515123813840,"number":4,"cdate":1515123813840,"id":"SyCvNOnQf","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Comment","forum":"r1gs9JgRZ","replyto":"rJwXkeOgM","signatures":["ICLR.cc/2018/Conference/Paper203/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper203/Authors"],"content":{"title":"Review Response","comment":"We thank the reviewer for the feedback.  As newer hardware such as Nvidia’s V100 and TitanV become more widely available, we should be able to see a speedup in training time. Performance results for GEMM, RNN, and CNN layers are available at DeepBench. Depending on the layer size and batch size, MP hardware can achieve 2~6x speedup for a given layer, we will add this information to the paper. We are working on measuring the improvement in end-to-end model training using MP hardware and more optimized libraries and frameworks - the studies in this paper used either older hardware, or Volta GPUs but very early libraries and frameworks with MP support.  These measurements are targeted for a subsequent publication as they couldn’t make the ICLR deadline. \n \nWhen it comes to the need for FP32 accumulation, while some networks did not need it others lost a few percentage points of accuracy when accumulating in fp16.  We will add this mention to the paper, but to maximize the success of initial training in MP we recommend employing all three proposed techniques.\n \nThank you for pointing out typos, we will address them in this paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515123891898,"tcdate":1515123683379,"number":3,"cdate":1515123683379,"id":"SyoJNu2Xf","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Comment","forum":"r1gs9JgRZ","replyto":"SyetJ_Pbz","signatures":["ICLR.cc/2018/Conference/Paper203/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper203/Authors"],"content":{"title":"Thanks ","comment":"Hello Stephen,\n\nThanks for pointing out that the advantage of this technique is not limited to reduction in memory only. We will add some more statements in the paper highlighting the potential speedup with hardware that supports mixed precision training. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515123522582,"tcdate":1515123522582,"number":2,"cdate":1515123522582,"id":"ryjSm_hQz","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Comment","forum":"r1gs9JgRZ","replyto":"SkSMlWcgG","signatures":["ICLR.cc/2018/Conference/Paper203/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper203/Authors"],"content":{"title":"Review Response","comment":"Thank you for your review and valuable feedback.  We are working on obtaining speedup numbers for mixed precision training with libraries and training frameworks that have been more extensively optimized for mixed precision (experiments in this study that were run on Volta GPUs used libraries and frameworks that had preliminary optimization for mixed precision).  \n\nInitial performance numbers are available in DeepBench which indicate a 2~6x speedup for an operation depending on layer size and batch size, as long as the layer is not limited by latency (as stated in the paper, mixed-precision improves performance for 2 out of 3 potential performance limiters - memory or arithmetic throughput, with latency being the third one).  For layers limited by memory bandwidth, as you point out, upper bound on speedup is 2x. The upper bound on speedups on Volta GPUs is 8x, if the operation is limited by floating point arithmetic. Full network speedups will be somewhat lower, depending on how many layers are limited by memory bandwidth or latency.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515123395905,"tcdate":1515123395905,"number":1,"cdate":1515123395905,"id":"S1nTzd2Qf","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Comment","forum":"r1gs9JgRZ","replyto":"SJQ3bonlG","signatures":["ICLR.cc/2018/Conference/Paper203/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper203/Authors"],"content":{"title":"Review Response ","comment":"Thank you for the review and comments.  The focus of the studies in the paper, as you point out, was to describe and validate the procedure for training with mixed precision without losing accuracy.  Experiments were run with libraries and frameworks that had preliminary support for mixed precision.  As shown in DeepBench (https://github.com/baidu-research/DeepBench), depending on the layer size and batch size, MP hardware can achieve 2~6x speedup for a layer that’s not latency-limited. We will add this mention and pointer to DeepBench results to the paper.  Measuring end to end speedups with more optimized frameworks is the focus for future work.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1512697738105,"tcdate":1512697719624,"number":1,"cdate":1512697719624,"id":"SyetJ_Pbz","invitation":"ICLR.cc/2018/Conference/-/Paper203/Public_Comment","forum":"r1gs9JgRZ","replyto":"SkSMlWcgG","signatures":["~Stephen_Merity1"],"readers":["everyone"],"writers":["~Stephen_Merity1"],"content":{"title":"Overall advantage is both memory and speed","comment":"You note in negatives that \"the overall advantage is only a 2x reduction in memory\". The paper notes (though only in the introduction section) that \"Performance (speed) ... is limited by one of three factors: arithmetic bandwidth, memory bandwidth, or latency\", with reduced precision helping two. Specifically, FP16 improves memory bandwidth by only requiring half the data to be shuffled about and that on modern GPUs the FP16 throughput can be 2 to 8 times faster than FP32. Hence, the potential benefit is actually far more than just reducing memory, though the methods and techniques noted in the paper are required in order to have models that can sanely train using FP16."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642408382,"tcdate":1511989675010,"number":3,"cdate":1511989675010,"id":"SJQ3bonlG","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Review","forum":"r1gs9JgRZ","replyto":"r1gs9JgRZ","signatures":["ICLR.cc/2018/Conference/Paper203/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Computation improvements for training neural nets","rating":"7: Good paper, accept","review":"The paper presents three techniques to train and test neural networks using half precision format (FP16) while not losing accuracy. This allows to train and compute networks faster, and potentially create larger models that use less computation and energy.\n\nThe proposed techniques are rigorously evaluated in several tasks, including CNNs for classification and object detection, RNNs for machine translation, language generation and speech recognition, and generative adversarial networks. The paper consistently shows that the accuracy of training and validation matches the baseline using single precision (FP32), which is the common practice.\n\nThe paper is missing results comparing training and testing speeds in all these models, to illustrate the benefits of using the proposed techniques. It would be very valuable to add the baseline wall-time to the tables, together with the obtained wall-time for training and testing using the proposed techniques. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642408423,"tcdate":1511817228832,"number":2,"cdate":1511817228832,"id":"SkSMlWcgG","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Review","forum":"r1gs9JgRZ","replyto":"r1gs9JgRZ","signatures":["ICLR.cc/2018/Conference/Paper203/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Mixed Precision Training","rating":"5: Marginally below acceptance threshold","review":"The paper provides methods for training deep networks using half-precision floating point numbers without losing model accuracy or changing the model hyper-parameters. The main ideas are to use a master copy of weights when updating the weights, scaling the loss before back-prop and using full precision variables to store products. Experiments are performed on a large number of state-of-art deep networks, tasks and datasets which show that the proposed mixed precision training does provide the same accuracy at half the memory.\n\nPositives\n- The experimental evaluation is fairly exhaustive on a large number of deep networks, tasks and datasets and the proposed training preserves the accuracy of all the tested networks at half the memory cost.\n\nNegatives\n- The overall technical contribution is fairly small and are ideas that are regularly implemented when optimizing systems.\n- The overall advantage is only a 2x reduction in memory which can be gained by using smaller batches at the cost of extra compute. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515642408464,"tcdate":1511681822582,"number":1,"cdate":1511681822582,"id":"rJwXkeOgM","invitation":"ICLR.cc/2018/Conference/-/Paper203/Official_Review","forum":"r1gs9JgRZ","replyto":"r1gs9JgRZ","signatures":["ICLR.cc/2018/Conference/Paper203/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Exhaustive experiments validate simple techniques","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper considers the problem of training neural networks in mixed precision (MP), using both 16-bit floating point (FP16) and 32-bit floating point (FP32). The paper proposes three techniques for training networks in mixed precision: first, keep a master copy of network parameters in FP32; second, use loss scaling to ensure that gradients are representable using the limited range of FP16; third, compute dot products and reductions with FP32 accumulation. \n\nUsing these techniques allows the authors to match the results of traditional FP32 training on a wide variety of tasks without modifying any training hyperparameters. The authors show results on ImageNet classification (with AlexNet, VGG, GoogLeNet, Inception-v1, Inception-v3, and ResNet-50), VOC object detection (with Faster R-CNN and Multibox SSD), speech recognition in English and Mandarin (with CNN+GRU), English to French machine translation (with multilayer LSTMs), language modeling on the 1 Billion Words dataset (with a bigLSTM), and generative adversarial networks on CelebFaces (with DCGAN).\n\nPros:\n- Three simple techniques to use for mixed-precision training\n- Matches performance of traditional FP32 training without modifying any hyperparameters\n- Very extensive experiments on a wide variety of tasks\n\nCons:\n- Experiments do not validate the necessity of FP32 accumulation\n- No comparison of training time speedup from mixed precision\n\nWith new hardware (such as NVIDIA’s Volta architecture) providing large computational speedups for MP computation, I expect that MP training will become standard practice in deep learning in the near future. Naively porting FP32 training recipes can fail due to the reduced numeric range of FP16 arithmetic; however by adopting the techniques of this paper, practitioners will be able to migrate their existing FP32 training pipelines to MP without modifying any hyperparameters. I expect these techniques to be hugely impactful as more people begin migrating to new MP hardware.\n\nThe experiments in this paper are very exhaustive, covering nearly every major application of deep learning. Matching state-of-the-art results on so many tasks increases my confidence that I will be able to apply these techniques to my own tasks and architectures to achieve stable MP training.\n\nMy first concern with the paper is that there are no experiments to demonstrate the necessity of FP32 accumulation. With an FP32 master copy of the weights and loss scaling, can all arithmetic be performed solely in FP16, or are there some tasks where training will still diverge?\n\nMy second concern is that there is no comparison of training-time speedup using MP. The main reason that MP is interesting is because new hardware promises to accelerate it. If people are willing to endure the extra engineering overhead of implementing the techniques from this paper, what kind of practical speedups can they expect to see from their workloads? NVIDIA’s marketing material claims that the Tensor Cores in the V100 offer an 8x speedup over its general-purpose CUDA cores (https://www.nvidia.com/en-us/data-center/tesla-v100/). Since in this paper some operations are performed in FP32 (weight updates, batch normalization) and other operations are bound by memory and not compute bandwidth, what kinds of speedups do you see in practice when moving from FP32 to MP on V100?\n\nMy other concerns are minor. Mandarin speech recognition results are reported on “our internal test set”. Is there any previously published work on this dataset, or any publicly available test set for this task?\n\nThe notation around the Inception architectures should be clarified. According to [3] and [4], “Inception-v1” and “GoogLeNet” both refer to the architecture used in [1]. The architecture used in [2] is referred to as “BN-Inception” by [3] and “Inception-v2” by [4]. “Inception-v3” is the architecture from [3], which is not currently cited. To improve clarity in Table 1, I suggest renaming “GoogLeNet” to “Inception-v1”, changing “Inception-v1” to “Inception-v2”, and adding explicit citations to all rows of the table.\n\nIn Section 4.3 the authors note that “half-precision storage format may act as a regularizer during training”. Though the effect is most obvious from the speech recognition experiments in Section 4.3, MP also achieves slightly higher performance than baseline for all ImageNet models but Inception-v1 and for both object detection models; these results add support to the idea of FP16 as a regularizer.\n\nMinor typos:\nSection 3.3, Paragraph 3: “either FP16 or FP16 math” -> “either FP16 or FP32 math”\nSection 4.1, Paragraph 4: “ pre-ativation” -> “pre-activation”\n\nOverall this is a strong paper, and I believe that it will be impactful as MP hardware becomes more widely used.\n\n\nReferences\n\n[1] Szegedy et al, “Going Deeper with Convolutions”, CVPR 2015\n[2] Ioffe and Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, ICML 2015\n[3] Szegedy et al, “Rethinking the Inception Architecture for Computer Vision”, CVPR 2016\n[4] Szegedy et al, “Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning”, ICLR 2016 Workshop","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]}},{"tddate":null,"ddate":null,"tmdate":1515189145634,"tcdate":1509059224064,"number":203,"cdate":1509739428924,"id":"r1gs9JgRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1gs9JgRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Mixed Precision Training","abstract":"Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.","pdf":"/pdf/31684511882e3dd8211cc5ded06babe8ad503be9.pdf","paperhash":"anonymous|mixed_precision_training","_bibtex":"@article{\n  anonymous2018mixed,\n  title={Mixed Precision Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1gs9JgRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper203/Authors"],"keywords":["Half precision","float16","Convolutional neural networks","Recurrent neural networks"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}