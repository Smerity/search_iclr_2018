{"notes":[{"tddate":null,"ddate":null,"tmdate":1516645409039,"tcdate":1516645409039,"number":8,"cdate":1516645409039,"id":"rkKX3j7SM","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"BywMmCPzf","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/AnonReviewer1"],"content":{"title":"repy","comment":"I appreciate the author's response and updates to the paper, and I apologize for my confusion over action classification vs action localization; this explanation makes the Charades experiments more convincing, and I've increased my score from 5 to 6 accordingly."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1516580917377,"tcdate":1516580917377,"number":7,"cdate":1516580917377,"id":"HJ6Ve2MHz","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"HJwcGAPMG","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/AnonReviewer2"],"content":{"title":"reply","comment":"I would like to thank the authors for their reply. The new experiments with randomly dropout baseline look compiling. My only concern is the performance of the random baseline in Table 3 is as good as the best skip GRU regarding mAP. The latest revision has cleared some of my concerns in the initial review. I decided to increase the review score from 5 to 6."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1513864950847,"tcdate":1513771790781,"number":5,"cdate":1513771790781,"id":"BywMmCPzf","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"HkmeI2vxM","signatures":["ICLR.cc/2018/Conference/Paper121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/Authors"],"content":{"title":"Reply to AnonReviewer1","comment":"Q: Include prior published results of the same tasks\n\nA: Following the suggestion from the reviewer, we added previously published results and their implementation context  for all tasks mentioned by the reviewer: MNIST, Charades, UCF-101 and IMDB. They were added to the tables when possible, or to the discussion only otherwise (e.g. results from other works wouldn’t match the table layout for IMDB). \n\n\nQ: low performance on Charades dataset\n\nA: Regarding the performance in temporal action localization task on Charades, we would like to highlight that we report results for the action localization task (i.e. a many-to-many task where an output is emitted for each input frame). However, the results reported in [5] belong to the action classification task (i.e. a many-to-one task where a single output is emitted for the whole video). To the best of our knowledge, the best result for a CNN+LSTM architecture like ours on the localization task is 9.60% (Table 2 in [6], results without post-processing). However, in [6] they use both streams from a Two-Stream CNN (RGB+Flow) whereas we use the RGB stream only. Although this yields a 0.58% mAP increase with respect to our best performing model, using both streams results in approximately a 2x increase in computation (# FLOPs) and memory footprint, plus an additional pre-processing step to compute the optical flow from the input frames. We are not aware of any other work using RGB only with CNN+LSTM on this dataset and task. It is also interesting to notice that our models learn which frames need to be attended without being given explicit motion information as input."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1513771700915,"tcdate":1513771700915,"number":4,"cdate":1513771700915,"id":"SyThG0DMz","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"BkfgO-FgG","signatures":["ICLR.cc/2018/Conference/Paper121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/Authors"],"content":{"title":"Reply to AnonReviewer3","comment":"Q: Wall-clock time for inferencing should be reported.\n\nA: Wall-clock timing is highly dependent on factors such as hardware, framework and implementation, thus making it difficult to isolate the impact of the model. This is why we originally reported the number of sequential steps (i.e. state updates) performed by each model. As an alternative to wall-clock timing, we updated the manuscript to report the number of floating point operations (FLOPs) per sequence. This measure is also independent of external factors such as hardware/software while being representative of the computational load of each model. Although we believe that wall-clock time is not very informative, we are willing to report it if the reviewer still thinks that it will improve the quality of the paper.\n\n\n\nQ: Cite paper by Bengio et al.\n\nA: The paper by Bengio et al. is cited for the ST estimator in the updated version of the manuscript."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1513771663338,"tcdate":1513771663338,"number":3,"cdate":1513771663338,"id":"HJwcGAPMG","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"SyUH1Qjez","signatures":["ICLR.cc/2018/Conference/Paper121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/Authors"],"content":{"title":"Reply to AnonReviewer 2","comment":"Q: Comparison with baselines randomly dropping inputs is missing.\n\nA: We actually have reported the random input dropping baseline for the MNIST task. In the revised version of the paper, we have added results of random dropping baseline for the adding task and Charades. In all cases, the proposed model learns the best ways to skip states (instead randomly) and demonstrated clear performance gains over the random dropping baseline. Note here we assume when random dropping is done, both the input and the state update operation are skipped. We do not consider the option that only input is dropped and the state is still updated since it does not achieve the objective of saving computation. \n\n\n\nQ: the three experiments included in the main paper seemed cherry-picked. \n\nA: We have included a large diverse set of experiments in the Appendix including signal frequency discrimination, sentiment analysis on IMDB movie reviews (text), and video action classification. Far from cherry-picking test results, our goal is to demonstrate the general applicability of the proposed model in various tasks involving data of different modalities (signal, text, and video). We will be happy to move any of the experiments from Appendix to the main paper.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1513771570804,"tcdate":1513771570804,"number":2,"cdate":1513771570804,"id":"HJoVzADMM","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/Authors"],"content":{"title":"On the scale of the experiments & updates to the paper","comment":"We thank reviewers for their valuable comments. We respond to the main concerns below and in individual replies to each reviewer. \n\nR2 and R3 \nQ: Task scale and model size too small. Should run experiments with a large number of hidden units.\n\nA:\n- SkipRNN has indeed a larger advantage over regular RNNs when the model is larger, since the computational savings of skipped states are larger. However, this does not necessarily require RNNs with thousands of hidden units, as the main bulk of computation may come from other associated components of the full architecture, e.g. the CNN encoder in video tasks. It’s important to note when a RNN state update is skipped for a time step, all its preceding elements in the computation graph for that time step are also skipped. For example, in the case of CNN encoder in video tasks, the computational cost of these elements is typically very significant.  The updated version of the manuscript reports the actual # of FLOPs/sequence and shows how SkipRNN models can result in large savings in computation-intensive tasks such as video action localization. We estimated the # of floating point operations based on the actual operations involved in the input encoder and the RNN model. \n\n- Please also note that the size of the studied RNNs in our paper is the same as or even larger than those reported in related methods, e.g. [1, 2, 3]. The largest model in these works is composed by 2 LSTM layers with 256 units each, while we have reported results for 2 layers with 512 units each in the appendix.\nWe believe that the size of the considered tasks is also comparable to those in [1, 2, 3]. Despite some of them using larger datasets in terms of number of examples, their inputs have low dimensionality (e.g. 300-d pre-trained word embeddings) compared to the ones in some of our experiments (e.g. up to 4096-d for video tasks). \n\n\n\nUpdates to the paper:\n\n- Add FLOPs to the tables\n- Moved the description of the random skipping baseline to the beginning of the experiments section\n- Add skipping baselines for the adding task (plus discussion)\n- Add skipping baselines for Charades (plus discussion)\n- Evaluate models on Charades sampling 100 frames/video instead of 25, which should be more accurate for studying models performing different number of state updates.\n- Add SOTA results for recurrent models on MNIST\n- Add comparison to Sigurdsson et al. (CVPR 2017) for Charades\n- Cite prior work and SOTA for IMDB\n- Add SOTA results on UCF-101 (Carreira & Zisserman, 2017)\n\n\n\n\n[1] Neil et al., Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences, NIPS 2017\n[2] Yu et al., Learning to Skim Text, ACL 2017\n[3] Anonymous authors, Neural Speed Reading via Skim-RNN, ICLR 2018 submission\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1516580936297,"tcdate":1511890750499,"number":3,"cdate":1511890750499,"id":"SyUH1Qjez","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Review","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps. The proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time steps. The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks.\n\n\n\nStrength:\n- The experimental results on the simple skip RNNs have shown a good improvement over the previous results.\n\nWeakness:\n- Although the paper shows that skip RNN worked well, I found the appropriate baseline is lacking here. Comparable baselines, I believe, are regular LSTM/GRU whose inputs are randomly dropped out during training.\n\n- Most of the experiments in the main paper are on toy tasks with small LSTMs. I thought the main selling point of the method is the computational gain. Would it make more sense to show that on large RNNs with thousands of hidden units? After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry-picked, and it will be good to include more NLP tasks.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1515642395582,"tcdate":1511753705761,"number":2,"cdate":1511753705761,"id":"BkfgO-FgG","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Review","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer3"],"readers":["everyone"],"content":{"title":"interesting idea. requires more experiment to be convincing.","rating":"6: Marginally above acceptance threshold","review":"This paper proposes an idea to do faster RNN inference via skip RNN state updates. \nI like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance. But the experiments are not convincing enough. First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task. I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters. Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim.\n\nMinor -- \nCite Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation by Yoshua Bengio, Nicholas Leonard and Aaron Courville for straight-through estimator.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1516645623326,"tcdate":1511667178652,"number":1,"cdate":1511667178652,"id":"HkmeI2vxM","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Review","forum":"HkwVAXyCW","replyto":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference/Paper121/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting model, but lacking comparison to prior work","rating":"6: Marginally above acceptance threshold","review":"UPDATE: Following the author's response I've increased my score from 5 to 6. The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5]).\n\nThe paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time. At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped. The use of a straight-through estimator allows the model to be trained with standard backpropagation. The number of state updates that the model learns to use can be controlled with an auxiliary loss function. Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates.\n\nPros:\n- Task of reducing computation by skipping inputs is interesting\n- Model is novel and interesting\n- Experiments on multiple tasks and datasets confirm the efficacy of the method\n- Skipping behavior can be controlled via an auxiliary loss term\n- Paper is clearly written\n\nCons:\n- Missing comparison to prior work on sequential MNIST\n- Low performance on Charades dataset, no comparison to prior work\n- No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification\n\nThe task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained. Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants. The use of an auxiliary loss to control the number of state updates is interesting; since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.\n\nHowever, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work.\n\nOn permuted MNIST, Table 2 could include results from [1-4]. Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task.\n\nFor Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation. All methods work on “fc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge”, and the best-performing method (Skip GRU) achieves 9.02 mAP. This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP. I don’t expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does. However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else.\n\nIn a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks.\n\nThough experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline. However I feel that the authors should include some discussion of other published results.\n\nOn the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author’s own baselines. I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.\n\n\nReferences\n\n[1] Le et al, “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”, arXiv 2015\n[2] Arjovsky et al, “Unitary Evolution Recurrent Neural Networks”, ICML 2016\n[3] Cooijmans et al, “Recurrent Batch Normalization”, ICLR 2017\n[4] Zhang et al, “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS 2016\n[5] Sigurdsson et al, “Hollywood in homes: Crowdsourcing data collection for activity understanding”, ECCV 2016\n[6] Sigurdsson et al, “Asynchronous temporal fields for action recognition”, CVPR 2017","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1510921663759,"tcdate":1510908834096,"number":1,"cdate":1510908834096,"id":"B15iQQ2kf","invitation":"ICLR.cc/2018/Conference/-/Paper121/Official_Comment","forum":"HkwVAXyCW","replyto":"HkTO-f31G","signatures":["ICLR.cc/2018/Conference/Paper121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper121/Authors"],"content":{"title":"reply to question about Equation 1","comment":"Thanks for pointing this out. Throughout the description of the model we use a generic parametric state transition model, S. Please note that S is not the transition model for the vanilla RNN, but a \"placeholder\" to be filled with the transition model of the RNN architecture being used (LSTM or GRU for the experiments in the paper). Should you use the vanilla RNN, S would be exactly what you wrote -- including the non-linearity. This formulation is similar to the one in [1] and abstracts our proposal from the underlying RNN cell.\n\n[1] Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]}},{"tddate":null,"ddate":null,"tmdate":1513864983532,"tcdate":1509010991173,"number":121,"cdate":1509739470504,"id":"HkwVAXyCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkwVAXyCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code will be made publicly available upon publication of this work.","pdf":"/pdf/53881ae3ffa1f5a4c26aadd12c2ec0c3d5452725.pdf","TL;DR":"A modification for existing RNN architectures which allows them to skip state updates while preserving the performance of the original architectures.","paperhash":"anonymous|skip_rnn_learning_to_skip_state_updates_in_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018skip,\n  title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkwVAXyCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper121/Authors"],"keywords":["recurrent neural networks","dynamic learning","conditional computation"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}