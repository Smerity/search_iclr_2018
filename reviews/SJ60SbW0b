{"notes":[{"tddate":null,"ddate":null,"tmdate":1514619699798,"tcdate":1514619699798,"number":3,"cdate":1514619699798,"id":"r1nN7a4Qz","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Comment","forum":"SJ60SbW0b","replyto":"Sk3pyHoez","signatures":["ICLR.cc/2018/Conference/Paper676/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper676/Authors"],"content":{"title":"Scaling to ImageNet and the Role of the Beta Hyperparameter","comment":"Before directly addressing their concerns, we direct the reviewer’s attention to the newly added Sections E, F, & G in the supplementary material of our revised paper draft. These sections include new experiments that illustrate the effect of varying the beta hyperparameter, demonstrate the strength of our approach on the larger scale Inception network for the ILSVRC 2014 classification challenge, and further highlight the effectiveness of our approach in diagnosing model failure modes.\n\nThe phrase “simple datasets” is difficult to interpret; all datasets used in this paper are standard benchmark datasets in computer vision and NLP. We share the reviewer’s desire to further analyze the strength of our framework within computer vision however, for this initial outline of our framework, we have opted to showcase breadth across modalities instead of depth. That said, please consult Section F of our supplementary materials to see visualizations of attention masks trained on top of the Inception network architecture for ImageNet classification. Notably, the results demonstrate that our sample-specific attention masks identify regions of the input space critical to correct classification.\n\nRepeating from a separate comment, LANs trained with an insufficiently large value of beta would accurately reproduce F network outputs without providing useful attention masks; conversely, overly large values of beta dismiss too much information in the input and make reproducing the original network outputs incredibly difficult. Please refer to Section E of the supplementary material for some visualizations illustrating the effect of beta on the resulting attention masks. In general, we make a default assumption that there is a single, if not small range of, beta value that can adequately produce the latent attention mechanisms of the pre-trained network. The parameter does require tuning for different models and, accordingly, we utilize different values of beta across our experiments.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1514619365397,"tcdate":1514619365397,"number":2,"cdate":1514619365397,"id":"rJakzaEmM","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Comment","forum":"SJ60SbW0b","replyto":"HkkF8qngG","signatures":["ICLR.cc/2018/Conference/Paper676/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper676/Authors"],"content":{"title":"Role of the Beta Hyperparameter","comment":"Before directly addressing their concerns, we direct the reviewer’s attention to the newly added Sections E, F, & G in the supplementary material of our revised paper draft. These sections include new experiments that illustrate the effect of varying the beta hyperparameter, demonstrate the strength of our approach on the larger scale Inception network for the ILSVRC 2014 classification challenge, and further highlight the effectiveness of our approach in diagnosing model failure modes.\n\nIn general, we found that the setting of the beta hyperparameter, weighting the amount of input corruption against the reconstructing the outputs of the pre-trained F network, was the single critical factor in determining convergence to good mask structures. LANs trained with an insufficiently large value of beta would accurately reproduce F network outputs without providing useful attention masks; conversely, overly large values of beta dismiss too much information in the input and make reproducing the original network outputs incredibly difficult. Fortunately, since our approach does not require the re-training of the original network altogether, the grid search over potential beta values is relatively simple, though results must be evaluated qualitatively. Please refer to Section E of the supplementary material for some visualizations illustrating the effect of beta on the resulting attention masks. Accordingly, a fruitful direction for subsequent research involves identifying metrics or alternate loss functions that can better measure the interpretability of the resulting masks thereby minimizing the amount of manual inspection needed to diagnose pre-trained models within our framework.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1514619535746,"tcdate":1514359378861,"number":1,"cdate":1514359378861,"id":"ByjI56eXG","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Comment","forum":"SJ60SbW0b","replyto":"Hy3t5U5gz","signatures":["ICLR.cc/2018/Conference/Paper676/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper676/Authors"],"content":{"title":"Usage and Cost of Modeling Latent Attention","comment":"Before directly addressing their concerns, we direct the reviewer’s attention to the newly added Sections E, F, & G in the supplementary material of our revised paper draft. These sections include new experiments that illustrate the effect of varying the beta hyperparameter, demonstrate the strength of our approach on the larger scale Inception network for the ILSVRC 2014 classification challenge, and further highlight the effectiveness of our approach in diagnosing model failure modes.\n\nThe purpose of a LAN is to diagnose failure modes in trained neural networks. Consider a neural network that must make inferences in the healthcare domain, potentially having direct and immediate consequence to patients. When such a network makes any inference, it is paramount that there is an understanding of *why*. Identifying life-threatening illnesses or selecting an optimal course of treatment are just a few examples of decisions that must be made with as much transparency as possible. In capturing the importance in each dimension of a network’s input, our attention masks are a step forward in this direction of interpretable and understandable models. Please consult Section G of our supplementary materials for a demonstration of how sample-specific attention masks successfully identify model failure modes.\n\nWith regards to the cost of a LAN, we note that the training time is itself a sunk cost that offers the potential of yielding information about a potentially erroneous model. In our work, we outline  two methods for learning attention masks. While one prescribes an entirely new network to train a mapping from input to attention masks, the other simply learns the attention mask for a single input directly. While we make no claims on the ability for a LAN to deliver a novel architecture that could remedy the problems of an existing network, it can certainly identify failure points from the dataset and provide sufficient motivation for abandoning an existing network architecture.\n\nSince the training objective would remain unchanged, we currently do not believe that using different structures for the LAN will result in significantly different attention masks. That said, the prohibitive cost of searching over the space of possible LAN architectures is, in part, mitigated by our second “sample-specific” approach for learning attention masks directly without constructing an entirely new network. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1515642490519,"tcdate":1511986808127,"number":3,"cdate":1511986808127,"id":"HkkF8qngG","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Review","forum":"SJ60SbW0b","replyto":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference/Paper676/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Attention masks for diagnosing neural nets","rating":"7: Good paper, accept","review":"The paper presents the formulation of Latent Attention Masks, which is a framework for understanding the importance of input structure in neural networks. The framework takes a pre-trained network F as target of the analysis, and trains another network A that generates masks for inputs. The goal of these masks is to remove parts of the input without changing the response of F. Generated masks are helpful to interpret the preferred patterns of neural networks as well as diagnose modes of error.\n\nThe paper is very well motivated and the formulation and experiments are well presented too. The experiments are conducted in small benchmarks and using simple fully connected networks. It would be interesting to report and discuss convergence properties of the proposed framework. Also, insights of what are the foreseeable challenges on scaling up the framework to real world scenarios.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1515642490555,"tcdate":1511899075938,"number":2,"cdate":1511899075938,"id":"Sk3pyHoez","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Review","forum":"SJ60SbW0b","replyto":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference/Paper676/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The authors of this paper proposed a data-driven black-box visualization scheme. The paper primarily focuses on neural network models in the experiment section. The proposed method iteratively optimize learnable masks for each training example to find the most relevant content in the input that was \"attended\" by the neural network.  The authors empirically demonstrated their method on image and text classification tasks. \n\nStrength:\n           - The paper is well-written and easy to follow. \n           - The qualitative analysis of the experimental results nicely illustrated how the learnt latent attention masks match with our intuition about how neural networks make its classification predictions.\n\n        Weakness:\n           - Most of the experiments in the paper are performed on small neural networks and simple datesets. I found the method will be more compiling if the authors can show visualization results on ImageNet models. Besides simple object recognition tasks, other more interesting tasks to test out the proposed visualization method are object detection models like end-to-end fast R-CNN, video classification models, and image-captioning models. Overall, the current set of experiments are limited to showcase the effectiveness of the proposed method.\n           - It is unclear how the hyperparameter is chosen for the proposed method. How does the \\beta affect the visualization quality? It would be great to show a range of samples from high to low beta values. Does it require tuning for different visualization samples? Does it vary over different datasets?\n  ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1515642490593,"tcdate":1511840388443,"number":1,"cdate":1511840388443,"id":"Hy3t5U5gz","invitation":"ICLR.cc/2018/Conference/-/Paper676/Official_Review","forum":"SJ60SbW0b","replyto":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference/Paper676/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper presented a general method for visualizing an arbitrary neural network's inner mechanisms.","rating":"4: Ok but not good enough - rejection","review":"The main contribution of the paper is to propose to learn a Latent Attention Network (LAN) that can help to visualize the inner structure of a deep neural network. To this end, the paper propose a novel training objective that can learn to tell the importance of each dimension of input. It is very interesting. However, one question is what is the potential usage of the model? Since the model need to train an another network to visualize the structure of a trained neural network, it is expensive, and I don't think the model can help use to design a better structure (at least the experiments did not show this point). And maybe different structures of LAN will produce different understanding of the trained model. Hence people are still not sure what kind of structure is the most helpful.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]}},{"tddate":null,"ddate":null,"tmdate":1514619225711,"tcdate":1509131733416,"number":676,"cdate":1509739163859,"id":"SJ60SbW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJ60SbW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Modeling Latent Attention Within Neural Networks","abstract":"Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned mechanisms that contribute to such effective behaviors and more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \"attention masks\" support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision and natural language processing. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network's underlying decision-making process irrespective of the data modality.","pdf":"/pdf/7f1cd7921f4c93578b7e0a50a2105858b328300b.pdf","TL;DR":"We develop a technique to visualize attention mechanisms in arbitrary neural networks. ","paperhash":"anonymous|modeling_latent_attention_within_neural_networks","_bibtex":"@article{\n  anonymous2018modeling,\n  title={Modeling Latent Attention Within Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJ60SbW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper676/Authors"],"keywords":["deep learning","neural network","attention","attention mechanism","interpretability","visualization"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}