{"notes":[{"tddate":null,"ddate":null,"tmdate":1512408230890,"tcdate":1512405287963,"number":1,"cdate":1512405287963,"id":"r1e4YlmWG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Comment","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper461/Authors"],"content":{"title":"CIFAR-10 Experimental Results","comment":"Here are some results on the CIFAR-10 dataset. \n\n== Error Rates ==  \nVanilla Improved GAN:    0.2513 ± 0.0037\nBasic Self-Training:           0.2471 ± 0.0002\nImproved Self-Training:   0.2231 ± 0.0029\n\n== Improvements Over Vanilla Improved GAN ==  \nVanilla Improved GAN:    0\nBasic Self-Training:           0.0042 ± 0.0039\nImproved Self-Training:   0.0282 ± 0.0008\n\nThe results are averaged over 2 different seeds and the error margins represent one standard deviation. \n\nHere, we notice significant improvement of our Improved Self-Training method over the original vanilla Improved GAN."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/9eea44e8faccda09f56d87273930d504597bbab6.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222658485,"tcdate":1512013997552,"number":3,"cdate":1512013997552,"id":"r1UheZ6gG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Review","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea but limited novelty and impact","rating":"3: Clear rejection","review":"This paper presents a self-training scheme for GANs and tests it on image (NIST) data.\n\nSelf-training is a well-known and usually effective way to learn models in a semi-supervised setting. It makes a lot of sense to try this with GANs, which have also been shown to help train Deep Learning methods.\n\nThe novelty seems quite limited, as both components (GANs and self-training) are well-known and their combination, given the context, is a fairly obvious baseline. The small changes described in Section 4 are not especially motivated and seem rather minor. [btw you have a repeated sentence at the end of that section]\n\nExperiments are also quite limited. An obvious baseline would be to try self-training on a non-GAN model, in order to determine the influence of both components on the performance. Results seem quite inconclusive: the variances are so large that all method perform essentially equivalently. On the other hand, starting with 10 labelled examples seems to work marginally better than 20. This is a bit weird and would justify at least a mention, and idealy some investigation.\n\nIn summary, both novelty and impact seem limited. The idea makes a lot of sense though, so it would be great to expand on these preliminary results and explore the use of GANs in semi-supervised learning in a more thorough manner.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/9eea44e8faccda09f56d87273930d504597bbab6.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222658528,"tcdate":1511845800138,"number":2,"cdate":1511845800138,"id":"S1e2kO9gG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Review","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper presents to combine self-learning and GAN. The basic idea is to first use GAN to generate data, and then infer the pseudo label, and finally use the pseudo labeled data to enhance the learning process. Experiments are conducted on one image data set. The paper contains several deficiencies.","rating":"4: Ok but not good enough - rejection","review":"The paper presents to combine self-learning and GAN. The basic idea is to first use GAN to generate data, and then infer the pseudo label, and finally use the pseudo labeled data to enhance the learning process. Experiments are conducted on one image data set. The paper contains several deficiencies.\n\n1.\tThe experiment is weak. Firstly, only one data set is employed for evaluation, which is hard to justify the applicability of the proposed approach. Secondly, the compared methods are too few and do not include many state-of-the-art SSL methods like graph-based approaches. Thirdly, in these cases, the results in table 1 contain evident redundancy. Fourthly, the performance improvement over compared method is not significant and the result is based on 3 splits of data set, which is obviously not convincing and involves large variance. \n2.\tThe paper claims that ‘when paired with deep, semi-supervised learning has had a few success’. I do not agree with such a claim. There are many success SSL deep learning studies on embedding. They are not included in the discussions. \n3.\tThe layout of the paper could be improved. For example, there are too many empty spaces in the paper. \n4.\tOverall technically the proposed approach is a bit straightforward and does not bring too much novelty.\n5.\tThe format of references is not consistent. For example, some conference has short name, while some does not have. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/9eea44e8faccda09f56d87273930d504597bbab6.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222658570,"tcdate":1511806595832,"number":1,"cdate":1511806595832,"id":"SknKUAteG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Review","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper presents a straight-forward application of existing self-training approach to GAN. Although the proposed approaches are sound, the technical contribution of this paper is low, and the experiments are weak.","rating":"3: Clear rejection","review":"This paper proposes to use self-training strategies for using unlabeled data in GAN. Experiments on only one data set, i.e., MNIST, are conducted \n\nPros:\n* Studying how to use unlabeled data to improve performance of GAN is of technical importance. The use of the self-training in GAN for exploiting unlabeled data is sound.\n \nCons:\n* The novelty and technical contribution is low. The unlabeled data are exploited by off-the-shelf self-training strategies, where the base learner is fixed to GAN. Using GAN does not make the self-training strategy special to the existing self-training approaches. Thus, the proposed approaches are actually a straight application of the existing techniques. In fact, It would be more interesting if the unlabeled data could be employed to the “G” and “A” in GAN.\n\n* In each self-training iteration, GAN needs to be retrained, whose computational cost is high..\n\n* Only one data set is used in the experiment. Some widely-used datasets, like SVHN or CIFAR-10, are not used in the experiment. \n\n* Important baseline methods are missing. The proposed methods should be evaluated with the state-of-the-art semi-supervised deep learning methods, such as those mentioned in related work section.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/9eea44e8faccda09f56d87273930d504597bbab6.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1509739291173,"tcdate":1509120571884,"number":461,"cdate":1509739288512,"id":"ry4S90l0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/9eea44e8faccda09f56d87273930d504597bbab6.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}