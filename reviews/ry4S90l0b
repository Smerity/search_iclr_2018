{"notes":[{"tddate":null,"ddate":null,"tmdate":1513805386246,"tcdate":1513800727009,"number":4,"cdate":1513800727009,"id":"B11QNrOfM","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Comment","forum":"ry4S90l0b","replyto":"SknKUAteG","signatures":["ICLR.cc/2018/Conference/Paper461/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper461/Authors"],"content":{"title":"Simple does not necessarily mean low novelty","comment":"Regarding the Cons:\n\n* One important part of the paper is the use of generated data. Furthermore, while it might seem like a straightforward approach, there is no real guarantee that these methods would work as well unless we have empirical proofs. The second method, in particular, is a complex one that might go wrong in the settings of GAN. \n\n* There is indeed a computational cost but training many times is already something that is commonly done e.g. when searching for hyperparameters. Also, with the advance of computer hardware and parallel processing machine learning libraries like Chainer, this problem will become less important. However, decreasing the computational time while keeping the same performance is something that can be investigated in future work. \n\n* We agree, we have added results for CIFAR-10. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1513800395212,"tcdate":1513800395212,"number":3,"cdate":1513800395212,"id":"Sk7Azr_ff","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Comment","forum":"ry4S90l0b","replyto":"S1e2kO9gG","signatures":["ICLR.cc/2018/Conference/Paper461/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper461/Authors"],"content":{"title":"The new experiments on CIFAR-10 should add power to the experiments","comment":"1. Firstly, yes, we agree and we have added results for CIFAR-10, see above. Secondly, what we wanted to show was the success of self-training on the Improved GAN which already does some semi-supervised learning. Thirdly and fourthly, the results might seem similar for both self-training methods but they still show an improvement over a non-self-trained GAN which is one of the goals of our paper. The difference is more important with the CIFAR-10 results. \n2. We did not say \"has had few success\", we said \"has had a few success\". The former means that there was little success while the latter means that there were successes, which is what we claim. If the question is on the choice of the word \"few\" vs \"many\", then okay we can change \"few\" to \"many\". \n3. Okay, we will rearrange the layout. \n4. The Basic Self-Training scheme might seem obvious and straightforward but the second self-training method should not be considered obvious: label inversion, disagreement calculation and multiple subset candidates is not necessarily something that anyone can think about on top of their head. Furthermore, theoretical justifications exist in the original published paper. \n5. Okay, we will fix the references. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1513800243434,"tcdate":1513800243434,"number":2,"cdate":1513800243434,"id":"rkoVGHOMf","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Comment","forum":"ry4S90l0b","replyto":"r1UheZ6gG","signatures":["ICLR.cc/2018/Conference/Paper461/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper461/Authors"],"content":{"title":"The second method might not be as obvious as it seems","comment":"Although it might make a lot of sense, it seems that no paper has been published about the combination of both. Many things seem obvious in hindsight but a priori we cannot know for sure if these things will work out or not. \n\nThe Basic Self-Training method may seem obvious but the Improved Self-Training method is not obvious. Thinking about selecting multiple subsets of unlabelled data and inverting their labels to check their effect on the decision boundary is not something that people would immediately think about. In fact, it is not even obvious why this might work. The original paper testing this method presented theoretical justification as to why it is a good idea to try on simple hypotheses but not on a complex one such as GANs. \n\nOne of the goals of the paper was to test the self-training method specifically on GANs to see how it can be applied to them. Using a baseline that is not a GAN does not seem to provide information towards that goal. The MNIST results appear equivalent in both self-training methods but they still show an improvement over a non-self-trained GAN which is one of the goals we wanted to achieve with this paper.\n\nAlthough one method seems obvious to use, no other publications seem to have done a similar proof of concept. Moreover, the second self-training method used should not be thought of as obvious; label inversion, disagreement calculation and multiple subset candidates is not necessarily something that anyone can think about on top of their head. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1512408230890,"tcdate":1512405287963,"number":1,"cdate":1512405287963,"id":"r1e4YlmWG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Comment","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper461/Authors"],"content":{"title":"CIFAR-10 Experimental Results","comment":"Here are some results on the CIFAR-10 dataset. \n\n== Error Rates ==  \nVanilla Improved GAN:    0.2513 ± 0.0037\nBasic Self-Training:           0.2471 ± 0.0002\nImproved Self-Training:   0.2231 ± 0.0029\n\n== Improvements Over Vanilla Improved GAN ==  \nVanilla Improved GAN:    0\nBasic Self-Training:           0.0042 ± 0.0039\nImproved Self-Training:   0.0282 ± 0.0008\n\nThe results are averaged over 2 different seeds and the error margins represent one standard deviation. \n\nHere, we notice significant improvement of our Improved Self-Training method over the original vanilla Improved GAN."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1515642452156,"tcdate":1512013997552,"number":3,"cdate":1512013997552,"id":"r1UheZ6gG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Review","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea but limited novelty and impact","rating":"3: Clear rejection","review":"This paper presents a self-training scheme for GANs and tests it on image (NIST) data.\n\nSelf-training is a well-known and usually effective way to learn models in a semi-supervised setting. It makes a lot of sense to try this with GANs, which have also been shown to help train Deep Learning methods.\n\nThe novelty seems quite limited, as both components (GANs and self-training) are well-known and their combination, given the context, is a fairly obvious baseline. The small changes described in Section 4 are not especially motivated and seem rather minor. [btw you have a repeated sentence at the end of that section]\n\nExperiments are also quite limited. An obvious baseline would be to try self-training on a non-GAN model, in order to determine the influence of both components on the performance. Results seem quite inconclusive: the variances are so large that all method perform essentially equivalently. On the other hand, starting with 10 labelled examples seems to work marginally better than 20. This is a bit weird and would justify at least a mention, and idealy some investigation.\n\nIn summary, both novelty and impact seem limited. The idea makes a lot of sense though, so it would be great to expand on these preliminary results and explore the use of GANs in semi-supervised learning in a more thorough manner.\n\n[Response read -- thanks]","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1515642452202,"tcdate":1511845800138,"number":2,"cdate":1511845800138,"id":"S1e2kO9gG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Review","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper presents to combine self-learning and GAN. The basic idea is to first use GAN to generate data, and then infer the pseudo label, and finally use the pseudo labeled data to enhance the learning process. Experiments are conducted on one image data set. The paper contains several deficiencies.","rating":"4: Ok but not good enough - rejection","review":"The paper presents to combine self-learning and GAN. The basic idea is to first use GAN to generate data, and then infer the pseudo label, and finally use the pseudo labeled data to enhance the learning process. Experiments are conducted on one image data set. The paper contains several deficiencies.\n\n1.\tThe experiment is weak. Firstly, only one data set is employed for evaluation, which is hard to justify the applicability of the proposed approach. Secondly, the compared methods are too few and do not include many state-of-the-art SSL methods like graph-based approaches. Thirdly, in these cases, the results in table 1 contain evident redundancy. Fourthly, the performance improvement over compared method is not significant and the result is based on 3 splits of data set, which is obviously not convincing and involves large variance. \n2.\tThe paper claims that ‘when paired with deep, semi-supervised learning has had a few success’. I do not agree with such a claim. There are many success SSL deep learning studies on embedding. They are not included in the discussions. \n3.\tThe layout of the paper could be improved. For example, there are too many empty spaces in the paper. \n4.\tOverall technically the proposed approach is a bit straightforward and does not bring too much novelty.\n5.\tThe format of references is not consistent. For example, some conference has short name, while some does not have. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1515642452239,"tcdate":1511806595832,"number":1,"cdate":1511806595832,"id":"SknKUAteG","invitation":"ICLR.cc/2018/Conference/-/Paper461/Official_Review","forum":"ry4S90l0b","replyto":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference/Paper461/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper presents a straight-forward application of existing self-training approach to GAN. Although the proposed approaches are sound, the technical contribution of this paper is low, and the experiments are weak.","rating":"3: Clear rejection","review":"This paper proposes to use self-training strategies for using unlabeled data in GAN. Experiments on only one data set, i.e., MNIST, are conducted \n\nPros:\n* Studying how to use unlabeled data to improve performance of GAN is of technical importance. The use of the self-training in GAN for exploiting unlabeled data is sound.\n \nCons:\n* The novelty and technical contribution is low. The unlabeled data are exploited by off-the-shelf self-training strategies, where the base learner is fixed to GAN. Using GAN does not make the self-training strategy special to the existing self-training approaches. Thus, the proposed approaches are actually a straight application of the existing techniques. In fact, It would be more interesting if the unlabeled data could be employed to the “G” and “A” in GAN.\n\n* In each self-training iteration, GAN needs to be retrained, whose computational cost is high..\n\n* Only one data set is used in the experiment. Some widely-used datasets, like SVHN or CIFAR-10, are not used in the experiment. \n\n* Important baseline methods are missing. The proposed methods should be evaluated with the state-of-the-art semi-supervised deep learning methods, such as those mentioned in related work section.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1513796799367,"tcdate":1509120571884,"number":461,"cdate":1509739288512,"id":"ry4S90l0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry4S90l0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"A Self-Training Method for Semi-Supervised GANs","abstract":"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation. ","pdf":"/pdf/f7fe525a4e832170d2bd670f569e26f20099a15e.pdf","paperhash":"anonymous|a_selftraining_method_for_semisupervised_gans","_bibtex":"@article{\n  anonymous2018a,\n  title={A Self-Training Method for Semi-Supervised GANs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry4S90l0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper461/Authors"],"keywords":["self-training","generative adversarial networks","semi-supervised"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}