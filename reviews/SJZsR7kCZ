{"notes":[{"tddate":null,"ddate":null,"tmdate":1515154373389,"tcdate":1515154373389,"number":6,"cdate":1515154373389,"id":"H1a6sypQf","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Comment","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper122/Authors"],"content":{"title":"Info update","comment":"Changes are made to the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514316784524,"tcdate":1514316784524,"number":5,"cdate":1514316784524,"id":"SkdxEmgmM","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Comment","forum":"SJZsR7kCZ","replyto":"Syn5-0_lM","signatures":["ICLR.cc/2018/Conference/Paper122/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper122/Authors"],"content":{"title":"Pruning time:","comment":"The number of iterations mentioned for pruning is for the whole network and most of the compression happens in the first 10 iterations, meaning that the method is not so time-consuming as it may seem from the total runtime reported."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513328500308,"tcdate":1513328500308,"number":4,"cdate":1513328500308,"id":"rkhukfWfG","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Comment","forum":"SJZsR7kCZ","replyto":"Syn5-0_lM","signatures":["ICLR.cc/2018/Conference/Paper122/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper122/Authors"],"content":{"title":"Answers to above all mentioned points:","comment":"1) We removed unimportant weights (smaller than a given threshold). Or, we can say both have the same meaning.\n\n2) It is really an interesting idea and might perform faster, however, considering the complexity of the network, there might be convergence problem as we would change the sparsity abruptly. Indeed, we could try it out with an experiment. \n\n3) We used the simple heuristic of quantifying the importance of weights using their absolute values. We could try the other ways in future work.\n\n4) We will add it in our next version.\n\n5) We will take this in our next version."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513256442587,"tcdate":1513256442587,"number":3,"cdate":1513256442587,"id":"H1X-IeeGz","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Comment","forum":"SJZsR7kCZ","replyto":"BkYtPqKez","signatures":["ICLR.cc/2018/Conference/Paper122/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper122/Authors"],"content":{"title":"Answers to points 1, 2, 3 and 4","comment":"1)  It’s a typo. It took 35 hours for MNIST. We will correct it in our next revision.\n\n2)  Following points highlights the differences between the existing and our approach.\n\n     -\t  We evaluate different threshold initialization methods for  weight pruning. To determine those thresholds, we \n         conducted an experiment in which we calculate the minimum achievable sparsity in each layer.\n\n     -\t  We explore different clustering techniques to find shared weights. We examine the impact of density based \n          meanshift clustering and unsupervised k-means clustering with random and linear centroid initialization.\n\n     -\t We also evaluated different weight sharing possibilities. First, only within a layer, that is finding shared weights \n         among the multiple connections within a layer (Han et al.) and second, across all the layers, that is, finding \n         shared weights among multiple connections across all the layers. We found that the second method \n         outperforms the first one.\n\n     -\t We show the trade-off between the number of clusters by state-of-the-art weight sharing technique (k-means \n        clustering with linear centroid initialization) and network performance. We also proposed and implemented \n        ways to improve it.  \n\n     -\t  We compress and evaluate our method on a fully convolutional network performing semantic segmentation \n         and we are not aware of any state-of-the-art technique that obtains good compression rates for such networks.\n\n3)  We successfully demonstrated the flexibility of our method by testing it on fully convolutional network performing other task than classification. \n\nWe also outperformed the existing pruning method (Han et al.) not only in terms of compression statistics but also in accuracy results. \n\nHowever, a better comparison could be done with some other network / dataset, such as inception and ImageNet, but that the focus was indeed on the segmentation.\n\n4) Currently, there is no date set that could adequately captures the complexity of real-world urban scenes [1]. Cityscapes is a benchmark suite and large-scale dataset to address the understanding of complex urban street scenes and there was no experiment performed on this very relevant dataset. So, we focus to use cityscapes high quality images in our experiments and address the problem of real time computation with limited hardware resources in autonomous driving\n\nAlso, in this research work, one of our main goals was to perform compression on networks performing some other tasks than just classification. So, unlike all the previous works, where compression is only performed on networks performing classification, we evaluated and performed compression on networks for semantic segmentation. \n\nReferences: [1] Cordts, Marius, et al. \"The cityscapes dataset for semantic urban scene understanding.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1513240258262,"tcdate":1513240258262,"number":2,"cdate":1513240258262,"id":"HyqT8n1fM","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Comment","forum":"SJZsR7kCZ","replyto":"rJrltU5gf","signatures":["ICLR.cc/2018/Conference/Paper122/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper122/Authors"],"content":{"title":"Answers to points 2, 3 and 4","comment":"2) This is for the whole network. In each iteration, we performed pruning and retraining on each layer simultaneously. We will clarify this in the next version. Moreover, most of the compression happens in the first 10 iterations, meaning that the method is not so time-consuming as it may seem from the total runtime reported.\n\n3) We will correct it in our next revision.\n\n4) Yes, it would be really interesting to see how our compression works on residual connections. This could be our future research work.\n\nIn this research work, one of our main goal was to perform compression on networks performing some other tasks than just classification. So, unlike all the previous works, where compression is only performed on networks performing classification, we also evaluated and performed compression on networks for semantic segmentation. In this work, we tried to address the problem of real time computation with limited hardware resources in autonomous driving.  Semantic segmentation is greatly useful for understanding scenes in autonomous driving. So, we tried to compress a network performing semantic segmentation on Cityscapes dataset.\n\nAlso, we are not aware of any state-of-the-art technique that obtains good compression rates for fully convolutional networks, so we were interested to see how much compression could be achieved on a network without any fully connected layer. Thus, we decided to compress the fully convolutional network.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"ddate":null,"tddate":1513182654208,"tmdate":1513240131089,"tcdate":1513182317716,"number":1,"cdate":1513182317716,"id":"ryU_V00bf","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Comment","forum":"SJZsR7kCZ","replyto":"rJrltU5gf","signatures":["ICLR.cc/2018/Conference/Paper122/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper122/Authors"],"content":{"title":"Answer to point 1","comment":"Here, we would like to highlight the differences between the Han[1] and our approach for the weight sharing stage: \n\nYes, we evaluated the different initialization methods. And, we also evaluated different weight sharing possibilities. First, only within a layer, that is finding shared weights among the multiple connections within a layer and second, across all the layers, that is, finding shared weights among multiple connections across all the layers. We found that the second method outperforms the first one in our case, however, Han[1] stated and used the first one. Comparison of weight sharing techniques discussed above:\n\n\nWeight sharing techniques for LeNet on Mnist\t                  Number of clusters found\t     Accuracy achieved\n\nk-means with linear initialization within layers [Han]\t                       24\t                                         99.14%\nk-means with linear initialization across all the layers [ours]\t       25\t                                         99.28%\n\nWe further improved our k-means with linear initialization across all the layers by checking the possibility of reducing down the number of shared weights. For this, we added one more step to the pipeline, that is, pruning of the codebook. For LeNet on Mnist, we reduced the number of shared weights from 25 to 15 by applying codebook pruning with accuracy loss of just 0.01%. So, our approach gives the optimal trade-off between number of shared weights and loss of accuracy.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642395682,"tcdate":1511839980562,"number":3,"cdate":1511839980562,"id":"rJrltU5gf","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Review","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good work but much similarity with an existing work","rating":"5: Marginally below acceptance threshold","review":"This paper inherits the framework proposed by Han[1]. A pruning, weight sharing, quantization pipeline is refined at each stage. At the pruning stage, by taking into account difference in the distribution across the layers, this paper propose a dynamic threshold pruning, which partially avoids mistakenly pruning important connections. As for the weight sharing stage, this paper explores several ways to initialize the clustering method. The introduction of error tolerance gives us more fine-grained control over the compression process.\n\nHere are some issues to be paid attention to:\n\n1. The overall pipeline including the last two stage looks quite similar to Han[1]. Though different initialization methods are tested in this paper, final conclusion does not change.\n\n2. The dynamic threshold pruning seems to be very time-consuming. As indicated from the paper, only 42 iterations for MNIST and 32 iterations for Cityscapes are required. Whether these number works for each layer or total network should be clarified.\n\n3. Fig 7(a) says it's error rate while it plots accuracy rate.\n\n4. Experiments on popular network structure such as residual connection should be conducted, as they are widely used nowadays.\n\n\nReferences:\n[1] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015 \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642395719,"tcdate":1511790464992,"number":2,"cdate":1511790464992,"id":"BkYtPqKez","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Review","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/AnonReviewer1"],"readers":["everyone"],"content":{"title":"iterative deep compression","rating":"4: Ok but not good enough - rejection","review":"The paper presents a method for iteratively pruning redundant weights in deep networks. The method is primarily based on a 3-step pipeline to achieve this objective. These three steps consist of pruning, weight sharing and quantization. The authors demonstrate reduction in model size and number of parameters significantly while only undergoing minor decrease in accuracy.\n\nSome of the main points of concern are below :\n\n - Computational complexity - The proposed method of iterative pruning seems quite computationally expensive. In the conclusion, it is mentioned that it takes 35 days of training for MNIST. This seems extremely high, and given this, it is unclear if there is much benefit in further reduction in model sizes and parameters (by the proposed method) than those obtained by existing method such as Han etal.\n\n - The novelty in the paper is quite limited and is mainly based on combining existing methods for pruning, weight sharing and quantization. The main difference from existing method seems to be the inclusion of layerwise threshold for weight pruning instead of using a single global threshold.\n\n - The results shown in Table 2 do not indicate much difference in terms of number of parameters between the proposed method and that of Han etal. For instance, the number of overall remaining parameters is 6.5% for the proposed method versus 8% for Deep Compression. As a result, the impact of the proposed method seems quite limited. \n\n - The paper in the title and abstract refers to segmentation as the main area of focus. However, there does not seem to be much related to it except an experiment on the CityScapes dataset.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642395758,"tcdate":1511739796237,"number":1,"cdate":1511739796237,"id":"Syn5-0_lM","invitation":"ICLR.cc/2018/Conference/-/Paper122/Official_Review","forum":"SJZsR7kCZ","replyto":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference/Paper122/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Pruning NNs with layer-dependent parametrization, combined with other approaches for space reduction (weight sharing, quantization) significantly decrease space requirements without hurting accuracy. However, this comes at the cost of significant pruning time.","rating":"6: Marginally above acceptance threshold","review":"quality: this paper is of good quality\nclarity: this paper is very clear\noriginality: this paper combines original ideas with existing approaches for pruning to obtain dramatic space reduction in NN parameters.\nsignificance: this paper seems significant.\n\nPROS\n- a new approach to sparsifying that considers different thresholds for each layer\n- a systematic, empirical method to obtain optimal sparsity levels for a given neural network on a task.\n- Very interesting and extensive experiments that validate the reasoning behind the described approach, with a detailed analysis of each step of the algorithm.\n\nCONS\n- Pruning time. Although the authors argue that the pruning algorithm is not prohibitive, I would argue that >1 month to prune LeNet-5 for MNIST is certainly daunting in many settings. It would benefit the experimental section to use another dataset than MNIST (e.g. CIFAR-10) for the image recognition experiment.\n- It is unclear whether this approach will always work well; for some neural nets, the currently used sparsification method (thresholding) may not perform well, leading to very little final sparsification to maintain good performance.\n- The search for the optimal sparsity in each level seems akin to a brute-force search. Although possibly inevitable, it would be valuable to discuss whether or not this approach can be refined.\n\nMain questions\n- You mention removing \"unimportant and redundant weights\" in the pruning step; in this case, do unimportant and redundant have the same meaning (smaller than a given threshold), or does redundancy have another meaning (e.g. (Mariet, Sra, 2016))?\n- Algorithm 1 finds the best sparsity for a given layer that maintains a certain accuracy. Have you tried using a binary search for the best sparsity instead of simply decreasing the sparsity by 1% at each step? If there is a simple correlation between sparsity and accuracy, that might be faster; if there isn't (which would be believable given the complexity of neural nets), it would be valuable to confirm this with an experiment.\n- Have you tried other pruning methods than thresholding to decide on the optimal sparsity in each layer?\n- Could you please report the final accuracy of both models in Table 2?\n\nNitpicks:\n- paragraph break in page 4 would be helpful.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1514316568251,"tcdate":1509011097527,"number":122,"cdate":1509739469966,"id":"SJZsR7kCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJZsR7kCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation","abstract":"Machine learning and in particular deep learning approaches have outperformed many traditional techniques in accomplishing complex tasks such as\nimage classfication, natural language processing or speech recognition. Most of the state-of-the art deep networks have complex architecture and use a vast number of parameters to reach this superior performance. Though these networks use a large number of learnable parameters, those parameters present significant redundancy. Therefore, it is possible to compress the network without much affecting its accuracy by eliminating those redundant and unimportant parameters.\nIn this work, we propose a three stage compression pipeline, which consists of pruning, weight sharing and quantization to compress deep neural networks.\nOur novel pruning technique combines magnitude based ones with dense sparse dense ideas and iteratively finds for each layer its achievable sparsity instead of selecting a single threshold for the whole network.\nUnlike previous works, where compression is only applied on networks performing classification, we evaluate and perform compression on networks for classification as well as semantic segmentation, which is greatly useful for understanding scenes in autonomous driving.\nWe tested our method on LeNet-5 and FCNs, performing classification and semantic segmentation, respectively. With LeNet-5 on MNIST, pruning reduces the number of parameters by 15.3 times and storage requirement from 1.7 MB to 0.006 MB with accuracy loss of 0.03%. With FCN8 on Cityscapes, we decrease the number of parameters by 8 times and reduce the storage requirement from 537.47 MB to 18.23 MB with class-wise intersection-over-union (IoU) loss of 4.93% on the validation data.","pdf":"/pdf/7ce1eda4d9f66faa86b70eaab4ce82bc625c621d.pdf","paperhash":"anonymous|iterative_deep_compression_compressing_deep_networks_for_classification_and_semantic_segmentation","_bibtex":"@article{\n  anonymous2018iterative,\n  title={Iterative Deep Compression : Compressing Deep Networks for Classification and Semantic Segmentation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZsR7kCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper122/Authors"],"keywords":[]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}