{"notes":[{"tddate":null,"ddate":null,"tmdate":1512715773708,"tcdate":1512715773708,"number":3,"cdate":1512715773708,"id":"rJIWI2PWM","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Comment","forum":"HyUNwulC-","replyto":"Hkr1wGOeG","signatures":["ICLR.cc/2018/Conference/Paper310/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper310/Authors"],"content":{"title":"Significant quantity of research indicates non-linear recurrence not necessary for sequence problems","comment":"We contest the limited technical novelty of this work. It is true that parallel scan is \"a key primitive in many parallel algorithms\"[1] and has been heavily studied and optimized. Parallel linear recurrence is a lesser known application of the widely popular parallel scan algorithm. Neural nets are hugely dependent on high performance parallel computational primitives such as matrix multiplication and convolution. We believe the first application of this classic parallel algorithm to a field dependent on fast parallel algorithms is a novel idea; otherwise someone else would have published this paper in the previous 30+ years that both parallel linear recurrence and RNNs have existed.\n\nBeyond the new architectures introduced in the paper, we applied parallel linear recurrence (PLR) to SRU and QRNN and note that it could also be applied to strongly-typed RNNs. Further, we show that PLR can also accelerate (the currently uninvestigated) architectures involving on h_t = A_t h_{t-1} + x_t for square matrices A_t.\n\nThe broader question is \"how limiting is it that PLR cannot accelerate LSTMs, GRUs, vanilla RNNs, or other non-linear RNN models?\". We do not think this will limit the applicability of PLR within RNNs. A significant amount of recent research (listed below in [2]) has matched or surpassed the performance of non-linear RNNs with models with only linear sequential dependency. Given this body of research, our belief has shifted from \"RNNs depend on sequential non-linearity\" to \"there is no evidence that sequential non-linearity is necessary, and there is a fair amount of evidence it is not necessary\". With this in mind, we believe PLR's incompatibility with non-linear RNNs is not a major practical limitation as we expect linear surrogate RNNs to continue growing in popularity due to their fast training times and good performance. We also think this work will accelerate the growing popularity of linear surrogate RNNs.\n\n[1]\nhttp://people.cs.vt.edu/yongcao/teaching/cs5234/spring2013/slides/Lecture10.pdf\n\n[2]\nSequential models with linear dependendencies with experimental\nperformance on par with non-linear RNNs. Most models listed trained in\nsignificantly less time than non-linear RNN.\n\nStrongly-typed RN https://arxiv.org/abs/1602.02218 (language\nmodelling)\n\nByteNet https://arxiv.org/abs/1610.10099 (state of the art (SotA)\ncharacter level language model on Hutter Prize, SotA character to\ncharacter machine on WMT)\n\nQuasi-RNN https://arxiv.org/abs/1611.01576 (sentiment classification,\nlanguage modelling, machine translation)\n\nConvolutional Sequence to Sequence Learning\nhttps://arxiv.org/abs/1705.03122 (machine translation, outperforms\nLSTM)\n\nAttention Is All You Need https://arxiv.org/abs/1706.03762 (SotA\nmachine translation on WMT)\n\nWaveNet https://arxiv.org/abs/1609.03499 (high fidelity audio\ngeneration)\n\nSimple Recurrent Unit https://arxiv.org/abs/1709.02755 (matches or\noutperforms LSTM on sequence classification, question answering,\nlanguage modelling, machine translation, speech recognition). PLR\ncan significantly accelerate already fast SRU training.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1512715458334,"tcdate":1512715458334,"number":2,"cdate":1512715458334,"id":"BycaVhwZz","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Comment","forum":"HyUNwulC-","replyto":"ry7sCqtgM","signatures":["ICLR.cc/2018/Conference/Paper310/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper310/Authors"],"content":{"title":"Example tasks for which truncated backprop through time causes problems","comment":"We agree that you can often \"get away with\" backprop through time\n(BPTT) truncated at several hundred time steps for many sequential\nproblems, even when the inherent sequence length of the data is very\nlong.\n\nSome problems which can benefit from additional sequence length:\n\n* Medical waveforms are often sampled at greater than 1KHz. This means\n  relatively short recordings create very long sequences. These\n  sequences may be used for a sequence classification task which makes\n  it difficult to use truncated BPTT. Sequence classification on very\n  long sequences must either handle the entire sequence, classify\n  subsequences (suboptimal as label may only be determined by part of\n  the sequence), or down-sample the sequence data (suboptimal because\n  it loses information). The 2016 PhysioNet Challenge\n  (https://physionet.org/challenge/2016/) involved classifying EEGs\n  sampled at 2KHz for 5-120s for a total of 10K-240K events per\n  sequence. It would be difficult to apply neural nets to such a\n  problem without a technique to parallelize over timesteps.  An even\n  more extreme dataset is 90 minutes @ 30KHz (= 160 million steps) of\n  neural recordings of a mouse: http://data.cortexlab.net/dualPhase3/ .\n  In general, consider some task involving sensor data. Now consider the\n  same phenomena but measured at 10X the frequency. There is now\n  more information available, but this additional information is only accessible if\n  the researcher has tools that can deal with a 10X longer sequence with\n  10X longer dependencies.\n\n* Example future machine learning task: Generate a (text) review of a\n  2+ hour movie, including comments on dialogue and\n  cinematography. Even with significant downsampling of both frames\n  and audio, a 2 hour movie contains 7200 frames at 1 frame/sec and an\n  average of 9000 words\n  (http://kaylinwalker.com/long-winded-actors-and-movies-with-the-most-dialogue/).\n  We believe parallel sequential methods would be hugely useful for\n  such a task.\n\n* I am not an expert, but I believe reinforcement learning on long\n  episodes with sparse rewards could benefit from less episode truncation.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1512714533020,"tcdate":1512714533020,"number":1,"cdate":1512714533020,"id":"r1T7-3PbG","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Comment","forum":"HyUNwulC-","replyto":"SyAgjAtgG","signatures":["ICLR.cc/2018/Conference/Paper310/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper310/Authors"],"content":{"title":"Models which benefit from parallel linear recurrence have demonstrated strong experimental performance","comment":"We feel the very impressive performance of SRUs and QRNNs on a variety of large-scale tasks demonstrates the applicability and usefulness of our work. We could have replicated their results in language modelling and machine translation with faster training times, but we believe that showing large speedup factors for these models is sufficient evidence for the value of parallel linear recurrence.\n\nWe argue more strongly that a non-linearity recurrence is unnecessary than we do that \"rotation free\" RNNs are just as powerful as RNNs with non-diagonal weight matrices. However, SRUs are \"rotation free\" linear recurrences with performance equal or superior to LSTM and other non-linear RNNs on 6 sequence classification datasets, the SQuAD question answering dataset, Penn Treebank language modelling, Switchboard-1 speech recognition, and WMT English->German translation.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1515642429758,"tcdate":1511807734369,"number":3,"cdate":1511807734369,"id":"SyAgjAtgG","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Review","forum":"HyUNwulC-","replyto":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference/Paper310/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Faster RNNs, with novel insights on need for nonlinear recurrence; novel and clear presentation","rating":"7: Good paper, accept","review":"This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation. The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM. Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997). A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure.\n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal. Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures.\n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs. While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets.\n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1515642429806,"tcdate":1511792283550,"number":2,"cdate":1511792283550,"id":"ry7sCqtgM","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Review","forum":"HyUNwulC-","replyto":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference/Paper310/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Authors propose a method to make recurrent learning over 1000s and more time steps possible.","rating":"7: Good paper, accept","review":"# Summary and Assessment\n\nThe paper addresses an important issue–that of making learning of recurrent networks tractable for sequence lengths well beyond 1’000s of time steps. A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor. \nThe authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator. Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively.\nIn the following, the authors explore the landscape of RNNs satisfying the necessary conditions. The performance is investigated in terms of wall clock time. Further, experimental results of problems with previously untacked sequence lengths are reported.\n\nThe paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies.\nTo me, the execution seems sound. The experiments back up the claim.\n\n## Minor\n- I challenge the claim that thousands and millions of time steps are a common issue in “robotics, remote sensing, control systems, speech recognition, medicine and finance”, as claimed in the first paragraph of the introduction. IMHO, most problems in these domains get away with a few hundred time steps; nevertheless, I’d appreciate a few examples where this is a case to better justify the method.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1515642429845,"tcdate":1511691997090,"number":1,"cdate":1511691997090,"id":"Hkr1wGOeG","invitation":"ICLR.cc/2018/Conference/-/Paper310/Official_Review","forum":"HyUNwulC-","replyto":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference/Paper310/AnonReviewer2"],"readers":["everyone"],"content":{"title":"simple but effective method for RNN speed up","rating":"6: Marginally above acceptance threshold","review":"This paper focuses on accelerating RNN by applying the method from Blelloch (1990). The application is straightforward and thus technical novelty of this paper is limited. But the results are impressive. \n\nOne concern is the proposed technique is only applied for few types of RNNs which may limit its applications in practice. Could the authors comment on this potential limitation?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]}},{"tddate":null,"ddate":null,"tmdate":1514858551761,"tcdate":1509095214338,"number":310,"cdate":1509739369384,"id":"HyUNwulC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyUNwulC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Parallelizing Linear Recurrent Neural Nets Over Sequence Length","abstract":"Recurrent neural networks (RNNs) are widely used to model sequential data but\ntheir non-linear dependencies between sequence elements prevent parallelizing\ntraining over sequence length. We show the training of RNNs with only linear\nsequential dependencies can be parallelized over the sequence length using the\nparallel scan algorithm, leading to rapid training on long sequences even with\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\nshow that it can be applied to immediately speed up training and inference of\nseveral state of the art RNN architectures by up to 9x.  We abstract recent work\non linear RNNs into a new framework of linear surrogate RNNs and develop a\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\nutilizes parallel linear recurrence.  We extend sequence learning to new\nextremely long sequence regimes that were previously out of reach by\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\nwith a one million timestep dependency.\n","pdf":"/pdf/9aee1e969723c3f15788291964f572eca0339080.pdf","TL;DR":"use parallel scan to parallelize linear recurrent neural nets. train model on length 1 million dependency","paperhash":"anonymous|parallelizing_linear_recurrent_neural_nets_over_sequence_length","_bibtex":"@article{\n  anonymous2018parallelizing,\n  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyUNwulC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper310/Authors"],"keywords":["rnn","sequence","parallel","qrnn","sru","gilr","gilr-lstm"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}