{"notes":[{"tddate":null,"ddate":null,"tmdate":1512456443977,"tcdate":1512456443977,"number":3,"cdate":1512456443977,"id":"ByV--6Xbz","invitation":"ICLR.cc/2018/Conference/-/Paper1136/Official_Review","forum":"rJIgf7bAZ","replyto":"rJIgf7bAZ","signatures":["ICLR.cc/2018/Conference/Paper1136/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, but not impactful","rating":"4: Ok but not good enough - rejection","review":"This paper proposes what is essentially an off-policy method for learning options in complex continuous problems.  The idea is to use policy gradient style algorithms to update a suite of options using relatively \n\nOn the positive side, I like the core idea of this paper.  The idea of updating multiple options at once is a good one.  I think the authors should definitely continue to investigate this line of work.  I also appreciated that the authors took the time to try and visualize what was learned.  The paper is generally well-written and easy to read.\n\nOn the negative side: ultimately, the algorithm doesn't seem to work all that well.  Empirically, the method doesn't seem to perform substantially better than other algorithms, although there seems to be some slight advantage.  A clearly missing comparison would be something like TRPO or DDPG.\n\nFigure 1 was helpful in understanding marginalization and the forward algorithm.  Thanks.\n\nWas there really only 4 options that were learned?  How would this scale to more?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An inference-based policy gradient method for learning options","abstract":"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","pdf":"/pdf/7ff2f7d7dba366ae35b85d4dbac7d2a46c59007e.pdf","TL;DR":"We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.","paperhash":"anonymous|an_inferencebased_policy_gradient_method_for_learning_options","_bibtex":"@article{\n  anonymous2018an,\n  title={An inference-based policy gradient method for learning options},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJIgf7bAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1136/Authors"],"keywords":["reinforcement learning","hierarchy","options","inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222558629,"tcdate":1511977136742,"number":2,"cdate":1511977136742,"id":"B1F3lO2lG","invitation":"ICLR.cc/2018/Conference/-/Paper1136/Official_Review","forum":"rJIgf7bAZ","replyto":"rJIgf7bAZ","signatures":["ICLR.cc/2018/Conference/Paper1136/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper is well written and presents a good extension of infernece based option discovery. However, the results are not convincing and there is a crucial issue in the assumptions of the algorithm. ","rating":"3: Clear rejection","review":"The paper presents a new policy gradient technique for learning options. The option index is treated as latent variable and, in order to compute the policy gradient, the option distribution for the current sample is computed by using a forward pass. Hence, a single sample can be used to update all options and not just the option that has been used for this sample.\n \nThe idea of the paper is good but the novelty is limited. As noted by the authors, the idea of using inference for option discovery has already been presented in Daniel2016. Note that the option discovery process is Daniel2016 is not limited to linear sub-policies, only the policy update strategy is. So the main contribution is to use a new policy update strategy, i.e., policy gradients, for inference based option discovery. Thats fine but should be stated more clearly in the paper. The paper is also written very well and the topic is relevant for the ICLR conference. \n\nHowever, the paper has two main problems:\n- The results are not convincing. In most domains, the performance is similar to the A3C algorithm (which does not use inference based option discovery), so the impact of this paper seems limited. \n\n- One of the main assumptions of the algorithm is wrong. The assumption is that rewards from the past are not correlated with actions in the future conditioned on the state s_t (otherwise we would always have a correlation) ,which  is needed to use the policy gradient theorem. The assumption is only true for MDPs. However, using the option index as latent variable yields a PoMDP. There, this assumption does not hold any more. Example: Reward at time step t-1 depends on the action, which again depends on the option o_t-1. Action at time step t depends on o_t. Hence, there is a strong correlation between reward r_t-1 and action a_t+1 as o_t and o_t+1 are strongly correlated. o_t is not a conditional variable of the policy as it is not part of the state, thats why this assumption does not work any more.\n\nSummary: The paper is well written and presents a good extension of inference based option discovery. However, the results are not convincing and there is a crucial issue in the assumptions of the algorithm. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An inference-based policy gradient method for learning options","abstract":"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","pdf":"/pdf/7ff2f7d7dba366ae35b85d4dbac7d2a46c59007e.pdf","TL;DR":"We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.","paperhash":"anonymous|an_inferencebased_policy_gradient_method_for_learning_options","_bibtex":"@article{\n  anonymous2018an,\n  title={An inference-based policy gradient method for learning options},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJIgf7bAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1136/Authors"],"keywords":["reinforcement learning","hierarchy","options","inference"]}},{"tddate":null,"ddate":null,"tmdate":1512222558672,"tcdate":1511826733700,"number":1,"cdate":1511826733700,"id":"B1LVBmqgf","invitation":"ICLR.cc/2018/Conference/-/Paper1136/Official_Review","forum":"rJIgf7bAZ","replyto":"rJIgf7bAZ","signatures":["ICLR.cc/2018/Conference/Paper1136/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposes a new algorithm for discovering options, but the benefits of the algorithm are not clear empirically.","rating":"3: Clear rejection","review":"This paper treats option discovery as being analogous to discovering useful latent variables.  The proposed formulation assumes there is a policy over options, which invokes an option’s policy to select actions at each timestep until the option’s termination function is activated.  A contribution of this paper is to learn all possible options that might have caused an observed trajectory, and to update parameters for all these pertinent option-policies with backprop.  The proposed method, IOPG, is compared to A3C and the option-critic (OC) on four continuous control tasks in Mujoco, and IOPG has the best performance on one of the four domains.\n\nThe primary weakness of this paper is the absence of performance or conceptual improvements in exchange for the additional complexity of using options.  The only domain where IOPG outperforms both A3C and OC is the Walker2D-v1 domain, and the reported performance on that domain (~800) is far below the performance of other methods (shown on OpenAI’s Gym site or in the PPO paper). Also, there is not much analysis on what kind of options are learned with this approach, beyond noting that the options seem clustered on tSNE plots.  Given the close match between the A3C agent and the IOPG agent on the other three domains, I expect that the system is mostly relying on the base A3C components with limited contributions from the extensions introduced in the network for options.  \n\nThe clarity of the paper’s contributions could be improved. The contribution of options might be made more clearly in smaller domains or in more detailed experiments. How is the termination beta provided from the network?  How frequently did the policy over options switch between them?  How was the number of options selected, and what happens when the number of possible options is varied from 1 to 4 or beyond 4?  To what extent was there overlap in the learned policies to realize the proposed algorithmic benefit of learning multiple option-policies from the same transitions?  The results in this paper do not provide strong support for using the proposed method.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"An inference-based policy gradient method for learning options","abstract":"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","pdf":"/pdf/7ff2f7d7dba366ae35b85d4dbac7d2a46c59007e.pdf","TL;DR":"We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.","paperhash":"anonymous|an_inferencebased_policy_gradient_method_for_learning_options","_bibtex":"@article{\n  anonymous2018an,\n  title={An inference-based policy gradient method for learning options},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJIgf7bAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1136/Authors"],"keywords":["reinforcement learning","hierarchy","options","inference"]}},{"tddate":null,"ddate":null,"tmdate":1510092380040,"tcdate":1509138929632,"number":1136,"cdate":1510092359581,"id":"rJIgf7bAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJIgf7bAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"An inference-based policy gradient method for learning options","abstract":"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.","pdf":"/pdf/7ff2f7d7dba366ae35b85d4dbac7d2a46c59007e.pdf","TL;DR":"We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.","paperhash":"anonymous|an_inferencebased_policy_gradient_method_for_learning_options","_bibtex":"@article{\n  anonymous2018an,\n  title={An inference-based policy gradient method for learning options},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJIgf7bAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1136/Authors"],"keywords":["reinforcement learning","hierarchy","options","inference"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}