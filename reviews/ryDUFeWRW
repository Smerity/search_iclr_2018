{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642478807,"tcdate":1511835943388,"number":3,"cdate":1511835943388,"id":"H1y4FS9gf","invitation":"ICLR.cc/2018/Conference/-/Paper610/Official_Review","forum":"ryDUFeWRW","replyto":"ryDUFeWRW","signatures":["ICLR.cc/2018/Conference/Paper610/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A paper that aims to address an important task but fails to deliver the expectations","rating":"4: Ok but not good enough - rejection","review":"Summary:\nThis paper studies the problem of transfer learning on time series forecasting. The proposed solution works by first training multiple-layer LSTM network on a related task with large training dataset and then fine-tuning the last several LSTM layers on the target task. Experiment results on real world application data demonstrate relatively good performance of the proposed method.\n\n\nDetailed Comments: \n\nTransfer learning on time series data is an interesting direction to explore.\n\nThe claim that \"recent results on time-series forecasting using LSTM only apply a single layer of LSTM\" in the introduction is not true. Deep LSTM with more than one LSTM recurrent layers have been used for time series forecasting in [2], [3], [4] and [5].\n\nThe statement that state-of-the-art time series methods  train a single mode for each time series is not accurate. For example, in Vector Auto-Regressive (VAR), one of the most popular time series models, a single model is trained for all time series. Besides, training a single model on multiple time series with similar behavior might be beneficial as the model can explore the correlation between time series, examples can be found in [3] and [6].\n\nThe experiment conducted on the PG&E dataset  (Table 1, 2, Figure 2) is not a strictly defined \"transfer learning\" task [1]. The two tasks on the PG&E dataset, i.e., forecasting on group A and group B, are randomly chosen from the same data source. It is very easy to transfer knowledge in this scenarios, and therefore cannot demonstrate the effectiveness of the proposed solution. \n\nThe proposed method fails to demonstrate the advantages of transfer learning.  In Figure 2(a), though the proposed approach \"AnB\" is better than \"Single\", this might simply because \"AnB\" is trained using more data. In addition, \"AnB\" gets the worse result than Baseline when training size is greater than 50%.\n\nIn Figure 3, the proposed method is worse than more than half of the baselines on the M3 dataset. Though the proposed method requires less training data, it will be helpful if the model can outperform other baselines leveraging transfer learning (for example pre-train on PG&E and fine-tuning on the M3 dataset). \n\nThe description and evaluation of the experiments on the M3 dataset are *very* brief and lack lots of necessary details. This might due to the space limitation, but it would be better if more detail can be included in the supplementary material.\n\nInsufficient explanation of experimental settings.  \n- The paper contains a limited discussion of the online tool (DeepCast), which is one of the main claimed contributions. \n- \"Daily interaction\" that appears in Section 4.2 is never explained.\n- The explanation about baseline methods (e.g., those in Figure 3, Figure 5) is *very* limited.\n- Legend is missing in Figure 4 and the visualization of clustering does not seem to be good as points with different colors mixing with others.\n\nD7: Inconsistent notation.\nIn section 2, the symbols are abused.\n-  X v.s. \\mathcal{X}, Y v.s. \\mathcal{Y}, D v.s. \\mathcal{D}. \n- T is used to represent both \"task\" and \"target domain\",\n- The equation might be: y(t) = g(t; \\alpha_i^g) + ....\n- subscript t is used to index sample in section 2, while superscript i is used in section 3. \n\nIn Section 3:\n- SMAPE is not explained. \nIn Section 4.1.\n-  \"A3B\" -> \"AnB\" \n\nIn short,  many claims in the paper are not fully convincing or sometimes even misleading; the novelty of the proposed approach is limited and the improvement is not significant. Besides, the paper provides insufficient information about baselines and evaluation of the experiments. This makes the experimental results less convincing. Lastly, the notation is not consistent and presentation can be improved. \n\n[1] Lisa Torrey, Jude Shavlik. Transfer Learning. \n[2] Nikolay Laptev, et al. Time-series Extreme Event Forecasting with Neural Networks at Uber\n[3] Haiyang Yu, et al. Spatiotemporal Recurrent Convolutional Networks for Traffic Prediction in Transportation Networks\n[4] Rose Yu, et al. Deep Learning: A General Framework for Extreme Traffic Forecasting\n[5] Amir Ghaderi, et al. Deep Forecast: Deep Learning-based Spatio-temporal Forecasting\n[6] Hsiang-Fu Yu, Nikhil Rao. Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER","abstract":"Reliable and accurate time-series forecasting is critical in many fields including energy, finance, and manufacturing. Many time-series tasks, however, suffer from a limited amount of training data (i.e., the cold start problem) resulting in poor forecasting performance. Recently, convolutional neural networks (CNNs) have shown outstanding image classification performance even on tasks with small-scale training sets. The performance can be attributed to transfer learning through CNNsâ€™ ability to learn rich mid-level image representations. However, no prior work exists on general transfer learning for time-series forecasting. In this paper, motivated by recent success of transfer learning in CNN model and image-related tasks, we for the first time show how time-series representations learned with Long Short Term Memory (LSTM) on large-scale datasets can be efficiently transferred to other time-series forecasting tasks with limited amount of training data. We also validate that despite differences in time-series statistics and tasks in the datasets, the transferred representation leads to significantly improved forecasting results outperforming majority of the best time-series methods on the public M3 and other datasets. Our online universal forecasting tool, DeepCast, will leverage transfer learning to provide accurate forecasts for a diverse set of time series where classical methods were computationally infeasible or inapplicable due to short training history.","pdf":"/pdf/ce8d8d7e5efb72326ea5252f1d0d55a896cbbd58.pdf","paperhash":"anonymous|deepcast_universal_timeseries_forecaster","_bibtex":"@article{\n  anonymous2018deepcast:,\n  title={DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDUFeWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper610/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642478844,"tcdate":1511780894983,"number":2,"cdate":1511780894983,"id":"Bkv7fOtlz","invitation":"ICLR.cc/2018/Conference/-/Paper610/Official_Review","forum":"ryDUFeWRW","replyto":"ryDUFeWRW","signatures":["ICLR.cc/2018/Conference/Paper610/AnonReviewer2"],"readers":["everyone"],"content":{"title":"More an application paper than a fundamental methodological contribution","rating":"4: Ok but not good enough - rejection","review":"The authors investigate transfer learning for LSTM models applied to timeseries data. In terms of significance, this is an interesting topic, and the authors do a good job in motivating this work. However, their investigation focuses exclusively on residential electrical load time series. Therefore, calling their methodology a \"universal\" time-series forecaster is an overstatement. In particular,\n- in contrary to their statement 2 (c) on page 2, the authors do not demonstrate cross-domain learning; the learning is all performed on data from the same domain\n- their experimental set-up is such that transfer learning can't go wrong, particularly the first experiment in which they split a data set of 118K time series into two parts and transfer representations learned on the first part onto the second one. In practice, when transferring representations between completely different domains, the benefits (reduction of variance) might actually be outweighed by the bias of the low-level representations. The study of how many levels of the base models should be retrained on the target domain is a step in this direction; do the authors have any suggestions how to choose the optimal 'n' in practice, particularly if little is known about the target domain?\n- even within the load forecasting domain, the authors should seek for generalizations of their results. E.g., how well do representations for hourly data transfer to higher-resolution data? How well do representations for higher aggregation levels (e.g. at the market level) transform to the load of individual customers (residential and industrial), and vice versa?\n- another question on the modeling side: why not using a fully-connected network instead of an LSTM? That would be closer to the traditional neural network-based approaches for load forecasting which have been successfully deployed for ~20 years.\n\nA few more specific comments:\n- in Figure 2 (c), I didn't understand how the Seasonal/Sparse/Noisy classification was obtained. A bit more detail might be helpful. What was the number of timeseries in each of the classes?\n- in Figure 3, do I understand correctly that DeepCast used the same model for all 3K timeseries in M3? Why wasn't transfer learning used (i.e. using the base model from A1 and learning the higher-level representations based on M3)?\n- I couldn't follow the geometric interpretation in Figure 4 (b). It makes sense that the states in those two layers are clustered according to weekdays, but what is the difference then between them?\n- In Figure 5, a comparison in terms of computation time would be valuable\n\nSummary:\n+ Paper considers interesting question, with relevance for representation learning. Writing is good, notation is clear, experiments are carefully described.\n- The actual investigation is application-specific; cross-domain transfer learning isn't studied.\n- Experiments confirm that transfer learning works in situations where one would expect it, but they don't provide further insights into when, more generally, the bias-variance trade-off will pay off for LSTMs applied to timeseries\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER","abstract":"Reliable and accurate time-series forecasting is critical in many fields including energy, finance, and manufacturing. Many time-series tasks, however, suffer from a limited amount of training data (i.e., the cold start problem) resulting in poor forecasting performance. Recently, convolutional neural networks (CNNs) have shown outstanding image classification performance even on tasks with small-scale training sets. The performance can be attributed to transfer learning through CNNsâ€™ ability to learn rich mid-level image representations. However, no prior work exists on general transfer learning for time-series forecasting. In this paper, motivated by recent success of transfer learning in CNN model and image-related tasks, we for the first time show how time-series representations learned with Long Short Term Memory (LSTM) on large-scale datasets can be efficiently transferred to other time-series forecasting tasks with limited amount of training data. We also validate that despite differences in time-series statistics and tasks in the datasets, the transferred representation leads to significantly improved forecasting results outperforming majority of the best time-series methods on the public M3 and other datasets. Our online universal forecasting tool, DeepCast, will leverage transfer learning to provide accurate forecasts for a diverse set of time series where classical methods were computationally infeasible or inapplicable due to short training history.","pdf":"/pdf/ce8d8d7e5efb72326ea5252f1d0d55a896cbbd58.pdf","paperhash":"anonymous|deepcast_universal_timeseries_forecaster","_bibtex":"@article{\n  anonymous2018deepcast:,\n  title={DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDUFeWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper610/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1515642478880,"tcdate":1511721841176,"number":1,"cdate":1511721841176,"id":"rkY_oK_lf","invitation":"ICLR.cc/2018/Conference/-/Paper610/Official_Review","forum":"ryDUFeWRW","replyto":"ryDUFeWRW","signatures":["ICLR.cc/2018/Conference/Paper610/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Paper attempts to tackle an important transfer learning problem for time series forecasting but it is not ready for a publication in its current form","rating":"4: Ok but not good enough - rejection","review":"This paper studies transfer learning for time series using an LSTM models. In particular, paper demonstrates that it is possible to improve model performance by training LSTM model on one set of time series, freezing first n layers and then training the rest on the other set of time series that is the desired target task.\n\nThe paper is clearly written and tackles an important problem of forecasting large datasets of time series. However, quality of the paper and novelty of the results is not sufficient for acceptance to ICLR.\n\n1. The paper is primarily experimental in nature and does not provide any theoretical motivation or insights. This is fine so long as either methodology is novel and experimental results are extensive. However, paper applies a very simple well-known strategy of transfer learning (learn on source - freeze n layers - learn on target). No additional, insights are provided. For instance, only very simple feature viz is performed. What feature representations are learned? I think there is a lot of room for interesting analysis here which is lacking in this work.\n\n2. Experiments are carried out on a single dataset where target and source are basically the same. The datasets is essentially randomly split into target and source. To prove effectiveness of transfer it would be more interesting to use completely different tasks. Say electricity data and finance data. I also suggest that authors do not use M3 dataset - it is tiny and hardly presents any interesting statistically significant results.\n\n3. End of paragraph 1 in section 3 \"All time series are ... min-max scaled to [0, 1] interval\". This is subtle point that is often overlooked but this transformation is dangerous for real setting. It leaks information about the labels (scale of the future data) into training. This makes me skeptical about experimental results overall.\n\nMinor points:\n\n1. What is \"daily interactions\"? This is not explained in the paper.\n\n2. Why is it called \"universal\"? Clearly it can not forecast any time series. Just the ones observed in data. I am not sure any sort of universality has been established in the paper.\n\n3. Not sure I agree with the claim about computational resources. Why not just train a single model per time-series in parallel? These models might be a lot smaller (e.g. HoltWinters which seems to work well in your experiments) and hence cheaper to train. Since training is in parallel, it might be a lot faster than SGD training of LSTMs which is inherently sequential. Even if one is constrained by amount of parallelism available a good implementation of HW model applied sequentially might be faster than LSTM training. I also do not understand what are gains in terms of memory usage. This is not explained at all in the paper.\n\n4. HoltWinters seems to outperform LSTM models. Any insights why? Also I think it is worth explicitly discussing this in text.\n\n5. End of page 1 \"This process will tend to work if the features are general, ...\". This is probably a necessary but not sufficent condition. In particular, you need target and source distributions to be sufficiently close.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER","abstract":"Reliable and accurate time-series forecasting is critical in many fields including energy, finance, and manufacturing. Many time-series tasks, however, suffer from a limited amount of training data (i.e., the cold start problem) resulting in poor forecasting performance. Recently, convolutional neural networks (CNNs) have shown outstanding image classification performance even on tasks with small-scale training sets. The performance can be attributed to transfer learning through CNNsâ€™ ability to learn rich mid-level image representations. However, no prior work exists on general transfer learning for time-series forecasting. In this paper, motivated by recent success of transfer learning in CNN model and image-related tasks, we for the first time show how time-series representations learned with Long Short Term Memory (LSTM) on large-scale datasets can be efficiently transferred to other time-series forecasting tasks with limited amount of training data. We also validate that despite differences in time-series statistics and tasks in the datasets, the transferred representation leads to significantly improved forecasting results outperforming majority of the best time-series methods on the public M3 and other datasets. Our online universal forecasting tool, DeepCast, will leverage transfer learning to provide accurate forecasts for a diverse set of time series where classical methods were computationally infeasible or inapplicable due to short training history.","pdf":"/pdf/ce8d8d7e5efb72326ea5252f1d0d55a896cbbd58.pdf","paperhash":"anonymous|deepcast_universal_timeseries_forecaster","_bibtex":"@article{\n  anonymous2018deepcast:,\n  title={DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDUFeWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper610/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739202514,"tcdate":1509128526906,"number":610,"cdate":1509739199852,"id":"ryDUFeWRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryDUFeWRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER","abstract":"Reliable and accurate time-series forecasting is critical in many fields including energy, finance, and manufacturing. Many time-series tasks, however, suffer from a limited amount of training data (i.e., the cold start problem) resulting in poor forecasting performance. Recently, convolutional neural networks (CNNs) have shown outstanding image classification performance even on tasks with small-scale training sets. The performance can be attributed to transfer learning through CNNsâ€™ ability to learn rich mid-level image representations. However, no prior work exists on general transfer learning for time-series forecasting. In this paper, motivated by recent success of transfer learning in CNN model and image-related tasks, we for the first time show how time-series representations learned with Long Short Term Memory (LSTM) on large-scale datasets can be efficiently transferred to other time-series forecasting tasks with limited amount of training data. We also validate that despite differences in time-series statistics and tasks in the datasets, the transferred representation leads to significantly improved forecasting results outperforming majority of the best time-series methods on the public M3 and other datasets. Our online universal forecasting tool, DeepCast, will leverage transfer learning to provide accurate forecasts for a diverse set of time series where classical methods were computationally infeasible or inapplicable due to short training history.","pdf":"/pdf/ce8d8d7e5efb72326ea5252f1d0d55a896cbbd58.pdf","paperhash":"anonymous|deepcast_universal_timeseries_forecaster","_bibtex":"@article{\n  anonymous2018deepcast:,\n  title={DEEPCAST: UNIVERSAL TIME-SERIES FORECASTER},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryDUFeWRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper610/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}