{"notes":[{"tddate":null,"ddate":null,"tmdate":1512430891828,"tcdate":1512430891828,"number":3,"cdate":1512430891828,"id":"H1VNp87bz","invitation":"ICLR.cc/2018/Conference/-/Paper1088/Official_Review","forum":"rkZB1XbRZ","replyto":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference/Paper1088/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Clarification needed for data-dependent privacy guarantee.","rating":"7: Good paper, accept","review":"This paper considers the problem of private learning and uses the PATE framework to achieve differential privacy. The dataset is partitioned and multiple learning algorithms produce so-called teacher classifiers. The labels produced by the teachers are aggregated in a differentially private manner and the aggregated labels are then used to train a student classifier, which forms the final output. The novelty of this work is a refined aggregation process, which is improved in three ways:\na) Gaussian instead of Laplace noise is used to achieve differential privacy.\nb) Queries to the aggregator are \"filtered\" so that the limited privacy budget is only expended on queries where the teachers are confident and the student is uncertain or wrong.\nc) A data-dependent privacy analysis is used to attain sharper bounds on the privacy loss with each query.\n\nI think this is a nice modular framework form private learning, with significant refinements relative to previous work that make the algorithm more practical. On this basis, I think the paper should be accepted. However, I think some clarification is needed with regard to item c above:\n\nTheorem 2 gives a data-dependent privacy guarantee. That is, if there is one label backed by a clear majority of teachers, then the privacy loss (as measured by Renyi divergence) is low. This data-dependent privacy guarantee is likely to be much tighter than the data-independent guarantee.\nHowever, since the privacy guarantee now depends on the data, it is itself sensitive information. How is this issue resolved? If the final privacy guarantee is data-dependent, then this is very different to the way differential privacy is usually applied. This would resemble the \"privacy odometer\" setting of Rogers-Roth-Ullman-Vadhan [ https://arxiv.org/abs/1605.08294 ]. \nAnother way to resolve this would be to have an output-dependent privacy guarantee. That is, the privacy guarantee would depend only on public information, rather than the private data. The widely-used \"sparse vector\" technique [ http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf#page=59 ] does this.\nIn any case, this is an important issue that needs to be clarified, as it is not clear to me how this is resolved.\n\nThe algorithm in this work is similar to the so-called median mechanism [ https://www.cis.upenn.edu/~aaroth/Papers/onlineprivacy.pdf ] and private multiplicative weights [ http://mrtz.org/papers/HR10mult.pdf ]. These works also involve a \"student\" being trained using sensitive data with queries being answered in a differentially private manner. And, in particular, these works also filter out uninformative queries using the sparse vector technique. It would be helpful to add a comparison.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222554493,"tcdate":1511795148510,"number":2,"cdate":1511795148510,"id":"r1ERFjYef","invitation":"ICLR.cc/2018/Conference/-/Paper1088/Official_Review","forum":"rkZB1XbRZ","replyto":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference/Paper1088/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This work investigates scalable applications of PATE ","rating":"6: Marginally above acceptance threshold","review":"Summary:\nIn this work, PATE, an approach for learning with privacy,  is modified to scale its application to real-world data sets. This is done by leveraging the synergy between privacy and utility, to make better use of the privacy budget spent when transferring knowledge from teachers to the student. Two aggregation mechanisms are introduced for this reason.  It is demonstrated that sampling from a Gaussian distribution (instead from a Laplacian distribution) facilitates the aggregation of teacher votes in tasks with large number of output classes. \n\non the positive side:\n\nHaving scalable models is important, especially models that can be applied to data with privacy concerns. The extension of an approach for learning with privacy to make it scalable is of merit. The paper is well written, and the idea of the model is clear. \n\n\non the negative side:\n\nIn the introduction, the authors introduce the problem by the importance of privacy issues in medical and health care data. This is for sure an important topic. However, in the following paper, the model is applied no neither medical nor healthcare data. The authors mention that the original model PATE was applied to medical record and census data with the UCI diabetes and adult data set. I personally would prefer to see the proposed model applied to this kind of data sets as well. \n\nminor comments: \n\nFigure 2, legend needs to be outside the Figure, in the current Figure a lot is covered by the legend","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222554540,"tcdate":1511746196271,"number":1,"cdate":1511746196271,"id":"r1nc91tez","invitation":"ICLR.cc/2018/Conference/-/Paper1088/Official_Review","forum":"rkZB1XbRZ","replyto":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference/Paper1088/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Novel techniques to improve private learning with PATE","rating":"6: Marginally above acceptance threshold","review":"The paper proposes novel techniques for private learning with PATE framework. Two key ideas in the paper include the use of Gaussian noise for the aggregation mechanism in PATE instead of Laplace noise and selective answering strategy by teacher ensemble. In the experiments, the efficacy of the proposed techniques has been demonstrated. I am not familiar with privacy learning but it is interesting to see that more concentrated distribution (Gaussian) and clever aggregators provide better utility-privacy tradeoff. \n\n1. As for noise distribution, I am wondering if the variance of the distribution also plays a role to keep good utility-privacy trade-off. It would be great to discuss and show experimental results for utility-privacy tradeoff with different variances of Laplace and Gaussian noise.\n\n2. It would be great to have an intuitive explanation about differential privacy and selective aggregation mechanisms with examples. \n\n3. It would be great if there is an explanation about the privacy cost for selective aggregation. Intuitively, if teacher ensemble does not answer, it seems that it would reveal the fact that teachers do not agree, and thus spend some privacy cost.\n\n\n\n\n\n\n\n\n\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092380963,"tcdate":1509138261294,"number":1088,"cdate":1510092360098,"id":"rkZB1XbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkZB1XbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Scalable Private Learning with PATE","abstract":"Recently, there has been increased attention to the privacy concerns raised by machine learning (ML) models trained on highly sensitive data, such as medical records or personal information. To resolve those concerns, one attractive approach is the Private Aggregation of Teacher Ensembles (PATE), which has shown that knowledge from an ensemble’s aggregated answers can be transferred to train models with strong differential-privacy guarantees. Yet, while promising, PATE applications have so far been limited to simple classification tasks like MNIST; its scalability to other tasks was unclear because of inherent limitations of the noise distributions proposed and its dependency on accurate aggregation and voting.\n\nIn this work, we enable scalable applications of PATE. For this, we leverage two key insights: aggregation mechanisms with concentrated noise may mitigate these limitations and an ensemble of teachers designed to answer only questions on which they generally agree can still successfully transfer their knowledge to the student. Intuitively, such consensus answers also ought to incur lower privacy costs. With new noisy mechanisms and tighter privacy analyses, we utilize these insights to greatly improve PATE’s tradeoffs thereby leading to better scalability.\n\nIn experiments, we improve the state-of-the-art on privacy-preserving ML benchmarks, and we also demonstrate the successful application of PATE with our new ideas to a real-world task with imbalanced and partly mislabeled data involving hundreds of classes.","pdf":"/pdf/85d216249a97e2df4929959351d82854c163e077.pdf","paperhash":"anonymous|scalable_private_learning_with_pate","_bibtex":"@article{\n  anonymous2018scalable,\n  title={Scalable Private Learning with PATE},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkZB1XbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1088/Authors"],"keywords":["privacy","differential privacy","machine learning","deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}