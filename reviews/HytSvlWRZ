{"notes":[{"tddate":null,"ddate":null,"tmdate":1515088922043,"tcdate":1515088922043,"number":7,"cdate":1515088922043,"id":"Skzmnk2mf","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Warm Reminder","comment":"We wish all the reviewers happy new year and we are looking forward to addressing any new comments to our responses posted previously. If you have any comments, feel free to let us know. Thank you very much."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514330609269,"tcdate":1514330609269,"number":6,"cdate":1514330609269,"id":"SyFl9LxQf","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Overall Response","comment":"We would like to thank all reviewers for all the valuable and constructive comments. By following the suggestions, we have significantly extended our experiments, revised our paper and addressed questions from all the reviewers. Below we summarize the improvements in this revision:\n\nMajor Improvements:\n\n1. For synthetic experiments (Table 1), we provide additional metrics to evaluate the recovery achieved by different methods. Results show that SN outperforms all methods in various metrics, with comparable margins. \n\n2. We largely extend experiments and compare SN with three DNN baselines (naive, with censoring, and with censoring + low-rank) for both synthetic and real data. Results (Table 2 and 5) show SN outperforms all baselines.\n\n3. We verify the benefit of considering target boundedness in all sets of methods considered: (1) Single-task and multi-task shallow methods; (2) Deep methods. Results (Table 2 and 5) show performance improvement when considering the boundedness of targets.\n\n4.  We’ve extended more detailed proof outlines for convergence properties for both asymptotic and non-asymptotic cases in Appendix.\n\nMinor Improvements:\n\n1. We solve the counter-intuitive experimental observation by fixing numerical issues in multi-task algorithms and update results in Table 2 and 5.\n\n2. We remeasure the computation speed more accurately and update results in Table 6.\n\n3. For all the confusions we made previously, we have made it more clear in the revised version.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514267252460,"tcdate":1514267252460,"number":5,"cdate":1514267252460,"id":"rJ2_MDJXf","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"H189gvyQM","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Response for Reviewer 3 (Part 2)","comment":"\nQ5:  In section 2.2 and 4 there is some confusion between iteration indices and samples indices\n \nA5: In our online algorithm, since each iteration a new sample comes in, the iteration indices and samples indices are identical. We apologize for the confusion caused and have clarified it in the revised paper.\n\nQ6: Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables.\n \nA6: Thanks for pointing out. This paper discussed the most basic nonnegativity-censored regression, which coincides with a RELU-based form (1). The effectiveness of this simplest lower bound has been verified in experiments.\n\nHowever, there is no obstacle to extend the developed methodology to general Tobit models with both lower and upper bounds, and to multiple regression targets where each target has varying bounds: that will only affect the form of likelihood function in Section 2. We will certainly extend Subspace Network to account for those cases in future work.\n \nQ7: Figure 2, the scale of the improvement of the subspace difference is quite tiny. The loss function of Figure 2.b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimization.\n \nA7: (1) Subspace difference is defined as ||U-U_i||_F / ||U||_F, which is divided by the Frobenius norm of U matrix after getting the difference, therefore, it is in small scale; (2) Iteration-wise difference is defined as ||U_i - U_i-1||_F / ||U||_F, similar with (1); (3) our loss is plotted per each single sample passing through the online algorithm: please note the important difference with typical DNN training curves, where loss values are plotted per batch or even per epoch and thus the curve looks smoother. Our training loss, subspace difference between groundtruth, and the iteration-wise subspace differences all steadily decrease as more data points are fed in, showing healthy convergence and fitting our theoretical results well.\n \nIn the revised version, please further refer to the two new, more robust mutual coherence-based metrics introduced in Section 4.1, in terms of which much sharper margins are achieved by Subspace Network, suggesting that our recovered subspaces are significantly better aligned with the groundtruth.\n                                                    \nQ8. The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is not discussed nor analyzed.\n \nA8: For real data, since there is no known ground truth of the rank, we choose the rank R by selecting one that leads to overall best performance in cross-validation. We display the effect of rank assumption on Subspace Network performance in real data, in Table 8 of Appendix. The overall robustness of SN to rank assumptions is fairly remarkable: the performance of SN under all rank assumptions appears to be competitive, consistently outperforming DNNs under the same rank assumptions and other baselines.                                                                            \n                \nQ9: The synthetic example of page 7 is potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.  \n \nA9: One goal of the synthetic experiments is to verify if our model could correctly recover the underlying low rank parameter subspaces: that is why we generate data in the controlled way. The synthetic results align with our theory. Note that the third synthetic experiment in Section 4.1 (“Benefits of Going Deep”) shows an interesting example, that a multi-layer SN performs the best even the data is generated using the one-layer model.\n \nMore importantly, the practical effectiveness of Subspace Network is validated by the real data experiments, where no data generation process is assumed and no underlying parameter (e.g., rank, layer number) is pre-known. Subspace Network proves to automatically discover latent low-rank subspaces from data and achieves superior predictions. We also compare our model with several DNN baselines in the revised paper, and still achieverperformance margins over them.\n \nQ10: The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations).\n \nA10: Thanks very much for pointing out. There was some unintentional bug in measuring the algorithm running time. We realized and corrected it right after submission. We apologize for this careless mistake and have reported the correct running time in the revised version, Table 6 in Appendix."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514307964634,"tcdate":1514266765878,"number":4,"cdate":1514266765878,"id":"H189gvyQM","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"SkwZAL4ef","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Response for Reviewer 3 (Part 1)","comment":"Thanks very much for your suggestions. We have addressed all your concerns below.  Most notably, we have significantly enriched our experiments to testify the performance advantages of SN over DNNs in real data, and to analyze how much each component (censored, low-rank, and online feed-forward training) accounts for this performance gain. We have also revised our paper and corrected typos. Overall, we hope that our revised version has manifested its value as a unique and effective predictive method of neurodegenerative disorders.\n \nQ1: Clinical scores are defined on discrete scales: the Gaussian assumption for the cost function is thus not appropriate for the proposed application.\n \nA1:  As defined in (1), the Gaussian assumption is not enforced on the scores, but on the noise \\epsilon that we assume in the latent space. It is a standard assumption in subspace sensing, and the standard deviation hyperparameter controls how accurately the low-rank subspace assumption can fit the data. It does not solely determine the distribution of y, and other noise models can also be assumed here. We apologize for the confusion and have revised the paper to make it clearer.\n\nQ2: The paper does not show evidence about improved predictive performance and generalization when accounting for the target boundedness\n \nA2: We extended experiments to compare results between considering target boundedness (Censored) and not (Uncensored) for both single-task and multi-task models. In either scenario, we reported the best performance (LS+L1 for single task and Multi Trace for multi-task in our case) in Table 2. In the revised version, we further compared SN with several DNN baselines, where the benefit of setting censored regression goals is also found to be evident in DNN settings. Please see next response for details.\n \nQ3: The authors should provide a more rigorous benchmark including non-linear prediction approaches\n \nA3: As suggested, we compared with three DNN baselines (naive, with censoring, and with censoring + low-rank) for both synthetic and real data in the paragraph “Benefits of Going deep” in Section 4.1 and “Performance” in Section 4.2 of the revised paper, in addition to the existing nonlinear Tobit censored regression model. The comparisons of three baselines indicate that both censored regression and low-rank assumption improve DNN’s performance on the given MTL task. Meanwhile, SN clearly outperforms all three, even DNN equipped with censoring + low-rank, suggesting the advantage of our proposed online one-pass sensing and feed-forward training strategy. The performance advantage of SN over DNNs is also confirmed across different rank assumptions in Table 8 in Appendix.\n\nQ4: What is the interest of predicting baseline (or 6 months at best) cognitive scores from brain imaging data?\n \nA4: Thanks for pointing out. We have revised the paper to refer readers interested in this setting to relevant clinical references. The predictive modeling paradigm that we used in the paper is a rather common setting in clinical studies of neurodegenerative diseases such as Alzheimer’s disease (AD), e.g.,\n \n[1] Stonnington, C. M., Chu, C., Klöppel, S., Jack, C. R., Ashburner, J., Frackowiak, R. S., & Alzheimer Disease Neuroimaging Initiative. (2010). Predicting clinical scores from magnetic resonance scans in Alzheimer's disease. Neuroimage, 51(4), 1405-1413.\n[2] Orrù, G., Pettersson-Yeo, W., Marquand, A. F., Sartori, G., & Mechelli, A. (2012). Using support vector machine to identify imaging biomarkers of neurological and psychiatric disease: a critical review. Neuroscience & Biobehavioral Reviews, 36(4), 1140-1152.\n[3] Zhang, D., Shen, D., & Alzheimer's Disease Neuroimaging Initiative. (2012). Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer's disease. NeuroImage, 59(2), 895-907.\n[4] Zhou, J., Liu, J., Narayan, V. A., Ye, J., & Alzheimer's Disease Neuroimaging Initiative. (2013). Modeling disease progression via multi-task learning. NeuroImage, 78, 233-248.\n \nThe rationale behind this setting is as follows. For example, the diagnosis of AD requires autopsy confirmation, which is not applicable on live patients. Hence many cognitive measures have been designed to evaluate the cognitive status of a patient. These measures are important criteria for clinical diagnosis of probable AD.  These cognitive status/scores can be considered as phenotypes that are tangled with complicated neurological pathologies in the brain. Currently there are many hypotheses the pathological pathways of AD progression over time, but we are far from understanding the ultimate cause and thus the studies of associations between cognitive scores and neuroimages are critical in understanding the progression and predictability of the disease. The models can reveal important insights and may lead to novel target for therapeutic intervention and drug developments. \n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514307893208,"tcdate":1514266394920,"number":3,"cdate":1514266394920,"id":"ByQ7kvkmf","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"BJl30LJ7G","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Response for Reviewer 1 (Part 2)","comment":"\nQ6. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. Extensive experiments could be added.\n \nA6: Thanks for the comments. We have extended our experiments by comparing SN with several baseline DNNs (in previous comments) and results verify that SN outperforms all three variants DNNs.\n \nWe note that the MTL, like the one proposed in this paper, is typically used for solve learning problems with insufficient training data. Nowadays this is very typical in medical research, and is one motivation for us to design this method. The ADNI data used in our paper is so far the largest cohort collected for Alzheimer’s disease study, despite that it still only has less than 1000 patients available due to the expensive data collection process. The ADNI dataset is widely used for building machine learning models, where researchers proposed many algorithms to tackle challenges arising from the small sample-size:\n \n[1] Huang, S., Li, J., Sun, L., Ye, J., Fleisher, A., Wu, T., ... & Alzheimer's Disease NeuroImaging Initiative. (2010). Learning brain connectivity of Alzheimer's disease by sparse inverse covariance estimation. NeuroImage, 50(3), 935-949.\n[2] Stonnington, C. M., Chu, C., Klöppel, S., Jack, C. R., Ashburner, J., Frackowiak, R. S., & Alzheimer Disease Neuroimaging Initiative. (2010). Predicting clinical scores from magnetic resonance scans in Alzheimer's disease. Neuroimage, 51(4), 1405-1413.\n[3] Orrù, G., Pettersson-Yeo, W., Marquand, A. F., Sartori, G., & Mechelli, A. (2012). Using support vector machine to identify imaging biomarkers of neurological and psychiatric disease: a critical review. Neuroscience & Biobehavioral Reviews, 36(4), 1140-1152.\n[4] Zhang, D., Shen, D., & Alzheimer's Disease Neuroimaging Initiative. (2012). Multi-modal MTL for joint prediction of multiple regression and classification variables in Alzheimer's disease. NeuroImage, 59(2), 895-907.\n[5] Zhou, J., Liu, J., Narayan, V. A., Ye, J., & Alzheimer's Disease Neuroimaging Initiative. (2013). Modeling disease progression via MTL. NeuroImage, 78, 233-248.\n[6] Liu, M., & Zhang, D. (2016). Pairwise constraint-guided sparse learning for feature selection. IEEE transactions on cybernetics, 46(1), 298-310.\n[7] Zheng, X., Shi, J., Li, Y., Liu, X., & Zhang, Q. (2016, April). Multi-modality stacked deep polynomial network based feature learning for Alzheimer's disease diagnosis. In Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on (pp. 851-854). IEEE.\n \nWe do plan to collaborate with clinical partners to collect larger neurodegenerative datasets and apply the proposed method.\n \nQ7. Add 1-Layer SN results\n \nA7: The results of 1-layer SN have been added: see Table 2 and Table 5.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514307862078,"tcdate":1514266280313,"number":2,"cdate":1514266280313,"id":"BJl30LJ7G","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"B1r0SU9gz","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Response for Reviewer 1 (Part 1)","comment":"\nThanks very much for appreciating the novelty of our work, and for your very insightful and constructive comments. We have addressed all your questions below. In particular, we have significantly enriched our experiments as suggested. All results consistently suggest the performance advantage and robustness of Subspace Network over state-of-art linear /nonlinear methods, as well as DNNs. We have also revised the paper and corrected typos. \n\nQ1: The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new.\n \nA1: SN was mainly built on the work series of online subspace sensing, where low-rank assumption was enforced on the input space, e.g., Mardani et al. (2015), Shen et al. (2016). Motivated by the popularity of low-rank parameter space in MTL, we introduced the first-of-its-kind combination of online subspace sensing (mostly focusing on input space), and low-rank parameter assumption for MTL: we believe their marriage to be new.\n                     \nQ2: The proof part (Section 2.2) can be extended with more details in Appendix.\n \nA2: We’ve extended more detailed proof outlines in Appendix. At some steps of the proofs, we point to the important key result to refer to. Proofs are provided for self-containedness only.\n\nQ3: In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others.\n \nA3: We have provided additional experimental results after further tuning all three networks. In addition to subspace difference, we added two new metrics: (1) the maximum mutual coherence of all column pairs from two matrices, as a classical measurement on how correlated the two matrices' column subspaces are; (2) the mean mutual coherence of all column pairs from two matrices. Note that the two mutual coherence-based metrics are more robust since they are immune to linear transformations of subspace coordinates, to which the L2-based subspace difference is not immune to. Results can be found in Table 1, showing the clear advantage of SN over f-MLP and rf-MLP under all three metrics. The performance margins of SN in terms of maximum/mean mutual coherences are remarkably more visible than under L2-based difference.\n\nQ4: In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation.\n \nA4: Thanks so much for pointing out. We found there were numerical convergence issues with the optimization algorithms for MTL models.  We have fixed the problem and updated the results in Table 3, where the performance turns now intuitive.\n\nWe also extended our experiments to compare results between considering target boundedness (Censored) and not (Uncensored) for both single-task and multi-task models. In either scenario, we reported the best performance (LS+L1 for single task linear, and Multi Trace for multi-task in our case) in Table 2.\n \nQ5: Extra models like Deep Networks with/without matrix factorization could be added.\n \nA5: Thanks very much for your suggestion. We have added them as suggested. Please refer to the new paragraph “Benefits of Going deep” in Section 4.1 and “Performance” in Section 4.2 in the revised paper. In sum, we compared with three DNN baselines (naive, with censoring, and with censoring + low-rank) for both synthetic and real data. The comparisons of three baselines indicated that both censored regression and low-rank assumption improved DNN’s performance on the given MTL task. Meanwhile, SN clearly outperformed all three, even DNN equipped with censoring + low-rank, suggesting the advantage of our proposed online one-pass sensing and feed-forward training strategy. The performance advantage of SN over DNNs was confirmed across different rank assumptions, and across both synthetic and real data.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514267387421,"tcdate":1514265803851,"number":1,"cdate":1514265803851,"id":"BkE02U1Xf","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Comment","forum":"HytSvlWRZ","replyto":"r1z2QSOlz","signatures":["ICLR.cc/2018/Conference/Paper599/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper599/Authors"],"content":{"title":"Response for Reviewer 2","comment":" \nThanks very much for your suggestions. We have addressed all your concerns below. \n\nQ1. The paper is not self-contained. The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1. However, for some key steps in the proof, they refer to other references. If this is due to space limitation in the main text, they may want to provide a complete proof in the appendix.\n \nA1: Thanks very much for your comment. In Appendix of the revised paper, please find the  more detailed long version of the proof outlines of both asymptotic and non-asymptotic convergence properties in Appendix. We expect that the new proof has better self-containedness.\n\nAt some steps of the proof, we pointed to the important key results to refer to (in precise forms, e.g., Lemma 2 of Mardani et al. (2013) ). In order to focus on the key contribution of this paper, we did not include the detailed assumptions and proofs of all those intermediate theorems/lemma that we used. The reasons are: (1) they can be very lengthy; (2) they were well-established results in other relevant literature and were not our innovations; (3) those intermediate results were not tightly related the main contributions of this paper (SN model). We believe that the current proof outline has already captured all main proof ideas and should be easy to follow for readers of interests. \n\nQ2. The experiments are unconvincing. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. A major merit of DNN is that it can automatically extract useful features. However, in this experiment, the features are handcrafted before they are fed into the models. Thus, I would like to see a comparison between SN with vanilla DNN.\n\nA2: Thanks for your comments. We agree that one major merit of DNN is to automatically extract features from images, that demonstrated huge success in many domains. Such capability is based on the availability of large labeled training data. In the medical research domain, however, such labeled data is rarely available, especially in the challenging disease of neurodegenerative diseases such as Alzheimer’s disease (AD) and Parkinson. The ADNI data used in our paper is so far the largest cohort collected for Alzheimer’s disease study, and however has less than 1000 patients available for building predictive models due to the expensive data collection process. Due to extreme high dimension of an MRI image (voxel size: 512x512x16 = 4,194,304), most studies use region-of-interests features extracted by existing neuroimaging tools, instead of raw imaging data for studying progression of a disease. As such, a majority amount of AD study performs predictive modeling using extracted features:\n\n[1] Duchesne, S., Caroli, A., Geroldi, C., Collins, D. L., & Frisoni, G. B. (2009). Relating one-year cognitive change in mild cognitive impairment to baseline MRI features. Neuroimage, 47(4), 1363-1370.\n[2] Stonnington, C. M., Chu, C., Klöppel, S., Jack, C. R., Ashburner, J., Frackowiak, R. S., & Alzheimer Disease Neuroimaging Initiative. (2010). Predicting clinical scores from magnetic resonance scans in Alzheimer's disease. Neuroimage, 51(4), 1405-1413.\n[3] Orrù, G., Pettersson-Yeo, W., Marquand, A. F., Sartori, G., & Mechelli, A. (2012). Using support vector machine to identify imaging biomarkers of neurological and psychiatric disease: a critical review. Neuroscience & Biobehavioral Reviews, 36(4), 1140-1152.\n[4] Zhang, D., Shen, D., & Alzheimer's Disease Neuroimaging Initiative. (2012). Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer's disease. NeuroImage, 59(2), 895-907.\n[5]  Zhou, J., Liu, J., Narayan, V. A., Ye, J., & Alzheimer's Disease Neuroimaging Initiative. (2013). Modeling disease progression via multi-task learning. NeuroImage, 78, 233-248.\n\nWe also improve our experiments by comparing with three DNN baselines (naive, with censoring, and with censoring + low-rank) in both synthetic and real data, please refer to the new paragraph “Benefits of Going deep” in Section 4.1 and “Performance” in Section 4.2 in the revised paper. The comparisons of three baselines indicate that both censored regression and low-rank assumption improve DNN’s performance on the given MTL task. Meanwhile, Subspace Network clearly outperforms all three, even DNN equipped with censoring + low-rank, suggesting the advantage of our proposed online one-pass sensing and feed-forward training strategy. The performance advantage of Subspace Network is confirmed across different rank assumptions, and across both synthetic and real data.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1515642476947,"tcdate":1511839181292,"number":3,"cdate":1511839181292,"id":"B1r0SU9gz","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Review","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper introduces a multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded.","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded.\n\nSome novel contributions:\n1. Layer by layer feedforward training process, no back-prop.\n2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data)\n\nWeakness :\n1. The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literature.\n2. The proof part(Section 2.2) can be extended with more details in Appendix.\n3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others. \n4. Typo: In Table2,3,5, Multi-l_{2,1} (denotes the L2,1 norm) were written wrong.\n5. In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation. \n6. Extra models like Deep Networks with/without matrix factorization could be added. ( As proposed model is a deep model, the lack of comparison with deep methods is dubious)\n7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin. Extensive experiments could be added.\n8. The performance on One-Layer Subspace Network (with only the input features) could be added. \n\nConclusion:\nThough with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1515642476988,"tcdate":1511703465551,"number":2,"cdate":1511703465551,"id":"r1z2QSOlz","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Review","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. The writing needs more elaboration. The experiments are unconvincing.","rating":"5: Marginally below acceptance threshold","review":"The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. The topic is important. Experiments on real data show improvements compared to several traditional approaches.\n\nMy major concerns are as follows.\n\n1. The paper is not self-contained. The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1. However, for some key steps in the proof, they refer to other references. If this is due to space limitation in the main text, they may want to provide a complete proof in the appendix.\n\n2. The experiments are unconvincing. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. A major merit of DNN is that it can automatically extract useful features. However, in this experiment, the features are handcrafted before they are fed into the models. Thus, I would like to see a comparison between SN with vanilla DNN. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1515642477025,"tcdate":1511448063449,"number":1,"cdate":1511448063449,"id":"SkwZAL4ef","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Review","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea which is however not clearly developed. Incremental results.","rating":"4: Ok but not good enough - rejection","review":"This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. \nDifferently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. The framework is represented by a feed-forward deep architecture analogous to a residual network. At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables.\n\nThe idea of explicitly accounting for the boundedness of clinical scores is interesting, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales. For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application. \nFurthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. \nThe proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, …). \n\nOverall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders. Moreover, the proposed experimental paradigm appears flawed. What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)?\n\nOther remarks. \n\n- In section 2.2 and 4 there is some confusion between iteration indices and samples indices “i”. \n\n- Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables.  \n\n-  Figure 2, synthetic data. The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations. The loss function of Figure 2.b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure. These aspects may be a sign of convergence issues. \n\n- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is a crucial parameters that is however not discussed nor analysed in the paper. \n\n- The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.  In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting.\n\n- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Is there anything that I am missing?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1514263619990,"tcdate":1509128001453,"number":599,"cdate":1509739206660,"id":"HytSvlWRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/523f895a9b8741be3e63a328364e6cede5b42c86.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}