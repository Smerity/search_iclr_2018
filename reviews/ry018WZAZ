{"notes":[{"tddate":null,"ddate":null,"tmdate":1515302214424,"tcdate":1515301965118,"number":5,"cdate":1515301965118,"id":"S1S83QJVz","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Comment","forum":"ry018WZAZ","replyto":"B1cNJbz7G","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper677/AnonReviewer2"],"content":{"title":"Reply","comment":"Thank you for such detailed response and for adding the new experiment results. Overall, I believe this kind of empirical results on applying active learning to deep NLP models would be useful to the community."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1514438449775,"tcdate":1514438449775,"number":4,"cdate":1514438449775,"id":"B1cNJbz7G","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Comment","forum":"ry018WZAZ","replyto":"ry7bzE8eG","signatures":["ICLR.cc/2018/Conference/Paper677/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper677/Authors"],"content":{"title":"More experimental results are added to strengthen the empirical evidence","comment":"We would like to thank Reviewer 2 for taking the time to leave a clear and thoughtful review. We have improved the draft per your comments and would like to reply to each specific point in turn:\n\n1. While the architecture and active learning contributions may appear on a technical level to be orthogonal contributions, in practice they are closely linked. Active learning requires frequent retraining of the model, during which time new annotations cannot be collected; faster training speed makes more frequent training of the model affordable, which in turn allows collection of more informative labels. Whereas the use of recurrent network in word-level encoder and chain CRF in tag decoder have been considered as a standard approach in the literature (from Collobert et al 2011 to more recent papers like Lample et al 2016, Chiu and Nichols 2015, Yang et al 2016), we demonstrate that convolutional word encoder and LSTM decoder provides 4x speedup in training time with very minimal loss of performance in terms of F1 score.\n\nPer your criticism and to reinforce the significance of the speedup, we have implemented standard CNN-LSTM-CRF model and added its training speed to Table 4, so that the magnitude of the speedup could be demonstrated.\n\n2. We have also added LSTM-LSTM-LSTM experiment on OntoNotes 5.0-English in Table 4. We did not run this experiment in our initial draft since we focus on finding computationally efficient architectures, and it was clear from CoNLL-2003 English experiments that LSTM-LSTM-LSTM is much more computationally expensive than other competitors; but we agree that this result is still informative.\n\n3. In response to your question about variance, we have replicated the active learning experiments multiple times, and added a learning curve plot with error bars; please refer to Appendix B and Figure 6 of the updated paper. While learning curves from active learning methods are indeed close to each other, there is a noticeable trend that MNLP (our proposal) and BALD outperforms traditional LC in early rounds of data acquisition. Also note that MNLP we propose is much more computationally efficient because it requires only a single forward pass, whereas BALD requires multiple forward passes.\n\n4. We agree with your point that the SUBMOD method ought to be explained briefly in the main text and not relegated only to the Appendix. Per your criticism, we’ve added a paragraph to section 4 describing the approach. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1514438256965,"tcdate":1514438256965,"number":3,"cdate":1514438256965,"id":"H1Y_RgzXG","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Comment","forum":"ry018WZAZ","replyto":"S1G9tRFgG","signatures":["ICLR.cc/2018/Conference/Paper677/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper677/Authors"],"content":{"title":"Our results give actionable insights to the practice of deep learning","comment":"Thank you for taking the time to review our paper. We are glad that you recognize the soundness of the approaches and the experimental evaluation. \n\nWe would like to address the reviewer’s main concerns:\n\n1. Regarding “No conclusion can be drawn by comparing with the 4 different strategies”:\n\nOur experiments demonstrate that active learning can produce results comparable to the state of the art methods while using only 25% of the data. Moreover, we demonstrate that combining representativeness-based submodular optimization methods with uncertainty-based heuristics conferred no additional advantage. \n\nWe believe that these are in and of themselves significant conclusions. We agree that the Bayesian approach (BALD) and least confidence approaches (LC and MNLP) produce similar learning curves, but we do not believe that this renders the result inconclusive. To the contrary, parity in performance strongly favors the least confidence approaches owing to their computational advantages; BALD requires multiple forward passes to produce an uncertainty estimate, whereas LC and MNLP require only a single forward pass. \n\nWe have added error bars to our learning curve (Appendix B, Figure 6) in our updated paper, and it can be seen that there is a noticeable trend which MNLP (our proposal) and BALD outperform traditional LC in early active learning rounds.\n\n2. Regarding the novelty of the paper:\n\nIn this paper, we explore several methods for active learning. These include uncertainty-based heuristics, which we emphasize in the main text and which yield compelling experimental results. We also include a representativeness-based sampling method using submodular optimization that is reported in the main text and described in detail in the appendix. In the revised version we have included some of this detail in the main body of the paper. \n\nIt so happens that the simpler approaches outperform our more technically complex contributions. The tendency not to publish compelling results produced by simple methods creates a strong selection bias that might misrepresent the state of the art and can encourages authors to overcomplicate methodology unnecessarily. While the winning method in our paper doesn’t offer substantial mathematical novelty, we hope that the reviewer will appreciate that our work demonstrates empirical rigor. Our results give actionable insights to the deep learning and NLP communities that are not described in the prior literature.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1514437913853,"tcdate":1514437913853,"number":2,"cdate":1514437913853,"id":"B1M7plzmz","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Comment","forum":"ry018WZAZ","replyto":"HklWpb5lz","signatures":["ICLR.cc/2018/Conference/Paper677/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper677/Authors"],"content":{"title":"further analysis of LSTM decoder is provided","comment":"We’d like to thank Reviewer 3 for a thoughtful and constructive review of our paper. We are glad that the reviewer recognized the importance of using a lightweight network to support the incremental training required for our active learning strategy. \n\nThe reviewer asks an interesting question; “LSTM decoder seems to have slight advantage over CRF decoder although LSTM does not output the best tag sequence. It'd be good to comment on this.”\n\nIndeed, our greedily decoded tag sequences from the LSTM decoder may not be the best tag sequence with respect to the LSTM’s predictions whereas the chain CRF can output the best sequence with respect to its predictions via dynamic programming. There are a few important points to make here:\n\n1. We experimented with multiple beam sizes for LSTM decoders, and found that greedy decoding is very competitive to beam search; beam search with size 16 was only marginally better than greedily decoded tags in terms of F1 score. Producing the best sequence is equivalent to setting an arbitrarily large beam width. Out experiments indicate that empirically, increasing the beam width beyond 2 helps only marginally.  This suggests that the intractability of finding best sequence from the LSTM decoder is not a significant practical concern. We added these results, plotting the F1 score vs beam width in Appendix A of the updated paper.\n\n2. The LSTM decoder is a more expressive model than the chain CRF model since it models long-range dependencies (vs a single step for CRF), and thus the LSTM’s best sequence may be considerably better than the best sequence according to the CRF. So even if we do not get the very best sequence from the LSTM, it can still outperform the CRF. Indeed, we found in our experiments that when the encoder is fixed, our LSTM decoder outperforms our CRF decoder (Table 3 and 4, compare CNN-CNN-CRF against CNN-CNN-LSTM).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1514437782856,"tcdate":1514437782856,"number":1,"cdate":1514437782856,"id":"rJJs3gG7M","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Comment","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper677/Authors"],"content":{"title":"General Reply to All Reviewers","comment":"We would like to thank the reviewers for taking the time to review our paper and give thoughtful comments. We are encouraged to see that two reviewers rate the paper as above the acceptance threshold and that all three reviewers recognized the clear demonstration of the empirical benefits of applying active learning when training deep models for named entity recognition. The reviewers also left some insightful critical comments. We have addressed these in the revised draft and responded to each reviewer in the respective threads."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"ddate":null,"tddate":1511820717252,"tmdate":1516086537067,"tcdate":1511820535716,"number":3,"cdate":1511820535716,"id":"HklWpb5lz","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Review","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer3"],"readers":["everyone"],"content":{"title":"the ideas are simple but seems to work well empirically","rating":"6: Marginally above acceptance threshold","review":"This paper introduces a lightweight neural network that achieves state-of-the-art performance on NER. The network allows efficient active incremental training, which significantly reduces the amount of training data needed to match state-of-the-art performance.\n\nThe paper is well-written. The ideas are simple, but they seem to work well in the experiments. Interestingly, LSTM decoder seems to have slight advantage over CRF decoder although LSTM does not output the best tag sequence. It'd be good to comment on this.\n\n* After rebuttal\nThank you for your response and revision of the paper. I think the empirical study could be useful to the community.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515909213252,"tcdate":1511807369555,"number":2,"cdate":1511807369555,"id":"S1G9tRFgG","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Review","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper studies the application of different existing active learning strategies for the deep models for NER. This paper has several strong and weak points listed in the reviews","rating":"6: Marginally above acceptance threshold","review":"This paper studies the application of different existing active learning strategies for the deep models for NER.\n\nPros:\n* Active learning may be used for improving the performance of deep models for NER in practice\n* All the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for NER\n\nCons:\n* The novelty of this paper is marginal. The proposed approaches turn out to be a combination of existing active learning strategies for selecting data to query with the existing deep model for NER. \n* No conclusion can be drawn by comparing with the 4 different strategies.\n\n======= After rebuttal  ================\n\nThank you for the clarification and revision on this paper. It looks better now.\n\nI understand that the purpose of this paper is to give actionable insights to the practice of deep learning. However, since AL itself is a meta learning framework and neural net as the base learner has been shown to be effective for AL, the novelty and contribution of a general discussion of applying AL for deep neural nets is marginal.  What I really expected is a tightly-coupled active learning strategy that is specially designed for the particular deep neural network structure used for NER. Apparently, however, none of the strategies used in this work is designed for this purpose (e.g., the query strategy or model update strategy should at least reflex some properties of deep learning or NER). Thus, it is still below my expectation. \n\nAnyway, since the authors had attempted to improved this paper, and the results may provide some information to practice, I would like to slightly raise my rating to give this attempt a chance.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1515642490728,"tcdate":1511567867025,"number":1,"cdate":1511567867025,"id":"ry7bzE8eG","invitation":"ICLR.cc/2018/Conference/-/Paper677/Official_Review","forum":"ry018WZAZ","replyto":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference/Paper677/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Active learning for BiLSTM-based NER model","rating":"7: Good paper, accept","review":"Summary:\nThis paper applies active learning to a deep neural model (CNN-CNN-LSTM) for  named-entity recognition, which allows the model to match state-of-the-art performance with about 25% of the full training data.\n\nStrength:\nThe paper is relatively easy to follow. Experiments show significant reduction of training samples needed.\n\nWeaknesses:\nAbout half of the content is used to explain the CNN-CNN-LSTM architecture, which seems orthogonal to the active learning angle, except for the efficiency gain from replacing the CRF with an LSTM.\n\nThe difference in performance among the sampling strategies (as shown in Figure 4) seems very tiny. Therefore, it is difficult to tell what we can really learn from these empirical results.\n\nOther questions and comments:\nIn Table 4: Why is the performance of LSTM-LSTM-LSTM not reported for OntoNotes 5.0, or was the model simply too inefficient? \n\nHow is the variance of the model performance? At the early stage of active learning, the model uses as few as 1% of the training samples, which might cause large variance in terms of dev/test accuracy. \n\nThe SUBMOD method is not properly explained in Section 4. As one of the active learning techniques being compared in experiments, it might be better to formally describe the approach instead of putting it in the appendix.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]}},{"tddate":null,"ddate":null,"tmdate":1514437408506,"tcdate":1509131749590,"number":677,"cdate":1509739163305,"id":"ry018WZAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry018WZAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Active Learning for Named Entity Recognition","abstract":"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a  long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.","pdf":"/pdf/ebb403212f8f5bd3bcee9c84a90d19593471dd02.pdf","TL;DR":"We introduce a lightweight architecture for named entity recognition and carry out incremental active learning, which is able to match state-of-the-art performance with just 25% of the original training data.","paperhash":"anonymous|deep_active_learning_for_named_entity_recognition","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Active Learning for Named Entity Recognition},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry018WZAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper677/Authors"],"keywords":["active learning","deep learning","named entity recognition"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}